# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=1", "--one_hot=0", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1909021694, receiver_embed_dim=32, save_run=0, temp_decay=1.0, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1909021694, receiver_embed_dim=32, save_run=False, temp_decay=1.0, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.9647164344787598, acc=0.09377777576446533, loss=2.9647164344787598
test: epoch 1, loss 3.521529197692871, acc=0.07500000298023224, loss=3.521529197692871
train: epoch 2, loss 2.420518159866333, acc=0.16272221505641937, loss=2.420518159866333
test: epoch 2, loss 3.050062417984009, acc=0.10555555671453476, loss=3.050062417984009
train: epoch 3, loss 2.210193157196045, acc=0.20505554974079132, loss=2.210193157196045
test: epoch 3, loss 2.97202467918396, acc=0.1111111119389534, loss=2.97202467918396
train: epoch 4, loss 2.0765395164489746, acc=0.23177777230739594, loss=2.0765395164489746
test: epoch 4, loss 2.6075832843780518, acc=0.11666666716337204, loss=2.6075832843780518
train: epoch 5, loss 1.9716203212738037, acc=0.25788888335227966, loss=1.9716203212738037
test: epoch 5, loss 2.7827858924865723, acc=0.11944444477558136, loss=2.7827858924865723
train: epoch 6, loss 1.8920029401779175, acc=0.28422221541404724, loss=1.8920029401779175
test: epoch 6, loss 2.5930240154266357, acc=0.14166666567325592, loss=2.5930240154266357
train: epoch 7, loss 1.8385446071624756, acc=0.2963888943195343, loss=1.8385446071624756
test: epoch 7, loss 2.490701675415039, acc=0.14444445073604584, loss=2.490701675415039
train: epoch 8, loss 1.7893801927566528, acc=0.31122222542762756, loss=1.7893801927566528
test: epoch 8, loss 2.6194891929626465, acc=0.14722222089767456, loss=2.6194891929626465
train: epoch 9, loss 1.7394154071807861, acc=0.32761111855506897, loss=1.7394154071807861
test: epoch 9, loss 2.6214568614959717, acc=0.16388888657093048, loss=2.6214568614959717
train: epoch 10, loss 1.6989842653274536, acc=0.3328889012336731, loss=1.6989842653274536
test: epoch 10, loss 2.4136903285980225, acc=0.15833333134651184, loss=2.4136903285980225
train: epoch 11, loss 1.6498451232910156, acc=0.35394445061683655, loss=1.6498451232910156
test: epoch 11, loss 2.3903751373291016, acc=0.15833333134651184, loss=2.3903751373291016
train: epoch 12, loss 1.6244436502456665, acc=0.3550555408000946, loss=1.6244436502456665
test: epoch 12, loss 2.324272394180298, acc=0.16944444179534912, loss=2.324272394180298
train: epoch 13, loss 1.5962151288986206, acc=0.36888888478279114, loss=1.5962151288986206
test: epoch 13, loss 2.3373422622680664, acc=0.17499999701976776, loss=2.3373422622680664
train: epoch 14, loss 1.5628081560134888, acc=0.38361111283302307, loss=1.5628081560134888
test: epoch 14, loss 2.249127149581909, acc=0.21111111342906952, loss=2.249127149581909
train: epoch 15, loss 1.5547183752059937, acc=0.38661110401153564, loss=1.5547183752059937
test: epoch 15, loss 2.1889331340789795, acc=0.1666666716337204, loss=2.1889331340789795
train: epoch 16, loss 1.532942771911621, acc=0.39383333921432495, loss=1.532942771911621
test: epoch 16, loss 2.302689790725708, acc=0.21111111342906952, loss=2.302689790725708
train: epoch 17, loss 1.5026922225952148, acc=0.4021666646003723, loss=1.5026922225952148
test: epoch 17, loss 2.2513427734375, acc=0.20000000298023224, loss=2.2513427734375
train: epoch 18, loss 1.4768176078796387, acc=0.4156111180782318, loss=1.4768176078796387
test: epoch 18, loss 2.3576500415802, acc=0.17499999701976776, loss=2.3576500415802
train: epoch 19, loss 1.4788812398910522, acc=0.41644445061683655, loss=1.4788812398910522
test: epoch 19, loss 2.1465463638305664, acc=0.24166665971279144, loss=2.1465463638305664
train: epoch 20, loss 1.4512991905212402, acc=0.4249444305896759, loss=1.4512991905212402
test: epoch 20, loss 2.1503872871398926, acc=0.17222222685813904, loss=2.1503872871398926
train: epoch 21, loss 1.4233049154281616, acc=0.4293888807296753, loss=1.4233049154281616
test: epoch 21, loss 2.1107094287872314, acc=0.21111111342906952, loss=2.1107094287872314
train: epoch 22, loss 1.4066171646118164, acc=0.43966665863990784, loss=1.4066171646118164
test: epoch 22, loss 2.0836856365203857, acc=0.1944444477558136, loss=2.0836856365203857
train: epoch 23, loss 1.396849274635315, acc=0.449444442987442, loss=1.396849274635315
test: epoch 23, loss 2.2297308444976807, acc=0.2083333283662796, loss=2.2297308444976807
train: epoch 24, loss 1.3738430738449097, acc=0.44955554604530334, loss=1.3738430738449097
test: epoch 24, loss 2.143141984939575, acc=0.21388888359069824, loss=2.143141984939575
train: epoch 25, loss 1.3415846824645996, acc=0.46877777576446533, loss=1.3415846824645996
test: epoch 25, loss 2.242002487182617, acc=0.1944444477558136, loss=2.242002487182617
train: epoch 26, loss 1.348109245300293, acc=0.4620000123977661, loss=1.348109245300293
test: epoch 26, loss 2.0881006717681885, acc=0.16111111640930176, loss=2.0881006717681885
train: epoch 27, loss 1.3392044305801392, acc=0.46833333373069763, loss=1.3392044305801392
test: epoch 27, loss 2.218179702758789, acc=0.23888888955116272, loss=2.218179702758789
train: epoch 28, loss 1.3311522006988525, acc=0.47466665506362915, loss=1.3311522006988525
test: epoch 28, loss 2.113208770751953, acc=0.1805555522441864, loss=2.113208770751953
train: epoch 29, loss 1.3005932569503784, acc=0.476500004529953, loss=1.3005932569503784
test: epoch 29, loss 2.223526954650879, acc=0.15000000596046448, loss=2.223526954650879
train: epoch 30, loss 1.2872391939163208, acc=0.4865555465221405, loss=1.2872391939163208
test: epoch 30, loss 2.1104207038879395, acc=0.21944443881511688, loss=2.1104207038879395
train: epoch 31, loss 1.297307014465332, acc=0.4841666519641876, loss=1.297307014465332
test: epoch 31, loss 2.081089496612549, acc=0.2083333283662796, loss=2.081089496612549
train: epoch 32, loss 1.2787636518478394, acc=0.49211111664772034, loss=1.2787636518478394
test: epoch 32, loss 1.9774621725082397, acc=0.26944443583488464, loss=1.9774621725082397
train: epoch 33, loss 1.263119101524353, acc=0.4986666738986969, loss=1.263119101524353
test: epoch 33, loss 2.2070353031158447, acc=0.18333333730697632, loss=2.2070353031158447
train: epoch 34, loss 1.243344783782959, acc=0.507777750492096, loss=1.243344783782959
test: epoch 34, loss 2.2969553470611572, acc=0.22777777910232544, loss=2.2969553470611572
train: epoch 35, loss 1.2526863813400269, acc=0.49594444036483765, loss=1.2526863813400269
test: epoch 35, loss 2.037498712539673, acc=0.24166665971279144, loss=2.037498712539673
train: epoch 36, loss 1.2394754886627197, acc=0.5044999718666077, loss=1.2394754886627197
test: epoch 36, loss 2.106689929962158, acc=0.2083333283662796, loss=2.106689929962158
train: epoch 37, loss 1.235897421836853, acc=0.5055000185966492, loss=1.235897421836853
test: epoch 37, loss 2.196169137954712, acc=0.23888888955116272, loss=2.196169137954712
train: epoch 38, loss 1.2231029272079468, acc=0.5137222409248352, loss=1.2231029272079468
test: epoch 38, loss 1.9046885967254639, acc=0.21944443881511688, loss=1.9046885967254639
train: epoch 39, loss 1.2153760194778442, acc=0.5193333625793457, loss=1.2153760194778442
test: epoch 39, loss 1.8966244459152222, acc=0.28611111640930176, loss=1.8966244459152222
train: epoch 40, loss 1.18588125705719, acc=0.5276666879653931, loss=1.18588125705719
test: epoch 40, loss 1.8452212810516357, acc=0.2916666567325592, loss=1.8452212810516357
train: epoch 41, loss 1.1923651695251465, acc=0.5280555486679077, loss=1.1923651695251465
test: epoch 41, loss 2.0256528854370117, acc=0.2611111104488373, loss=2.0256528854370117
train: epoch 42, loss 1.1937708854675293, acc=0.527055561542511, loss=1.1937708854675293
test: epoch 42, loss 1.9871792793273926, acc=0.21666666865348816, loss=1.9871792793273926
train: epoch 43, loss 1.1662137508392334, acc=0.539222240447998, loss=1.1662137508392334
test: epoch 43, loss 1.8437551259994507, acc=0.3583333194255829, loss=1.8437551259994507
train: epoch 44, loss 1.1563148498535156, acc=0.5414999723434448, loss=1.1563148498535156
test: epoch 44, loss 2.115004539489746, acc=0.2777777910232544, loss=2.115004539489746
train: epoch 45, loss 1.1504679918289185, acc=0.5442222356796265, loss=1.1504679918289185
test: epoch 45, loss 2.0372023582458496, acc=0.26944443583488464, loss=2.0372023582458496
train: epoch 46, loss 1.1493738889694214, acc=0.5420555472373962, loss=1.1493738889694214
test: epoch 46, loss 1.863791584968567, acc=0.2888889014720917, loss=1.863791584968567
train: epoch 47, loss 1.1379194259643555, acc=0.5452777743339539, loss=1.1379194259643555
test: epoch 47, loss 1.8517279624938965, acc=0.2888889014720917, loss=1.8517279624938965
train: epoch 48, loss 1.135902762413025, acc=0.549833357334137, loss=1.135902762413025
test: epoch 48, loss 1.907591462135315, acc=0.2638888955116272, loss=1.907591462135315
train: epoch 49, loss 1.1231335401535034, acc=0.5612221956253052, loss=1.1231335401535034
test: epoch 49, loss 1.9108588695526123, acc=0.2777777910232544, loss=1.9108588695526123
train: epoch 50, loss 1.0910218954086304, acc=0.5645555257797241, loss=1.0910218954086304
test: epoch 50, loss 1.8978265523910522, acc=0.2638888955116272, loss=1.8978265523910522
train: epoch 51, loss 1.0894395112991333, acc=0.5645555257797241, loss=1.0894395112991333
test: epoch 51, loss 1.9252949953079224, acc=0.3027777671813965, loss=1.9252949953079224
train: epoch 52, loss 1.092972993850708, acc=0.5683333277702332, loss=1.092972993850708
test: epoch 52, loss 1.9162137508392334, acc=0.25833332538604736, loss=1.9162137508392334
train: epoch 53, loss 1.0910030603408813, acc=0.5705000162124634, loss=1.0910030603408813
test: epoch 53, loss 1.9519643783569336, acc=0.28333333134651184, loss=1.9519643783569336
train: epoch 54, loss 1.0885910987854004, acc=0.5752778053283691, loss=1.0885910987854004
test: epoch 54, loss 1.7995702028274536, acc=0.28333333134651184, loss=1.7995702028274536
train: epoch 55, loss 1.0689876079559326, acc=0.5755555629730225, loss=1.0689876079559326
test: epoch 55, loss 1.939408779144287, acc=0.2888889014720917, loss=1.939408779144287
train: epoch 56, loss 1.073265790939331, acc=0.5794444680213928, loss=1.073265790939331
test: epoch 56, loss 1.8556874990463257, acc=0.31111112236976624, loss=1.8556874990463257
train: epoch 57, loss 1.0479587316513062, acc=0.5853333473205566, loss=1.0479587316513062
test: epoch 57, loss 1.9332696199417114, acc=0.25833332538604736, loss=1.9332696199417114
train: epoch 58, loss 1.042526364326477, acc=0.5869444608688354, loss=1.042526364326477
test: epoch 58, loss 1.7806540727615356, acc=0.3444444537162781, loss=1.7806540727615356
train: epoch 59, loss 1.0378690958023071, acc=0.5892778038978577, loss=1.0378690958023071
test: epoch 59, loss 1.9470174312591553, acc=0.2222222238779068, loss=1.9470174312591553
train: epoch 60, loss 1.0276437997817993, acc=0.5933333039283752, loss=1.0276437997817993
test: epoch 60, loss 1.77232027053833, acc=0.3027777671813965, loss=1.77232027053833
train: epoch 61, loss 1.0209077596664429, acc=0.5933889150619507, loss=1.0209077596664429
test: epoch 61, loss 1.8728984594345093, acc=0.2666666805744171, loss=1.8728984594345093
train: epoch 62, loss 1.037655234336853, acc=0.596833348274231, loss=1.037655234336853
test: epoch 62, loss 1.9004311561584473, acc=0.25, loss=1.9004311561584473
train: epoch 63, loss 1.0256966352462769, acc=0.6012222170829773, loss=1.0256966352462769
test: epoch 63, loss 1.7497581243515015, acc=0.2888889014720917, loss=1.7497581243515015
train: epoch 64, loss 0.9969741702079773, acc=0.6054444313049316, loss=0.9969741702079773
test: epoch 64, loss 1.7669929265975952, acc=0.2916666567325592, loss=1.7669929265975952
train: epoch 65, loss 0.9929118156433105, acc=0.6121110916137695, loss=0.9929118156433105
test: epoch 65, loss 1.817493200302124, acc=0.3361110985279083, loss=1.817493200302124
train: epoch 66, loss 1.009804606437683, acc=0.6042777895927429, loss=1.009804606437683
test: epoch 66, loss 1.6995142698287964, acc=0.3222222328186035, loss=1.6995142698287964
train: epoch 67, loss 1.0077593326568604, acc=0.6105555295944214, loss=1.0077593326568604
test: epoch 67, loss 1.818103551864624, acc=0.2750000059604645, loss=1.818103551864624
train: epoch 68, loss 0.9601663947105408, acc=0.6192777752876282, loss=0.9601663947105408
test: epoch 68, loss 1.9956269264221191, acc=0.2527777850627899, loss=1.9956269264221191
train: epoch 69, loss 0.9692366719245911, acc=0.6157222390174866, loss=0.9692366719245911
test: epoch 69, loss 1.8427250385284424, acc=0.2944444417953491, loss=1.8427250385284424
train: epoch 70, loss 0.9553154706954956, acc=0.6211110949516296, loss=0.9553154706954956
test: epoch 70, loss 1.7262394428253174, acc=0.3499999940395355, loss=1.7262394428253174
train: epoch 71, loss 0.9539501070976257, acc=0.6186110973358154, loss=0.9539501070976257
test: epoch 71, loss 1.8964954614639282, acc=0.2666666805744171, loss=1.8964954614639282
train: epoch 72, loss 0.9528371691703796, acc=0.6300555467605591, loss=0.9528371691703796
test: epoch 72, loss 1.6905544996261597, acc=0.3305555582046509, loss=1.6905544996261597
train: epoch 73, loss 0.9461684226989746, acc=0.6231111288070679, loss=0.9461684226989746
test: epoch 73, loss 1.8459056615829468, acc=0.31111112236976624, loss=1.8459056615829468
train: epoch 74, loss 0.9385481476783752, acc=0.6252222061157227, loss=0.9385481476783752
test: epoch 74, loss 1.7554376125335693, acc=0.35555556416511536, loss=1.7554376125335693
train: epoch 75, loss 0.9488746523857117, acc=0.6287222504615784, loss=0.9488746523857117
test: epoch 75, loss 1.781530499458313, acc=0.2944444417953491, loss=1.781530499458313
train: epoch 76, loss 0.9243146777153015, acc=0.6348333358764648, loss=0.9243146777153015
test: epoch 76, loss 1.8449736833572388, acc=0.28333333134651184, loss=1.8449736833572388
train: epoch 77, loss 0.9153375625610352, acc=0.636555552482605, loss=0.9153375625610352
test: epoch 77, loss 1.822695255279541, acc=0.2944444417953491, loss=1.822695255279541
train: epoch 78, loss 0.951533317565918, acc=0.6275555491447449, loss=0.951533317565918
test: epoch 78, loss 1.8135603666305542, acc=0.25555557012557983, loss=1.8135603666305542
train: epoch 79, loss 0.9357024431228638, acc=0.6331666707992554, loss=0.9357024431228638
test: epoch 79, loss 1.7729038000106812, acc=0.3027777671813965, loss=1.7729038000106812
train: epoch 80, loss 0.9144431352615356, acc=0.6340000033378601, loss=0.9144431352615356
test: epoch 80, loss 1.730983853340149, acc=0.26944443583488464, loss=1.730983853340149
train: epoch 81, loss 0.8971592783927917, acc=0.6494444608688354, loss=0.8971592783927917
test: epoch 81, loss 1.7519398927688599, acc=0.3472222089767456, loss=1.7519398927688599
train: epoch 82, loss 0.9034188985824585, acc=0.6418333053588867, loss=0.9034188985824585
test: epoch 82, loss 1.832266926765442, acc=0.2777777910232544, loss=1.832266926765442
train: epoch 83, loss 0.8873206377029419, acc=0.6537222266197205, loss=0.8873206377029419
test: epoch 83, loss 1.6432510614395142, acc=0.28611111640930176, loss=1.6432510614395142
train: epoch 84, loss 0.8858449459075928, acc=0.6489444375038147, loss=0.8858449459075928
test: epoch 84, loss 1.8361952304840088, acc=0.3055555522441864, loss=1.8361952304840088
train: epoch 85, loss 0.9043899774551392, acc=0.6424444317817688, loss=0.9043899774551392
test: epoch 85, loss 1.7673475742340088, acc=0.28333333134651184, loss=1.7673475742340088
train: epoch 86, loss 0.895622193813324, acc=0.6475555300712585, loss=0.895622193813324
test: epoch 86, loss 1.7398426532745361, acc=0.2916666567325592, loss=1.7398426532745361
train: epoch 87, loss 0.8721922039985657, acc=0.6571111083030701, loss=0.8721922039985657
test: epoch 87, loss 1.6687084436416626, acc=0.35277777910232544, loss=1.6687084436416626
train: epoch 88, loss 0.8700611591339111, acc=0.6561111211776733, loss=0.8700611591339111
test: epoch 88, loss 1.7901611328125, acc=0.26944443583488464, loss=1.7901611328125
train: epoch 89, loss 0.8677173256874084, acc=0.6553888916969299, loss=0.8677173256874084
test: epoch 89, loss 1.7042039632797241, acc=0.35555556416511536, loss=1.7042039632797241
train: epoch 90, loss 0.88229900598526, acc=0.6598888635635376, loss=0.88229900598526
test: epoch 90, loss 1.6037752628326416, acc=0.29722222685813904, loss=1.6037752628326416
train: epoch 91, loss 0.8398605585098267, acc=0.6685000061988831, loss=0.8398605585098267
test: epoch 91, loss 1.7691688537597656, acc=0.3055555522441864, loss=1.7691688537597656
train: epoch 92, loss 0.8491716980934143, acc=0.6617777943611145, loss=0.8491716980934143
test: epoch 92, loss 1.5803883075714111, acc=0.3083333373069763, loss=1.5803883075714111
train: epoch 93, loss 0.8514746427536011, acc=0.6623888611793518, loss=0.8514746427536011
test: epoch 93, loss 1.6767182350158691, acc=0.3333333432674408, loss=1.6767182350158691
train: epoch 94, loss 0.873383104801178, acc=0.6578333377838135, loss=0.873383104801178
test: epoch 94, loss 1.768782377243042, acc=0.30000001192092896, loss=1.768782377243042
train: epoch 95, loss 0.8580318689346313, acc=0.6632221937179565, loss=0.8580318689346313
test: epoch 95, loss 1.9097495079040527, acc=0.29722222685813904, loss=1.9097495079040527
train: epoch 96, loss 0.8609909415245056, acc=0.6633889079093933, loss=0.8609909415245056
test: epoch 96, loss 1.69414484500885, acc=0.2805555462837219, loss=1.69414484500885
train: epoch 97, loss 0.8243538737297058, acc=0.6752777695655823, loss=0.8243538737297058
test: epoch 97, loss 1.6374644041061401, acc=0.2944444417953491, loss=1.6374644041061401
train: epoch 98, loss 0.817843496799469, acc=0.6737777590751648, loss=0.817843496799469
test: epoch 98, loss 1.6030945777893066, acc=0.28611111640930176, loss=1.6030945777893066
train: epoch 99, loss 0.8261330127716064, acc=0.6730555295944214, loss=0.8261330127716064
test: epoch 99, loss 1.533362865447998, acc=0.375, loss=1.533362865447998
train: epoch 100, loss 0.8408464789390564, acc=0.6676111221313477, loss=0.8408464789390564
test: epoch 100, loss 1.5683834552764893, acc=0.35277777910232544, loss=1.5683834552764893
train: epoch 101, loss 0.8049267530441284, acc=0.6813333630561829, loss=0.8049267530441284
test: epoch 101, loss 1.6288001537322998, acc=0.3472222089767456, loss=1.6288001537322998
train: epoch 102, loss 0.8079676628112793, acc=0.6810555458068848, loss=0.8079676628112793
test: epoch 102, loss 1.7280285358428955, acc=0.3499999940395355, loss=1.7280285358428955
train: epoch 103, loss 0.8111003637313843, acc=0.6864444613456726, loss=0.8111003637313843
test: epoch 103, loss 1.9723552465438843, acc=0.2638888955116272, loss=1.9723552465438843
train: epoch 104, loss 0.8167489171028137, acc=0.6791666746139526, loss=0.8167489171028137
test: epoch 104, loss 1.6084588766098022, acc=0.33888888359069824, loss=1.6084588766098022
train: epoch 105, loss 0.7872619032859802, acc=0.6899999976158142, loss=0.7872619032859802
test: epoch 105, loss 1.6162365674972534, acc=0.31388887763023376, loss=1.6162365674972534
train: epoch 106, loss 0.7859892249107361, acc=0.6856666803359985, loss=0.7859892249107361
test: epoch 106, loss 1.6397430896759033, acc=0.3333333432674408, loss=1.6397430896759033
train: epoch 107, loss 0.775784432888031, acc=0.6919999718666077, loss=0.775784432888031
test: epoch 107, loss 1.5899436473846436, acc=0.4000000059604645, loss=1.5899436473846436
train: epoch 108, loss 0.791873574256897, acc=0.6931111216545105, loss=0.791873574256897
test: epoch 108, loss 1.6534881591796875, acc=0.3055555522441864, loss=1.6534881591796875
train: epoch 109, loss 0.7762448787689209, acc=0.69477778673172, loss=0.7762448787689209
test: epoch 109, loss 1.7080563306808472, acc=0.34166666865348816, loss=1.7080563306808472
train: epoch 110, loss 0.7882533669471741, acc=0.6886666417121887, loss=0.7882533669471741
test: epoch 110, loss 1.4937890768051147, acc=0.3444444537162781, loss=1.4937890768051147
train: epoch 111, loss 0.7739424109458923, acc=0.7016111016273499, loss=0.7739424109458923
test: epoch 111, loss 1.8047564029693604, acc=0.3027777671813965, loss=1.8047564029693604
train: epoch 112, loss 0.7850897908210754, acc=0.691444456577301, loss=0.7850897908210754
test: epoch 112, loss 1.9931732416152954, acc=0.25833332538604736, loss=1.9931732416152954
train: epoch 113, loss 0.7413517832756042, acc=0.7067221999168396, loss=0.7413517832756042
test: epoch 113, loss 1.5161129236221313, acc=0.3638888895511627, loss=1.5161129236221313
train: epoch 114, loss 0.7576582431793213, acc=0.704277753829956, loss=0.7576582431793213
test: epoch 114, loss 1.599000096321106, acc=0.31111112236976624, loss=1.599000096321106
train: epoch 115, loss 0.7672985196113586, acc=0.6970555782318115, loss=0.7672985196113586
test: epoch 115, loss 1.6502685546875, acc=0.3583333194255829, loss=1.6502685546875
train: epoch 116, loss 0.7502738833427429, acc=0.7067221999168396, loss=0.7502738833427429
test: epoch 116, loss 1.6930975914001465, acc=0.3888888955116272, loss=1.6930975914001465
train: epoch 117, loss 0.761166512966156, acc=0.7012222409248352, loss=0.761166512966156
test: epoch 117, loss 1.472368597984314, acc=0.3722222149372101, loss=1.472368597984314
train: epoch 118, loss 0.7446647882461548, acc=0.7066110968589783, loss=0.7446647882461548
test: epoch 118, loss 1.7695319652557373, acc=0.26944443583488464, loss=1.7695319652557373
train: epoch 119, loss 0.7330554723739624, acc=0.7129444479942322, loss=0.7330554723739624
test: epoch 119, loss 1.6773672103881836, acc=0.3499999940395355, loss=1.6773672103881836
train: epoch 120, loss 0.735197901725769, acc=0.7118889093399048, loss=0.735197901725769
test: epoch 120, loss 1.6893219947814941, acc=0.3583333194255829, loss=1.6893219947814941
train: epoch 121, loss 0.7280582785606384, acc=0.7114444375038147, loss=0.7280582785606384
test: epoch 121, loss 1.572200894355774, acc=0.3638888895511627, loss=1.572200894355774
train: epoch 122, loss 0.722740650177002, acc=0.7132222056388855, loss=0.722740650177002
test: epoch 122, loss 1.5068895816802979, acc=0.38055557012557983, loss=1.5068895816802979
train: epoch 123, loss 0.7414178848266602, acc=0.7086111307144165, loss=0.7414178848266602
test: epoch 123, loss 1.5343811511993408, acc=0.4472222328186035, loss=1.5343811511993408
train: epoch 124, loss 0.720630943775177, acc=0.7130555510520935, loss=0.720630943775177
test: epoch 124, loss 1.4616607427597046, acc=0.38055557012557983, loss=1.4616607427597046
train: epoch 125, loss 0.7243251800537109, acc=0.7143333554267883, loss=0.7243251800537109
test: epoch 125, loss 1.6362464427947998, acc=0.3722222149372101, loss=1.6362464427947998
train: epoch 126, loss 0.7092188596725464, acc=0.719944417476654, loss=0.7092188596725464
test: epoch 126, loss 1.737653136253357, acc=0.3611111044883728, loss=1.737653136253357
train: epoch 127, loss 0.7165622115135193, acc=0.7146666646003723, loss=0.7165622115135193
test: epoch 127, loss 1.4299780130386353, acc=0.3722222149372101, loss=1.4299780130386353
train: epoch 128, loss 0.7223751544952393, acc=0.7182777523994446, loss=0.7223751544952393
test: epoch 128, loss 1.5450849533081055, acc=0.3499999940395355, loss=1.5450849533081055
train: epoch 129, loss 0.7121521830558777, acc=0.718666672706604, loss=0.7121521830558777
test: epoch 129, loss 1.7426629066467285, acc=0.2777777910232544, loss=1.7426629066467285
train: epoch 130, loss 0.6969581842422485, acc=0.726111114025116, loss=0.6969581842422485
test: epoch 130, loss 1.4983408451080322, acc=0.38055557012557983, loss=1.4983408451080322
train: epoch 131, loss 0.6979272365570068, acc=0.7248888611793518, loss=0.6979272365570068
test: epoch 131, loss 1.5882222652435303, acc=0.4138889014720917, loss=1.5882222652435303
train: epoch 132, loss 0.6936244964599609, acc=0.7252222299575806, loss=0.6936244964599609
test: epoch 132, loss 1.6010547876358032, acc=0.3861111104488373, loss=1.6010547876358032
train: epoch 133, loss 0.681799054145813, acc=0.7286666631698608, loss=0.681799054145813
test: epoch 133, loss 1.4866001605987549, acc=0.3916666805744171, loss=1.4866001605987549
train: epoch 134, loss 0.7025639414787292, acc=0.7242222428321838, loss=0.7025639414787292
test: epoch 134, loss 1.5433143377304077, acc=0.32777777314186096, loss=1.5433143377304077
train: epoch 135, loss 0.6824610233306885, acc=0.7294999957084656, loss=0.6824610233306885
test: epoch 135, loss 1.4658936262130737, acc=0.3722222149372101, loss=1.4658936262130737
train: epoch 136, loss 0.6988570690155029, acc=0.72688889503479, loss=0.6988570690155029
test: epoch 136, loss 1.564984679222107, acc=0.42500001192092896, loss=1.564984679222107
train: epoch 137, loss 0.7157162427902222, acc=0.7240555286407471, loss=0.7157162427902222
test: epoch 137, loss 1.558158040046692, acc=0.3888888955116272, loss=1.558158040046692
train: epoch 138, loss 0.6866375207901001, acc=0.7296666502952576, loss=0.6866375207901001
test: epoch 138, loss 1.575806736946106, acc=0.3583333194255829, loss=1.575806736946106
train: epoch 139, loss 0.6748732924461365, acc=0.7377222180366516, loss=0.6748732924461365
test: epoch 139, loss 1.3684757947921753, acc=0.3777777850627899, loss=1.3684757947921753
train: epoch 140, loss 0.6759839653968811, acc=0.734499990940094, loss=0.6759839653968811
test: epoch 140, loss 1.6070510149002075, acc=0.35555556416511536, loss=1.6070510149002075
train: epoch 141, loss 0.6915347576141357, acc=0.730388879776001, loss=0.6915347576141357
test: epoch 141, loss 1.454227089881897, acc=0.36666667461395264, loss=1.454227089881897
train: epoch 142, loss 0.6738467216491699, acc=0.7351111173629761, loss=0.6738467216491699
test: epoch 142, loss 1.4799747467041016, acc=0.3861111104488373, loss=1.4799747467041016
train: epoch 143, loss 0.6665173768997192, acc=0.7380555272102356, loss=0.6665173768997192
test: epoch 143, loss 1.5809322595596313, acc=0.39444443583488464, loss=1.5809322595596313
train: epoch 144, loss 0.6595712304115295, acc=0.7416666746139526, loss=0.6595712304115295
test: epoch 144, loss 1.4859418869018555, acc=0.36666667461395264, loss=1.4859418869018555
train: epoch 145, loss 0.6723519563674927, acc=0.7426111102104187, loss=0.6723519563674927
test: epoch 145, loss 1.6510182619094849, acc=0.32499998807907104, loss=1.6510182619094849
train: epoch 146, loss 0.6774016618728638, acc=0.7388333082199097, loss=0.6774016618728638
test: epoch 146, loss 1.5133728981018066, acc=0.3361110985279083, loss=1.5133728981018066
train: epoch 147, loss 0.6573663353919983, acc=0.7428333163261414, loss=0.6573663353919983
test: epoch 147, loss 1.389103889465332, acc=0.4055555462837219, loss=1.389103889465332
train: epoch 148, loss 0.6499626636505127, acc=0.7456111311912537, loss=0.6499626636505127
test: epoch 148, loss 1.7025539875030518, acc=0.36666667461395264, loss=1.7025539875030518
train: epoch 149, loss 0.6499095559120178, acc=0.7448889017105103, loss=0.6499095559120178
test: epoch 149, loss 1.519415020942688, acc=0.4000000059604645, loss=1.519415020942688
train: epoch 150, loss 0.6200038194656372, acc=0.7568333148956299, loss=0.6200038194656372
test: epoch 150, loss 1.5171042680740356, acc=0.38333332538604736, loss=1.5171042680740356
