# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.99", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1045428495, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1045428495, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.947312116622925, acc=0.09883332997560501, loss=2.947312116622925
test: epoch 1, loss 3.872685432434082, acc=0.0694444477558136, loss=3.872685432434082
train: epoch 2, loss 2.2716195583343506, acc=0.177888885140419, loss=2.2716195583343506
test: epoch 2, loss 2.511674404144287, acc=0.13055555522441864, loss=2.511674404144287
train: epoch 3, loss 1.9629708528518677, acc=0.248222216963768, loss=1.9629708528518677
test: epoch 3, loss 2.2534587383270264, acc=0.19166666269302368, loss=2.2534587383270264
train: epoch 4, loss 1.7805453538894653, acc=0.2862222194671631, loss=1.7805453538894653
test: epoch 4, loss 2.1974453926086426, acc=0.20000000298023224, loss=2.1974453926086426
train: epoch 5, loss 1.6678048372268677, acc=0.32983332872390747, loss=1.6678048372268677
test: epoch 5, loss 1.8819539546966553, acc=0.22499999403953552, loss=1.8819539546966553
train: epoch 6, loss 1.5823038816452026, acc=0.3564999997615814, loss=1.5823038816452026
test: epoch 6, loss 2.004183292388916, acc=0.22777777910232544, loss=2.004183292388916
train: epoch 7, loss 1.521791934967041, acc=0.3816111087799072, loss=1.521791934967041
test: epoch 7, loss 1.9308899641036987, acc=0.25555557012557983, loss=1.9308899641036987
train: epoch 8, loss 1.4553017616271973, acc=0.4008888900279999, loss=1.4553017616271973
test: epoch 8, loss 1.9526923894882202, acc=0.2611111104488373, loss=1.9526923894882202
train: epoch 9, loss 1.3800724744796753, acc=0.42888888716697693, loss=1.3800724744796753
test: epoch 9, loss 1.6587579250335693, acc=0.26944443583488464, loss=1.6587579250335693
train: epoch 10, loss 1.3496527671813965, acc=0.44316667318344116, loss=1.3496527671813965
test: epoch 10, loss 1.843910574913025, acc=0.24166665971279144, loss=1.843910574913025
train: epoch 11, loss 1.3079687356948853, acc=0.45399999618530273, loss=1.3079687356948853
test: epoch 11, loss 1.7641804218292236, acc=0.2916666567325592, loss=1.7641804218292236
train: epoch 12, loss 1.2800548076629639, acc=0.47216665744781494, loss=1.2800548076629639
test: epoch 12, loss 2.0166866779327393, acc=0.26944443583488464, loss=2.0166866779327393
train: epoch 13, loss 1.2510020732879639, acc=0.4749999940395355, loss=1.2510020732879639
test: epoch 13, loss 1.6214913129806519, acc=0.3194444477558136, loss=1.6214913129806519
train: epoch 14, loss 1.234703540802002, acc=0.4859444499015808, loss=1.234703540802002
test: epoch 14, loss 1.6456390619277954, acc=0.2805555462837219, loss=1.6456390619277954
train: epoch 15, loss 1.1934565305709839, acc=0.49799999594688416, loss=1.1934565305709839
test: epoch 15, loss 1.6668874025344849, acc=0.28611111640930176, loss=1.6668874025344849
train: epoch 16, loss 1.1745697259902954, acc=0.5081666707992554, loss=1.1745697259902954
test: epoch 16, loss 1.4500073194503784, acc=0.3638888895511627, loss=1.4500073194503784
train: epoch 17, loss 1.1523061990737915, acc=0.5102221965789795, loss=1.1523061990737915
test: epoch 17, loss 1.5763041973114014, acc=0.31111112236976624, loss=1.5763041973114014
train: epoch 18, loss 1.1454846858978271, acc=0.5127221941947937, loss=1.1454846858978271
test: epoch 18, loss 1.7591286897659302, acc=0.35555556416511536, loss=1.7591286897659302
train: epoch 19, loss 1.127103567123413, acc=0.520111083984375, loss=1.127103567123413
test: epoch 19, loss 1.5363922119140625, acc=0.3333333432674408, loss=1.5363922119140625
train: epoch 20, loss 1.115525722503662, acc=0.5277777910232544, loss=1.115525722503662
test: epoch 20, loss 1.777654767036438, acc=0.3194444477558136, loss=1.777654767036438
train: epoch 21, loss 1.0951218605041504, acc=0.5371666550636292, loss=1.0951218605041504
test: epoch 21, loss 1.5004656314849854, acc=0.35277777910232544, loss=1.5004656314849854
train: epoch 22, loss 1.0729268789291382, acc=0.5456666946411133, loss=1.0729268789291382
test: epoch 22, loss 1.447479248046875, acc=0.4000000059604645, loss=1.447479248046875
train: epoch 23, loss 1.0594310760498047, acc=0.5534444451332092, loss=1.0594310760498047
test: epoch 23, loss 1.4219369888305664, acc=0.4027777910232544, loss=1.4219369888305664
train: epoch 24, loss 1.050282597541809, acc=0.5538333058357239, loss=1.050282597541809
test: epoch 24, loss 1.4204649925231934, acc=0.3499999940395355, loss=1.4204649925231934
train: epoch 25, loss 1.043760895729065, acc=0.5583333373069763, loss=1.043760895729065
test: epoch 25, loss 1.3910366296768188, acc=0.3499999940395355, loss=1.3910366296768188
train: epoch 26, loss 1.0349953174591064, acc=0.5661110877990723, loss=1.0349953174591064
test: epoch 26, loss 1.3334863185882568, acc=0.4027777910232544, loss=1.3334863185882568
train: epoch 27, loss 1.0264142751693726, acc=0.5649999976158142, loss=1.0264142751693726
test: epoch 27, loss 1.5119500160217285, acc=0.36666667461395264, loss=1.5119500160217285
train: epoch 28, loss 1.0036159753799438, acc=0.5759444236755371, loss=1.0036159753799438
test: epoch 28, loss 1.4471526145935059, acc=0.36944442987442017, loss=1.4471526145935059
train: epoch 29, loss 0.9920839667320251, acc=0.5834444165229797, loss=0.9920839667320251
test: epoch 29, loss 1.5443294048309326, acc=0.3611111044883728, loss=1.5443294048309326
train: epoch 30, loss 0.9857248663902283, acc=0.5893333554267883, loss=0.9857248663902283
test: epoch 30, loss 1.3974430561065674, acc=0.41111111640930176, loss=1.3974430561065674
train: epoch 31, loss 0.9674806594848633, acc=0.5923333168029785, loss=0.9674806594848633
test: epoch 31, loss 1.586495280265808, acc=0.36666667461395264, loss=1.586495280265808
train: epoch 32, loss 0.9746649265289307, acc=0.5920555591583252, loss=0.9746649265289307
test: epoch 32, loss 1.5069985389709473, acc=0.36666667461395264, loss=1.5069985389709473
train: epoch 33, loss 0.9551180601119995, acc=0.5966110825538635, loss=0.9551180601119995
test: epoch 33, loss 1.4923954010009766, acc=0.36944442987442017, loss=1.4923954010009766
train: epoch 34, loss 0.9446315765380859, acc=0.6024444699287415, loss=0.9446315765380859
test: epoch 34, loss 1.7564492225646973, acc=0.3638888895511627, loss=1.7564492225646973
train: epoch 35, loss 0.9299819469451904, acc=0.6074444651603699, loss=0.9299819469451904
test: epoch 35, loss 1.4896633625030518, acc=0.4000000059604645, loss=1.4896633625030518
train: epoch 36, loss 0.933964729309082, acc=0.6056110858917236, loss=0.933964729309082
test: epoch 36, loss 1.6836202144622803, acc=0.36944442987442017, loss=1.6836202144622803
train: epoch 37, loss 0.898700475692749, acc=0.6229444742202759, loss=0.898700475692749
test: epoch 37, loss 1.7722018957138062, acc=0.3611111044883728, loss=1.7722018957138062
train: epoch 38, loss 0.9211665391921997, acc=0.6129444241523743, loss=0.9211665391921997
test: epoch 38, loss 1.5555343627929688, acc=0.3638888895511627, loss=1.5555343627929688
train: epoch 39, loss 0.8980520963668823, acc=0.6206666827201843, loss=0.8980520963668823
test: epoch 39, loss 1.555367350578308, acc=0.3888888955116272, loss=1.555367350578308
train: epoch 40, loss 0.9006542563438416, acc=0.6236110925674438, loss=0.9006542563438416
test: epoch 40, loss 1.547704815864563, acc=0.39722222089767456, loss=1.547704815864563
train: epoch 41, loss 0.8991884589195251, acc=0.6214444637298584, loss=0.8991884589195251
test: epoch 41, loss 1.4990465641021729, acc=0.4138889014720917, loss=1.4990465641021729
train: epoch 42, loss 0.8621454834938049, acc=0.6325555443763733, loss=0.8621454834938049
test: epoch 42, loss 1.5642071962356567, acc=0.4027777910232544, loss=1.5642071962356567
train: epoch 43, loss 0.8679485321044922, acc=0.632888913154602, loss=0.8679485321044922
test: epoch 43, loss 1.4780775308609009, acc=0.40833333134651184, loss=1.4780775308609009
train: epoch 44, loss 0.8655896186828613, acc=0.6284444332122803, loss=0.8655896186828613
test: epoch 44, loss 1.4796273708343506, acc=0.3916666805744171, loss=1.4796273708343506
train: epoch 45, loss 0.8514909148216248, acc=0.6397777795791626, loss=0.8514909148216248
test: epoch 45, loss 1.5452672243118286, acc=0.4027777910232544, loss=1.5452672243118286
train: epoch 46, loss 0.8531275987625122, acc=0.6430000066757202, loss=0.8531275987625122
test: epoch 46, loss 1.431767463684082, acc=0.39722222089767456, loss=1.431767463684082
train: epoch 47, loss 0.8335739970207214, acc=0.6478888988494873, loss=0.8335739970207214
test: epoch 47, loss 1.4833906888961792, acc=0.4000000059604645, loss=1.4833906888961792
train: epoch 48, loss 0.824558436870575, acc=0.6560555696487427, loss=0.824558436870575
test: epoch 48, loss 1.4101786613464355, acc=0.4138889014720917, loss=1.4101786613464355
train: epoch 49, loss 0.8343884944915771, acc=0.6448888778686523, loss=0.8343884944915771
test: epoch 49, loss 1.4009647369384766, acc=0.40833333134651184, loss=1.4009647369384766
train: epoch 50, loss 0.8204448223114014, acc=0.6560555696487427, loss=0.8204448223114014
test: epoch 50, loss 1.6312719583511353, acc=0.39444443583488464, loss=1.6312719583511353
train: epoch 51, loss 0.7968026399612427, acc=0.6655555367469788, loss=0.7968026399612427
test: epoch 51, loss 1.6456172466278076, acc=0.4138889014720917, loss=1.6456172466278076
train: epoch 52, loss 0.8018712997436523, acc=0.6642777919769287, loss=0.8018712997436523
test: epoch 52, loss 1.4863922595977783, acc=0.3499999940395355, loss=1.4863922595977783
train: epoch 53, loss 0.7748756408691406, acc=0.6787222027778625, loss=0.7748756408691406
test: epoch 53, loss 1.403918981552124, acc=0.39722222089767456, loss=1.403918981552124
train: epoch 54, loss 0.7851738333702087, acc=0.6737777590751648, loss=0.7851738333702087
test: epoch 54, loss 1.5025465488433838, acc=0.4027777910232544, loss=1.5025465488433838
train: epoch 55, loss 0.7898638248443604, acc=0.6705555319786072, loss=0.7898638248443604
test: epoch 55, loss 1.544464111328125, acc=0.38055557012557983, loss=1.544464111328125
train: epoch 56, loss 0.7768594026565552, acc=0.6775000095367432, loss=0.7768594026565552
test: epoch 56, loss 1.4216688871383667, acc=0.4027777910232544, loss=1.4216688871383667
train: epoch 57, loss 0.757975161075592, acc=0.6802777647972107, loss=0.757975161075592
test: epoch 57, loss 1.3340568542480469, acc=0.4444444477558136, loss=1.3340568542480469
train: epoch 58, loss 0.758179783821106, acc=0.6834444403648376, loss=0.758179783821106
test: epoch 58, loss 1.4634575843811035, acc=0.4027777910232544, loss=1.4634575843811035
train: epoch 59, loss 0.7545434832572937, acc=0.6845555305480957, loss=0.7545434832572937
test: epoch 59, loss 1.5396925210952759, acc=0.4416666626930237, loss=1.5396925210952759
train: epoch 60, loss 0.7253846526145935, acc=0.694611132144928, loss=0.7253846526145935
test: epoch 60, loss 1.5113837718963623, acc=0.3055555522441864, loss=1.5113837718963623
train: epoch 61, loss 0.7460092306137085, acc=0.6848888993263245, loss=0.7460092306137085
test: epoch 61, loss 1.400231957435608, acc=0.40833333134651184, loss=1.400231957435608
train: epoch 62, loss 0.729098916053772, acc=0.694611132144928, loss=0.729098916053772
test: epoch 62, loss 1.3704255819320679, acc=0.4416666626930237, loss=1.3704255819320679
train: epoch 63, loss 0.7449191212654114, acc=0.6883333325386047, loss=0.7449191212654114
test: epoch 63, loss 1.4757188558578491, acc=0.4305555522441864, loss=1.4757188558578491
train: epoch 64, loss 0.7312242388725281, acc=0.6881111264228821, loss=0.7312242388725281
test: epoch 64, loss 1.5334677696228027, acc=0.39722222089767456, loss=1.5334677696228027
train: epoch 65, loss 0.7235464453697205, acc=0.6936666369438171, loss=0.7235464453697205
test: epoch 65, loss 1.4241713285446167, acc=0.39722222089767456, loss=1.4241713285446167
train: epoch 66, loss 0.7174960970878601, acc=0.6984444260597229, loss=0.7174960970878601
test: epoch 66, loss 1.4091074466705322, acc=0.4027777910232544, loss=1.4091074466705322
train: epoch 67, loss 0.7075387239456177, acc=0.7012777924537659, loss=0.7075387239456177
test: epoch 67, loss 1.4555109739303589, acc=0.43888887763023376, loss=1.4555109739303589
train: epoch 68, loss 0.7142574787139893, acc=0.6969444155693054, loss=0.7142574787139893
test: epoch 68, loss 1.3366117477416992, acc=0.49444442987442017, loss=1.3366117477416992
train: epoch 69, loss 0.7187800407409668, acc=0.6979444622993469, loss=0.7187800407409668
test: epoch 69, loss 1.493666648864746, acc=0.4416666626930237, loss=1.493666648864746
train: epoch 70, loss 0.7212139368057251, acc=0.6951666474342346, loss=0.7212139368057251
test: epoch 70, loss 1.3891756534576416, acc=0.43611112236976624, loss=1.3891756534576416
train: epoch 71, loss 0.7046312689781189, acc=0.7022777795791626, loss=0.7046312689781189
test: epoch 71, loss 1.4137599468231201, acc=0.43888887763023376, loss=1.4137599468231201
train: epoch 72, loss 0.7082089781761169, acc=0.7001110911369324, loss=0.7082089781761169
test: epoch 72, loss 1.5495758056640625, acc=0.4444444477558136, loss=1.5495758056640625
train: epoch 73, loss 0.6997488737106323, acc=0.7058888673782349, loss=0.6997488737106323
test: epoch 73, loss 1.4995195865631104, acc=0.4000000059604645, loss=1.4995195865631104
train: epoch 74, loss 0.7185277342796326, acc=0.6961110830307007, loss=0.7185277342796326
test: epoch 74, loss 1.5292249917984009, acc=0.4722222089767456, loss=1.5292249917984009
train: epoch 75, loss 0.6966263055801392, acc=0.7014999985694885, loss=0.6966263055801392
test: epoch 75, loss 1.3924776315689087, acc=0.39722222089767456, loss=1.3924776315689087
train: epoch 76, loss 0.6928595304489136, acc=0.7047777771949768, loss=0.6928595304489136
test: epoch 76, loss 1.5404376983642578, acc=0.3638888895511627, loss=1.5404376983642578
train: epoch 77, loss 0.6930265426635742, acc=0.7055000066757202, loss=0.6930265426635742
test: epoch 77, loss 1.4660098552703857, acc=0.4416666626930237, loss=1.4660098552703857
train: epoch 78, loss 0.6880456209182739, acc=0.7078333497047424, loss=0.6880456209182739
test: epoch 78, loss 1.4062626361846924, acc=0.4444444477558136, loss=1.4062626361846924
train: epoch 79, loss 0.7014845609664917, acc=0.7077777981758118, loss=0.7014845609664917
test: epoch 79, loss 1.5098775625228882, acc=0.43611112236976624, loss=1.5098775625228882
train: epoch 80, loss 0.6879547834396362, acc=0.7046666741371155, loss=0.6879547834396362
test: epoch 80, loss 1.5737519264221191, acc=0.4194444417953491, loss=1.5737519264221191
train: epoch 81, loss 0.667528510093689, acc=0.7160555720329285, loss=0.667528510093689
test: epoch 81, loss 1.7119642496109009, acc=0.4555555582046509, loss=1.7119642496109009
train: epoch 82, loss 0.681545615196228, acc=0.711222231388092, loss=0.681545615196228
test: epoch 82, loss 1.4891691207885742, acc=0.45277777314186096, loss=1.4891691207885742
train: epoch 83, loss 0.6619502902030945, acc=0.7170555591583252, loss=0.6619502902030945
test: epoch 83, loss 1.5660799741744995, acc=0.4416666626930237, loss=1.5660799741744995
train: epoch 84, loss 0.6832884550094604, acc=0.7096111178398132, loss=0.6832884550094604
test: epoch 84, loss 1.4641832113265991, acc=0.41111111640930176, loss=1.4641832113265991
train: epoch 85, loss 0.66523277759552, acc=0.7161666750907898, loss=0.66523277759552
test: epoch 85, loss 1.4466863870620728, acc=0.4611110985279083, loss=1.4466863870620728
train: epoch 86, loss 0.6690924167633057, acc=0.7126666903495789, loss=0.6690924167633057
test: epoch 86, loss 1.4782122373580933, acc=0.43888887763023376, loss=1.4782122373580933
train: epoch 87, loss 0.6535964608192444, acc=0.7222222089767456, loss=0.6535964608192444
test: epoch 87, loss 1.5483205318450928, acc=0.47777777910232544, loss=1.5483205318450928
train: epoch 88, loss 0.6713586449623108, acc=0.7178888916969299, loss=0.6713586449623108
test: epoch 88, loss 1.3199924230575562, acc=0.4583333432674408, loss=1.3199924230575562
train: epoch 89, loss 0.6591634154319763, acc=0.7160000205039978, loss=0.6591634154319763
test: epoch 89, loss 1.1887227296829224, acc=0.48055556416511536, loss=1.1887227296829224
train: epoch 90, loss 0.6733592748641968, acc=0.7194444537162781, loss=0.6733592748641968
test: epoch 90, loss 1.3153022527694702, acc=0.4861111044883728, loss=1.3153022527694702
train: epoch 91, loss 0.6629475951194763, acc=0.717555582523346, loss=0.6629475951194763
test: epoch 91, loss 1.4018645286560059, acc=0.46666666865348816, loss=1.4018645286560059
train: epoch 92, loss 0.667486846446991, acc=0.715666651725769, loss=0.667486846446991
test: epoch 92, loss 1.5040466785430908, acc=0.4694444537162781, loss=1.5040466785430908
train: epoch 93, loss 0.6610562205314636, acc=0.718999981880188, loss=0.6610562205314636
test: epoch 93, loss 1.2792166471481323, acc=0.4833333194255829, loss=1.2792166471481323
train: epoch 94, loss 0.6685493588447571, acc=0.7162777781486511, loss=0.6685493588447571
test: epoch 94, loss 1.2523356676101685, acc=0.4833333194255829, loss=1.2523356676101685
train: epoch 95, loss 0.6555689573287964, acc=0.7223888635635376, loss=0.6555689573287964
test: epoch 95, loss 1.404577374458313, acc=0.4749999940395355, loss=1.404577374458313
train: epoch 96, loss 0.6480245590209961, acc=0.7266666889190674, loss=0.6480245590209961
test: epoch 96, loss 1.3538353443145752, acc=0.4749999940395355, loss=1.3538353443145752
train: epoch 97, loss 0.6490173935890198, acc=0.7238888740539551, loss=0.6490173935890198
test: epoch 97, loss 1.5076175928115845, acc=0.4861111044883728, loss=1.5076175928115845
train: epoch 98, loss 0.6541458964347839, acc=0.7197222113609314, loss=0.6541458964347839
test: epoch 98, loss 1.4049891233444214, acc=0.4861111044883728, loss=1.4049891233444214
train: epoch 99, loss 0.6720805168151855, acc=0.7158889174461365, loss=0.6720805168151855
test: epoch 99, loss 1.3651982545852661, acc=0.47777777910232544, loss=1.3651982545852661
train: epoch 100, loss 0.6386913657188416, acc=0.7238333225250244, loss=0.6386913657188416
test: epoch 100, loss 1.4097208976745605, acc=0.4583333432674408, loss=1.4097208976745605
train: epoch 101, loss 0.6602162718772888, acc=0.7177222371101379, loss=0.6602162718772888
test: epoch 101, loss 1.4111617803573608, acc=0.45277777314186096, loss=1.4111617803573608
train: epoch 102, loss 0.6609898805618286, acc=0.7182222008705139, loss=0.6609898805618286
test: epoch 102, loss 1.5941799879074097, acc=0.48055556416511536, loss=1.5941799879074097
train: epoch 103, loss 0.6255812644958496, acc=0.7320555448532104, loss=0.6255812644958496
test: epoch 103, loss 1.32008957862854, acc=0.4611110985279083, loss=1.32008957862854
train: epoch 104, loss 0.6325799822807312, acc=0.7283333539962769, loss=0.6325799822807312
test: epoch 104, loss 1.3190479278564453, acc=0.48055556416511536, loss=1.3190479278564453
train: epoch 105, loss 0.6494528651237488, acc=0.7232778072357178, loss=0.6494528651237488
test: epoch 105, loss 1.5479120016098022, acc=0.4833333194255829, loss=1.5479120016098022
train: epoch 106, loss 0.646621823310852, acc=0.7223333120346069, loss=0.646621823310852
test: epoch 106, loss 1.4569580554962158, acc=0.4861111044883728, loss=1.4569580554962158
train: epoch 107, loss 0.6459910869598389, acc=0.7288888692855835, loss=0.6459910869598389
test: epoch 107, loss 1.299192190170288, acc=0.4749999940395355, loss=1.299192190170288
train: epoch 108, loss 0.6323308944702148, acc=0.7295555472373962, loss=0.6323308944702148
test: epoch 108, loss 1.5485484600067139, acc=0.4833333194255829, loss=1.5485484600067139
train: epoch 109, loss 0.6411317586898804, acc=0.7250000238418579, loss=0.6411317586898804
test: epoch 109, loss 1.3829944133758545, acc=0.49166667461395264, loss=1.3829944133758545
train: epoch 110, loss 0.6440567970275879, acc=0.7281110882759094, loss=0.6440567970275879
test: epoch 110, loss 1.3394657373428345, acc=0.48055556416511536, loss=1.3394657373428345
train: epoch 111, loss 0.6487179398536682, acc=0.7253888845443726, loss=0.6487179398536682
test: epoch 111, loss 1.2310913801193237, acc=0.47777777910232544, loss=1.2310913801193237
train: epoch 112, loss 0.638837456703186, acc=0.7285000085830688, loss=0.638837456703186
test: epoch 112, loss 1.4331769943237305, acc=0.43888887763023376, loss=1.4331769943237305
train: epoch 113, loss 0.6786922812461853, acc=0.7172222137451172, loss=0.6786922812461853
test: epoch 113, loss 1.4786854982376099, acc=0.4861111044883728, loss=1.4786854982376099
train: epoch 114, loss 0.6356486678123474, acc=0.7288333177566528, loss=0.6356486678123474
test: epoch 114, loss 1.4393173456192017, acc=0.47777777910232544, loss=1.4393173456192017
train: epoch 115, loss 0.6330029964447021, acc=0.730388879776001, loss=0.6330029964447021
test: epoch 115, loss 1.3359390497207642, acc=0.48055556416511536, loss=1.3359390497207642
train: epoch 116, loss 0.6328637003898621, acc=0.7276111245155334, loss=0.6328637003898621
test: epoch 116, loss 1.3611212968826294, acc=0.4833333194255829, loss=1.3611212968826294
train: epoch 117, loss 0.642122745513916, acc=0.7246111035346985, loss=0.642122745513916
test: epoch 117, loss 1.3285419940948486, acc=0.48055556416511536, loss=1.3285419940948486
train: epoch 118, loss 0.6795511841773987, acc=0.7152777910232544, loss=0.6795511841773987
test: epoch 118, loss 1.3931471109390259, acc=0.4833333194255829, loss=1.3931471109390259
train: epoch 119, loss 0.6353235840797424, acc=0.7255555391311646, loss=0.6353235840797424
test: epoch 119, loss 1.3325737714767456, acc=0.4833333194255829, loss=1.3325737714767456
train: epoch 120, loss 0.6097690463066101, acc=0.738777756690979, loss=0.6097690463066101
test: epoch 120, loss 1.3279403448104858, acc=0.4833333194255829, loss=1.3279403448104858
train: epoch 121, loss 0.6181256771087646, acc=0.7348333597183228, loss=0.6181256771087646
test: epoch 121, loss 1.3004487752914429, acc=0.47777777910232544, loss=1.3004487752914429
train: epoch 122, loss 0.6403769254684448, acc=0.7246666550636292, loss=0.6403769254684448
test: epoch 122, loss 1.3875880241394043, acc=0.4833333194255829, loss=1.3875880241394043
train: epoch 123, loss 0.6684522032737732, acc=0.7197222113609314, loss=0.6684522032737732
test: epoch 123, loss 1.3519961833953857, acc=0.4833333194255829, loss=1.3519961833953857
train: epoch 124, loss 0.605842649936676, acc=0.7368888854980469, loss=0.605842649936676
test: epoch 124, loss 1.3759406805038452, acc=0.47777777910232544, loss=1.3759406805038452
train: epoch 125, loss 0.6202751994132996, acc=0.7340555787086487, loss=0.6202751994132996
test: epoch 125, loss 1.5273449420928955, acc=0.4861111044883728, loss=1.5273449420928955
train: epoch 126, loss 0.6098827719688416, acc=0.7354999780654907, loss=0.6098827719688416
test: epoch 126, loss 1.4284145832061768, acc=0.4722222089767456, loss=1.4284145832061768
train: epoch 127, loss 0.614681601524353, acc=0.7327777743339539, loss=0.614681601524353
test: epoch 127, loss 1.3507248163223267, acc=0.4833333194255829, loss=1.3507248163223267
train: epoch 128, loss 0.6521914601325989, acc=0.719944417476654, loss=0.6521914601325989
test: epoch 128, loss 1.242713451385498, acc=0.48055556416511536, loss=1.242713451385498
train: epoch 129, loss 0.6481334567070007, acc=0.7226666808128357, loss=0.6481334567070007
test: epoch 129, loss 1.43890380859375, acc=0.4722222089767456, loss=1.43890380859375
train: epoch 130, loss 0.6568244695663452, acc=0.7146111130714417, loss=0.6568244695663452
test: epoch 130, loss 1.3796337842941284, acc=0.4888888895511627, loss=1.3796337842941284
train: epoch 131, loss 0.6415016651153564, acc=0.7218888998031616, loss=0.6415016651153564
test: epoch 131, loss 1.4174997806549072, acc=0.4749999940395355, loss=1.4174997806549072
train: epoch 132, loss 0.6554486751556396, acc=0.7176666855812073, loss=0.6554486751556396
test: epoch 132, loss 3.1877706050872803, acc=0.30000001192092896, loss=3.1877706050872803
train: epoch 133, loss 0.6775475144386292, acc=0.7118889093399048, loss=0.6775475144386292
test: epoch 133, loss 1.3423477411270142, acc=0.4833333194255829, loss=1.3423477411270142
train: epoch 134, loss 0.6392515897750854, acc=0.7271111011505127, loss=0.6392515897750854
test: epoch 134, loss 1.334845781326294, acc=0.4833333194255829, loss=1.334845781326294
train: epoch 135, loss 0.6271170973777771, acc=0.7299444675445557, loss=0.6271170973777771
test: epoch 135, loss 1.390586495399475, acc=0.47777777910232544, loss=1.390586495399475
train: epoch 136, loss 0.6247886419296265, acc=0.7296110987663269, loss=0.6247886419296265
test: epoch 136, loss 1.5308624505996704, acc=0.47777777910232544, loss=1.5308624505996704
train: epoch 137, loss 0.6371411681175232, acc=0.7217777967453003, loss=0.6371411681175232
test: epoch 137, loss 1.3110908269882202, acc=0.48055556416511536, loss=1.3110908269882202
train: epoch 138, loss 0.6027817130088806, acc=0.7379444241523743, loss=0.6027817130088806
test: epoch 138, loss 1.4558531045913696, acc=0.4833333194255829, loss=1.4558531045913696
train: epoch 139, loss 0.6323210597038269, acc=0.7233889102935791, loss=0.6323210597038269
test: epoch 139, loss 1.5097731351852417, acc=0.4833333194255829, loss=1.5097731351852417
train: epoch 140, loss 0.6294547319412231, acc=0.7239999771118164, loss=0.6294547319412231
test: epoch 140, loss 1.3368180990219116, acc=0.4861111044883728, loss=1.3368180990219116
train: epoch 141, loss 0.620945394039154, acc=0.7303333282470703, loss=0.620945394039154
test: epoch 141, loss 1.448110818862915, acc=0.43611112236976624, loss=1.448110818862915
train: epoch 142, loss 0.646511971950531, acc=0.7221111059188843, loss=0.646511971950531
test: epoch 142, loss 1.360335111618042, acc=0.4833333194255829, loss=1.360335111618042
train: epoch 143, loss 0.6210702657699585, acc=0.7277777791023254, loss=0.6210702657699585
test: epoch 143, loss 1.4541078805923462, acc=0.46388888359069824, loss=1.4541078805923462
train: epoch 144, loss 0.6098631620407104, acc=0.7316111326217651, loss=0.6098631620407104
test: epoch 144, loss 1.3277415037155151, acc=0.4833333194255829, loss=1.3277415037155151
train: epoch 145, loss 0.6047331690788269, acc=0.7358333468437195, loss=0.6047331690788269
test: epoch 145, loss 1.2633448839187622, acc=0.4833333194255829, loss=1.2633448839187622
train: epoch 146, loss 0.609657883644104, acc=0.7307778000831604, loss=0.609657883644104
test: epoch 146, loss 1.5825378894805908, acc=0.48055556416511536, loss=1.5825378894805908
train: epoch 147, loss 0.6109210848808289, acc=0.7343888878822327, loss=0.6109210848808289
test: epoch 147, loss 1.3255466222763062, acc=0.4833333194255829, loss=1.3255466222763062
train: epoch 148, loss 0.5816600918769836, acc=0.7444999814033508, loss=0.5816600918769836
test: epoch 148, loss 1.4675577878952026, acc=0.4833333194255829, loss=1.4675577878952026
train: epoch 149, loss 0.6151612997055054, acc=0.7353333234786987, loss=0.6151612997055054
test: epoch 149, loss 1.50860595703125, acc=0.4833333194255829, loss=1.50860595703125
train: epoch 150, loss 0.6283405423164368, acc=0.7327222228050232, loss=0.6283405423164368
test: epoch 150, loss 1.3606122732162476, acc=0.4833333194255829, loss=1.3606122732162476
