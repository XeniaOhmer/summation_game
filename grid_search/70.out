# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.99", "--one_hot=1", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1011764258, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1011764258, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.4520342350006104, acc=0.04794444516301155, loss=3.4520342350006104
test: epoch 1, loss 3.6303040981292725, acc=0.03888889029622078, loss=3.6303040981292725
train: epoch 2, loss 2.7727386951446533, acc=0.12416666746139526, loss=2.7727386951446533
test: epoch 2, loss 2.751026153564453, acc=0.08611111342906952, loss=2.751026153564453
train: epoch 3, loss 2.1054201126098633, acc=0.2352222204208374, loss=2.1054201126098633
test: epoch 3, loss 2.3563287258148193, acc=0.1388888955116272, loss=2.3563287258148193
train: epoch 4, loss 1.8499066829681396, acc=0.2829444408416748, loss=1.8499066829681396
test: epoch 4, loss 2.32893443107605, acc=0.15000000596046448, loss=2.32893443107605
train: epoch 5, loss 1.7339009046554565, acc=0.30944445729255676, loss=1.7339009046554565
test: epoch 5, loss 2.308515787124634, acc=0.16388888657093048, loss=2.308515787124634
train: epoch 6, loss 1.6360042095184326, acc=0.3448888957500458, loss=1.6360042095184326
test: epoch 6, loss 2.24249267578125, acc=0.18333333730697632, loss=2.24249267578125
train: epoch 7, loss 1.5696974992752075, acc=0.36544445157051086, loss=1.5696974992752075
test: epoch 7, loss 2.13651704788208, acc=0.18888889253139496, loss=2.13651704788208
train: epoch 8, loss 1.5044081211090088, acc=0.39088889956474304, loss=1.5044081211090088
test: epoch 8, loss 2.0130136013031006, acc=0.21388888359069824, loss=2.0130136013031006
train: epoch 9, loss 1.4578840732574463, acc=0.4042222201824188, loss=1.4578840732574463
test: epoch 9, loss 1.9219944477081299, acc=0.2527777850627899, loss=1.9219944477081299
train: epoch 10, loss 1.3907452821731567, acc=0.4317777752876282, loss=1.3907452821731567
test: epoch 10, loss 1.9110759496688843, acc=0.25555557012557983, loss=1.9110759496688843
train: epoch 11, loss 1.358803153038025, acc=0.4404444396495819, loss=1.358803153038025
test: epoch 11, loss 1.8397780656814575, acc=0.25555557012557983, loss=1.8397780656814575
train: epoch 12, loss 1.3193851709365845, acc=0.4666111171245575, loss=1.3193851709365845
test: epoch 12, loss 1.8078752756118774, acc=0.2777777910232544, loss=1.8078752756118774
train: epoch 13, loss 1.264864444732666, acc=0.48222222924232483, loss=1.264864444732666
test: epoch 13, loss 1.7384300231933594, acc=0.28333333134651184, loss=1.7384300231933594
train: epoch 14, loss 1.2254638671875, acc=0.4983888864517212, loss=1.2254638671875
test: epoch 14, loss 1.7527574300765991, acc=0.2916666567325592, loss=1.7527574300765991
train: epoch 15, loss 1.198076844215393, acc=0.5080000162124634, loss=1.198076844215393
test: epoch 15, loss 1.71145498752594, acc=0.2944444417953491, loss=1.71145498752594
train: epoch 16, loss 1.1705965995788574, acc=0.5206111073493958, loss=1.1705965995788574
test: epoch 16, loss 1.6740927696228027, acc=0.3055555522441864, loss=1.6740927696228027
train: epoch 17, loss 1.125898838043213, acc=0.5345555543899536, loss=1.125898838043213
test: epoch 17, loss 1.6702470779418945, acc=0.3027777671813965, loss=1.6702470779418945
train: epoch 18, loss 1.1017472743988037, acc=0.550000011920929, loss=1.1017472743988037
test: epoch 18, loss 1.6984572410583496, acc=0.3083333373069763, loss=1.6984572410583496
train: epoch 19, loss 1.0837868452072144, acc=0.5527777671813965, loss=1.0837868452072144
test: epoch 19, loss 1.6226712465286255, acc=0.3083333373069763, loss=1.6226712465286255
train: epoch 20, loss 1.0624865293502808, acc=0.5583333373069763, loss=1.0624865293502808
test: epoch 20, loss 1.5970361232757568, acc=0.3166666626930237, loss=1.5970361232757568
train: epoch 21, loss 1.033172845840454, acc=0.5672777891159058, loss=1.033172845840454
test: epoch 21, loss 1.613335371017456, acc=0.3222222328186035, loss=1.613335371017456
train: epoch 22, loss 1.026149034500122, acc=0.5715000033378601, loss=1.026149034500122
test: epoch 22, loss 1.6302845478057861, acc=0.31388887763023376, loss=1.6302845478057861
train: epoch 23, loss 1.0095475912094116, acc=0.5817221999168396, loss=1.0095475912094116
test: epoch 23, loss 1.585178017616272, acc=0.3194444477558136, loss=1.585178017616272
train: epoch 24, loss 0.9905072450637817, acc=0.5853333473205566, loss=0.9905072450637817
test: epoch 24, loss 1.6622394323349, acc=0.3166666626930237, loss=1.6622394323349
train: epoch 25, loss 0.9776977896690369, acc=0.5948888659477234, loss=0.9776977896690369
test: epoch 25, loss 1.6117571592330933, acc=0.3305555582046509, loss=1.6117571592330933
train: epoch 26, loss 0.9626699090003967, acc=0.5964444279670715, loss=0.9626699090003967
test: epoch 26, loss 1.5972810983657837, acc=0.34166666865348816, loss=1.5972810983657837
train: epoch 27, loss 0.948642373085022, acc=0.6022777557373047, loss=0.948642373085022
test: epoch 27, loss 1.5792341232299805, acc=0.32777777314186096, loss=1.5792341232299805
train: epoch 28, loss 0.9363529086112976, acc=0.6068888902664185, loss=0.9363529086112976
test: epoch 28, loss 1.598738431930542, acc=0.33888888359069824, loss=1.598738431930542
train: epoch 29, loss 0.9350802898406982, acc=0.6126111149787903, loss=0.9350802898406982
test: epoch 29, loss 1.585646390914917, acc=0.3333333432674408, loss=1.585646390914917
train: epoch 30, loss 0.9204396605491638, acc=0.6172778010368347, loss=0.9204396605491638
test: epoch 30, loss 1.5972918272018433, acc=0.33888888359069824, loss=1.5972918272018433
train: epoch 31, loss 0.911320149898529, acc=0.6221110820770264, loss=0.911320149898529
test: epoch 31, loss 1.6037929058074951, acc=0.3361110985279083, loss=1.6037929058074951
train: epoch 32, loss 0.8995029926300049, acc=0.6272777915000916, loss=0.8995029926300049
test: epoch 32, loss 1.6471155881881714, acc=0.33888888359069824, loss=1.6471155881881714
train: epoch 33, loss 0.8852493762969971, acc=0.6302777528762817, loss=0.8852493762969971
test: epoch 33, loss 1.6385576725006104, acc=0.3444444537162781, loss=1.6385576725006104
train: epoch 34, loss 0.8732009530067444, acc=0.6353333592414856, loss=0.8732009530067444
test: epoch 34, loss 1.5574240684509277, acc=0.34166666865348816, loss=1.5574240684509277
train: epoch 35, loss 0.8687453269958496, acc=0.6416666507720947, loss=0.8687453269958496
test: epoch 35, loss 1.6952003240585327, acc=0.34166666865348816, loss=1.6952003240585327
train: epoch 36, loss 0.8468400239944458, acc=0.6441110968589783, loss=0.8468400239944458
test: epoch 36, loss 1.6324049234390259, acc=0.34166666865348816, loss=1.6324049234390259
train: epoch 37, loss 0.8437677621841431, acc=0.6495555639266968, loss=0.8437677621841431
test: epoch 37, loss 1.690507411956787, acc=0.35555556416511536, loss=1.690507411956787
train: epoch 38, loss 0.8430874943733215, acc=0.6483333110809326, loss=0.8430874943733215
test: epoch 38, loss 1.585625410079956, acc=0.3444444537162781, loss=1.585625410079956
train: epoch 39, loss 0.8332566618919373, acc=0.6531111001968384, loss=0.8332566618919373
test: epoch 39, loss 1.6136428117752075, acc=0.3638888895511627, loss=1.6136428117752075
train: epoch 40, loss 0.8208624720573425, acc=0.657444417476654, loss=0.8208624720573425
test: epoch 40, loss 1.7334885597229004, acc=0.35555556416511536, loss=1.7334885597229004
train: epoch 41, loss 0.8190171122550964, acc=0.6575555801391602, loss=0.8190171122550964
test: epoch 41, loss 1.6145023107528687, acc=0.3638888895511627, loss=1.6145023107528687
train: epoch 42, loss 0.8158167600631714, acc=0.6623888611793518, loss=0.8158167600631714
test: epoch 42, loss 1.6175920963287354, acc=0.375, loss=1.6175920963287354
train: epoch 43, loss 0.8019317984580994, acc=0.6660555601119995, loss=0.8019317984580994
test: epoch 43, loss 1.6426335573196411, acc=0.38055557012557983, loss=1.6426335573196411
train: epoch 44, loss 0.7898500561714172, acc=0.670722246170044, loss=0.7898500561714172
test: epoch 44, loss 1.5740784406661987, acc=0.3638888895511627, loss=1.5740784406661987
train: epoch 45, loss 0.7948935627937317, acc=0.6728888750076294, loss=0.7948935627937317
test: epoch 45, loss 1.6099246740341187, acc=0.35555556416511536, loss=1.6099246740341187
train: epoch 46, loss 0.785872220993042, acc=0.6723333597183228, loss=0.785872220993042
test: epoch 46, loss 1.5929248332977295, acc=0.35277777910232544, loss=1.5929248332977295
train: epoch 47, loss 0.7816702723503113, acc=0.6728888750076294, loss=0.7816702723503113
test: epoch 47, loss 1.5694142580032349, acc=0.3722222149372101, loss=1.5694142580032349
train: epoch 48, loss 0.7870228886604309, acc=0.6717222332954407, loss=0.7870228886604309
test: epoch 48, loss 1.6991668939590454, acc=0.3722222149372101, loss=1.6991668939590454
train: epoch 49, loss 0.7703389525413513, acc=0.6790555715560913, loss=0.7703389525413513
test: epoch 49, loss 1.593422532081604, acc=0.3638888895511627, loss=1.593422532081604
train: epoch 50, loss 0.7588152885437012, acc=0.683055579662323, loss=0.7588152885437012
test: epoch 50, loss 1.6946784257888794, acc=0.36944442987442017, loss=1.6946784257888794
train: epoch 51, loss 0.7488459348678589, acc=0.6847777962684631, loss=0.7488459348678589
test: epoch 51, loss 1.6613729000091553, acc=0.3777777850627899, loss=1.6613729000091553
train: epoch 52, loss 0.7590845823287964, acc=0.6840555667877197, loss=0.7590845823287964
test: epoch 52, loss 1.6487433910369873, acc=0.3638888895511627, loss=1.6487433910369873
train: epoch 53, loss 0.7468081116676331, acc=0.6857222318649292, loss=0.7468081116676331
test: epoch 53, loss 1.7355974912643433, acc=0.38333332538604736, loss=1.7355974912643433
train: epoch 54, loss 0.7535490989685059, acc=0.6842777729034424, loss=0.7535490989685059
test: epoch 54, loss 1.5729072093963623, acc=0.38055557012557983, loss=1.5729072093963623
train: epoch 55, loss 0.7305847406387329, acc=0.6941666603088379, loss=0.7305847406387329
test: epoch 55, loss 1.591750144958496, acc=0.38333332538604736, loss=1.591750144958496
train: epoch 56, loss 0.7333486676216125, acc=0.6894444227218628, loss=0.7333486676216125
test: epoch 56, loss 1.7180043458938599, acc=0.38333332538604736, loss=1.7180043458938599
train: epoch 57, loss 0.7360779643058777, acc=0.6903333067893982, loss=0.7360779643058777
test: epoch 57, loss 1.6441810131072998, acc=0.38055557012557983, loss=1.6441810131072998
train: epoch 58, loss 0.7299457788467407, acc=0.6864444613456726, loss=0.7299457788467407
test: epoch 58, loss 1.6600240468978882, acc=0.36666667461395264, loss=1.6600240468978882
train: epoch 59, loss 0.724384605884552, acc=0.6893333196640015, loss=0.724384605884552
test: epoch 59, loss 1.759988784790039, acc=0.3861111104488373, loss=1.759988784790039
train: epoch 60, loss 0.7106308341026306, acc=0.6955000162124634, loss=0.7106308341026306
test: epoch 60, loss 1.6793122291564941, acc=0.3861111104488373, loss=1.6793122291564941
train: epoch 61, loss 0.7140589356422424, acc=0.6947222352027893, loss=0.7140589356422424
test: epoch 61, loss 1.726440191268921, acc=0.3777777850627899, loss=1.726440191268921
train: epoch 62, loss 0.7163255214691162, acc=0.6910555362701416, loss=0.7163255214691162
test: epoch 62, loss 1.773414969444275, acc=0.38055557012557983, loss=1.773414969444275
train: epoch 63, loss 0.7099953889846802, acc=0.6983333230018616, loss=0.7099953889846802
test: epoch 63, loss 1.7447227239608765, acc=0.39444443583488464, loss=1.7447227239608765
train: epoch 64, loss 0.7131603956222534, acc=0.7016666531562805, loss=0.7131603956222534
test: epoch 64, loss 1.6872605085372925, acc=0.38055557012557983, loss=1.6872605085372925
train: epoch 65, loss 0.7017666101455688, acc=0.702833354473114, loss=0.7017666101455688
test: epoch 65, loss 1.67133629322052, acc=0.38333332538604736, loss=1.67133629322052
train: epoch 66, loss 0.7061437368392944, acc=0.7005000114440918, loss=0.7061437368392944
test: epoch 66, loss 1.764620304107666, acc=0.3916666805744171, loss=1.764620304107666
train: epoch 67, loss 0.6981619000434875, acc=0.7016111016273499, loss=0.6981619000434875
test: epoch 67, loss 1.6532944440841675, acc=0.3722222149372101, loss=1.6532944440841675
train: epoch 68, loss 0.6966174244880676, acc=0.7066110968589783, loss=0.6966174244880676
test: epoch 68, loss 1.8092551231384277, acc=0.38055557012557983, loss=1.8092551231384277
train: epoch 69, loss 0.6978380680084229, acc=0.7032777667045593, loss=0.6978380680084229
test: epoch 69, loss 1.7174218893051147, acc=0.38333332538604736, loss=1.7174218893051147
train: epoch 70, loss 0.6756026148796082, acc=0.7097777724266052, loss=0.6756026148796082
test: epoch 70, loss 1.6776591539382935, acc=0.375, loss=1.6776591539382935
train: epoch 71, loss 0.6766894459724426, acc=0.710277795791626, loss=0.6766894459724426
test: epoch 71, loss 1.8709003925323486, acc=0.38333332538604736, loss=1.8709003925323486
train: epoch 72, loss 0.6823740005493164, acc=0.7082222104072571, loss=0.6823740005493164
test: epoch 72, loss 1.7981069087982178, acc=0.38333332538604736, loss=1.7981069087982178
train: epoch 73, loss 0.6841846108436584, acc=0.7076666951179504, loss=0.6841846108436584
test: epoch 73, loss 1.715646743774414, acc=0.38055557012557983, loss=1.715646743774414
train: epoch 74, loss 0.6842508912086487, acc=0.7072222232818604, loss=0.6842508912086487
test: epoch 74, loss 1.6215671300888062, acc=0.3777777850627899, loss=1.6215671300888062
train: epoch 75, loss 0.6790422797203064, acc=0.7095000147819519, loss=0.6790422797203064
test: epoch 75, loss 1.672843098640442, acc=0.38333332538604736, loss=1.672843098640442
train: epoch 76, loss 0.6673587560653687, acc=0.7146111130714417, loss=0.6673587560653687
test: epoch 76, loss 1.6622422933578491, acc=0.3722222149372101, loss=1.6622422933578491
train: epoch 77, loss 0.674446165561676, acc=0.7144444584846497, loss=0.674446165561676
test: epoch 77, loss 1.838042974472046, acc=0.38055557012557983, loss=1.838042974472046
train: epoch 78, loss 0.6787940263748169, acc=0.7113333344459534, loss=0.6787940263748169
test: epoch 78, loss 1.6982645988464355, acc=0.3861111104488373, loss=1.6982645988464355
train: epoch 79, loss 0.6704874038696289, acc=0.7124444246292114, loss=0.6704874038696289
test: epoch 79, loss 1.690037727355957, acc=0.38333332538604736, loss=1.690037727355957
train: epoch 80, loss 0.6613297462463379, acc=0.7171666622161865, loss=0.6613297462463379
test: epoch 80, loss 1.8386811017990112, acc=0.38055557012557983, loss=1.8386811017990112
train: epoch 81, loss 0.6696453094482422, acc=0.7147777676582336, loss=0.6696453094482422
test: epoch 81, loss 1.6612956523895264, acc=0.39722222089767456, loss=1.6612956523895264
train: epoch 82, loss 0.6622330546379089, acc=0.7159444689750671, loss=0.6622330546379089
test: epoch 82, loss 1.7513912916183472, acc=0.3861111104488373, loss=1.7513912916183472
train: epoch 83, loss 0.6545604467391968, acc=0.7206110954284668, loss=0.6545604467391968
test: epoch 83, loss 1.7218892574310303, acc=0.3888888955116272, loss=1.7218892574310303
train: epoch 84, loss 0.662458062171936, acc=0.7204444408416748, loss=0.662458062171936
test: epoch 84, loss 1.640036702156067, acc=0.3888888955116272, loss=1.640036702156067
train: epoch 85, loss 0.6603572368621826, acc=0.722611129283905, loss=0.6603572368621826
test: epoch 85, loss 1.7836805582046509, acc=0.3777777850627899, loss=1.7836805582046509
train: epoch 86, loss 0.6592994928359985, acc=0.7196111083030701, loss=0.6592994928359985
test: epoch 86, loss 1.7006486654281616, acc=0.3888888955116272, loss=1.7006486654281616
train: epoch 87, loss 0.6478975415229797, acc=0.7253333330154419, loss=0.6478975415229797
test: epoch 87, loss 1.72097647190094, acc=0.3916666805744171, loss=1.72097647190094
train: epoch 88, loss 0.6489664912223816, acc=0.7265555262565613, loss=0.6489664912223816
test: epoch 88, loss 1.7087976932525635, acc=0.39722222089767456, loss=1.7087976932525635
train: epoch 89, loss 0.6508175134658813, acc=0.722777783870697, loss=0.6508175134658813
test: epoch 89, loss 1.8318158388137817, acc=0.3888888955116272, loss=1.8318158388137817
train: epoch 90, loss 0.6437663435935974, acc=0.7264444231987, loss=0.6437663435935974
test: epoch 90, loss 1.8377102613449097, acc=0.39722222089767456, loss=1.8377102613449097
train: epoch 91, loss 0.6438338160514832, acc=0.7307778000831604, loss=0.6438338160514832
test: epoch 91, loss 1.7023158073425293, acc=0.39444443583488464, loss=1.7023158073425293
train: epoch 92, loss 0.6527004241943359, acc=0.7232221961021423, loss=0.6527004241943359
test: epoch 92, loss 1.7282087802886963, acc=0.4027777910232544, loss=1.7282087802886963
train: epoch 93, loss 0.6494695544242859, acc=0.7267777919769287, loss=0.6494695544242859
test: epoch 93, loss 1.7737650871276855, acc=0.39444443583488464, loss=1.7737650871276855
train: epoch 94, loss 0.6494023203849792, acc=0.7247777581214905, loss=0.6494023203849792
test: epoch 94, loss 1.7760615348815918, acc=0.4000000059604645, loss=1.7760615348815918
train: epoch 95, loss 0.6455953121185303, acc=0.7245555520057678, loss=0.6455953121185303
test: epoch 95, loss 1.8404592275619507, acc=0.4055555462837219, loss=1.8404592275619507
train: epoch 96, loss 0.63753342628479, acc=0.725777804851532, loss=0.63753342628479
test: epoch 96, loss 1.7954902648925781, acc=0.4138889014720917, loss=1.7954902648925781
train: epoch 97, loss 0.630955159664154, acc=0.7269444465637207, loss=0.630955159664154
test: epoch 97, loss 1.7428348064422607, acc=0.4055555462837219, loss=1.7428348064422607
train: epoch 98, loss 0.6151198744773865, acc=0.7353888750076294, loss=0.6151198744773865
test: epoch 98, loss 1.7055776119232178, acc=0.4055555462837219, loss=1.7055776119232178
train: epoch 99, loss 0.6299514770507812, acc=0.7310000061988831, loss=0.6299514770507812
test: epoch 99, loss 1.721492886543274, acc=0.4055555462837219, loss=1.721492886543274
train: epoch 100, loss 0.6315215826034546, acc=0.7315000295639038, loss=0.6315215826034546
test: epoch 100, loss 1.768154263496399, acc=0.40833333134651184, loss=1.768154263496399
train: epoch 101, loss 0.6294772624969482, acc=0.7352777719497681, loss=0.6294772624969482
test: epoch 101, loss 1.7724043130874634, acc=0.4138889014720917, loss=1.7724043130874634
train: epoch 102, loss 0.6221287250518799, acc=0.7386666536331177, loss=0.6221287250518799
test: epoch 102, loss 1.63525390625, acc=0.4000000059604645, loss=1.63525390625
train: epoch 103, loss 0.6274304986000061, acc=0.7328333258628845, loss=0.6274304986000061
test: epoch 103, loss 1.6983355283737183, acc=0.4166666567325592, loss=1.6983355283737183
train: epoch 104, loss 0.6131382584571838, acc=0.7368888854980469, loss=0.6131382584571838
test: epoch 104, loss 1.7723153829574585, acc=0.4138889014720917, loss=1.7723153829574585
train: epoch 105, loss 0.6262211799621582, acc=0.7328888773918152, loss=0.6262211799621582
test: epoch 105, loss 1.8511404991149902, acc=0.4194444417953491, loss=1.8511404991149902
train: epoch 106, loss 0.6223070025444031, acc=0.7361666560173035, loss=0.6223070025444031
test: epoch 106, loss 1.7041687965393066, acc=0.4055555462837219, loss=1.7041687965393066
train: epoch 107, loss 0.6249325275421143, acc=0.7322221994400024, loss=0.6249325275421143
test: epoch 107, loss 1.6851400136947632, acc=0.4194444417953491, loss=1.6851400136947632
train: epoch 108, loss 0.6198605895042419, acc=0.7358333468437195, loss=0.6198605895042419
test: epoch 108, loss 1.81592857837677, acc=0.4055555462837219, loss=1.81592857837677
train: epoch 109, loss 0.6068153977394104, acc=0.7401111125946045, loss=0.6068153977394104
test: epoch 109, loss 1.6241425275802612, acc=0.4194444417953491, loss=1.6241425275802612
train: epoch 110, loss 0.6200507283210754, acc=0.7357777953147888, loss=0.6200507283210754
test: epoch 110, loss 1.8248724937438965, acc=0.41111111640930176, loss=1.8248724937438965
train: epoch 111, loss 0.6231645941734314, acc=0.7393333315849304, loss=0.6231645941734314
test: epoch 111, loss 1.723647117614746, acc=0.4194444417953491, loss=1.723647117614746
train: epoch 112, loss 0.6137547492980957, acc=0.7400000095367432, loss=0.6137547492980957
test: epoch 112, loss 1.7770328521728516, acc=0.4138889014720917, loss=1.7770328521728516
train: epoch 113, loss 0.6085229516029358, acc=0.7373889088630676, loss=0.6085229516029358
test: epoch 113, loss 1.6434167623519897, acc=0.42500001192092896, loss=1.6434167623519897
train: epoch 114, loss 0.6100013256072998, acc=0.7367222309112549, loss=0.6100013256072998
test: epoch 114, loss 1.6704051494598389, acc=0.4277777671813965, loss=1.6704051494598389
train: epoch 115, loss 0.6149994730949402, acc=0.7351111173629761, loss=0.6149994730949402
test: epoch 115, loss 1.9167197942733765, acc=0.41111111640930176, loss=1.9167197942733765
train: epoch 116, loss 0.6080300211906433, acc=0.7402222156524658, loss=0.6080300211906433
test: epoch 116, loss 1.7907403707504272, acc=0.42500001192092896, loss=1.7907403707504272
train: epoch 117, loss 0.6086429357528687, acc=0.7411666512489319, loss=0.6086429357528687
test: epoch 117, loss 1.6823525428771973, acc=0.4138889014720917, loss=1.6823525428771973
train: epoch 118, loss 0.6132631301879883, acc=0.7369999885559082, loss=0.6132631301879883
test: epoch 118, loss 1.8152580261230469, acc=0.4166666567325592, loss=1.8152580261230469
train: epoch 119, loss 0.6134183406829834, acc=0.7371666431427002, loss=0.6134183406829834
test: epoch 119, loss 1.7329597473144531, acc=0.42500001192092896, loss=1.7329597473144531
train: epoch 120, loss 0.5970888137817383, acc=0.7449444532394409, loss=0.5970888137817383
test: epoch 120, loss 1.7448869943618774, acc=0.42500001192092896, loss=1.7448869943618774
train: epoch 121, loss 0.6037054657936096, acc=0.7456111311912537, loss=0.6037054657936096
test: epoch 121, loss 1.7353885173797607, acc=0.4166666567325592, loss=1.7353885173797607
train: epoch 122, loss 0.6004852056503296, acc=0.7426111102104187, loss=0.6004852056503296
test: epoch 122, loss 1.7561185359954834, acc=0.43888887763023376, loss=1.7561185359954834
train: epoch 123, loss 0.6032979488372803, acc=0.7443888783454895, loss=0.6032979488372803
test: epoch 123, loss 1.737189769744873, acc=0.42500001192092896, loss=1.737189769744873
train: epoch 124, loss 0.5969733595848083, acc=0.745722234249115, loss=0.5969733595848083
test: epoch 124, loss 1.5580708980560303, acc=0.43611112236976624, loss=1.5580708980560303
train: epoch 125, loss 0.5964382290840149, acc=0.7463889122009277, loss=0.5964382290840149
test: epoch 125, loss 2.0111329555511475, acc=0.4333333373069763, loss=2.0111329555511475
train: epoch 126, loss 0.5952829122543335, acc=0.7419999837875366, loss=0.5952829122543335
test: epoch 126, loss 1.7517330646514893, acc=0.4333333373069763, loss=1.7517330646514893
train: epoch 127, loss 0.5799046754837036, acc=0.7504444718360901, loss=0.5799046754837036
test: epoch 127, loss 1.6717475652694702, acc=0.43888887763023376, loss=1.6717475652694702
train: epoch 128, loss 0.5953698754310608, acc=0.7428333163261414, loss=0.5953698754310608
test: epoch 128, loss 1.8222670555114746, acc=0.4444444477558136, loss=1.8222670555114746
train: epoch 129, loss 0.5820401310920715, acc=0.7481111288070679, loss=0.5820401310920715
test: epoch 129, loss 1.6924686431884766, acc=0.4416666626930237, loss=1.6924686431884766
train: epoch 130, loss 0.5984923839569092, acc=0.7440555691719055, loss=0.5984923839569092
test: epoch 130, loss 1.8062360286712646, acc=0.43611112236976624, loss=1.8062360286712646
train: epoch 131, loss 0.5861876010894775, acc=0.7459999918937683, loss=0.5861876010894775
test: epoch 131, loss 1.6968731880187988, acc=0.4333333373069763, loss=1.6968731880187988
train: epoch 132, loss 0.5794777274131775, acc=0.7506666779518127, loss=0.5794777274131775
test: epoch 132, loss 1.8555569648742676, acc=0.43888887763023376, loss=1.8555569648742676
train: epoch 133, loss 0.579194188117981, acc=0.7508888840675354, loss=0.579194188117981
test: epoch 133, loss 1.657929539680481, acc=0.4305555522441864, loss=1.657929539680481
train: epoch 134, loss 0.5809961557388306, acc=0.7511666417121887, loss=0.5809961557388306
test: epoch 134, loss 1.739256501197815, acc=0.43888887763023376, loss=1.739256501197815
train: epoch 135, loss 0.579797625541687, acc=0.7480555772781372, loss=0.579797625541687
test: epoch 135, loss 1.7430959939956665, acc=0.43888887763023376, loss=1.7430959939956665
train: epoch 136, loss 0.5777235627174377, acc=0.7521666884422302, loss=0.5777235627174377
test: epoch 136, loss 1.7163145542144775, acc=0.4444444477558136, loss=1.7163145542144775
train: epoch 137, loss 0.5830420851707458, acc=0.7482222318649292, loss=0.5830420851707458
test: epoch 137, loss 1.7130584716796875, acc=0.43611112236976624, loss=1.7130584716796875
train: epoch 138, loss 0.5732541680335999, acc=0.7508333325386047, loss=0.5732541680335999
test: epoch 138, loss 1.7301958799362183, acc=0.43888887763023376, loss=1.7301958799362183
train: epoch 139, loss 0.5700349807739258, acc=0.7548888921737671, loss=0.5700349807739258
test: epoch 139, loss 1.6134555339813232, acc=0.4444444477558136, loss=1.6134555339813232
train: epoch 140, loss 0.5711752772331238, acc=0.7533888816833496, loss=0.5711752772331238
test: epoch 140, loss 1.7664886713027954, acc=0.43611112236976624, loss=1.7664886713027954
train: epoch 141, loss 0.5829374194145203, acc=0.7515555620193481, loss=0.5829374194145203
test: epoch 141, loss 1.8248249292373657, acc=0.4472222328186035, loss=1.8248249292373657
train: epoch 142, loss 0.5780053734779358, acc=0.7521111369132996, loss=0.5780053734779358
test: epoch 142, loss 1.5696613788604736, acc=0.4555555582046509, loss=1.5696613788604736
train: epoch 143, loss 0.5699060559272766, acc=0.7540000081062317, loss=0.5699060559272766
test: epoch 143, loss 1.9872157573699951, acc=0.4444444477558136, loss=1.9872157573699951
train: epoch 144, loss 0.5819243788719177, acc=0.7524999976158142, loss=0.5819243788719177
test: epoch 144, loss 1.6587096452713013, acc=0.4444444477558136, loss=1.6587096452713013
train: epoch 145, loss 0.5674945116043091, acc=0.7554444670677185, loss=0.5674945116043091
test: epoch 145, loss 1.7622698545455933, acc=0.4472222328186035, loss=1.7622698545455933
train: epoch 146, loss 0.5665425658226013, acc=0.7537222504615784, loss=0.5665425658226013
test: epoch 146, loss 1.7065379619598389, acc=0.4416666626930237, loss=1.7065379619598389
train: epoch 147, loss 0.5706002712249756, acc=0.7507222294807434, loss=0.5706002712249756
test: epoch 147, loss 1.8464757204055786, acc=0.44999998807907104, loss=1.8464757204055786
train: epoch 148, loss 0.5696722865104675, acc=0.7527222037315369, loss=0.5696722865104675
test: epoch 148, loss 1.669662356376648, acc=0.4333333373069763, loss=1.669662356376648
train: epoch 149, loss 0.5736554861068726, acc=0.7513889074325562, loss=0.5736554861068726
test: epoch 149, loss 1.6477822065353394, acc=0.44999998807907104, loss=1.6477822065353394
train: epoch 150, loss 0.5622118711471558, acc=0.7562777996063232, loss=0.5622118711471558
test: epoch 150, loss 1.6640527248382568, acc=0.45277777314186096, loss=1.6640527248382568
