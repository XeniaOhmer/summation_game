# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1.5", "--temp_decay=0.995", "--one_hot=0", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1722309193, receiver_embed_dim=32, save_run=0, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1722309193, receiver_embed_dim=32, save_run=False, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.757326364517212, acc=0.12172222137451172, loss=2.757326364517212
test: epoch 1, loss 4.518414497375488, acc=0.07222222536802292, loss=4.518414497375488
train: epoch 2, loss 2.1630358695983887, acc=0.2163333296775818, loss=2.1630358695983887
test: epoch 2, loss 4.637460231781006, acc=0.06666667014360428, loss=4.637460231781006
train: epoch 3, loss 1.93921959400177, acc=0.26544445753097534, loss=1.93921959400177
test: epoch 3, loss 4.446638584136963, acc=0.08888889104127884, loss=4.446638584136963
train: epoch 4, loss 1.7873096466064453, acc=0.3101666569709778, loss=1.7873096466064453
test: epoch 4, loss 4.097540855407715, acc=0.10000000149011612, loss=4.097540855407715
train: epoch 5, loss 1.7098673582077026, acc=0.33250001072883606, loss=1.7098673582077026
test: epoch 5, loss 3.6946120262145996, acc=0.12222222238779068, loss=3.6946120262145996
train: epoch 6, loss 1.6183573007583618, acc=0.36250001192092896, loss=1.6183573007583618
test: epoch 6, loss 4.1167893409729, acc=0.11666666716337204, loss=4.1167893409729
train: epoch 7, loss 1.5477724075317383, acc=0.3844444453716278, loss=1.5477724075317383
test: epoch 7, loss 4.088811874389648, acc=0.12222222238779068, loss=4.088811874389648
train: epoch 8, loss 1.483167290687561, acc=0.4142777919769287, loss=1.483167290687561
test: epoch 8, loss 4.251307964324951, acc=0.11388888955116272, loss=4.251307964324951
train: epoch 9, loss 1.4315494298934937, acc=0.4423888921737671, loss=1.4315494298934937
test: epoch 9, loss 4.182398319244385, acc=0.12222222238779068, loss=4.182398319244385
train: epoch 10, loss 1.407489538192749, acc=0.4500555694103241, loss=1.407489538192749
test: epoch 10, loss 4.2852630615234375, acc=0.11388888955116272, loss=4.2852630615234375
train: epoch 11, loss 1.368427038192749, acc=0.46266666054725647, loss=1.368427038192749
test: epoch 11, loss 4.041934490203857, acc=0.11666666716337204, loss=4.041934490203857
train: epoch 12, loss 1.3252696990966797, acc=0.4754999876022339, loss=1.3252696990966797
test: epoch 12, loss 3.8803138732910156, acc=0.1388888955116272, loss=3.8803138732910156
train: epoch 13, loss 1.3031936883926392, acc=0.4933333396911621, loss=1.3031936883926392
test: epoch 13, loss 4.208329200744629, acc=0.1388888955116272, loss=4.208329200744629
train: epoch 14, loss 1.284428358078003, acc=0.4938333332538605, loss=1.284428358078003
test: epoch 14, loss 3.5856664180755615, acc=0.1388888955116272, loss=3.5856664180755615
train: epoch 15, loss 1.2609591484069824, acc=0.5086666941642761, loss=1.2609591484069824
test: epoch 15, loss 3.7601165771484375, acc=0.14444445073604584, loss=3.7601165771484375
train: epoch 16, loss 1.2341934442520142, acc=0.5199999809265137, loss=1.2341934442520142
test: epoch 16, loss 3.960956573486328, acc=0.11666666716337204, loss=3.960956573486328
train: epoch 17, loss 1.2272323369979858, acc=0.5249999761581421, loss=1.2272323369979858
test: epoch 17, loss 4.0186567306518555, acc=0.13611111044883728, loss=4.0186567306518555
train: epoch 18, loss 1.2156496047973633, acc=0.5288333296775818, loss=1.2156496047973633
test: epoch 18, loss 3.5858523845672607, acc=0.13611111044883728, loss=3.5858523845672607
train: epoch 19, loss 1.202418565750122, acc=0.5337222218513489, loss=1.202418565750122
test: epoch 19, loss 3.213087320327759, acc=0.12222222238779068, loss=3.213087320327759
train: epoch 20, loss 1.2035157680511475, acc=0.5394444465637207, loss=1.2035157680511475
test: epoch 20, loss 3.452646255493164, acc=0.13611111044883728, loss=3.452646255493164
train: epoch 21, loss 1.1765376329421997, acc=0.5444444417953491, loss=1.1765376329421997
test: epoch 21, loss 3.4637746810913086, acc=0.16388888657093048, loss=3.4637746810913086
train: epoch 22, loss 1.1590981483459473, acc=0.5534999966621399, loss=1.1590981483459473
test: epoch 22, loss 3.7530906200408936, acc=0.13333334028720856, loss=3.7530906200408936
train: epoch 23, loss 1.148561716079712, acc=0.5562777519226074, loss=1.148561716079712
test: epoch 23, loss 3.526566743850708, acc=0.14166666567325592, loss=3.526566743850708
train: epoch 24, loss 1.1316653490066528, acc=0.5668888688087463, loss=1.1316653490066528
test: epoch 24, loss 3.4890239238739014, acc=0.16944444179534912, loss=3.4890239238739014
train: epoch 25, loss 1.1542881727218628, acc=0.5572777986526489, loss=1.1542881727218628
test: epoch 25, loss 3.2725491523742676, acc=0.1527777761220932, loss=3.2725491523742676
train: epoch 26, loss 1.1141366958618164, acc=0.565500020980835, loss=1.1141366958618164
test: epoch 26, loss 3.1788361072540283, acc=0.16944444179534912, loss=3.1788361072540283
train: epoch 27, loss 1.1192265748977661, acc=0.5720555782318115, loss=1.1192265748977661
test: epoch 27, loss 3.5241591930389404, acc=0.16388888657093048, loss=3.5241591930389404
train: epoch 28, loss 1.1150342226028442, acc=0.5770000219345093, loss=1.1150342226028442
test: epoch 28, loss 3.0232350826263428, acc=0.16111111640930176, loss=3.0232350826263428
train: epoch 29, loss 1.1210620403289795, acc=0.5659999847412109, loss=1.1210620403289795
test: epoch 29, loss 3.2790820598602295, acc=0.14444445073604584, loss=3.2790820598602295
train: epoch 30, loss 1.116624355316162, acc=0.5772777795791626, loss=1.116624355316162
test: epoch 30, loss 3.181267738342285, acc=0.15000000596046448, loss=3.181267738342285
train: epoch 31, loss 1.0819380283355713, acc=0.5841666460037231, loss=1.0819380283355713
test: epoch 31, loss 3.346996784210205, acc=0.14166666567325592, loss=3.346996784210205
train: epoch 32, loss 1.0860768556594849, acc=0.5858333110809326, loss=1.0860768556594849
test: epoch 32, loss 2.9934239387512207, acc=0.16388888657093048, loss=2.9934239387512207
train: epoch 33, loss 1.0962891578674316, acc=0.5786666870117188, loss=1.0962891578674316
test: epoch 33, loss 2.637436628341675, acc=0.16111111640930176, loss=2.637436628341675
train: epoch 34, loss 1.0911351442337036, acc=0.5802222490310669, loss=1.0911351442337036
test: epoch 34, loss 2.906522035598755, acc=0.1805555522441864, loss=2.906522035598755
train: epoch 35, loss 1.0772684812545776, acc=0.5920555591583252, loss=1.0772684812545776
test: epoch 35, loss 3.0417308807373047, acc=0.18611110746860504, loss=3.0417308807373047
train: epoch 36, loss 1.0908740758895874, acc=0.5861666798591614, loss=1.0908740758895874
test: epoch 36, loss 3.2305784225463867, acc=0.16388888657093048, loss=3.2305784225463867
train: epoch 37, loss 1.092340350151062, acc=0.5870555639266968, loss=1.092340350151062
test: epoch 37, loss 3.1984364986419678, acc=0.14722222089767456, loss=3.1984364986419678
train: epoch 38, loss 1.0833476781845093, acc=0.5863333344459534, loss=1.0833476781845093
test: epoch 38, loss 2.9026787281036377, acc=0.18333333730697632, loss=2.9026787281036377
train: epoch 39, loss 1.0616116523742676, acc=0.5953333377838135, loss=1.0616116523742676
test: epoch 39, loss 3.1845669746398926, acc=0.16944444179534912, loss=3.1845669746398926
train: epoch 40, loss 1.067290186882019, acc=0.5948333144187927, loss=1.067290186882019
test: epoch 40, loss 2.7886486053466797, acc=0.13611111044883728, loss=2.7886486053466797
train: epoch 41, loss 1.0522537231445312, acc=0.598111093044281, loss=1.0522537231445312
test: epoch 41, loss 2.9352729320526123, acc=0.19166666269302368, loss=2.9352729320526123
train: epoch 42, loss 1.0731544494628906, acc=0.5887777805328369, loss=1.0731544494628906
test: epoch 42, loss 3.0430848598480225, acc=0.15555556118488312, loss=3.0430848598480225
train: epoch 43, loss 1.0543878078460693, acc=0.5950000286102295, loss=1.0543878078460693
test: epoch 43, loss 2.8713881969451904, acc=0.15833333134651184, loss=2.8713881969451904
train: epoch 44, loss 1.0640285015106201, acc=0.5953333377838135, loss=1.0640285015106201
test: epoch 44, loss 3.1370198726654053, acc=0.15833333134651184, loss=3.1370198726654053
train: epoch 45, loss 1.0812315940856934, acc=0.58561110496521, loss=1.0812315940856934
test: epoch 45, loss 2.653999090194702, acc=0.17499999701976776, loss=2.653999090194702
train: epoch 46, loss 1.0638220310211182, acc=0.5956110954284668, loss=1.0638220310211182
test: epoch 46, loss 3.056532621383667, acc=0.1944444477558136, loss=3.056532621383667
train: epoch 47, loss 1.0596482753753662, acc=0.5919444561004639, loss=1.0596482753753662
test: epoch 47, loss 3.1286308765411377, acc=0.21111111342906952, loss=3.1286308765411377
train: epoch 48, loss 1.060667872428894, acc=0.5967222452163696, loss=1.060667872428894
test: epoch 48, loss 2.804140329360962, acc=0.18333333730697632, loss=2.804140329360962
train: epoch 49, loss 1.0670102834701538, acc=0.5912222266197205, loss=1.0670102834701538
test: epoch 49, loss 2.703655481338501, acc=0.18333333730697632, loss=2.703655481338501
train: epoch 50, loss 1.059899091720581, acc=0.593833327293396, loss=1.059899091720581
test: epoch 50, loss 2.5500974655151367, acc=0.22777777910232544, loss=2.5500974655151367
train: epoch 51, loss 1.0664136409759521, acc=0.5871666669845581, loss=1.0664136409759521
test: epoch 51, loss 2.7888782024383545, acc=0.20000000298023224, loss=2.7888782024383545
train: epoch 52, loss 1.090537428855896, acc=0.5845555663108826, loss=1.090537428855896
test: epoch 52, loss 2.7950782775878906, acc=0.17777778208255768, loss=2.7950782775878906
train: epoch 53, loss 1.0817313194274902, acc=0.5899444222450256, loss=1.0817313194274902
test: epoch 53, loss 2.7529959678649902, acc=0.19722221791744232, loss=2.7529959678649902
train: epoch 54, loss 1.0596262216567993, acc=0.5978888869285583, loss=1.0596262216567993
test: epoch 54, loss 2.8168623447418213, acc=0.18888889253139496, loss=2.8168623447418213
train: epoch 55, loss 1.0537455081939697, acc=0.5910555720329285, loss=1.0537455081939697
test: epoch 55, loss 2.823608875274658, acc=0.17222222685813904, loss=2.823608875274658
train: epoch 56, loss 1.062289834022522, acc=0.5906111001968384, loss=1.062289834022522
test: epoch 56, loss 2.298734188079834, acc=0.1944444477558136, loss=2.298734188079834
train: epoch 57, loss 1.0601391792297363, acc=0.5922222137451172, loss=1.0601391792297363
test: epoch 57, loss 2.3724141120910645, acc=0.17777778208255768, loss=2.3724141120910645
train: epoch 58, loss 1.0587669610977173, acc=0.5960000157356262, loss=1.0587669610977173
test: epoch 58, loss 2.5988948345184326, acc=0.20555555820465088, loss=2.5988948345184326
train: epoch 59, loss 1.0665448904037476, acc=0.5868889093399048, loss=1.0665448904037476
test: epoch 59, loss 2.4984796047210693, acc=0.1944444477558136, loss=2.4984796047210693
train: epoch 60, loss 1.0755596160888672, acc=0.5855555534362793, loss=1.0755596160888672
test: epoch 60, loss 2.2485523223876953, acc=0.21388888359069824, loss=2.2485523223876953
train: epoch 61, loss 1.0500787496566772, acc=0.5953333377838135, loss=1.0500787496566772
test: epoch 61, loss 2.3771274089813232, acc=0.20277777314186096, loss=2.3771274089813232
train: epoch 62, loss 1.0736043453216553, acc=0.5825555324554443, loss=1.0736043453216553
test: epoch 62, loss 2.2500336170196533, acc=0.25, loss=2.2500336170196533
train: epoch 63, loss 1.0644415616989136, acc=0.5876666903495789, loss=1.0644415616989136
test: epoch 63, loss 2.4726719856262207, acc=0.16388888657093048, loss=2.4726719856262207
train: epoch 64, loss 1.0679771900177002, acc=0.5980555415153503, loss=1.0679771900177002
test: epoch 64, loss 2.707167387008667, acc=0.21111111342906952, loss=2.707167387008667
train: epoch 65, loss 1.0627928972244263, acc=0.590499997138977, loss=1.0627928972244263
test: epoch 65, loss 2.5271456241607666, acc=0.1805555522441864, loss=2.5271456241607666
train: epoch 66, loss 1.0602741241455078, acc=0.5918333530426025, loss=1.0602741241455078
test: epoch 66, loss 2.893634080886841, acc=0.18611110746860504, loss=2.893634080886841
train: epoch 67, loss 1.0548090934753418, acc=0.5945555567741394, loss=1.0548090934753418
test: epoch 67, loss 2.131028890609741, acc=0.22777777910232544, loss=2.131028890609741
train: epoch 68, loss 1.0643960237503052, acc=0.5937222242355347, loss=1.0643960237503052
test: epoch 68, loss 2.2300047874450684, acc=0.2361111044883728, loss=2.2300047874450684
train: epoch 69, loss 1.0396991968154907, acc=0.5953888893127441, loss=1.0396991968154907
test: epoch 69, loss 2.6850874423980713, acc=0.24166665971279144, loss=2.6850874423980713
train: epoch 70, loss 1.0654772520065308, acc=0.586222231388092, loss=1.0654772520065308
test: epoch 70, loss 2.37819242477417, acc=0.18333333730697632, loss=2.37819242477417
train: epoch 71, loss 1.055639624595642, acc=0.5909444689750671, loss=1.055639624595642
test: epoch 71, loss 2.1458933353424072, acc=0.22777777910232544, loss=2.1458933353424072
train: epoch 72, loss 1.0961813926696777, acc=0.5828889012336731, loss=1.0961813926696777
test: epoch 72, loss 2.2736334800720215, acc=0.20555555820465088, loss=2.2736334800720215
train: epoch 73, loss 1.0812021493911743, acc=0.5882777571678162, loss=1.0812021493911743
test: epoch 73, loss 2.146610736846924, acc=0.2083333283662796, loss=2.146610736846924
train: epoch 74, loss 1.070858359336853, acc=0.5858888626098633, loss=1.070858359336853
test: epoch 74, loss 2.3404452800750732, acc=0.23055554926395416, loss=2.3404452800750732
train: epoch 75, loss 1.0923346281051636, acc=0.5752778053283691, loss=1.0923346281051636
test: epoch 75, loss 2.093029737472534, acc=0.20000000298023224, loss=2.093029737472534
train: epoch 76, loss 1.0592154264450073, acc=0.5952222347259521, loss=1.0592154264450073
test: epoch 76, loss 2.596139669418335, acc=0.18333333730697632, loss=2.596139669418335
train: epoch 77, loss 1.0687024593353271, acc=0.5870555639266968, loss=1.0687024593353271
test: epoch 77, loss 1.967099905014038, acc=0.23333333432674408, loss=1.967099905014038
train: epoch 78, loss 1.0603314638137817, acc=0.5928888916969299, loss=1.0603314638137817
test: epoch 78, loss 2.2841577529907227, acc=0.19166666269302368, loss=2.2841577529907227
train: epoch 79, loss 1.090484857559204, acc=0.5751110911369324, loss=1.090484857559204
test: epoch 79, loss 2.102918863296509, acc=0.23888888955116272, loss=2.102918863296509
train: epoch 80, loss 1.0643823146820068, acc=0.5938888788223267, loss=1.0643823146820068
test: epoch 80, loss 2.227494716644287, acc=0.2222222238779068, loss=2.227494716644287
train: epoch 81, loss 1.076107382774353, acc=0.5902222394943237, loss=1.076107382774353
test: epoch 81, loss 1.9577535390853882, acc=0.2750000059604645, loss=1.9577535390853882
train: epoch 82, loss 1.0859460830688477, acc=0.5839999914169312, loss=1.0859460830688477
test: epoch 82, loss 2.0451979637145996, acc=0.24444444477558136, loss=2.0451979637145996
train: epoch 83, loss 1.0823564529418945, acc=0.5864999890327454, loss=1.0823564529418945
test: epoch 83, loss 2.1431360244750977, acc=0.24166665971279144, loss=2.1431360244750977
train: epoch 84, loss 1.047385573387146, acc=0.592555582523346, loss=1.047385573387146
test: epoch 84, loss 1.937049388885498, acc=0.2527777850627899, loss=1.937049388885498
train: epoch 85, loss 1.0840957164764404, acc=0.5878888964653015, loss=1.0840957164764404
test: epoch 85, loss 1.8494852781295776, acc=0.3194444477558136, loss=1.8494852781295776
train: epoch 86, loss 1.0693926811218262, acc=0.5913333296775818, loss=1.0693926811218262
test: epoch 86, loss 2.150742530822754, acc=0.1805555522441864, loss=2.150742530822754
train: epoch 87, loss 1.0585066080093384, acc=0.5909444689750671, loss=1.0585066080093384
test: epoch 87, loss 1.9918670654296875, acc=0.2888889014720917, loss=1.9918670654296875
train: epoch 88, loss 1.0439858436584473, acc=0.5953333377838135, loss=1.0439858436584473
test: epoch 88, loss 2.301706552505493, acc=0.2222222238779068, loss=2.301706552505493
train: epoch 89, loss 1.0726910829544067, acc=0.5888888835906982, loss=1.0726910829544067
test: epoch 89, loss 1.8129533529281616, acc=0.25, loss=1.8129533529281616
train: epoch 90, loss 1.0620129108428955, acc=0.5971111059188843, loss=1.0620129108428955
test: epoch 90, loss 1.970513939857483, acc=0.3166666626930237, loss=1.970513939857483
train: epoch 91, loss 1.0422810316085815, acc=0.5961111187934875, loss=1.0422810316085815
test: epoch 91, loss 1.7727768421173096, acc=0.3027777671813965, loss=1.7727768421173096
train: epoch 92, loss 1.0418460369110107, acc=0.5956110954284668, loss=1.0418460369110107
test: epoch 92, loss 1.7833685874938965, acc=0.2777777910232544, loss=1.7833685874938965
train: epoch 93, loss 1.080626368522644, acc=0.5828889012336731, loss=1.080626368522644
test: epoch 93, loss 1.8504592180252075, acc=0.2750000059604645, loss=1.8504592180252075
train: epoch 94, loss 1.0519574880599976, acc=0.6008333563804626, loss=1.0519574880599976
test: epoch 94, loss 1.8257460594177246, acc=0.3027777671813965, loss=1.8257460594177246
train: epoch 95, loss 1.0621414184570312, acc=0.5888333320617676, loss=1.0621414184570312
test: epoch 95, loss 1.7185903787612915, acc=0.3611111044883728, loss=1.7185903787612915
train: epoch 96, loss 0.9967041015625, acc=0.6037222146987915, loss=0.9967041015625
test: epoch 96, loss 1.7868291139602661, acc=0.31388887763023376, loss=1.7868291139602661
train: epoch 97, loss 1.026037335395813, acc=0.6040555834770203, loss=1.026037335395813
test: epoch 97, loss 1.5922819375991821, acc=0.3583333194255829, loss=1.5922819375991821
train: epoch 98, loss 1.007627010345459, acc=0.6010555624961853, loss=1.007627010345459
test: epoch 98, loss 1.7769842147827148, acc=0.3222222328186035, loss=1.7769842147827148
train: epoch 99, loss 1.0171701908111572, acc=0.6018333435058594, loss=1.0171701908111572
test: epoch 99, loss 1.8965942859649658, acc=0.28611111640930176, loss=1.8965942859649658
train: epoch 100, loss 1.0040968656539917, acc=0.6078333258628845, loss=1.0040968656539917
test: epoch 100, loss 1.9554671049118042, acc=0.3083333373069763, loss=1.9554671049118042
train: epoch 101, loss 1.01975417137146, acc=0.6082777976989746, loss=1.01975417137146
test: epoch 101, loss 1.8027470111846924, acc=0.3361110985279083, loss=1.8027470111846924
train: epoch 102, loss 1.042765736579895, acc=0.5947777628898621, loss=1.042765736579895
test: epoch 102, loss 1.800657868385315, acc=0.3305555582046509, loss=1.800657868385315
train: epoch 103, loss 1.0373989343643188, acc=0.6016666889190674, loss=1.0373989343643188
test: epoch 103, loss 1.9217153787612915, acc=0.26944443583488464, loss=1.9217153787612915
train: epoch 104, loss 1.013815999031067, acc=0.6110555529594421, loss=1.013815999031067
test: epoch 104, loss 1.7534667253494263, acc=0.3583333194255829, loss=1.7534667253494263
train: epoch 105, loss 1.0038102865219116, acc=0.609499990940094, loss=1.0038102865219116
test: epoch 105, loss 1.8843624591827393, acc=0.3194444477558136, loss=1.8843624591827393
train: epoch 106, loss 1.0281345844268799, acc=0.5996111035346985, loss=1.0281345844268799
test: epoch 106, loss 1.6835367679595947, acc=0.3333333432674408, loss=1.6835367679595947
train: epoch 107, loss 0.9991489052772522, acc=0.6057778000831604, loss=0.9991489052772522
test: epoch 107, loss 1.8472548723220825, acc=0.2638888955116272, loss=1.8472548723220825
train: epoch 108, loss 1.0337660312652588, acc=0.6027777791023254, loss=1.0337660312652588
test: epoch 108, loss 1.6606272459030151, acc=0.34166666865348816, loss=1.6606272459030151
train: epoch 109, loss 0.9995875358581543, acc=0.6058889031410217, loss=0.9995875358581543
test: epoch 109, loss 1.5874061584472656, acc=0.35555556416511536, loss=1.5874061584472656
train: epoch 110, loss 1.014506459236145, acc=0.6033889055252075, loss=1.014506459236145
test: epoch 110, loss 1.6931281089782715, acc=0.3888888955116272, loss=1.6931281089782715
train: epoch 111, loss 0.9959484934806824, acc=0.6077777743339539, loss=0.9959484934806824
test: epoch 111, loss 1.6350531578063965, acc=0.2638888955116272, loss=1.6350531578063965
train: epoch 112, loss 0.9902939200401306, acc=0.6117777824401855, loss=0.9902939200401306
test: epoch 112, loss 1.5783737897872925, acc=0.3027777671813965, loss=1.5783737897872925
train: epoch 113, loss 1.0134447813034058, acc=0.6069444417953491, loss=1.0134447813034058
test: epoch 113, loss 1.4218040704727173, acc=0.3916666805744171, loss=1.4218040704727173
train: epoch 114, loss 0.9942516684532166, acc=0.6095555424690247, loss=0.9942516684532166
test: epoch 114, loss 1.4953936338424683, acc=0.38333332538604736, loss=1.4953936338424683
train: epoch 115, loss 0.9819457530975342, acc=0.6201111078262329, loss=0.9819457530975342
test: epoch 115, loss 1.7333261966705322, acc=0.38055557012557983, loss=1.7333261966705322
train: epoch 116, loss 0.9832481145858765, acc=0.617111086845398, loss=0.9832481145858765
test: epoch 116, loss 1.5079954862594604, acc=0.3722222149372101, loss=1.5079954862594604
train: epoch 117, loss 0.9782408475875854, acc=0.6141111254692078, loss=0.9782408475875854
test: epoch 117, loss 1.4904471635818481, acc=0.3916666805744171, loss=1.4904471635818481
train: epoch 118, loss 1.0057307481765747, acc=0.6022777557373047, loss=1.0057307481765747
test: epoch 118, loss 1.5796468257904053, acc=0.34166666865348816, loss=1.5796468257904053
train: epoch 119, loss 0.9722388982772827, acc=0.6191111207008362, loss=0.9722388982772827
test: epoch 119, loss 1.5764299631118774, acc=0.38055557012557983, loss=1.5764299631118774
train: epoch 120, loss 0.9748663306236267, acc=0.6166666746139526, loss=0.9748663306236267
test: epoch 120, loss 1.828743577003479, acc=0.32499998807907104, loss=1.828743577003479
train: epoch 121, loss 0.9798645973205566, acc=0.6183888912200928, loss=0.9798645973205566
test: epoch 121, loss 1.5236449241638184, acc=0.4444444477558136, loss=1.5236449241638184
train: epoch 122, loss 0.9863298535346985, acc=0.6171666383743286, loss=0.9863298535346985
test: epoch 122, loss 1.4607157707214355, acc=0.3305555582046509, loss=1.4607157707214355
train: epoch 123, loss 0.9940924048423767, acc=0.6125555634498596, loss=0.9940924048423767
test: epoch 123, loss 1.5214976072311401, acc=0.3444444537162781, loss=1.5214976072311401
train: epoch 124, loss 0.9524114727973938, acc=0.6217222213745117, loss=0.9524114727973938
test: epoch 124, loss 1.459519863128662, acc=0.36944442987442017, loss=1.459519863128662
train: epoch 125, loss 0.9823406934738159, acc=0.6141111254692078, loss=0.9823406934738159
test: epoch 125, loss 1.6471422910690308, acc=0.3888888955116272, loss=1.6471422910690308
train: epoch 126, loss 0.9737866520881653, acc=0.6194444298744202, loss=0.9737866520881653
test: epoch 126, loss 1.6310110092163086, acc=0.3083333373069763, loss=1.6310110092163086
train: epoch 127, loss 0.9916631579399109, acc=0.6133333444595337, loss=0.9916631579399109
test: epoch 127, loss 1.4113235473632812, acc=0.4000000059604645, loss=1.4113235473632812
train: epoch 128, loss 0.9539037346839905, acc=0.629111111164093, loss=0.9539037346839905
test: epoch 128, loss 1.4876519441604614, acc=0.41111111640930176, loss=1.4876519441604614
train: epoch 129, loss 0.9556161761283875, acc=0.6232222318649292, loss=0.9556161761283875
test: epoch 129, loss 1.5627092123031616, acc=0.3861111104488373, loss=1.5627092123031616
train: epoch 130, loss 0.9922520518302917, acc=0.6109444499015808, loss=0.9922520518302917
test: epoch 130, loss 1.4170849323272705, acc=0.4027777910232544, loss=1.4170849323272705
train: epoch 131, loss 0.9778913259506226, acc=0.6202222108840942, loss=0.9778913259506226
test: epoch 131, loss 1.3803805112838745, acc=0.41111111640930176, loss=1.3803805112838745
train: epoch 132, loss 0.9445424675941467, acc=0.6216111183166504, loss=0.9445424675941467
test: epoch 132, loss 1.5177780389785767, acc=0.3583333194255829, loss=1.5177780389785767
train: epoch 133, loss 0.9488382339477539, acc=0.6198889017105103, loss=0.9488382339477539
test: epoch 133, loss 1.3859467506408691, acc=0.4166666567325592, loss=1.3859467506408691
train: epoch 134, loss 0.9492498636245728, acc=0.621999979019165, loss=0.9492498636245728
test: epoch 134, loss 1.3555846214294434, acc=0.3916666805744171, loss=1.3555846214294434
train: epoch 135, loss 0.9660965800285339, acc=0.6234999895095825, loss=0.9660965800285339
test: epoch 135, loss 1.3791569471359253, acc=0.38333332538604736, loss=1.3791569471359253
train: epoch 136, loss 0.9293767213821411, acc=0.6295555830001831, loss=0.9293767213821411
test: epoch 136, loss 1.4220211505889893, acc=0.4194444417953491, loss=1.4220211505889893
train: epoch 137, loss 0.9478335976600647, acc=0.6238333582878113, loss=0.9478335976600647
test: epoch 137, loss 1.3480671644210815, acc=0.375, loss=1.3480671644210815
train: epoch 138, loss 0.9401639103889465, acc=0.6261110901832581, loss=0.9401639103889465
test: epoch 138, loss 1.3775438070297241, acc=0.36944442987442017, loss=1.3775438070297241
train: epoch 139, loss 0.9853916764259338, acc=0.617888867855072, loss=0.9853916764259338
test: epoch 139, loss 1.645201325416565, acc=0.3888888955116272, loss=1.645201325416565
train: epoch 140, loss 0.9269472360610962, acc=0.6317777633666992, loss=0.9269472360610962
test: epoch 140, loss 1.497251033782959, acc=0.39722222089767456, loss=1.497251033782959
train: epoch 141, loss 0.9301733374595642, acc=0.6301110982894897, loss=0.9301733374595642
test: epoch 141, loss 1.4755078554153442, acc=0.39722222089767456, loss=1.4755078554153442
train: epoch 142, loss 0.9823876619338989, acc=0.6225555539131165, loss=0.9823876619338989
test: epoch 142, loss 1.5271714925765991, acc=0.3611111044883728, loss=1.5271714925765991
train: epoch 143, loss 0.9421615600585938, acc=0.6304444670677185, loss=0.9421615600585938
test: epoch 143, loss 1.3918832540512085, acc=0.4555555582046509, loss=1.3918832540512085
train: epoch 144, loss 0.9421068429946899, acc=0.633388876914978, loss=0.9421068429946899
test: epoch 144, loss 1.4026691913604736, acc=0.4583333432674408, loss=1.4026691913604736
train: epoch 145, loss 0.8897976279258728, acc=0.6521111130714417, loss=0.8897976279258728
test: epoch 145, loss 1.3465054035186768, acc=0.4138889014720917, loss=1.3465054035186768
train: epoch 146, loss 0.921133279800415, acc=0.6302222013473511, loss=0.921133279800415
test: epoch 146, loss 1.5381139516830444, acc=0.39722222089767456, loss=1.5381139516830444
train: epoch 147, loss 0.9259527921676636, acc=0.6359444260597229, loss=0.9259527921676636
test: epoch 147, loss 1.4586139917373657, acc=0.44999998807907104, loss=1.4586139917373657
train: epoch 148, loss 0.8973827362060547, acc=0.6455000042915344, loss=0.8973827362060547
test: epoch 148, loss 1.5213065147399902, acc=0.3888888955116272, loss=1.5213065147399902
train: epoch 149, loss 0.8966589570045471, acc=0.6432222127914429, loss=0.8966589570045471
test: epoch 149, loss 1.5027544498443604, acc=0.3888888955116272, loss=1.5027544498443604
train: epoch 150, loss 0.915720522403717, acc=0.6437777876853943, loss=0.915720522403717
test: epoch 150, loss 1.4684504270553589, acc=0.3722222149372101, loss=1.4684504270553589
