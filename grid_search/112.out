# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=1", "--one_hot=1", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=394732508, receiver_embed_dim=128, save_run=0, temp_decay=1.0, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=394732508, receiver_embed_dim=128, save_run=False, temp_decay=1.0, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.3838863372802734, acc=0.052000001072883606, loss=3.3838863372802734
test: epoch 1, loss 3.0475919246673584, acc=0.07500000298023224, loss=3.0475919246673584
train: epoch 2, loss 2.256960868835449, acc=0.16772222518920898, loss=2.256960868835449
test: epoch 2, loss 2.283522367477417, acc=0.1388888955116272, loss=2.283522367477417
train: epoch 3, loss 1.781010389328003, acc=0.28094443678855896, loss=1.781010389328003
test: epoch 3, loss 2.0245981216430664, acc=0.1805555522441864, loss=2.0245981216430664
train: epoch 4, loss 1.5664039850234985, acc=0.3411666750907898, loss=1.5664039850234985
test: epoch 4, loss 2.110699415206909, acc=0.20555555820465088, loss=2.110699415206909
train: epoch 5, loss 1.4358704090118408, acc=0.39072221517562866, loss=1.4358704090118408
test: epoch 5, loss 2.047879695892334, acc=0.2222222238779068, loss=2.047879695892334
train: epoch 6, loss 1.36787748336792, acc=0.41972222924232483, loss=1.36787748336792
test: epoch 6, loss 2.110124349594116, acc=0.2222222238779068, loss=2.110124349594116
train: epoch 7, loss 1.3152871131896973, acc=0.445166677236557, loss=1.3152871131896973
test: epoch 7, loss 2.1347880363464355, acc=0.2222222238779068, loss=2.1347880363464355
train: epoch 8, loss 1.2677263021469116, acc=0.4629444479942322, loss=1.2677263021469116
test: epoch 8, loss 2.0089199542999268, acc=0.23055554926395416, loss=2.0089199542999268
train: epoch 9, loss 1.2261502742767334, acc=0.4754444360733032, loss=1.2261502742767334
test: epoch 9, loss 2.108046293258667, acc=0.24444444477558136, loss=2.108046293258667
train: epoch 10, loss 1.1851708889007568, acc=0.4991111159324646, loss=1.1851708889007568
test: epoch 10, loss 2.1090853214263916, acc=0.21944443881511688, loss=2.1090853214263916
train: epoch 11, loss 1.1514713764190674, acc=0.5120000243186951, loss=1.1514713764190674
test: epoch 11, loss 2.150219202041626, acc=0.2222222238779068, loss=2.150219202041626
train: epoch 12, loss 1.1273953914642334, acc=0.5214444398880005, loss=1.1273953914642334
test: epoch 12, loss 2.2264931201934814, acc=0.25555557012557983, loss=2.2264931201934814
train: epoch 13, loss 1.1176854372024536, acc=0.5312222242355347, loss=1.1176854372024536
test: epoch 13, loss 2.2175257205963135, acc=0.24166665971279144, loss=2.2175257205963135
train: epoch 14, loss 1.096278190612793, acc=0.5329999923706055, loss=1.096278190612793
test: epoch 14, loss 2.2063562870025635, acc=0.27222222089767456, loss=2.2063562870025635
train: epoch 15, loss 1.0763260126113892, acc=0.5391111373901367, loss=1.0763260126113892
test: epoch 15, loss 2.1496925354003906, acc=0.24722221493721008, loss=2.1496925354003906
train: epoch 16, loss 1.046258568763733, acc=0.5556111335754395, loss=1.046258568763733
test: epoch 16, loss 2.2442867755889893, acc=0.25, loss=2.2442867755889893
train: epoch 17, loss 1.0390753746032715, acc=0.5556111335754395, loss=1.0390753746032715
test: epoch 17, loss 2.2639126777648926, acc=0.26944443583488464, loss=2.2639126777648926
train: epoch 18, loss 1.0414541959762573, acc=0.5575555562973022, loss=1.0414541959762573
test: epoch 18, loss 2.273149013519287, acc=0.2611111104488373, loss=2.273149013519287
train: epoch 19, loss 1.014072299003601, acc=0.5680555701255798, loss=1.014072299003601
test: epoch 19, loss 2.2257933616638184, acc=0.2750000059604645, loss=2.2257933616638184
train: epoch 20, loss 1.0039637088775635, acc=0.5689444541931152, loss=1.0039637088775635
test: epoch 20, loss 2.280075788497925, acc=0.26944443583488464, loss=2.280075788497925
train: epoch 21, loss 0.9957088232040405, acc=0.5731111168861389, loss=0.9957088232040405
test: epoch 21, loss 2.1718175411224365, acc=0.2666666805744171, loss=2.1718175411224365
train: epoch 22, loss 0.972646176815033, acc=0.5797777771949768, loss=0.972646176815033
test: epoch 22, loss 2.192399740219116, acc=0.27222222089767456, loss=2.192399740219116
train: epoch 23, loss 0.9639495015144348, acc=0.5786111354827881, loss=0.9639495015144348
test: epoch 23, loss 2.2110118865966797, acc=0.27222222089767456, loss=2.2110118865966797
train: epoch 24, loss 0.9622076749801636, acc=0.582111120223999, loss=0.9622076749801636
test: epoch 24, loss 2.283193588256836, acc=0.2666666805744171, loss=2.283193588256836
train: epoch 25, loss 0.9517312049865723, acc=0.5886111259460449, loss=0.9517312049865723
test: epoch 25, loss 2.1467366218566895, acc=0.28611111640930176, loss=2.1467366218566895
train: epoch 26, loss 0.9340831637382507, acc=0.5896666646003723, loss=0.9340831637382507
test: epoch 26, loss 2.2332334518432617, acc=0.2777777910232544, loss=2.2332334518432617
train: epoch 27, loss 0.9184593558311462, acc=0.593666672706604, loss=0.9184593558311462
test: epoch 27, loss 2.208127975463867, acc=0.28611111640930176, loss=2.208127975463867
train: epoch 28, loss 0.9325650334358215, acc=0.5928333401679993, loss=0.9325650334358215
test: epoch 28, loss 2.351149797439575, acc=0.28611111640930176, loss=2.351149797439575
train: epoch 29, loss 0.9084081053733826, acc=0.5998888611793518, loss=0.9084081053733826
test: epoch 29, loss 2.3412420749664307, acc=0.2916666567325592, loss=2.3412420749664307
train: epoch 30, loss 0.8963907957077026, acc=0.6038333177566528, loss=0.8963907957077026
test: epoch 30, loss 2.41973614692688, acc=0.28611111640930176, loss=2.41973614692688
train: epoch 31, loss 0.905286431312561, acc=0.5998888611793518, loss=0.905286431312561
test: epoch 31, loss 2.404196262359619, acc=0.2750000059604645, loss=2.404196262359619
train: epoch 32, loss 0.8961862325668335, acc=0.6082777976989746, loss=0.8961862325668335
test: epoch 32, loss 2.175727128982544, acc=0.3027777671813965, loss=2.175727128982544
train: epoch 33, loss 0.8715627789497375, acc=0.6107777953147888, loss=0.8715627789497375
test: epoch 33, loss 2.1489098072052, acc=0.28611111640930176, loss=2.1489098072052
train: epoch 34, loss 0.8840159177780151, acc=0.6065000295639038, loss=0.8840159177780151
test: epoch 34, loss 2.4794275760650635, acc=0.3055555522441864, loss=2.4794275760650635
train: epoch 35, loss 0.8897920846939087, acc=0.6044444441795349, loss=0.8897920846939087
test: epoch 35, loss 2.263390064239502, acc=0.3055555522441864, loss=2.263390064239502
train: epoch 36, loss 0.8771365880966187, acc=0.6100555658340454, loss=0.8771365880966187
test: epoch 36, loss 2.2087242603302, acc=0.31388887763023376, loss=2.2087242603302
train: epoch 37, loss 0.8724057674407959, acc=0.6087222099304199, loss=0.8724057674407959
test: epoch 37, loss 2.3120083808898926, acc=0.31388887763023376, loss=2.3120083808898926
train: epoch 38, loss 0.8558767437934875, acc=0.6140000224113464, loss=0.8558767437934875
test: epoch 38, loss 2.162520408630371, acc=0.2944444417953491, loss=2.162520408630371
train: epoch 39, loss 0.856823742389679, acc=0.6156111359596252, loss=0.856823742389679
test: epoch 39, loss 2.341817855834961, acc=0.31111112236976624, loss=2.341817855834961
train: epoch 40, loss 0.8406156301498413, acc=0.6241666674613953, loss=0.8406156301498413
test: epoch 40, loss 2.323267936706543, acc=0.32777777314186096, loss=2.323267936706543
train: epoch 41, loss 0.8405171036720276, acc=0.6268333196640015, loss=0.8405171036720276
test: epoch 41, loss 2.0628459453582764, acc=0.3333333432674408, loss=2.0628459453582764
train: epoch 42, loss 0.8210926055908203, acc=0.629111111164093, loss=0.8210926055908203
test: epoch 42, loss 2.2900161743164062, acc=0.3361110985279083, loss=2.2900161743164062
train: epoch 43, loss 0.8213882446289062, acc=0.6338889002799988, loss=0.8213882446289062
test: epoch 43, loss 2.1660220623016357, acc=0.3305555582046509, loss=2.1660220623016357
train: epoch 44, loss 0.8257240056991577, acc=0.6319444179534912, loss=0.8257240056991577
test: epoch 44, loss 2.2531259059906006, acc=0.32499998807907104, loss=2.2531259059906006
train: epoch 45, loss 0.815287172794342, acc=0.6382777690887451, loss=0.815287172794342
test: epoch 45, loss 2.2930848598480225, acc=0.3333333432674408, loss=2.2930848598480225
train: epoch 46, loss 0.8044201731681824, acc=0.6420000195503235, loss=0.8044201731681824
test: epoch 46, loss 2.152712106704712, acc=0.3333333432674408, loss=2.152712106704712
train: epoch 47, loss 0.8099966645240784, acc=0.6414999961853027, loss=0.8099966645240784
test: epoch 47, loss 2.28163743019104, acc=0.3222222328186035, loss=2.28163743019104
train: epoch 48, loss 0.802830159664154, acc=0.6422222256660461, loss=0.802830159664154
test: epoch 48, loss 2.250924825668335, acc=0.3472222089767456, loss=2.250924825668335
train: epoch 49, loss 0.8065602779388428, acc=0.6446666717529297, loss=0.8065602779388428
test: epoch 49, loss 2.3072941303253174, acc=0.33888888359069824, loss=2.3072941303253174
train: epoch 50, loss 0.7895709872245789, acc=0.6463333368301392, loss=0.7895709872245789
test: epoch 50, loss 2.14078688621521, acc=0.33888888359069824, loss=2.14078688621521
train: epoch 51, loss 0.7888810634613037, acc=0.6451666951179504, loss=0.7888810634613037
test: epoch 51, loss 2.262349843978882, acc=0.3444444537162781, loss=2.262349843978882
train: epoch 52, loss 0.7999551296234131, acc=0.637666642665863, loss=0.7999551296234131
test: epoch 52, loss 2.2705070972442627, acc=0.33888888359069824, loss=2.2705070972442627
train: epoch 53, loss 0.7742389440536499, acc=0.6504999995231628, loss=0.7742389440536499
test: epoch 53, loss 2.3598155975341797, acc=0.3444444537162781, loss=2.3598155975341797
train: epoch 54, loss 0.7774107456207275, acc=0.6505555510520935, loss=0.7774107456207275
test: epoch 54, loss 2.3933825492858887, acc=0.3444444537162781, loss=2.3933825492858887
train: epoch 55, loss 0.8002485632896423, acc=0.6396111249923706, loss=0.8002485632896423
test: epoch 55, loss 2.258517026901245, acc=0.3305555582046509, loss=2.258517026901245
train: epoch 56, loss 0.7716531753540039, acc=0.6526666879653931, loss=0.7716531753540039
test: epoch 56, loss 2.3063700199127197, acc=0.3583333194255829, loss=2.3063700199127197
train: epoch 57, loss 0.7715219855308533, acc=0.652055561542511, loss=0.7715219855308533
test: epoch 57, loss 2.528177261352539, acc=0.3472222089767456, loss=2.528177261352539
train: epoch 58, loss 0.7747533917427063, acc=0.6503333449363708, loss=0.7747533917427063
test: epoch 58, loss 2.3588979244232178, acc=0.36666667461395264, loss=2.3588979244232178
train: epoch 59, loss 0.7646312713623047, acc=0.6567777991294861, loss=0.7646312713623047
test: epoch 59, loss 2.545544385910034, acc=0.3638888895511627, loss=2.545544385910034
train: epoch 60, loss 0.7764646410942078, acc=0.6501111388206482, loss=0.7764646410942078
test: epoch 60, loss 2.3835747241973877, acc=0.3611111044883728, loss=2.3835747241973877
train: epoch 61, loss 0.7686457633972168, acc=0.6532777547836304, loss=0.7686457633972168
test: epoch 61, loss 2.3912513256073, acc=0.36666667461395264, loss=2.3912513256073
train: epoch 62, loss 0.7579452395439148, acc=0.6569444537162781, loss=0.7579452395439148
test: epoch 62, loss 1.899474859237671, acc=0.36666667461395264, loss=1.899474859237671
train: epoch 63, loss 0.7619187831878662, acc=0.6547222137451172, loss=0.7619187831878662
test: epoch 63, loss 2.281951665878296, acc=0.36666667461395264, loss=2.281951665878296
train: epoch 64, loss 0.7574090957641602, acc=0.6558333039283752, loss=0.7574090957641602
test: epoch 64, loss 2.2935333251953125, acc=0.3722222149372101, loss=2.2935333251953125
train: epoch 65, loss 0.7536587119102478, acc=0.6556110978126526, loss=0.7536587119102478
test: epoch 65, loss 2.4282257556915283, acc=0.3611111044883728, loss=2.4282257556915283
train: epoch 66, loss 0.7574456334114075, acc=0.6573888659477234, loss=0.7574456334114075
test: epoch 66, loss 2.0197064876556396, acc=0.3722222149372101, loss=2.0197064876556396
train: epoch 67, loss 0.7484960556030273, acc=0.6617222428321838, loss=0.7484960556030273
test: epoch 67, loss 2.515364408493042, acc=0.3722222149372101, loss=2.515364408493042
train: epoch 68, loss 0.7458887696266174, acc=0.6619444489479065, loss=0.7458887696266174
test: epoch 68, loss 2.1553475856781006, acc=0.3722222149372101, loss=2.1553475856781006
train: epoch 69, loss 0.7468801736831665, acc=0.6633889079093933, loss=0.7468801736831665
test: epoch 69, loss 2.339284896850586, acc=0.3722222149372101, loss=2.339284896850586
train: epoch 70, loss 0.7512565851211548, acc=0.6592222452163696, loss=0.7512565851211548
test: epoch 70, loss 2.2251510620117188, acc=0.36944442987442017, loss=2.2251510620117188
train: epoch 71, loss 0.7336952090263367, acc=0.6646111011505127, loss=0.7336952090263367
test: epoch 71, loss 2.4386677742004395, acc=0.3638888895511627, loss=2.4386677742004395
train: epoch 72, loss 0.7419171333312988, acc=0.6627222299575806, loss=0.7419171333312988
test: epoch 72, loss 2.404069662094116, acc=0.375, loss=2.404069662094116
train: epoch 73, loss 0.7389901876449585, acc=0.6633333563804626, loss=0.7389901876449585
test: epoch 73, loss 2.506826877593994, acc=0.36666667461395264, loss=2.506826877593994
train: epoch 74, loss 0.7522228360176086, acc=0.6559444665908813, loss=0.7522228360176086
test: epoch 74, loss 2.417079448699951, acc=0.38055557012557983, loss=2.417079448699951
train: epoch 75, loss 0.7302086353302002, acc=0.6621666550636292, loss=0.7302086353302002
test: epoch 75, loss 2.2902591228485107, acc=0.375, loss=2.2902591228485107
train: epoch 76, loss 0.7245346903800964, acc=0.6667777895927429, loss=0.7245346903800964
test: epoch 76, loss 2.295294761657715, acc=0.38055557012557983, loss=2.295294761657715
train: epoch 77, loss 0.7289313673973083, acc=0.6635555624961853, loss=0.7289313673973083
test: epoch 77, loss 2.392129898071289, acc=0.3722222149372101, loss=2.392129898071289
train: epoch 78, loss 0.7370802760124207, acc=0.6627777814865112, loss=0.7370802760124207
test: epoch 78, loss 2.331045627593994, acc=0.38055557012557983, loss=2.331045627593994
train: epoch 79, loss 0.7273934483528137, acc=0.6664999723434448, loss=0.7273934483528137
test: epoch 79, loss 2.107736587524414, acc=0.3777777850627899, loss=2.107736587524414
train: epoch 80, loss 0.7358536124229431, acc=0.6651111245155334, loss=0.7358536124229431
test: epoch 80, loss 2.3288347721099854, acc=0.38333332538604736, loss=2.3288347721099854
train: epoch 81, loss 0.7370390295982361, acc=0.6640555262565613, loss=0.7370390295982361
test: epoch 81, loss 2.569357395172119, acc=0.3777777850627899, loss=2.569357395172119
train: epoch 82, loss 0.7308780550956726, acc=0.6612777709960938, loss=0.7308780550956726
test: epoch 82, loss 2.3200230598449707, acc=0.38055557012557983, loss=2.3200230598449707
train: epoch 83, loss 0.7309465408325195, acc=0.6657778024673462, loss=0.7309465408325195
test: epoch 83, loss 2.3777706623077393, acc=0.36944442987442017, loss=2.3777706623077393
train: epoch 84, loss 0.7254195809364319, acc=0.6675000190734863, loss=0.7254195809364319
test: epoch 84, loss 2.3881494998931885, acc=0.3722222149372101, loss=2.3881494998931885
train: epoch 85, loss 0.7208340167999268, acc=0.6707777976989746, loss=0.7208340167999268
test: epoch 85, loss 2.554455518722534, acc=0.375, loss=2.554455518722534
train: epoch 86, loss 0.735819399356842, acc=0.6661111116409302, loss=0.735819399356842
test: epoch 86, loss 2.2499232292175293, acc=0.3777777850627899, loss=2.2499232292175293
train: epoch 87, loss 0.7170253992080688, acc=0.664555549621582, loss=0.7170253992080688
test: epoch 87, loss 2.656059980392456, acc=0.375, loss=2.656059980392456
train: epoch 88, loss 0.718156099319458, acc=0.6666111350059509, loss=0.718156099319458
test: epoch 88, loss 2.5994086265563965, acc=0.3777777850627899, loss=2.5994086265563965
train: epoch 89, loss 0.7269466519355774, acc=0.6629999876022339, loss=0.7269466519355774
test: epoch 89, loss 2.482436418533325, acc=0.38055557012557983, loss=2.482436418533325
train: epoch 90, loss 0.7099812030792236, acc=0.6694444417953491, loss=0.7099812030792236
test: epoch 90, loss 2.686690330505371, acc=0.375, loss=2.686690330505371
train: epoch 91, loss 0.7132261395454407, acc=0.6677777767181396, loss=0.7132261395454407
test: epoch 91, loss 2.567918539047241, acc=0.38333332538604736, loss=2.567918539047241
train: epoch 92, loss 0.7282828688621521, acc=0.6706110835075378, loss=0.7282828688621521
test: epoch 92, loss 2.356788158416748, acc=0.38055557012557983, loss=2.356788158416748
train: epoch 93, loss 0.7141852974891663, acc=0.6685000061988831, loss=0.7141852974891663
test: epoch 93, loss 2.3303427696228027, acc=0.3888888955116272, loss=2.3303427696228027
train: epoch 94, loss 0.706134021282196, acc=0.6717777848243713, loss=0.706134021282196
test: epoch 94, loss 2.256000518798828, acc=0.3861111104488373, loss=2.256000518798828
train: epoch 95, loss 0.7004641890525818, acc=0.6739444732666016, loss=0.7004641890525818
test: epoch 95, loss 2.5808515548706055, acc=0.3722222149372101, loss=2.5808515548706055
train: epoch 96, loss 0.7138705849647522, acc=0.6711666584014893, loss=0.7138705849647522
test: epoch 96, loss 2.3682353496551514, acc=0.3777777850627899, loss=2.3682353496551514
train: epoch 97, loss 0.7224948406219482, acc=0.6730555295944214, loss=0.7224948406219482
test: epoch 97, loss 2.3421671390533447, acc=0.38333332538604736, loss=2.3421671390533447
train: epoch 98, loss 0.7122181057929993, acc=0.6717222332954407, loss=0.7122181057929993
test: epoch 98, loss 2.6088783740997314, acc=0.38055557012557983, loss=2.6088783740997314
train: epoch 99, loss 0.6972830295562744, acc=0.679888904094696, loss=0.6972830295562744
test: epoch 99, loss 2.366506338119507, acc=0.3861111104488373, loss=2.366506338119507
train: epoch 100, loss 0.6827533841133118, acc=0.680055558681488, loss=0.6827533841133118
test: epoch 100, loss 2.299050807952881, acc=0.3861111104488373, loss=2.299050807952881
train: epoch 101, loss 0.6909040212631226, acc=0.6819999814033508, loss=0.6909040212631226
test: epoch 101, loss 2.403069496154785, acc=0.3583333194255829, loss=2.403069496154785
train: epoch 102, loss 0.6807197332382202, acc=0.687333345413208, loss=0.6807197332382202
test: epoch 102, loss 2.462548017501831, acc=0.3722222149372101, loss=2.462548017501831
train: epoch 103, loss 0.679909348487854, acc=0.6889444589614868, loss=0.679909348487854
test: epoch 103, loss 2.565525770187378, acc=0.3777777850627899, loss=2.565525770187378
train: epoch 104, loss 0.6944836974143982, acc=0.6853333115577698, loss=0.6944836974143982
test: epoch 104, loss 2.745436191558838, acc=0.3888888955116272, loss=2.745436191558838
train: epoch 105, loss 0.6839067339897156, acc=0.6838333606719971, loss=0.6839067339897156
test: epoch 105, loss 2.462484836578369, acc=0.3861111104488373, loss=2.462484836578369
train: epoch 106, loss 0.6784002780914307, acc=0.6856666803359985, loss=0.6784002780914307
test: epoch 106, loss 2.3537943363189697, acc=0.38333332538604736, loss=2.3537943363189697
train: epoch 107, loss 0.6896193027496338, acc=0.6836110949516296, loss=0.6896193027496338
test: epoch 107, loss 2.685957670211792, acc=0.38333332538604736, loss=2.685957670211792
train: epoch 108, loss 0.6794558763504028, acc=0.6867222189903259, loss=0.6794558763504028
test: epoch 108, loss 2.2374024391174316, acc=0.38055557012557983, loss=2.2374024391174316
train: epoch 109, loss 0.6846312284469604, acc=0.6862778067588806, loss=0.6846312284469604
test: epoch 109, loss 2.5403330326080322, acc=0.3888888955116272, loss=2.5403330326080322
train: epoch 110, loss 0.6867910027503967, acc=0.6834444403648376, loss=0.6867910027503967
test: epoch 110, loss 2.46797513961792, acc=0.3861111104488373, loss=2.46797513961792
train: epoch 111, loss 0.6818673014640808, acc=0.6838889122009277, loss=0.6818673014640808
test: epoch 111, loss 2.209606885910034, acc=0.3722222149372101, loss=2.209606885910034
train: epoch 112, loss 0.6838933825492859, acc=0.6899444460868835, loss=0.6838933825492859
test: epoch 112, loss 2.234994649887085, acc=0.39444443583488464, loss=2.234994649887085
train: epoch 113, loss 0.6750481128692627, acc=0.6893888711929321, loss=0.6750481128692627
test: epoch 113, loss 2.2331864833831787, acc=0.38055557012557983, loss=2.2331864833831787
train: epoch 114, loss 0.687390148639679, acc=0.6901111006736755, loss=0.687390148639679
test: epoch 114, loss 2.3003556728363037, acc=0.38333332538604736, loss=2.3003556728363037
train: epoch 115, loss 0.6681324243545532, acc=0.6995000243186951, loss=0.6681324243545532
test: epoch 115, loss 2.5517618656158447, acc=0.375, loss=2.5517618656158447
train: epoch 116, loss 0.6560978889465332, acc=0.7020000219345093, loss=0.6560978889465332
test: epoch 116, loss 2.5708987712860107, acc=0.38055557012557983, loss=2.5708987712860107
train: epoch 117, loss 0.6566618084907532, acc=0.7039999961853027, loss=0.6566618084907532
test: epoch 117, loss 2.4784812927246094, acc=0.375, loss=2.4784812927246094
train: epoch 118, loss 0.6667211055755615, acc=0.7031111121177673, loss=0.6667211055755615
test: epoch 118, loss 2.2138235569000244, acc=0.3888888955116272, loss=2.2138235569000244
train: epoch 119, loss 0.6544117331504822, acc=0.7039999961853027, loss=0.6544117331504822
test: epoch 119, loss 2.5262773036956787, acc=0.38333332538604736, loss=2.5262773036956787
train: epoch 120, loss 0.651009738445282, acc=0.7053889036178589, loss=0.651009738445282
test: epoch 120, loss 2.171781301498413, acc=0.38055557012557983, loss=2.171781301498413
train: epoch 121, loss 0.655890703201294, acc=0.7054444551467896, loss=0.655890703201294
test: epoch 121, loss 2.621270179748535, acc=0.3722222149372101, loss=2.621270179748535
train: epoch 122, loss 0.6489667296409607, acc=0.7045000195503235, loss=0.6489667296409607
test: epoch 122, loss 2.521150827407837, acc=0.3777777850627899, loss=2.521150827407837
train: epoch 123, loss 0.6414604187011719, acc=0.7074999809265137, loss=0.6414604187011719
test: epoch 123, loss 2.6416702270507812, acc=0.3888888955116272, loss=2.6416702270507812
train: epoch 124, loss 0.6455163359642029, acc=0.7077222466468811, loss=0.6455163359642029
test: epoch 124, loss 2.281888723373413, acc=0.3777777850627899, loss=2.281888723373413
train: epoch 125, loss 0.6510278582572937, acc=0.7064444422721863, loss=0.6510278582572937
test: epoch 125, loss 2.3666720390319824, acc=0.38333332538604736, loss=2.3666720390319824
train: epoch 126, loss 0.6343016028404236, acc=0.7137222290039062, loss=0.6343016028404236
test: epoch 126, loss 2.564455509185791, acc=0.36944442987442017, loss=2.564455509185791
train: epoch 127, loss 0.6336225271224976, acc=0.7114999890327454, loss=0.6336225271224976
test: epoch 127, loss 2.368619918823242, acc=0.3583333194255829, loss=2.368619918823242
train: epoch 128, loss 0.6352949142456055, acc=0.715833306312561, loss=0.6352949142456055
test: epoch 128, loss 2.4842522144317627, acc=0.375, loss=2.4842522144317627
train: epoch 129, loss 0.6389243602752686, acc=0.7139444351196289, loss=0.6389243602752686
test: epoch 129, loss 2.4781596660614014, acc=0.39722222089767456, loss=2.4781596660614014
train: epoch 130, loss 0.6280788779258728, acc=0.7178333401679993, loss=0.6280788779258728
test: epoch 130, loss 2.399993658065796, acc=0.3777777850627899, loss=2.399993658065796
train: epoch 131, loss 0.6316483020782471, acc=0.7166110873222351, loss=0.6316483020782471
test: epoch 131, loss 2.406949520111084, acc=0.3916666805744171, loss=2.406949520111084
train: epoch 132, loss 0.6278494596481323, acc=0.7203333377838135, loss=0.6278494596481323
test: epoch 132, loss 2.3489468097686768, acc=0.3777777850627899, loss=2.3489468097686768
train: epoch 133, loss 0.6318751573562622, acc=0.7169444561004639, loss=0.6318751573562622
test: epoch 133, loss 2.6197268962860107, acc=0.3888888955116272, loss=2.6197268962860107
train: epoch 134, loss 0.6341671943664551, acc=0.7187222242355347, loss=0.6341671943664551
test: epoch 134, loss 2.2807698249816895, acc=0.3861111104488373, loss=2.2807698249816895
train: epoch 135, loss 0.6329185366630554, acc=0.7164444327354431, loss=0.6329185366630554
test: epoch 135, loss 2.5239505767822266, acc=0.3777777850627899, loss=2.5239505767822266
train: epoch 136, loss 0.6341550350189209, acc=0.7140555381774902, loss=0.6341550350189209
test: epoch 136, loss 2.6242592334747314, acc=0.3861111104488373, loss=2.6242592334747314
train: epoch 137, loss 0.6208384037017822, acc=0.7186111211776733, loss=0.6208384037017822
test: epoch 137, loss 2.4504947662353516, acc=0.3861111104488373, loss=2.4504947662353516
train: epoch 138, loss 0.6277500987052917, acc=0.7181666493415833, loss=0.6277500987052917
test: epoch 138, loss 2.4670064449310303, acc=0.3916666805744171, loss=2.4670064449310303
train: epoch 139, loss 0.6151819825172424, acc=0.7206666469573975, loss=0.6151819825172424
test: epoch 139, loss 2.6579697132110596, acc=0.3888888955116272, loss=2.6579697132110596
train: epoch 140, loss 0.642667293548584, acc=0.7148333191871643, loss=0.642667293548584
test: epoch 140, loss 2.62174129486084, acc=0.3916666805744171, loss=2.62174129486084
train: epoch 141, loss 0.6159805059432983, acc=0.7225555777549744, loss=0.6159805059432983
test: epoch 141, loss 2.4499669075012207, acc=0.39444443583488464, loss=2.4499669075012207
train: epoch 142, loss 0.6259458661079407, acc=0.7213333249092102, loss=0.6259458661079407
test: epoch 142, loss 2.4179444313049316, acc=0.3888888955116272, loss=2.4179444313049316
train: epoch 143, loss 0.616112470626831, acc=0.7261666655540466, loss=0.616112470626831
test: epoch 143, loss 2.332301616668701, acc=0.39444443583488464, loss=2.332301616668701
train: epoch 144, loss 0.6138553023338318, acc=0.7302777767181396, loss=0.6138553023338318
test: epoch 144, loss 2.4060468673706055, acc=0.4000000059604645, loss=2.4060468673706055
train: epoch 145, loss 0.6162337064743042, acc=0.7297777533531189, loss=0.6162337064743042
test: epoch 145, loss 2.5305662155151367, acc=0.39722222089767456, loss=2.5305662155151367
train: epoch 146, loss 0.6049556136131287, acc=0.7367222309112549, loss=0.6049556136131287
test: epoch 146, loss 2.561652660369873, acc=0.38333332538604736, loss=2.561652660369873
train: epoch 147, loss 0.5973659753799438, acc=0.737666666507721, loss=0.5973659753799438
test: epoch 147, loss 3.0704574584960938, acc=0.3916666805744171, loss=3.0704574584960938
train: epoch 148, loss 0.6076412796974182, acc=0.7357222437858582, loss=0.6076412796974182
test: epoch 148, loss 2.4368062019348145, acc=0.4055555462837219, loss=2.4368062019348145
train: epoch 149, loss 0.6025423407554626, acc=0.737500011920929, loss=0.6025423407554626
test: epoch 149, loss 2.5169031620025635, acc=0.4000000059604645, loss=2.5169031620025635
train: epoch 150, loss 0.6059534549713135, acc=0.7355555295944214, loss=0.6059534549713135
test: epoch 150, loss 2.526862382888794, acc=0.39444443583488464, loss=2.526862382888794
