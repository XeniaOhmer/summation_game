# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.995", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=2129276997, receiver_embed_dim=64, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=2129276997, receiver_embed_dim=64, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.7770657539367676, acc=0.10700000077486038, loss=2.7770657539367676
test: epoch 1, loss 3.182908535003662, acc=0.07777778059244156, loss=3.182908535003662
train: epoch 2, loss 2.071897268295288, acc=0.2118888944387436, loss=2.071897268295288
test: epoch 2, loss 2.4895851612091064, acc=0.15000000596046448, loss=2.4895851612091064
train: epoch 3, loss 1.7996968030929565, acc=0.2824999988079071, loss=1.7996968030929565
test: epoch 3, loss 2.231689691543579, acc=0.24166665971279144, loss=2.231689691543579
train: epoch 4, loss 1.620089054107666, acc=0.3408888876438141, loss=1.620089054107666
test: epoch 4, loss 2.0506694316864014, acc=0.24166665971279144, loss=2.0506694316864014
train: epoch 5, loss 1.5182803869247437, acc=0.3742222189903259, loss=1.5182803869247437
test: epoch 5, loss 1.8411465883255005, acc=0.3083333373069763, loss=1.8411465883255005
train: epoch 6, loss 1.4364606142044067, acc=0.4033888876438141, loss=1.4364606142044067
test: epoch 6, loss 1.850846529006958, acc=0.25, loss=1.850846529006958
train: epoch 7, loss 1.3408257961273193, acc=0.44477778673171997, loss=1.3408257961273193
test: epoch 7, loss 1.6627894639968872, acc=0.2916666567325592, loss=1.6627894639968872
train: epoch 8, loss 1.2937402725219727, acc=0.46433332562446594, loss=1.2937402725219727
test: epoch 8, loss 1.568669319152832, acc=0.2916666567325592, loss=1.568669319152832
train: epoch 9, loss 1.2237911224365234, acc=0.49149999022483826, loss=1.2237911224365234
test: epoch 9, loss 1.7067030668258667, acc=0.3722222149372101, loss=1.7067030668258667
train: epoch 10, loss 1.165909767150879, acc=0.5150555372238159, loss=1.165909767150879
test: epoch 10, loss 1.6550202369689941, acc=0.3194444477558136, loss=1.6550202369689941
train: epoch 11, loss 1.1089937686920166, acc=0.5365555286407471, loss=1.1089937686920166
test: epoch 11, loss 1.5736929178237915, acc=0.3777777850627899, loss=1.5736929178237915
train: epoch 12, loss 1.08138108253479, acc=0.5434444546699524, loss=1.08138108253479
test: epoch 12, loss 1.6295292377471924, acc=0.3166666626930237, loss=1.6295292377471924
train: epoch 13, loss 1.03670334815979, acc=0.5610555410385132, loss=1.03670334815979
test: epoch 13, loss 1.5540509223937988, acc=0.35555556416511536, loss=1.5540509223937988
train: epoch 14, loss 1.0237505435943604, acc=0.5692222118377686, loss=1.0237505435943604
test: epoch 14, loss 1.4795799255371094, acc=0.4027777910232544, loss=1.4795799255371094
train: epoch 15, loss 0.9906507730484009, acc=0.5866666436195374, loss=0.9906507730484009
test: epoch 15, loss 1.6317250728607178, acc=0.3499999940395355, loss=1.6317250728607178
train: epoch 16, loss 0.9737798571586609, acc=0.5918889045715332, loss=0.9737798571586609
test: epoch 16, loss 1.6353377103805542, acc=0.36666667461395264, loss=1.6353377103805542
train: epoch 17, loss 0.9399486780166626, acc=0.612333357334137, loss=0.9399486780166626
test: epoch 17, loss 1.7759546041488647, acc=0.3222222328186035, loss=1.7759546041488647
train: epoch 18, loss 0.9212474226951599, acc=0.6209444403648376, loss=0.9212474226951599
test: epoch 18, loss 1.6188284158706665, acc=0.4194444417953491, loss=1.6188284158706665
train: epoch 19, loss 0.9219520092010498, acc=0.6187222003936768, loss=0.9219520092010498
test: epoch 19, loss 1.8454076051712036, acc=0.2916666567325592, loss=1.8454076051712036
train: epoch 20, loss 0.9028208255767822, acc=0.6262221932411194, loss=0.9028208255767822
test: epoch 20, loss 1.4551126956939697, acc=0.41111111640930176, loss=1.4551126956939697
train: epoch 21, loss 0.8848598003387451, acc=0.6367777585983276, loss=0.8848598003387451
test: epoch 21, loss 1.4628684520721436, acc=0.3583333194255829, loss=1.4628684520721436
train: epoch 22, loss 0.873160719871521, acc=0.6380555629730225, loss=0.873160719871521
test: epoch 22, loss 1.5555427074432373, acc=0.3611111044883728, loss=1.5555427074432373
train: epoch 23, loss 0.8750776648521423, acc=0.6366111040115356, loss=0.8750776648521423
test: epoch 23, loss 1.5585832595825195, acc=0.35555556416511536, loss=1.5585832595825195
train: epoch 24, loss 0.8876421451568604, acc=0.6319444179534912, loss=0.8876421451568604
test: epoch 24, loss 1.5524691343307495, acc=0.4166666567325592, loss=1.5524691343307495
train: epoch 25, loss 0.8418101072311401, acc=0.6580555438995361, loss=0.8418101072311401
test: epoch 25, loss 1.5171390771865845, acc=0.4305555522441864, loss=1.5171390771865845
train: epoch 26, loss 0.8343493938446045, acc=0.6570555567741394, loss=0.8343493938446045
test: epoch 26, loss 1.4347158670425415, acc=0.4416666626930237, loss=1.4347158670425415
train: epoch 27, loss 0.8060764670372009, acc=0.6649444699287415, loss=0.8060764670372009
test: epoch 27, loss 1.6131887435913086, acc=0.35555556416511536, loss=1.6131887435913086
train: epoch 28, loss 0.8194079995155334, acc=0.6633333563804626, loss=0.8194079995155334
test: epoch 28, loss 1.3447259664535522, acc=0.4833333194255829, loss=1.3447259664535522
train: epoch 29, loss 0.7780832052230835, acc=0.6818333268165588, loss=0.7780832052230835
test: epoch 29, loss 1.4247833490371704, acc=0.4472222328186035, loss=1.4247833490371704
train: epoch 30, loss 0.8142675757408142, acc=0.6575000286102295, loss=0.8142675757408142
test: epoch 30, loss 1.5233619213104248, acc=0.40833333134651184, loss=1.5233619213104248
train: epoch 31, loss 0.7746375203132629, acc=0.6818333268165588, loss=0.7746375203132629
test: epoch 31, loss 1.4410784244537354, acc=0.4166666567325592, loss=1.4410784244537354
train: epoch 32, loss 0.8068019151687622, acc=0.6668333411216736, loss=0.8068019151687622
test: epoch 32, loss 1.4380625486373901, acc=0.4888888895511627, loss=1.4380625486373901
train: epoch 33, loss 0.776908278465271, acc=0.6787222027778625, loss=0.776908278465271
test: epoch 33, loss 1.3887842893600464, acc=0.4055555462837219, loss=1.3887842893600464
train: epoch 34, loss 0.7860617637634277, acc=0.6817777752876282, loss=0.7860617637634277
test: epoch 34, loss 1.5871188640594482, acc=0.43888887763023376, loss=1.5871188640594482
train: epoch 35, loss 0.7659381031990051, acc=0.6842777729034424, loss=0.7659381031990051
test: epoch 35, loss 1.4690786600112915, acc=0.42222222685813904, loss=1.4690786600112915
train: epoch 36, loss 0.7655391097068787, acc=0.6839444637298584, loss=0.7655391097068787
test: epoch 36, loss 1.5196937322616577, acc=0.3861111104488373, loss=1.5196937322616577
train: epoch 37, loss 0.7635136246681213, acc=0.6864444613456726, loss=0.7635136246681213
test: epoch 37, loss 1.3779833316802979, acc=0.4444444477558136, loss=1.3779833316802979
train: epoch 38, loss 0.7483106851577759, acc=0.6968333125114441, loss=0.7483106851577759
test: epoch 38, loss 1.5521689653396606, acc=0.42222222685813904, loss=1.5521689653396606
train: epoch 39, loss 0.7419018745422363, acc=0.6961666941642761, loss=0.7419018745422363
test: epoch 39, loss 1.5257805585861206, acc=0.3583333194255829, loss=1.5257805585861206
train: epoch 40, loss 0.7221745252609253, acc=0.7033888697624207, loss=0.7221745252609253
test: epoch 40, loss 1.242920160293579, acc=0.5, loss=1.242920160293579
train: epoch 41, loss 0.723034679889679, acc=0.7027222514152527, loss=0.723034679889679
test: epoch 41, loss 1.2560404539108276, acc=0.44999998807907104, loss=1.2560404539108276
train: epoch 42, loss 0.7342240810394287, acc=0.7007777690887451, loss=0.7342240810394287
test: epoch 42, loss 1.3873093128204346, acc=0.4749999940395355, loss=1.3873093128204346
train: epoch 43, loss 0.7310858368873596, acc=0.6987777948379517, loss=0.7310858368873596
test: epoch 43, loss 1.3351306915283203, acc=0.4833333194255829, loss=1.3351306915283203
train: epoch 44, loss 0.7343268394470215, acc=0.6982222199440002, loss=0.7343268394470215
test: epoch 44, loss 1.2457255125045776, acc=0.49444442987442017, loss=1.2457255125045776
train: epoch 45, loss 0.7139878869056702, acc=0.7097777724266052, loss=0.7139878869056702
test: epoch 45, loss 1.4408504962921143, acc=0.4194444417953491, loss=1.4408504962921143
train: epoch 46, loss 0.7030287981033325, acc=0.7126666903495789, loss=0.7030287981033325
test: epoch 46, loss 1.3485323190689087, acc=0.40833333134651184, loss=1.3485323190689087
train: epoch 47, loss 0.725256085395813, acc=0.706166684627533, loss=0.725256085395813
test: epoch 47, loss 1.1702722311019897, acc=0.519444465637207, loss=1.1702722311019897
train: epoch 48, loss 0.7033217549324036, acc=0.7163888812065125, loss=0.7033217549324036
test: epoch 48, loss 1.1503382921218872, acc=0.5083333253860474, loss=1.1503382921218872
train: epoch 49, loss 0.7163465023040771, acc=0.707444429397583, loss=0.7163465023040771
test: epoch 49, loss 1.1674270629882812, acc=0.5, loss=1.1674270629882812
train: epoch 50, loss 0.7325901389122009, acc=0.7048333287239075, loss=0.7325901389122009
test: epoch 50, loss 1.2863438129425049, acc=0.519444465637207, loss=1.2863438129425049
train: epoch 51, loss 0.7104682922363281, acc=0.7162222266197205, loss=0.7104682922363281
test: epoch 51, loss 1.0997440814971924, acc=0.5527777671813965, loss=1.0997440814971924
train: epoch 52, loss 0.680422306060791, acc=0.7203888893127441, loss=0.680422306060791
test: epoch 52, loss 1.3660242557525635, acc=0.5111111402511597, loss=1.3660242557525635
train: epoch 53, loss 0.6796921491622925, acc=0.7242222428321838, loss=0.6796921491622925
test: epoch 53, loss 0.9910586476325989, acc=0.574999988079071, loss=0.9910586476325989
train: epoch 54, loss 0.6615219712257385, acc=0.7294999957084656, loss=0.6615219712257385
test: epoch 54, loss 1.2325007915496826, acc=0.5444444417953491, loss=1.2325007915496826
train: epoch 55, loss 0.6523885130882263, acc=0.7325555682182312, loss=0.6523885130882263
test: epoch 55, loss 1.1586099863052368, acc=0.4138889014720917, loss=1.1586099863052368
train: epoch 56, loss 0.6884713172912598, acc=0.7177777886390686, loss=0.6884713172912598
test: epoch 56, loss 1.2372058629989624, acc=0.4305555522441864, loss=1.2372058629989624
train: epoch 57, loss 0.6598356366157532, acc=0.7284444570541382, loss=0.6598356366157532
test: epoch 57, loss 1.176932454109192, acc=0.5444444417953491, loss=1.176932454109192
train: epoch 58, loss 0.6642408967018127, acc=0.731166660785675, loss=0.6642408967018127
test: epoch 58, loss 1.1520875692367554, acc=0.5083333253860474, loss=1.1520875692367554
train: epoch 59, loss 0.6537078022956848, acc=0.7329999804496765, loss=0.6537078022956848
test: epoch 59, loss 1.128126859664917, acc=0.5611110925674438, loss=1.128126859664917
train: epoch 60, loss 0.6627698540687561, acc=0.7321666479110718, loss=0.6627698540687561
test: epoch 60, loss 1.2445858716964722, acc=0.4972222149372101, loss=1.2445858716964722
train: epoch 61, loss 0.6419283747673035, acc=0.7388333082199097, loss=0.6419283747673035
test: epoch 61, loss 1.14744234085083, acc=0.5083333253860474, loss=1.14744234085083
train: epoch 62, loss 0.6534019112586975, acc=0.7321666479110718, loss=0.6534019112586975
test: epoch 62, loss 1.2467994689941406, acc=0.5, loss=1.2467994689941406
train: epoch 63, loss 0.6633044481277466, acc=0.7338888645172119, loss=0.6633044481277466
test: epoch 63, loss 1.1190454959869385, acc=0.4611110985279083, loss=1.1190454959869385
train: epoch 64, loss 0.6389643549919128, acc=0.7412222027778625, loss=0.6389643549919128
test: epoch 64, loss 1.170772910118103, acc=0.44999998807907104, loss=1.170772910118103
train: epoch 65, loss 0.6156553030014038, acc=0.746222198009491, loss=0.6156553030014038
test: epoch 65, loss 1.1592916250228882, acc=0.5249999761581421, loss=1.1592916250228882
train: epoch 66, loss 0.6310680508613586, acc=0.741777777671814, loss=0.6310680508613586
test: epoch 66, loss 1.0693827867507935, acc=0.5666666626930237, loss=1.0693827867507935
train: epoch 67, loss 0.6176258325576782, acc=0.7495555281639099, loss=0.6176258325576782
test: epoch 67, loss 1.1353647708892822, acc=0.49166667461395264, loss=1.1353647708892822
train: epoch 68, loss 0.6034151911735535, acc=0.7534444332122803, loss=0.6034151911735535
test: epoch 68, loss 1.7392443418502808, acc=0.41111111640930176, loss=1.7392443418502808
train: epoch 69, loss 0.6444916129112244, acc=0.7394444346427917, loss=0.6444916129112244
test: epoch 69, loss 1.1271966695785522, acc=0.550000011920929, loss=1.1271966695785522
train: epoch 70, loss 0.6005060076713562, acc=0.7530555725097656, loss=0.6005060076713562
test: epoch 70, loss 1.214228630065918, acc=0.5138888955116272, loss=1.214228630065918
train: epoch 71, loss 0.6153300404548645, acc=0.7515000104904175, loss=0.6153300404548645
test: epoch 71, loss 1.1592298746109009, acc=0.5305555462837219, loss=1.1592298746109009
train: epoch 72, loss 0.62420254945755, acc=0.7450000047683716, loss=0.62420254945755
test: epoch 72, loss 1.1079314947128296, acc=0.550000011920929, loss=1.1079314947128296
train: epoch 73, loss 0.6225469708442688, acc=0.7426666617393494, loss=0.6225469708442688
test: epoch 73, loss 1.0705376863479614, acc=0.5694444179534912, loss=1.0705376863479614
train: epoch 74, loss 0.6073265075683594, acc=0.749666690826416, loss=0.6073265075683594
test: epoch 74, loss 1.0912985801696777, acc=0.5583333373069763, loss=1.0912985801696777
train: epoch 75, loss 0.5927348732948303, acc=0.757888913154602, loss=0.5927348732948303
test: epoch 75, loss 1.2542747259140015, acc=0.5083333253860474, loss=1.2542747259140015
train: epoch 76, loss 0.6124047636985779, acc=0.7521111369132996, loss=0.6124047636985779
test: epoch 76, loss 1.142088532447815, acc=0.5083333253860474, loss=1.142088532447815
train: epoch 77, loss 0.574247419834137, acc=0.7700555324554443, loss=0.574247419834137
test: epoch 77, loss 1.1009540557861328, acc=0.5333333611488342, loss=1.1009540557861328
train: epoch 78, loss 0.6013860702514648, acc=0.7584999799728394, loss=0.6013860702514648
test: epoch 78, loss 1.0522810220718384, acc=0.5249999761581421, loss=1.0522810220718384
train: epoch 79, loss 0.5960967540740967, acc=0.7559999823570251, loss=0.5960967540740967
test: epoch 79, loss 1.2093547582626343, acc=0.5249999761581421, loss=1.2093547582626343
train: epoch 80, loss 0.5838931798934937, acc=0.7622777819633484, loss=0.5838931798934937
test: epoch 80, loss 1.1493303775787354, acc=0.48055556416511536, loss=1.1493303775787354
train: epoch 81, loss 0.5688511729240417, acc=0.7708888649940491, loss=0.5688511729240417
test: epoch 81, loss 1.2500944137573242, acc=0.5027777552604675, loss=1.2500944137573242
train: epoch 82, loss 0.5991908311843872, acc=0.7614444494247437, loss=0.5991908311843872
test: epoch 82, loss 1.180844783782959, acc=0.47777777910232544, loss=1.180844783782959
train: epoch 83, loss 0.667963445186615, acc=0.7352777719497681, loss=0.667963445186615
test: epoch 83, loss 1.146140217781067, acc=0.5111111402511597, loss=1.146140217781067
train: epoch 84, loss 0.6662617921829224, acc=0.7347777485847473, loss=0.6662617921829224
test: epoch 84, loss 1.2554590702056885, acc=0.4472222328186035, loss=1.2554590702056885
train: epoch 85, loss 0.7032049894332886, acc=0.7135000228881836, loss=0.7032049894332886
test: epoch 85, loss 1.2945659160614014, acc=0.4722222089767456, loss=1.2945659160614014
train: epoch 86, loss 0.6055341958999634, acc=0.7513889074325562, loss=0.6055341958999634
test: epoch 86, loss 1.2306171655654907, acc=0.519444465637207, loss=1.2306171655654907
train: epoch 87, loss 0.5864452719688416, acc=0.7606666684150696, loss=0.5864452719688416
test: epoch 87, loss 1.2620720863342285, acc=0.5, loss=1.2620720863342285
train: epoch 88, loss 0.5833086371421814, acc=0.7616666555404663, loss=0.5833086371421814
test: epoch 88, loss 1.3449547290802002, acc=0.44999998807907104, loss=1.3449547290802002
train: epoch 89, loss 0.5629240274429321, acc=0.7727222442626953, loss=0.5629240274429321
test: epoch 89, loss 1.235815167427063, acc=0.5888888835906982, loss=1.235815167427063
train: epoch 90, loss 0.5842189192771912, acc=0.7647777795791626, loss=0.5842189192771912
test: epoch 90, loss 1.2084778547286987, acc=0.5277777910232544, loss=1.2084778547286987
train: epoch 91, loss 0.6219884753227234, acc=0.746833324432373, loss=0.6219884753227234
test: epoch 91, loss 1.1211261749267578, acc=0.5277777910232544, loss=1.1211261749267578
train: epoch 92, loss 0.5576786398887634, acc=0.7753333449363708, loss=0.5576786398887634
test: epoch 92, loss 1.2356868982315063, acc=0.519444465637207, loss=1.2356868982315063
train: epoch 93, loss 0.5605593919754028, acc=0.7754444479942322, loss=0.5605593919754028
test: epoch 93, loss 1.386635661125183, acc=0.5083333253860474, loss=1.386635661125183
train: epoch 94, loss 0.5627864599227905, acc=0.7702222466468811, loss=0.5627864599227905
test: epoch 94, loss 1.2734088897705078, acc=0.4833333194255829, loss=1.2734088897705078
train: epoch 95, loss 0.5542751550674438, acc=0.7799999713897705, loss=0.5542751550674438
test: epoch 95, loss 1.2436143159866333, acc=0.5055555701255798, loss=1.2436143159866333
train: epoch 96, loss 0.5696509480476379, acc=0.7702777981758118, loss=0.5696509480476379
test: epoch 96, loss 1.0519599914550781, acc=0.49166667461395264, loss=1.0519599914550781
train: epoch 97, loss 0.5564594864845276, acc=0.7781111001968384, loss=0.5564594864845276
test: epoch 97, loss 1.1462703943252563, acc=0.4888888895511627, loss=1.1462703943252563
train: epoch 98, loss 0.6142229437828064, acc=0.7544999718666077, loss=0.6142229437828064
test: epoch 98, loss 1.233385443687439, acc=0.5222222208976746, loss=1.233385443687439
train: epoch 99, loss 0.5857424139976501, acc=0.7688888907432556, loss=0.5857424139976501
test: epoch 99, loss 1.32213294506073, acc=0.5138888955116272, loss=1.32213294506073
train: epoch 100, loss 0.5647519826889038, acc=0.777222216129303, loss=0.5647519826889038
test: epoch 100, loss 1.0350937843322754, acc=0.5361111164093018, loss=1.0350937843322754
train: epoch 101, loss 0.5503060817718506, acc=0.781166672706604, loss=0.5503060817718506
test: epoch 101, loss 1.2885442972183228, acc=0.4972222149372101, loss=1.2885442972183228
train: epoch 102, loss 0.5347731113433838, acc=0.7855555415153503, loss=0.5347731113433838
test: epoch 102, loss 1.2323479652404785, acc=0.49444442987442017, loss=1.2323479652404785
train: epoch 103, loss 0.5563925504684448, acc=0.7778333425521851, loss=0.5563925504684448
test: epoch 103, loss 1.5881792306900024, acc=0.4194444417953491, loss=1.5881792306900024
train: epoch 104, loss 0.5776953101158142, acc=0.7708888649940491, loss=0.5776953101158142
test: epoch 104, loss 1.1996949911117554, acc=0.5138888955116272, loss=1.1996949911117554
train: epoch 105, loss 0.5895096063613892, acc=0.762666642665863, loss=0.5895096063613892
test: epoch 105, loss 1.141686201095581, acc=0.5416666865348816, loss=1.141686201095581
train: epoch 106, loss 0.5064496994018555, acc=0.7982222437858582, loss=0.5064496994018555
test: epoch 106, loss 1.2232393026351929, acc=0.5222222208976746, loss=1.2232393026351929
train: epoch 107, loss 0.526172935962677, acc=0.788444459438324, loss=0.526172935962677
test: epoch 107, loss 1.4011057615280151, acc=0.43888887763023376, loss=1.4011057615280151
train: epoch 108, loss 0.5491858720779419, acc=0.7806666493415833, loss=0.5491858720779419
test: epoch 108, loss 1.0934468507766724, acc=0.5388888716697693, loss=1.0934468507766724
train: epoch 109, loss 0.5931195616722107, acc=0.7594444155693054, loss=0.5931195616722107
test: epoch 109, loss 1.1460418701171875, acc=0.5361111164093018, loss=1.1460418701171875
train: epoch 110, loss 0.5578237175941467, acc=0.773277759552002, loss=0.5578237175941467
test: epoch 110, loss 1.192852258682251, acc=0.5, loss=1.192852258682251
train: epoch 111, loss 0.5626837611198425, acc=0.769444465637207, loss=0.5626837611198425
test: epoch 111, loss 1.1669175624847412, acc=0.5305555462837219, loss=1.1669175624847412
train: epoch 112, loss 0.5705252885818481, acc=0.7688888907432556, loss=0.5705252885818481
test: epoch 112, loss 1.1568808555603027, acc=0.5888888835906982, loss=1.1568808555603027
train: epoch 113, loss 0.5331066846847534, acc=0.781166672706604, loss=0.5331066846847534
test: epoch 113, loss 1.1482371091842651, acc=0.5055555701255798, loss=1.1482371091842651
train: epoch 114, loss 0.5498856902122498, acc=0.7735555768013, loss=0.5498856902122498
test: epoch 114, loss 1.3216179609298706, acc=0.5333333611488342, loss=1.3216179609298706
train: epoch 115, loss 0.5699753761291504, acc=0.7666666507720947, loss=0.5699753761291504
test: epoch 115, loss 1.2489278316497803, acc=0.4611110985279083, loss=1.2489278316497803
train: epoch 116, loss 0.5025580525398254, acc=0.7946110963821411, loss=0.5025580525398254
test: epoch 116, loss 1.1960816383361816, acc=0.5249999761581421, loss=1.1960816383361816
train: epoch 117, loss 0.5284344553947449, acc=0.7849444150924683, loss=0.5284344553947449
test: epoch 117, loss 1.1716079711914062, acc=0.5388888716697693, loss=1.1716079711914062
train: epoch 118, loss 0.5614998936653137, acc=0.7736666798591614, loss=0.5614998936653137
test: epoch 118, loss 1.1279598474502563, acc=0.5416666865348816, loss=1.1279598474502563
train: epoch 119, loss 0.5372257232666016, acc=0.7754444479942322, loss=0.5372257232666016
test: epoch 119, loss 1.293162226676941, acc=0.4194444417953491, loss=1.293162226676941
train: epoch 120, loss 0.5705273747444153, acc=0.7641666531562805, loss=0.5705273747444153
test: epoch 120, loss 0.9747794270515442, acc=0.5555555820465088, loss=0.9747794270515442
train: epoch 121, loss 0.5418537855148315, acc=0.7786666750907898, loss=0.5418537855148315
test: epoch 121, loss 1.223312497138977, acc=0.5027777552604675, loss=1.223312497138977
train: epoch 122, loss 0.537747323513031, acc=0.777222216129303, loss=0.537747323513031
test: epoch 122, loss 1.022484302520752, acc=0.5888888835906982, loss=1.022484302520752
train: epoch 123, loss 0.5523131489753723, acc=0.7758888602256775, loss=0.5523131489753723
test: epoch 123, loss 1.297296166419983, acc=0.4833333194255829, loss=1.297296166419983
train: epoch 124, loss 0.5402766466140747, acc=0.7766666412353516, loss=0.5402766466140747
test: epoch 124, loss 1.122770071029663, acc=0.5472221970558167, loss=1.122770071029663
train: epoch 125, loss 0.5505410432815552, acc=0.7775555849075317, loss=0.5505410432815552
test: epoch 125, loss 1.2903369665145874, acc=0.5277777910232544, loss=1.2903369665145874
train: epoch 126, loss 0.5396600365638733, acc=0.7823888659477234, loss=0.5396600365638733
test: epoch 126, loss 1.0718144178390503, acc=0.574999988079071, loss=1.0718144178390503
train: epoch 127, loss 0.5221442580223083, acc=0.7931110858917236, loss=0.5221442580223083
test: epoch 127, loss 1.5511788129806519, acc=0.45277777314186096, loss=1.5511788129806519
train: epoch 128, loss 0.544543445110321, acc=0.7803888916969299, loss=0.544543445110321
test: epoch 128, loss 1.4513790607452393, acc=0.5055555701255798, loss=1.4513790607452393
train: epoch 129, loss 0.5758898854255676, acc=0.7663888931274414, loss=0.5758898854255676
test: epoch 129, loss 1.2008681297302246, acc=0.5416666865348816, loss=1.2008681297302246
train: epoch 130, loss 0.5228852033615112, acc=0.7867777943611145, loss=0.5228852033615112
test: epoch 130, loss 1.1073408126831055, acc=0.5416666865348816, loss=1.1073408126831055
train: epoch 131, loss 0.5248342156410217, acc=0.7832777500152588, loss=0.5248342156410217
test: epoch 131, loss 1.0608607530593872, acc=0.5222222208976746, loss=1.0608607530593872
train: epoch 132, loss 0.5470572710037231, acc=0.7763888835906982, loss=0.5470572710037231
test: epoch 132, loss 1.2131139039993286, acc=0.5416666865348816, loss=1.2131139039993286
train: epoch 133, loss 0.5302562117576599, acc=0.7854999899864197, loss=0.5302562117576599
test: epoch 133, loss 1.1097383499145508, acc=0.574999988079071, loss=1.1097383499145508
train: epoch 134, loss 0.5109225511550903, acc=0.78938889503479, loss=0.5109225511550903
test: epoch 134, loss 1.0368438959121704, acc=0.5888888835906982, loss=1.0368438959121704
train: epoch 135, loss 0.5234429240226746, acc=0.7860000133514404, loss=0.5234429240226746
test: epoch 135, loss 1.30757737159729, acc=0.5222222208976746, loss=1.30757737159729
train: epoch 136, loss 0.5308890342712402, acc=0.7816666960716248, loss=0.5308890342712402
test: epoch 136, loss 1.145272135734558, acc=0.5388888716697693, loss=1.145272135734558
train: epoch 137, loss 0.5540691018104553, acc=0.7746666669845581, loss=0.5540691018104553
test: epoch 137, loss 1.4961787462234497, acc=0.49444442987442017, loss=1.4961787462234497
train: epoch 138, loss 0.49368715286254883, acc=0.7995555400848389, loss=0.49368715286254883
test: epoch 138, loss 1.2674018144607544, acc=0.5361111164093018, loss=1.2674018144607544
train: epoch 139, loss 0.5238028764724731, acc=0.7867777943611145, loss=0.5238028764724731
test: epoch 139, loss 1.04856538772583, acc=0.5472221970558167, loss=1.04856538772583
train: epoch 140, loss 0.48973792791366577, acc=0.8016111254692078, loss=0.48973792791366577
test: epoch 140, loss 1.2007882595062256, acc=0.5361111164093018, loss=1.2007882595062256
train: epoch 141, loss 0.5022733211517334, acc=0.7964444160461426, loss=0.5022733211517334
test: epoch 141, loss 0.9798354506492615, acc=0.574999988079071, loss=0.9798354506492615
train: epoch 142, loss 0.5468751192092896, acc=0.7832221984863281, loss=0.5468751192092896
test: epoch 142, loss 1.1740670204162598, acc=0.5583333373069763, loss=1.1740670204162598
train: epoch 143, loss 0.4895523190498352, acc=0.8028333187103271, loss=0.4895523190498352
test: epoch 143, loss 1.0144866704940796, acc=0.605555534362793, loss=1.0144866704940796
train: epoch 144, loss 0.5081930756568909, acc=0.7952222228050232, loss=0.5081930756568909
test: epoch 144, loss 0.9863296747207642, acc=0.5861111283302307, loss=0.9863296747207642
train: epoch 145, loss 0.5090059638023376, acc=0.7919999957084656, loss=0.5090059638023376
test: epoch 145, loss 1.1256259679794312, acc=0.5694444179534912, loss=1.1256259679794312
train: epoch 146, loss 0.5307657122612, acc=0.7847777605056763, loss=0.5307657122612
test: epoch 146, loss 1.134940505027771, acc=0.5305555462837219, loss=1.134940505027771
train: epoch 147, loss 0.5335389375686646, acc=0.7848333120346069, loss=0.5335389375686646
test: epoch 147, loss 1.0426236391067505, acc=0.574999988079071, loss=1.0426236391067505
train: epoch 148, loss 0.49302345514297485, acc=0.8037777543067932, loss=0.49302345514297485
test: epoch 148, loss 1.1630398035049438, acc=0.5861111283302307, loss=1.1630398035049438
train: epoch 149, loss 0.5102177262306213, acc=0.792388916015625, loss=0.5102177262306213
test: epoch 149, loss 1.0477948188781738, acc=0.5722222328186035, loss=1.0477948188781738
train: epoch 150, loss 0.49197086691856384, acc=0.8030555844306946, loss=0.49197086691856384
test: epoch 150, loss 1.0313997268676758, acc=0.574999988079071, loss=1.0313997268676758
