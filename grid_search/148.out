# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=2.0", "--temp_decay=1", "--one_hot=1", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1324976868, receiver_embed_dim=128, save_run=0, temp_decay=1.0, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1324976868, receiver_embed_dim=128, save_run=False, temp_decay=1.0, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.8376166820526123, acc=0.12950000166893005, loss=2.8376166820526123
test: epoch 1, loss 4.663823127746582, acc=0.0972222238779068, loss=4.663823127746582
train: epoch 2, loss 1.4453668594360352, acc=0.4046666622161865, loss=1.4453668594360352
test: epoch 2, loss 4.183688163757324, acc=0.13055555522441864, loss=4.183688163757324
train: epoch 3, loss 1.1063883304595947, acc=0.5505555272102356, loss=1.1063883304595947
test: epoch 3, loss 4.3554182052612305, acc=0.14444445073604584, loss=4.3554182052612305
train: epoch 4, loss 0.9208017587661743, acc=0.632611095905304, loss=0.9208017587661743
test: epoch 4, loss 4.062655925750732, acc=0.1944444477558136, loss=4.062655925750732
train: epoch 5, loss 0.7853112816810608, acc=0.6946666836738586, loss=0.7853112816810608
test: epoch 5, loss 4.178066730499268, acc=0.18888889253139496, loss=4.178066730499268
train: epoch 6, loss 0.6812806129455566, acc=0.7398889064788818, loss=0.6812806129455566
test: epoch 6, loss 3.6093955039978027, acc=0.21944443881511688, loss=3.6093955039978027
train: epoch 7, loss 0.5916785597801208, acc=0.7792778015136719, loss=0.5916785597801208
test: epoch 7, loss 3.737792491912842, acc=0.1805555522441864, loss=3.737792491912842
train: epoch 8, loss 0.5563284158706665, acc=0.7923333048820496, loss=0.5563284158706665
test: epoch 8, loss 3.545198678970337, acc=0.2083333283662796, loss=3.545198678970337
train: epoch 9, loss 0.5014718770980835, acc=0.8098333477973938, loss=0.5014718770980835
test: epoch 9, loss 3.541640281677246, acc=0.20277777314186096, loss=3.541640281677246
train: epoch 10, loss 0.4573441445827484, acc=0.8349999785423279, loss=0.4573441445827484
test: epoch 10, loss 3.4331700801849365, acc=0.2777777910232544, loss=3.4331700801849365
train: epoch 11, loss 0.4155902862548828, acc=0.8481666445732117, loss=0.4155902862548828
test: epoch 11, loss 3.4156241416931152, acc=0.25, loss=3.4156241416931152
train: epoch 12, loss 0.3851504325866699, acc=0.8631666898727417, loss=0.3851504325866699
test: epoch 12, loss 3.2172913551330566, acc=0.3027777671813965, loss=3.2172913551330566
train: epoch 13, loss 0.35102224349975586, acc=0.8760555386543274, loss=0.35102224349975586
test: epoch 13, loss 3.034674882888794, acc=0.23888888955116272, loss=3.034674882888794
train: epoch 14, loss 0.329443097114563, acc=0.8845000267028809, loss=0.329443097114563
test: epoch 14, loss 3.1902785301208496, acc=0.2666666805744171, loss=3.1902785301208496
train: epoch 15, loss 0.30747202038764954, acc=0.8954444527626038, loss=0.30747202038764954
test: epoch 15, loss 3.4672040939331055, acc=0.2750000059604645, loss=3.4672040939331055
train: epoch 16, loss 0.28620031476020813, acc=0.9006111025810242, loss=0.28620031476020813
test: epoch 16, loss 3.150028944015503, acc=0.3027777671813965, loss=3.150028944015503
train: epoch 17, loss 0.26879429817199707, acc=0.9084444642066956, loss=0.26879429817199707
test: epoch 17, loss 3.28410267829895, acc=0.2638888955116272, loss=3.28410267829895
train: epoch 18, loss 0.2474759966135025, acc=0.9156666398048401, loss=0.2474759966135025
test: epoch 18, loss 3.2880704402923584, acc=0.2944444417953491, loss=3.2880704402923584
train: epoch 19, loss 0.23971885442733765, acc=0.9221110939979553, loss=0.23971885442733765
test: epoch 19, loss 3.1436145305633545, acc=0.3194444477558136, loss=3.1436145305633545
train: epoch 20, loss 0.21156588196754456, acc=0.9281111359596252, loss=0.21156588196754456
test: epoch 20, loss 2.7844271659851074, acc=0.2916666567325592, loss=2.7844271659851074
train: epoch 21, loss 0.2062406837940216, acc=0.9313333630561829, loss=0.2062406837940216
test: epoch 21, loss 2.7827391624450684, acc=0.29722222685813904, loss=2.7827391624450684
train: epoch 22, loss 0.1942533701658249, acc=0.9363889098167419, loss=0.1942533701658249
test: epoch 22, loss 3.1543118953704834, acc=0.28611111640930176, loss=3.1543118953704834
train: epoch 23, loss 0.1771974265575409, acc=0.9430555701255798, loss=0.1771974265575409
test: epoch 23, loss 2.9649205207824707, acc=0.30000001192092896, loss=2.9649205207824707
train: epoch 24, loss 0.17727264761924744, acc=0.945388913154602, loss=0.17727264761924744
test: epoch 24, loss 2.854543685913086, acc=0.32499998807907104, loss=2.854543685913086
train: epoch 25, loss 0.1745174527168274, acc=0.9447222352027893, loss=0.1745174527168274
test: epoch 25, loss 2.7376441955566406, acc=0.3499999940395355, loss=2.7376441955566406
train: epoch 26, loss 0.15794526040554047, acc=0.9501110911369324, loss=0.15794526040554047
test: epoch 26, loss 2.9804227352142334, acc=0.3638888895511627, loss=2.9804227352142334
train: epoch 27, loss 0.14266520738601685, acc=0.9540555477142334, loss=0.14266520738601685
test: epoch 27, loss 2.7950057983398438, acc=0.2916666567325592, loss=2.7950057983398438
train: epoch 28, loss 0.13958361744880676, acc=0.9555555582046509, loss=0.13958361744880676
test: epoch 28, loss 3.559004068374634, acc=0.30000001192092896, loss=3.559004068374634
train: epoch 29, loss 0.12288298457860947, acc=0.960277795791626, loss=0.12288298457860947
test: epoch 29, loss 3.0386078357696533, acc=0.3722222149372101, loss=3.0386078357696533
train: epoch 30, loss 0.1268215924501419, acc=0.961222231388092, loss=0.1268215924501419
test: epoch 30, loss 2.9487125873565674, acc=0.3916666805744171, loss=2.9487125873565674
train: epoch 31, loss 0.1232474073767662, acc=0.9635555744171143, loss=0.1232474073767662
test: epoch 31, loss 2.7522451877593994, acc=0.36666667461395264, loss=2.7522451877593994
train: epoch 32, loss 0.12827090919017792, acc=0.9606666564941406, loss=0.12827090919017792
test: epoch 32, loss 3.1771092414855957, acc=0.31388887763023376, loss=3.1771092414855957
train: epoch 33, loss 0.12270252406597137, acc=0.9618889093399048, loss=0.12270252406597137
test: epoch 33, loss 2.9810991287231445, acc=0.3916666805744171, loss=2.9810991287231445
train: epoch 34, loss 0.1097382977604866, acc=0.9653888940811157, loss=0.1097382977604866
test: epoch 34, loss 2.992331027984619, acc=0.3444444537162781, loss=2.992331027984619
train: epoch 35, loss 0.1047602966427803, acc=0.9668889045715332, loss=0.1047602966427803
test: epoch 35, loss 2.9341797828674316, acc=0.3888888955116272, loss=2.9341797828674316
train: epoch 36, loss 0.10464921593666077, acc=0.9677222371101379, loss=0.10464921593666077
test: epoch 36, loss 3.0245485305786133, acc=0.375, loss=3.0245485305786133
train: epoch 37, loss 0.1017022505402565, acc=0.9687777757644653, loss=0.1017022505402565
test: epoch 37, loss 3.203993082046509, acc=0.3722222149372101, loss=3.203993082046509
train: epoch 38, loss 0.0939297080039978, acc=0.9717222452163696, loss=0.0939297080039978
test: epoch 38, loss 2.8666067123413086, acc=0.3583333194255829, loss=2.8666067123413086
train: epoch 39, loss 0.0969671756029129, acc=0.9714444279670715, loss=0.0969671756029129
test: epoch 39, loss 2.8103227615356445, acc=0.4333333373069763, loss=2.8103227615356445
train: epoch 40, loss 0.0805148333311081, acc=0.9758889079093933, loss=0.0805148333311081
test: epoch 40, loss 2.8423893451690674, acc=0.41111111640930176, loss=2.8423893451690674
train: epoch 41, loss 0.09185567498207092, acc=0.973111093044281, loss=0.09185567498207092
test: epoch 41, loss 2.407684326171875, acc=0.4444444477558136, loss=2.407684326171875
train: epoch 42, loss 0.08052509278059006, acc=0.9746666550636292, loss=0.08052509278059006
test: epoch 42, loss 2.7234692573547363, acc=0.46388888359069824, loss=2.7234692573547363
train: epoch 43, loss 0.08310071378946304, acc=0.9746111035346985, loss=0.08310071378946304
test: epoch 43, loss 2.4588022232055664, acc=0.4722222089767456, loss=2.4588022232055664
train: epoch 44, loss 0.06483390182256699, acc=0.9787222146987915, loss=0.06483390182256699
test: epoch 44, loss 3.2850124835968018, acc=0.45277777314186096, loss=3.2850124835968018
train: epoch 45, loss 0.08542554080486298, acc=0.9751111268997192, loss=0.08542554080486298
test: epoch 45, loss 2.9958865642547607, acc=0.3861111104488373, loss=2.9958865642547607
train: epoch 46, loss 0.07879053056240082, acc=0.9780555367469788, loss=0.07879053056240082
test: epoch 46, loss 3.136699914932251, acc=0.43888887763023376, loss=3.136699914932251
train: epoch 47, loss 0.0674721747636795, acc=0.9793888926506042, loss=0.0674721747636795
test: epoch 47, loss 3.0786116123199463, acc=0.4472222328186035, loss=3.0786116123199463
train: epoch 48, loss 0.0691913291811943, acc=0.9800000190734863, loss=0.0691913291811943
test: epoch 48, loss 2.7942051887512207, acc=0.46666666865348816, loss=2.7942051887512207
train: epoch 49, loss 0.061985574662685394, acc=0.9797222018241882, loss=0.061985574662685394
test: epoch 49, loss 2.584505796432495, acc=0.44999998807907104, loss=2.584505796432495
train: epoch 50, loss 0.07133982330560684, acc=0.9796110987663269, loss=0.07133982330560684
test: epoch 50, loss 3.1064772605895996, acc=0.4333333373069763, loss=3.1064772605895996
train: epoch 51, loss 0.07321420311927795, acc=0.9792777895927429, loss=0.07321420311927795
test: epoch 51, loss 3.601194381713867, acc=0.46388888359069824, loss=3.601194381713867
train: epoch 52, loss 0.07387900352478027, acc=0.9807222485542297, loss=0.07387900352478027
test: epoch 52, loss 2.7413315773010254, acc=0.44999998807907104, loss=2.7413315773010254
train: epoch 53, loss 0.06224024295806885, acc=0.9825555682182312, loss=0.06224024295806885
test: epoch 53, loss 2.9026551246643066, acc=0.4861111044883728, loss=2.9026551246643066
train: epoch 54, loss 0.05732505023479462, acc=0.9836111068725586, loss=0.05732505023479462
test: epoch 54, loss 3.2605268955230713, acc=0.42222222685813904, loss=3.2605268955230713
train: epoch 55, loss 0.05241074785590172, acc=0.9854444265365601, loss=0.05241074785590172
test: epoch 55, loss 2.7838499546051025, acc=0.5111111402511597, loss=2.7838499546051025
train: epoch 56, loss 0.06469644606113434, acc=0.9830555319786072, loss=0.06469644606113434
test: epoch 56, loss 3.52070689201355, acc=0.4861111044883728, loss=3.52070689201355
train: epoch 57, loss 0.06627130508422852, acc=0.9826111197471619, loss=0.06627130508422852
test: epoch 57, loss 3.0674984455108643, acc=0.43611112236976624, loss=3.0674984455108643
train: epoch 58, loss 0.061700962483882904, acc=0.9842222332954407, loss=0.061700962483882904
test: epoch 58, loss 3.2135090827941895, acc=0.4611110985279083, loss=3.2135090827941895
train: epoch 59, loss 0.047101687639951706, acc=0.9866111278533936, loss=0.047101687639951706
test: epoch 59, loss 2.6105353832244873, acc=0.4833333194255829, loss=2.6105353832244873
train: epoch 60, loss 0.050648950040340424, acc=0.9869999885559082, loss=0.050648950040340424
test: epoch 60, loss 2.9680116176605225, acc=0.5333333611488342, loss=2.9680116176605225
train: epoch 61, loss 0.05173760652542114, acc=0.9875555634498596, loss=0.05173760652542114
test: epoch 61, loss 3.461016893386841, acc=0.519444465637207, loss=3.461016893386841
train: epoch 62, loss 0.06469986587762833, acc=0.98416668176651, loss=0.06469986587762833
test: epoch 62, loss 3.2368767261505127, acc=0.5055555701255798, loss=3.2368767261505127
train: epoch 63, loss 0.04524288326501846, acc=0.987333357334137, loss=0.04524288326501846
test: epoch 63, loss 2.1344127655029297, acc=0.605555534362793, loss=2.1344127655029297
train: epoch 64, loss 0.05625804513692856, acc=0.9853888750076294, loss=0.05625804513692856
test: epoch 64, loss 3.4376626014709473, acc=0.5138888955116272, loss=3.4376626014709473
train: epoch 65, loss 0.05510760098695755, acc=0.9854444265365601, loss=0.05510760098695755
test: epoch 65, loss 2.960225820541382, acc=0.5055555701255798, loss=2.960225820541382
train: epoch 66, loss 0.048100050538778305, acc=0.9881666898727417, loss=0.048100050538778305
test: epoch 66, loss 3.312854051589966, acc=0.49444442987442017, loss=3.312854051589966
train: epoch 67, loss 0.04846272990107536, acc=0.9869999885559082, loss=0.04846272990107536
test: epoch 67, loss 3.0425124168395996, acc=0.5527777671813965, loss=3.0425124168395996
train: epoch 68, loss 0.051085520535707474, acc=0.9878333210945129, loss=0.051085520535707474
test: epoch 68, loss 3.2608046531677246, acc=0.5, loss=3.2608046531677246
train: epoch 69, loss 0.042912598699331284, acc=0.9892222285270691, loss=0.042912598699331284
test: epoch 69, loss 3.1185412406921387, acc=0.5555555820465088, loss=3.1185412406921387
train: epoch 70, loss 0.04594229906797409, acc=0.9883888959884644, loss=0.04594229906797409
test: epoch 70, loss 2.8336808681488037, acc=0.5444444417953491, loss=2.8336808681488037
train: epoch 71, loss 0.044129129499197006, acc=0.9884999990463257, loss=0.044129129499197006
test: epoch 71, loss 3.1320905685424805, acc=0.5166666507720947, loss=3.1320905685424805
train: epoch 72, loss 0.045954085886478424, acc=0.9893888831138611, loss=0.045954085886478424
test: epoch 72, loss 2.6834568977355957, acc=0.5249999761581421, loss=2.6834568977355957
train: epoch 73, loss 0.04123760759830475, acc=0.9893888831138611, loss=0.04123760759830475
test: epoch 73, loss 3.172259569168091, acc=0.5305555462837219, loss=3.172259569168091
train: epoch 74, loss 0.04793834313750267, acc=0.9878888726234436, loss=0.04793834313750267
test: epoch 74, loss 2.838717222213745, acc=0.5333333611488342, loss=2.838717222213745
train: epoch 75, loss 0.03642435744404793, acc=0.9897222518920898, loss=0.03642435744404793
test: epoch 75, loss 2.747830867767334, acc=0.5305555462837219, loss=2.747830867767334
train: epoch 76, loss 0.039814867079257965, acc=0.9898889064788818, loss=0.039814867079257965
test: epoch 76, loss 2.7600269317626953, acc=0.5583333373069763, loss=2.7600269317626953
train: epoch 77, loss 0.029453380033373833, acc=0.9929444193840027, loss=0.029453380033373833
test: epoch 77, loss 2.9366414546966553, acc=0.550000011920929, loss=2.9366414546966553
train: epoch 78, loss 0.0438445582985878, acc=0.9897222518920898, loss=0.0438445582985878
test: epoch 78, loss 2.4373764991760254, acc=0.550000011920929, loss=2.4373764991760254
train: epoch 79, loss 0.04301917552947998, acc=0.9892222285270691, loss=0.04301917552947998
test: epoch 79, loss 3.1128811836242676, acc=0.5638889074325562, loss=3.1128811836242676
train: epoch 80, loss 0.03736323490738869, acc=0.9903333187103271, loss=0.03736323490738869
test: epoch 80, loss 2.7990305423736572, acc=0.6000000238418579, loss=2.7990305423736572
train: epoch 81, loss 0.042832113802433014, acc=0.9897778034210205, loss=0.042832113802433014
test: epoch 81, loss 2.6455576419830322, acc=0.5944444537162781, loss=2.6455576419830322
train: epoch 82, loss 0.040497999638319016, acc=0.9905555844306946, loss=0.040497999638319016
test: epoch 82, loss 3.6426453590393066, acc=0.5249999761581421, loss=3.6426453590393066
train: epoch 83, loss 0.04073528200387955, acc=0.9897778034210205, loss=0.04073528200387955
test: epoch 83, loss 2.311112642288208, acc=0.5805555582046509, loss=2.311112642288208
train: epoch 84, loss 0.03120381012558937, acc=0.991944432258606, loss=0.03120381012558937
test: epoch 84, loss 2.7562661170959473, acc=0.5833333134651184, loss=2.7562661170959473
train: epoch 85, loss 0.0419243760406971, acc=0.9897222518920898, loss=0.0419243760406971
test: epoch 85, loss 2.804335117340088, acc=0.6027777791023254, loss=2.804335117340088
train: epoch 86, loss 0.033420298248529434, acc=0.9922778010368347, loss=0.033420298248529434
test: epoch 86, loss 2.368973970413208, acc=0.6888889074325562, loss=2.368973970413208
train: epoch 87, loss 0.03969505801796913, acc=0.9909999966621399, loss=0.03969505801796913
test: epoch 87, loss 2.3882524967193604, acc=0.5944444537162781, loss=2.3882524967193604
train: epoch 88, loss 0.031404655426740646, acc=0.9925000071525574, loss=0.031404655426740646
test: epoch 88, loss 2.890373706817627, acc=0.6083333492279053, loss=2.890373706817627
train: epoch 89, loss 0.03957192599773407, acc=0.9909999966621399, loss=0.03957192599773407
test: epoch 89, loss 2.884225606918335, acc=0.5527777671813965, loss=2.884225606918335
train: epoch 90, loss 0.026165589690208435, acc=0.9936110973358154, loss=0.026165589690208435
test: epoch 90, loss 2.9529311656951904, acc=0.5833333134651184, loss=2.9529311656951904
train: epoch 91, loss 0.03631659597158432, acc=0.99144446849823, loss=0.03631659597158432
test: epoch 91, loss 2.35937237739563, acc=0.644444465637207, loss=2.35937237739563
train: epoch 92, loss 0.03138631954789162, acc=0.9929444193840027, loss=0.03138631954789162
test: epoch 92, loss 2.7971901893615723, acc=0.5972222089767456, loss=2.7971901893615723
train: epoch 93, loss 0.03450464457273483, acc=0.99144446849823, loss=0.03450464457273483
test: epoch 93, loss 3.5719473361968994, acc=0.6166666746139526, loss=3.5719473361968994
train: epoch 94, loss 0.02928384579718113, acc=0.9923333525657654, loss=0.02928384579718113
test: epoch 94, loss 3.225834608078003, acc=0.5944444537162781, loss=3.225834608078003
train: epoch 95, loss 0.03600367158651352, acc=0.9913889169692993, loss=0.03600367158651352
test: epoch 95, loss 2.3999197483062744, acc=0.5916666388511658, loss=2.3999197483062744
train: epoch 96, loss 0.030149996280670166, acc=0.9932222366333008, loss=0.030149996280670166
test: epoch 96, loss 2.6116695404052734, acc=0.6388888955116272, loss=2.6116695404052734
train: epoch 97, loss 0.036249805241823196, acc=0.9911666512489319, loss=0.036249805241823196
test: epoch 97, loss 2.8152506351470947, acc=0.6333333253860474, loss=2.8152506351470947
train: epoch 98, loss 0.036655575037002563, acc=0.9915000200271606, loss=0.036655575037002563
test: epoch 98, loss 1.9483941793441772, acc=0.6722221970558167, loss=1.9483941793441772
train: epoch 99, loss 0.02980400063097477, acc=0.992888867855072, loss=0.02980400063097477
test: epoch 99, loss 2.3577630519866943, acc=0.6277777552604675, loss=2.3577630519866943
train: epoch 100, loss 0.03506625443696976, acc=0.992111086845398, loss=0.03506625443696976
test: epoch 100, loss 2.388636589050293, acc=0.5833333134651184, loss=2.388636589050293
train: epoch 101, loss 0.028643427416682243, acc=0.9938333630561829, loss=0.028643427416682243
test: epoch 101, loss 2.2509074211120605, acc=0.6499999761581421, loss=2.2509074211120605
train: epoch 102, loss 0.03266425430774689, acc=0.9926666617393494, loss=0.03266425430774689
test: epoch 102, loss 2.5724937915802, acc=0.6277777552604675, loss=2.5724937915802
train: epoch 103, loss 0.02432595193386078, acc=0.9940000176429749, loss=0.02432595193386078
test: epoch 103, loss 2.310350179672241, acc=0.6472222208976746, loss=2.310350179672241
train: epoch 104, loss 0.025956448167562485, acc=0.9938889145851135, loss=0.025956448167562485
test: epoch 104, loss 1.8825795650482178, acc=0.6638888716697693, loss=1.8825795650482178
train: epoch 105, loss 0.027003446593880653, acc=0.9933333396911621, loss=0.027003446593880653
test: epoch 105, loss 2.3434767723083496, acc=0.625, loss=2.3434767723083496
train: epoch 106, loss 0.029671186581254005, acc=0.99272221326828, loss=0.029671186581254005
test: epoch 106, loss 2.9856953620910645, acc=0.6027777791023254, loss=2.9856953620910645
train: epoch 107, loss 0.02672230452299118, acc=0.9934444427490234, loss=0.02672230452299118
test: epoch 107, loss 2.4579226970672607, acc=0.6388888955116272, loss=2.4579226970672607
train: epoch 108, loss 0.02862708829343319, acc=0.992555558681488, loss=0.02862708829343319
test: epoch 108, loss 2.1910228729248047, acc=0.699999988079071, loss=2.1910228729248047
train: epoch 109, loss 0.027438748627901077, acc=0.9941666722297668, loss=0.027438748627901077
test: epoch 109, loss 2.7576825618743896, acc=0.6472222208976746, loss=2.7576825618743896
train: epoch 110, loss 0.04225107282400131, acc=0.9911666512489319, loss=0.04225107282400131
test: epoch 110, loss 2.133390426635742, acc=0.675000011920929, loss=2.133390426635742
train: epoch 111, loss 0.018166620284318924, acc=0.9957777857780457, loss=0.018166620284318924
test: epoch 111, loss 2.087791919708252, acc=0.6555555462837219, loss=2.087791919708252
train: epoch 112, loss 0.02833046391606331, acc=0.9937222003936768, loss=0.02833046391606331
test: epoch 112, loss 2.426966428756714, acc=0.6416666507720947, loss=2.426966428756714
train: epoch 113, loss 0.025686830282211304, acc=0.9930555820465088, loss=0.025686830282211304
test: epoch 113, loss 2.442387580871582, acc=0.6694444417953491, loss=2.442387580871582
train: epoch 114, loss 0.022429054602980614, acc=0.9949444532394409, loss=0.022429054602980614
test: epoch 114, loss 1.9302482604980469, acc=0.7194444537162781, loss=1.9302482604980469
train: epoch 115, loss 0.031571999192237854, acc=0.9932222366333008, loss=0.031571999192237854
test: epoch 115, loss 2.244680881500244, acc=0.6805555820465088, loss=2.244680881500244
train: epoch 116, loss 0.03107740357518196, acc=0.9930555820465088, loss=0.03107740357518196
test: epoch 116, loss 2.3193366527557373, acc=0.6333333253860474, loss=2.3193366527557373
train: epoch 117, loss 0.021194640547037125, acc=0.9950000047683716, loss=0.021194640547037125
test: epoch 117, loss 2.3998382091522217, acc=0.6888889074325562, loss=2.3998382091522217
train: epoch 118, loss 0.032035697251558304, acc=0.9926666617393494, loss=0.032035697251558304
test: epoch 118, loss 1.8506824970245361, acc=0.6722221970558167, loss=1.8506824970245361
train: epoch 119, loss 0.01966569386422634, acc=0.9949444532394409, loss=0.01966569386422634
test: epoch 119, loss 2.02915358543396, acc=0.6861110925674438, loss=2.02915358543396
train: epoch 120, loss 0.027220936492085457, acc=0.9938889145851135, loss=0.027220936492085457
test: epoch 120, loss 3.153892755508423, acc=0.6611111164093018, loss=3.153892755508423
train: epoch 121, loss 0.019671093672513962, acc=0.9948333501815796, loss=0.019671093672513962
test: epoch 121, loss 2.2152960300445557, acc=0.6638888716697693, loss=2.2152960300445557
train: epoch 122, loss 0.019788483157753944, acc=0.9956111311912537, loss=0.019788483157753944
test: epoch 122, loss 2.4420745372772217, acc=0.6555555462837219, loss=2.4420745372772217
train: epoch 123, loss 0.020461617037653923, acc=0.9948889017105103, loss=0.020461617037653923
test: epoch 123, loss 2.190661907196045, acc=0.6861110925674438, loss=2.190661907196045
train: epoch 124, loss 0.025416120886802673, acc=0.9936666488647461, loss=0.025416120886802673
test: epoch 124, loss 1.9496339559555054, acc=0.699999988079071, loss=1.9496339559555054
train: epoch 125, loss 0.022965921089053154, acc=0.9947777986526489, loss=0.022965921089053154
test: epoch 125, loss 1.90304434299469, acc=0.7333333492279053, loss=1.90304434299469
train: epoch 126, loss 0.02420404925942421, acc=0.9944999814033508, loss=0.02420404925942421
test: epoch 126, loss 1.930522084236145, acc=0.7166666388511658, loss=1.930522084236145
train: epoch 127, loss 0.017392419278621674, acc=0.9959999918937683, loss=0.017392419278621674
test: epoch 127, loss 2.369340419769287, acc=0.7111111283302307, loss=2.369340419769287
train: epoch 128, loss 0.020356034860014915, acc=0.9953333139419556, loss=0.020356034860014915
test: epoch 128, loss 2.218609094619751, acc=0.7416666746139526, loss=2.218609094619751
train: epoch 129, loss 0.020110642537474632, acc=0.995555579662323, loss=0.020110642537474632
test: epoch 129, loss 1.7036936283111572, acc=0.7444444298744202, loss=1.7036936283111572
train: epoch 130, loss 0.027081485837697983, acc=0.9942777752876282, loss=0.027081485837697983
test: epoch 130, loss 2.479614019393921, acc=0.6666666865348816, loss=2.479614019393921
train: epoch 131, loss 0.0223714467138052, acc=0.9944444298744202, loss=0.0223714467138052
test: epoch 131, loss 2.478743553161621, acc=0.6555555462837219, loss=2.478743553161621
train: epoch 132, loss 0.022483276203274727, acc=0.9949444532394409, loss=0.022483276203274727
test: epoch 132, loss 2.1441359519958496, acc=0.6805555820465088, loss=2.1441359519958496
train: epoch 133, loss 0.01985786110162735, acc=0.9952222108840942, loss=0.01985786110162735
test: epoch 133, loss 2.6630823612213135, acc=0.7166666388511658, loss=2.6630823612213135
train: epoch 134, loss 0.025389892980456352, acc=0.9942777752876282, loss=0.025389892980456352
test: epoch 134, loss 1.9272006750106812, acc=0.7083333134651184, loss=1.9272006750106812
train: epoch 135, loss 0.021507926285266876, acc=0.9959444403648376, loss=0.021507926285266876
test: epoch 135, loss 2.068251371383667, acc=0.7361111044883728, loss=2.068251371383667
train: epoch 136, loss 0.024569988250732422, acc=0.9945555329322815, loss=0.024569988250732422
test: epoch 136, loss 1.9133135080337524, acc=0.730555534362793, loss=1.9133135080337524
train: epoch 137, loss 0.027894727885723114, acc=0.9950000047683716, loss=0.027894727885723114
test: epoch 137, loss 1.8351484537124634, acc=0.7277777791023254, loss=1.8351484537124634
train: epoch 138, loss 0.02023277059197426, acc=0.9946110844612122, loss=0.02023277059197426
test: epoch 138, loss 2.044619083404541, acc=0.6944444179534912, loss=2.044619083404541
train: epoch 139, loss 0.020551659166812897, acc=0.9959999918937683, loss=0.020551659166812897
test: epoch 139, loss 1.9861632585525513, acc=0.7361111044883728, loss=1.9861632585525513
train: epoch 140, loss 0.01583077386021614, acc=0.995722234249115, loss=0.01583077386021614
test: epoch 140, loss 1.964875340461731, acc=0.75, loss=1.964875340461731
train: epoch 141, loss 0.027371950447559357, acc=0.9940000176429749, loss=0.027371950447559357
test: epoch 141, loss 2.4635047912597656, acc=0.7138888835906982, loss=2.4635047912597656
train: epoch 142, loss 0.018260790035128593, acc=0.9962777495384216, loss=0.018260790035128593
test: epoch 142, loss 2.258528709411621, acc=0.7277777791023254, loss=2.258528709411621
train: epoch 143, loss 0.023426761850714684, acc=0.9953333139419556, loss=0.023426761850714684
test: epoch 143, loss 1.8989946842193604, acc=0.7194444537162781, loss=1.8989946842193604
train: epoch 144, loss 0.02066045254468918, acc=0.9961110949516296, loss=0.02066045254468918
test: epoch 144, loss 1.8557898998260498, acc=0.7527777552604675, loss=1.8557898998260498
train: epoch 145, loss 0.020283807069063187, acc=0.9947222471237183, loss=0.020283807069063187
test: epoch 145, loss 1.9543179273605347, acc=0.7611111402511597, loss=1.9543179273605347
train: epoch 146, loss 0.01563374511897564, acc=0.9956666827201843, loss=0.01563374511897564
test: epoch 146, loss 1.781086802482605, acc=0.7388888597488403, loss=1.781086802482605
train: epoch 147, loss 0.01814880035817623, acc=0.995888888835907, loss=0.01814880035817623
test: epoch 147, loss 2.4908969402313232, acc=0.7111111283302307, loss=2.4908969402313232
train: epoch 148, loss 0.024868864566087723, acc=0.9937222003936768, loss=0.024868864566087723
test: epoch 148, loss 2.0130977630615234, acc=0.7277777791023254, loss=2.0130977630615234
train: epoch 149, loss 0.017192421481013298, acc=0.995888888835907, loss=0.017192421481013298
test: epoch 149, loss 1.757521152496338, acc=0.7916666865348816, loss=1.757521152496338
train: epoch 150, loss 0.011134865693747997, acc=0.9970555305480957, loss=0.011134865693747997
test: epoch 150, loss 1.7946743965148926, acc=0.7416666746139526, loss=1.7946743965148926
