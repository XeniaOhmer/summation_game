# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.99", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1890361300, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1890361300, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.558806896209717, acc=0.046888887882232666, loss=3.558806896209717
test: epoch 1, loss 3.5293755531311035, acc=0.05277777835726738, loss=3.5293755531311035
train: epoch 2, loss 3.498544216156006, acc=0.05022222176194191, loss=3.498544216156006
test: epoch 2, loss 3.322331428527832, acc=0.05277777835726738, loss=3.322331428527832
train: epoch 3, loss 3.4818379878997803, acc=0.04838888719677925, loss=3.4818379878997803
test: epoch 3, loss 3.140728235244751, acc=0.08055555820465088, loss=3.140728235244751
train: epoch 4, loss 3.433346748352051, acc=0.053611110895872116, loss=3.433346748352051
test: epoch 4, loss 3.2490715980529785, acc=0.07222222536802292, loss=3.2490715980529785
train: epoch 5, loss 3.282855749130249, acc=0.06105555593967438, loss=3.282855749130249
test: epoch 5, loss 3.754995584487915, acc=0.05833333358168602, loss=3.754995584487915
train: epoch 6, loss 3.1178297996520996, acc=0.08188889175653458, loss=3.1178297996520996
test: epoch 6, loss 3.9112889766693115, acc=0.05277777835726738, loss=3.9112889766693115
train: epoch 7, loss 2.9764351844787598, acc=0.09511110931634903, loss=2.9764351844787598
test: epoch 7, loss 3.9056918621063232, acc=0.05277777835726738, loss=3.9056918621063232
train: epoch 8, loss 2.887637138366699, acc=0.10111111402511597, loss=2.887637138366699
test: epoch 8, loss 3.7437522411346436, acc=0.05277777835726738, loss=3.7437522411346436
train: epoch 9, loss 2.8192195892333984, acc=0.1124444454908371, loss=2.8192195892333984
test: epoch 9, loss 3.680793046951294, acc=0.05000000074505806, loss=3.680793046951294
train: epoch 10, loss 2.7710046768188477, acc=0.11772222071886063, loss=2.7710046768188477
test: epoch 10, loss 3.689964532852173, acc=0.05833333358168602, loss=3.689964532852173
train: epoch 11, loss 2.7196950912475586, acc=0.1198333352804184, loss=2.7196950912475586
test: epoch 11, loss 3.4939229488372803, acc=0.06111111119389534, loss=3.4939229488372803
train: epoch 12, loss 2.680095672607422, acc=0.13233333826065063, loss=2.680095672607422
test: epoch 12, loss 3.5201566219329834, acc=0.0555555559694767, loss=3.5201566219329834
train: epoch 13, loss 2.6405458450317383, acc=0.13699999451637268, loss=2.6405458450317383
test: epoch 13, loss 3.4953932762145996, acc=0.05000000074505806, loss=3.4953932762145996
train: epoch 14, loss 2.6234090328216553, acc=0.1445000022649765, loss=2.6234090328216553
test: epoch 14, loss 3.435441732406616, acc=0.05277777835726738, loss=3.435441732406616
train: epoch 15, loss 2.6024329662323, acc=0.14516666531562805, loss=2.6024329662323
test: epoch 15, loss 3.358889102935791, acc=0.0555555559694767, loss=3.358889102935791
train: epoch 16, loss 2.5738627910614014, acc=0.14516666531562805, loss=2.5738627910614014
test: epoch 16, loss 3.4062581062316895, acc=0.05833333358168602, loss=3.4062581062316895
train: epoch 17, loss 2.564361333847046, acc=0.154388889670372, loss=2.564361333847046
test: epoch 17, loss 3.364847183227539, acc=0.07222222536802292, loss=3.364847183227539
train: epoch 18, loss 2.5548343658447266, acc=0.14755555987358093, loss=2.5548343658447266
test: epoch 18, loss 3.3457236289978027, acc=0.08055555820465088, loss=3.3457236289978027
train: epoch 19, loss 2.530548095703125, acc=0.1538333296775818, loss=2.530548095703125
test: epoch 19, loss 3.2801053524017334, acc=0.07500000298023224, loss=3.2801053524017334
train: epoch 20, loss 2.5260369777679443, acc=0.1511666625738144, loss=2.5260369777679443
test: epoch 20, loss 3.2607204914093018, acc=0.08055555820465088, loss=3.2607204914093018
train: epoch 21, loss 2.513883113861084, acc=0.1527777761220932, loss=2.513883113861084
test: epoch 21, loss 3.269526481628418, acc=0.0555555559694767, loss=3.269526481628418
train: epoch 22, loss 2.5147950649261475, acc=0.15061111748218536, loss=2.5147950649261475
test: epoch 22, loss 3.2221944332122803, acc=0.07500000298023224, loss=3.2221944332122803
train: epoch 23, loss 2.5040628910064697, acc=0.15777777135372162, loss=2.5040628910064697
test: epoch 23, loss 3.2636959552764893, acc=0.07222222536802292, loss=3.2636959552764893
train: epoch 24, loss 2.500685214996338, acc=0.1606111079454422, loss=2.500685214996338
test: epoch 24, loss 3.275615930557251, acc=0.0694444477558136, loss=3.275615930557251
train: epoch 25, loss 2.4915239810943604, acc=0.1548333317041397, loss=2.4915239810943604
test: epoch 25, loss 3.25216007232666, acc=0.07500000298023224, loss=3.25216007232666
train: epoch 26, loss 2.477632999420166, acc=0.1601666659116745, loss=2.477632999420166
test: epoch 26, loss 3.2522780895233154, acc=0.08888889104127884, loss=3.2522780895233154
train: epoch 27, loss 2.47255277633667, acc=0.15788888931274414, loss=2.47255277633667
test: epoch 27, loss 3.2086551189422607, acc=0.0833333358168602, loss=3.2086551189422607
train: epoch 28, loss 2.466515302658081, acc=0.16349999606609344, loss=2.466515302658081
test: epoch 28, loss 3.290301561355591, acc=0.0833333358168602, loss=3.290301561355591
train: epoch 29, loss 2.456287145614624, acc=0.1663888841867447, loss=2.456287145614624
test: epoch 29, loss 3.125835657119751, acc=0.07777778059244156, loss=3.125835657119751
train: epoch 30, loss 2.447227716445923, acc=0.1608888953924179, loss=2.447227716445923
test: epoch 30, loss 3.180203437805176, acc=0.08611111342906952, loss=3.180203437805176
train: epoch 31, loss 2.4384474754333496, acc=0.16677777469158173, loss=2.4384474754333496
test: epoch 31, loss 3.162874460220337, acc=0.08888889104127884, loss=3.162874460220337
train: epoch 32, loss 2.4281320571899414, acc=0.1709444373846054, loss=2.4281320571899414
test: epoch 32, loss 3.13962721824646, acc=0.10000000149011612, loss=3.13962721824646
train: epoch 33, loss 2.4028923511505127, acc=0.17394444346427917, loss=2.4028923511505127
test: epoch 33, loss 3.0472514629364014, acc=0.09166666865348816, loss=3.0472514629364014
train: epoch 34, loss 2.3882219791412354, acc=0.17561110854148865, loss=2.3882219791412354
test: epoch 34, loss 3.0654103755950928, acc=0.11666666716337204, loss=3.0654103755950928
train: epoch 35, loss 2.377393960952759, acc=0.17599999904632568, loss=2.377393960952759
test: epoch 35, loss 2.9894938468933105, acc=0.10833333432674408, loss=2.9894938468933105
train: epoch 36, loss 2.371260166168213, acc=0.17911110818386078, loss=2.371260166168213
test: epoch 36, loss 2.9111597537994385, acc=0.11666666716337204, loss=2.9111597537994385
train: epoch 37, loss 2.337707996368408, acc=0.18611110746860504, loss=2.337707996368408
test: epoch 37, loss 2.854538679122925, acc=0.11666666716337204, loss=2.854538679122925
train: epoch 38, loss 2.3449573516845703, acc=0.18872222304344177, loss=2.3449573516845703
test: epoch 38, loss 2.8406429290771484, acc=0.11944444477558136, loss=2.8406429290771484
train: epoch 39, loss 2.3126986026763916, acc=0.1919444501399994, loss=2.3126986026763916
test: epoch 39, loss 2.6877503395080566, acc=0.13055555522441864, loss=2.6877503395080566
train: epoch 40, loss 2.3152568340301514, acc=0.18894444406032562, loss=2.3152568340301514
test: epoch 40, loss 2.559187889099121, acc=0.12777778506278992, loss=2.559187889099121
train: epoch 41, loss 2.2832064628601074, acc=0.19494444131851196, loss=2.2832064628601074
test: epoch 41, loss 2.5573055744171143, acc=0.13611111044883728, loss=2.5573055744171143
train: epoch 42, loss 2.2578628063201904, acc=0.2007777839899063, loss=2.2578628063201904
test: epoch 42, loss 2.4470818042755127, acc=0.14166666567325592, loss=2.4470818042755127
train: epoch 43, loss 2.2533318996429443, acc=0.2007777839899063, loss=2.2533318996429443
test: epoch 43, loss 2.429570436477661, acc=0.15833333134651184, loss=2.429570436477661
train: epoch 44, loss 2.2522692680358887, acc=0.20844444632530212, loss=2.2522692680358887
test: epoch 44, loss 2.397047281265259, acc=0.14166666567325592, loss=2.397047281265259
train: epoch 45, loss 2.2192556858062744, acc=0.20677778124809265, loss=2.2192556858062744
test: epoch 45, loss 2.3775246143341064, acc=0.14722222089767456, loss=2.3775246143341064
train: epoch 46, loss 2.209810495376587, acc=0.21466666460037231, loss=2.209810495376587
test: epoch 46, loss 2.371145725250244, acc=0.15555556118488312, loss=2.371145725250244
train: epoch 47, loss 2.191897392272949, acc=0.21933333575725555, loss=2.191897392272949
test: epoch 47, loss 2.3214194774627686, acc=0.1527777761220932, loss=2.3214194774627686
train: epoch 48, loss 2.182609796524048, acc=0.22094444930553436, loss=2.182609796524048
test: epoch 48, loss 2.326700448989868, acc=0.15555556118488312, loss=2.326700448989868
train: epoch 49, loss 2.1580748558044434, acc=0.22466666996479034, loss=2.1580748558044434
test: epoch 49, loss 2.2620043754577637, acc=0.1666666716337204, loss=2.2620043754577637
train: epoch 50, loss 2.141709566116333, acc=0.22805555164813995, loss=2.141709566116333
test: epoch 50, loss 2.2419917583465576, acc=0.15833333134651184, loss=2.2419917583465576
train: epoch 51, loss 2.1325173377990723, acc=0.23805555701255798, loss=2.1325173377990723
test: epoch 51, loss 2.221637487411499, acc=0.15833333134651184, loss=2.221637487411499
train: epoch 52, loss 2.125997304916382, acc=0.2334444373846054, loss=2.125997304916382
test: epoch 52, loss 2.1652705669403076, acc=0.16944444179534912, loss=2.1652705669403076
train: epoch 53, loss 2.100914478302002, acc=0.23372222483158112, loss=2.100914478302002
test: epoch 53, loss 2.1652162075042725, acc=0.16944444179534912, loss=2.1652162075042725
train: epoch 54, loss 2.089474678039551, acc=0.24161110818386078, loss=2.089474678039551
test: epoch 54, loss 2.1403892040252686, acc=0.17499999701976776, loss=2.1403892040252686
train: epoch 55, loss 2.081514358520508, acc=0.24666666984558105, loss=2.081514358520508
test: epoch 55, loss 2.112138509750366, acc=0.17777778208255768, loss=2.112138509750366
train: epoch 56, loss 2.0524137020111084, acc=0.24744445085525513, loss=2.0524137020111084
test: epoch 56, loss 2.0992062091827393, acc=0.17777778208255768, loss=2.0992062091827393
train: epoch 57, loss 2.0362930297851562, acc=0.2529999911785126, loss=2.0362930297851562
test: epoch 57, loss 2.1300182342529297, acc=0.1805555522441864, loss=2.1300182342529297
train: epoch 58, loss 2.0238449573516846, acc=0.2547222077846527, loss=2.0238449573516846
test: epoch 58, loss 2.1075844764709473, acc=0.17777778208255768, loss=2.1075844764709473
train: epoch 59, loss 2.0369412899017334, acc=0.25555557012557983, loss=2.0369412899017334
test: epoch 59, loss 2.079144239425659, acc=0.17499999701976776, loss=2.079144239425659
train: epoch 60, loss 2.012582540512085, acc=0.26027777791023254, loss=2.012582540512085
test: epoch 60, loss 2.07735013961792, acc=0.17777778208255768, loss=2.07735013961792
train: epoch 61, loss 1.9860683679580688, acc=0.26644444465637207, loss=1.9860683679580688
test: epoch 61, loss 2.071727991104126, acc=0.16944444179534912, loss=2.071727991104126
train: epoch 62, loss 1.9957685470581055, acc=0.26794445514678955, loss=1.9957685470581055
test: epoch 62, loss 2.036954402923584, acc=0.19166666269302368, loss=2.036954402923584
train: epoch 63, loss 1.973228931427002, acc=0.2714444398880005, loss=1.973228931427002
test: epoch 63, loss 2.026093006134033, acc=0.18611110746860504, loss=2.026093006134033
train: epoch 64, loss 1.9477996826171875, acc=0.27388888597488403, loss=1.9477996826171875
test: epoch 64, loss 2.021305561065674, acc=0.18333333730697632, loss=2.021305561065674
train: epoch 65, loss 1.9388822317123413, acc=0.27461111545562744, loss=1.9388822317123413
test: epoch 65, loss 2.021620988845825, acc=0.1944444477558136, loss=2.021620988845825
train: epoch 66, loss 1.9359562397003174, acc=0.28655555844306946, loss=1.9359562397003174
test: epoch 66, loss 2.020012617111206, acc=0.17777778208255768, loss=2.020012617111206
train: epoch 67, loss 1.9277409315109253, acc=0.28833332657814026, loss=1.9277409315109253
test: epoch 67, loss 1.9907715320587158, acc=0.19722221791744232, loss=1.9907715320587158
train: epoch 68, loss 1.8972413539886475, acc=0.29411110281944275, loss=1.8972413539886475
test: epoch 68, loss 1.9929382801055908, acc=0.1944444477558136, loss=1.9929382801055908
train: epoch 69, loss 1.8991248607635498, acc=0.2909444570541382, loss=1.8991248607635498
test: epoch 69, loss 1.9854457378387451, acc=0.1944444477558136, loss=1.9854457378387451
train: epoch 70, loss 1.89670991897583, acc=0.2999444305896759, loss=1.89670991897583
test: epoch 70, loss 1.9664747714996338, acc=0.1944444477558136, loss=1.9664747714996338
train: epoch 71, loss 1.8668996095657349, acc=0.2989444434642792, loss=1.8668996095657349
test: epoch 71, loss 1.974323034286499, acc=0.1944444477558136, loss=1.974323034286499
train: epoch 72, loss 1.8659080266952515, acc=0.3043888807296753, loss=1.8659080266952515
test: epoch 72, loss 1.9675087928771973, acc=0.20000000298023224, loss=1.9675087928771973
train: epoch 73, loss 1.845096468925476, acc=0.3074444532394409, loss=1.845096468925476
test: epoch 73, loss 1.959202766418457, acc=0.20000000298023224, loss=1.959202766418457
train: epoch 74, loss 1.8476836681365967, acc=0.31683334708213806, loss=1.8476836681365967
test: epoch 74, loss 1.9529811143875122, acc=0.20555555820465088, loss=1.9529811143875122
train: epoch 75, loss 1.8296087980270386, acc=0.3159444332122803, loss=1.8296087980270386
test: epoch 75, loss 1.9475370645523071, acc=0.21111111342906952, loss=1.9475370645523071
train: epoch 76, loss 1.8236348628997803, acc=0.31305554509162903, loss=1.8236348628997803
test: epoch 76, loss 1.937990665435791, acc=0.21388888359069824, loss=1.937990665435791
train: epoch 77, loss 1.796242117881775, acc=0.32083332538604736, loss=1.796242117881775
test: epoch 77, loss 1.917812705039978, acc=0.21111111342906952, loss=1.917812705039978
train: epoch 78, loss 1.7984743118286133, acc=0.3251666724681854, loss=1.7984743118286133
test: epoch 78, loss 1.9270957708358765, acc=0.21388888359069824, loss=1.9270957708358765
train: epoch 79, loss 1.7825974225997925, acc=0.32288888096809387, loss=1.7825974225997925
test: epoch 79, loss 1.9360206127166748, acc=0.21666666865348816, loss=1.9360206127166748
train: epoch 80, loss 1.780881643295288, acc=0.332111120223999, loss=1.780881643295288
test: epoch 80, loss 1.9232960939407349, acc=0.20555555820465088, loss=1.9232960939407349
train: epoch 81, loss 1.7607098817825317, acc=0.3384999930858612, loss=1.7607098817825317
test: epoch 81, loss 1.9114238023757935, acc=0.21666666865348816, loss=1.9114238023757935
train: epoch 82, loss 1.7493304014205933, acc=0.3391111195087433, loss=1.7493304014205933
test: epoch 82, loss 1.9218475818634033, acc=0.21388888359069824, loss=1.9218475818634033
train: epoch 83, loss 1.749631404876709, acc=0.34283334016799927, loss=1.749631404876709
test: epoch 83, loss 1.91074800491333, acc=0.21666666865348816, loss=1.91074800491333
train: epoch 84, loss 1.7250505685806274, acc=0.3432222306728363, loss=1.7250505685806274
test: epoch 84, loss 1.8872647285461426, acc=0.21666666865348816, loss=1.8872647285461426
train: epoch 85, loss 1.7360317707061768, acc=0.3448888957500458, loss=1.7360317707061768
test: epoch 85, loss 1.8896520137786865, acc=0.21666666865348816, loss=1.8896520137786865
train: epoch 86, loss 1.7144324779510498, acc=0.3501666784286499, loss=1.7144324779510498
test: epoch 86, loss 1.864986538887024, acc=0.22777777910232544, loss=1.864986538887024
train: epoch 87, loss 1.7190451622009277, acc=0.35366666316986084, loss=1.7190451622009277
test: epoch 87, loss 1.8599642515182495, acc=0.2222222238779068, loss=1.8599642515182495
train: epoch 88, loss 1.700429081916809, acc=0.35366666316986084, loss=1.700429081916809
test: epoch 88, loss 1.8653470277786255, acc=0.22499999403953552, loss=1.8653470277786255
train: epoch 89, loss 1.68470299243927, acc=0.3546110987663269, loss=1.68470299243927
test: epoch 89, loss 1.8548052310943604, acc=0.23055554926395416, loss=1.8548052310943604
train: epoch 90, loss 1.6821202039718628, acc=0.3596111238002777, loss=1.6821202039718628
test: epoch 90, loss 1.8453689813613892, acc=0.22777777910232544, loss=1.8453689813613892
train: epoch 91, loss 1.6879583597183228, acc=0.3624444305896759, loss=1.6879583597183228
test: epoch 91, loss 1.8512462377548218, acc=0.23055554926395416, loss=1.8512462377548218
train: epoch 92, loss 1.6526473760604858, acc=0.3658333420753479, loss=1.6526473760604858
test: epoch 92, loss 1.855080485343933, acc=0.23333333432674408, loss=1.855080485343933
train: epoch 93, loss 1.6580454111099243, acc=0.3664444386959076, loss=1.6580454111099243
test: epoch 93, loss 1.8359330892562866, acc=0.23333333432674408, loss=1.8359330892562866
train: epoch 94, loss 1.6313965320587158, acc=0.3703888952732086, loss=1.6313965320587158
test: epoch 94, loss 1.8167060613632202, acc=0.2222222238779068, loss=1.8167060613632202
train: epoch 95, loss 1.6439268589019775, acc=0.3690555691719055, loss=1.6439268589019775
test: epoch 95, loss 1.8089874982833862, acc=0.23055554926395416, loss=1.8089874982833862
train: epoch 96, loss 1.6306748390197754, acc=0.382833331823349, loss=1.6306748390197754
test: epoch 96, loss 1.8031346797943115, acc=0.23888888955116272, loss=1.8031346797943115
train: epoch 97, loss 1.6248698234558105, acc=0.37466666102409363, loss=1.6248698234558105
test: epoch 97, loss 1.817527174949646, acc=0.23888888955116272, loss=1.817527174949646
train: epoch 98, loss 1.6171244382858276, acc=0.3820555508136749, loss=1.6171244382858276
test: epoch 98, loss 1.788161039352417, acc=0.23888888955116272, loss=1.788161039352417
train: epoch 99, loss 1.610374093055725, acc=0.3908333480358124, loss=1.610374093055725
test: epoch 99, loss 1.8161062002182007, acc=0.23888888955116272, loss=1.8161062002182007
train: epoch 100, loss 1.6085423231124878, acc=0.382999986410141, loss=1.6085423231124878
test: epoch 100, loss 1.7937885522842407, acc=0.2361111044883728, loss=1.7937885522842407
train: epoch 101, loss 1.5989536046981812, acc=0.3922777771949768, loss=1.5989536046981812
test: epoch 101, loss 1.789402961730957, acc=0.23055554926395416, loss=1.789402961730957
train: epoch 102, loss 1.6005653142929077, acc=0.3925555646419525, loss=1.6005653142929077
test: epoch 102, loss 1.8051848411560059, acc=0.23333333432674408, loss=1.8051848411560059
train: epoch 103, loss 1.595054268836975, acc=0.3908333480358124, loss=1.595054268836975
test: epoch 103, loss 1.8102777004241943, acc=0.24166665971279144, loss=1.8102777004241943
train: epoch 104, loss 1.558840036392212, acc=0.39872223138809204, loss=1.558840036392212
test: epoch 104, loss 1.8197004795074463, acc=0.24166665971279144, loss=1.8197004795074463
train: epoch 105, loss 1.5657095909118652, acc=0.4054444432258606, loss=1.5657095909118652
test: epoch 105, loss 1.787304401397705, acc=0.24166665971279144, loss=1.787304401397705
train: epoch 106, loss 1.5599721670150757, acc=0.40272220969200134, loss=1.5599721670150757
test: epoch 106, loss 1.805574893951416, acc=0.23888888955116272, loss=1.805574893951416
train: epoch 107, loss 1.5560650825500488, acc=0.4039444327354431, loss=1.5560650825500488
test: epoch 107, loss 1.8087384700775146, acc=0.23888888955116272, loss=1.8087384700775146
train: epoch 108, loss 1.5631548166275024, acc=0.4073333442211151, loss=1.5631548166275024
test: epoch 108, loss 1.805335521697998, acc=0.23888888955116272, loss=1.805335521697998
train: epoch 109, loss 1.553955316543579, acc=0.40666666626930237, loss=1.553955316543579
test: epoch 109, loss 1.8083345890045166, acc=0.24722221493721008, loss=1.8083345890045166
train: epoch 110, loss 1.533575415611267, acc=0.40905556082725525, loss=1.533575415611267
test: epoch 110, loss 1.8082997798919678, acc=0.23333333432674408, loss=1.8082997798919678
train: epoch 111, loss 1.5350253582000732, acc=0.4150555431842804, loss=1.5350253582000732
test: epoch 111, loss 1.8019471168518066, acc=0.23888888955116272, loss=1.8019471168518066
train: epoch 112, loss 1.5127437114715576, acc=0.41850000619888306, loss=1.5127437114715576
test: epoch 112, loss 1.8144452571868896, acc=0.24166665971279144, loss=1.8144452571868896
train: epoch 113, loss 1.5239895582199097, acc=0.4247777760028839, loss=1.5239895582199097
test: epoch 113, loss 1.7875577211380005, acc=0.24166665971279144, loss=1.7875577211380005
train: epoch 114, loss 1.509359359741211, acc=0.4225555658340454, loss=1.509359359741211
test: epoch 114, loss 1.8017690181732178, acc=0.2361111044883728, loss=1.8017690181732178
train: epoch 115, loss 1.5070304870605469, acc=0.42705556750297546, loss=1.5070304870605469
test: epoch 115, loss 1.8038991689682007, acc=0.23888888955116272, loss=1.8038991689682007
train: epoch 116, loss 1.4997164011001587, acc=0.4312777817249298, loss=1.4997164011001587
test: epoch 116, loss 1.7856945991516113, acc=0.24722221493721008, loss=1.7856945991516113
train: epoch 117, loss 1.4977891445159912, acc=0.4357222318649292, loss=1.4977891445159912
test: epoch 117, loss 1.7820703983306885, acc=0.24444444477558136, loss=1.7820703983306885
train: epoch 118, loss 1.4958525896072388, acc=0.42916667461395264, loss=1.4958525896072388
test: epoch 118, loss 1.7839672565460205, acc=0.24722221493721008, loss=1.7839672565460205
train: epoch 119, loss 1.4949750900268555, acc=0.4304444491863251, loss=1.4949750900268555
test: epoch 119, loss 1.791319489479065, acc=0.24722221493721008, loss=1.791319489479065
train: epoch 120, loss 1.486232876777649, acc=0.4311666786670685, loss=1.486232876777649
test: epoch 120, loss 1.7770817279815674, acc=0.23888888955116272, loss=1.7770817279815674
train: epoch 121, loss 1.480673909187317, acc=0.43683332204818726, loss=1.480673909187317
test: epoch 121, loss 1.780592918395996, acc=0.2527777850627899, loss=1.780592918395996
train: epoch 122, loss 1.4757349491119385, acc=0.4404999911785126, loss=1.4757349491119385
test: epoch 122, loss 1.7972347736358643, acc=0.25, loss=1.7972347736358643
train: epoch 123, loss 1.4765944480895996, acc=0.4407777786254883, loss=1.4765944480895996
test: epoch 123, loss 1.7962955236434937, acc=0.2527777850627899, loss=1.7962955236434937
train: epoch 124, loss 1.4635895490646362, acc=0.4429444372653961, loss=1.4635895490646362
test: epoch 124, loss 1.7898807525634766, acc=0.25, loss=1.7898807525634766
train: epoch 125, loss 1.471376657485962, acc=0.441611111164093, loss=1.471376657485962
test: epoch 125, loss 1.7827603816986084, acc=0.25833332538604736, loss=1.7827603816986084
train: epoch 126, loss 1.454575777053833, acc=0.4429444372653961, loss=1.454575777053833
test: epoch 126, loss 1.7612935304641724, acc=0.25555557012557983, loss=1.7612935304641724
train: epoch 127, loss 1.4486336708068848, acc=0.44922223687171936, loss=1.4486336708068848
test: epoch 127, loss 1.7878472805023193, acc=0.2527777850627899, loss=1.7878472805023193
train: epoch 128, loss 1.4292582273483276, acc=0.45399999618530273, loss=1.4292582273483276
test: epoch 128, loss 1.748417854309082, acc=0.2611111104488373, loss=1.748417854309082
train: epoch 129, loss 1.4507683515548706, acc=0.4514999985694885, loss=1.4507683515548706
test: epoch 129, loss 1.7755812406539917, acc=0.2666666805744171, loss=1.7755812406539917
train: epoch 130, loss 1.4338514804840088, acc=0.45838889479637146, loss=1.4338514804840088
test: epoch 130, loss 1.7619006633758545, acc=0.2611111104488373, loss=1.7619006633758545
train: epoch 131, loss 1.430724859237671, acc=0.4586111009120941, loss=1.430724859237671
test: epoch 131, loss 1.7719007730484009, acc=0.25555557012557983, loss=1.7719007730484009
train: epoch 132, loss 1.4244135618209839, acc=0.46000000834465027, loss=1.4244135618209839
test: epoch 132, loss 1.7654001712799072, acc=0.2666666805744171, loss=1.7654001712799072
train: epoch 133, loss 1.412642478942871, acc=0.4623333215713501, loss=1.412642478942871
test: epoch 133, loss 1.7805413007736206, acc=0.27222222089767456, loss=1.7805413007736206
train: epoch 134, loss 1.4229652881622314, acc=0.46566668152809143, loss=1.4229652881622314
test: epoch 134, loss 1.761819839477539, acc=0.2750000059604645, loss=1.761819839477539
train: epoch 135, loss 1.400931715965271, acc=0.4753333330154419, loss=1.400931715965271
test: epoch 135, loss 1.7471789121627808, acc=0.2666666805744171, loss=1.7471789121627808
train: epoch 136, loss 1.4029597043991089, acc=0.468666672706604, loss=1.4029597043991089
test: epoch 136, loss 1.7561719417572021, acc=0.2666666805744171, loss=1.7561719417572021
train: epoch 137, loss 1.4061622619628906, acc=0.4732222259044647, loss=1.4061622619628906
test: epoch 137, loss 1.7274143695831299, acc=0.26944443583488464, loss=1.7274143695831299
train: epoch 138, loss 1.3870625495910645, acc=0.4745555520057678, loss=1.3870625495910645
test: epoch 138, loss 1.7282464504241943, acc=0.2805555462837219, loss=1.7282464504241943
train: epoch 139, loss 1.3663381338119507, acc=0.47922220826148987, loss=1.3663381338119507
test: epoch 139, loss 1.7341288328170776, acc=0.26944443583488464, loss=1.7341288328170776
train: epoch 140, loss 1.3839410543441772, acc=0.4773888885974884, loss=1.3839410543441772
test: epoch 140, loss 1.7365455627441406, acc=0.2750000059604645, loss=1.7365455627441406
train: epoch 141, loss 1.375349760055542, acc=0.4871666729450226, loss=1.375349760055542
test: epoch 141, loss 1.7326582670211792, acc=0.28333333134651184, loss=1.7326582670211792
train: epoch 142, loss 1.3870165348052979, acc=0.47955554723739624, loss=1.3870165348052979
test: epoch 142, loss 1.7473945617675781, acc=0.2638888955116272, loss=1.7473945617675781
train: epoch 143, loss 1.3705672025680542, acc=0.484333336353302, loss=1.3705672025680542
test: epoch 143, loss 1.7554594278335571, acc=0.27222222089767456, loss=1.7554594278335571
train: epoch 144, loss 1.350893259048462, acc=0.4893888831138611, loss=1.350893259048462
test: epoch 144, loss 1.7285493612289429, acc=0.27222222089767456, loss=1.7285493612289429
train: epoch 145, loss 1.3588318824768066, acc=0.49227777123451233, loss=1.3588318824768066
test: epoch 145, loss 1.7352893352508545, acc=0.28333333134651184, loss=1.7352893352508545
train: epoch 146, loss 1.3723864555358887, acc=0.48811110854148865, loss=1.3723864555358887
test: epoch 146, loss 1.7389460802078247, acc=0.28333333134651184, loss=1.7389460802078247
train: epoch 147, loss 1.3472886085510254, acc=0.49666666984558105, loss=1.3472886085510254
test: epoch 147, loss 1.7298506498336792, acc=0.26944443583488464, loss=1.7298506498336792
train: epoch 148, loss 1.3549623489379883, acc=0.5007777810096741, loss=1.3549623489379883
test: epoch 148, loss 1.7322144508361816, acc=0.2805555462837219, loss=1.7322144508361816
train: epoch 149, loss 1.3287193775177002, acc=0.5049444437026978, loss=1.3287193775177002
test: epoch 149, loss 1.7296274900436401, acc=0.2805555462837219, loss=1.7296274900436401
train: epoch 150, loss 1.3408942222595215, acc=0.503944456577301, loss=1.3408942222595215
test: epoch 150, loss 1.6914639472961426, acc=0.2888889014720917, loss=1.6914639472961426
