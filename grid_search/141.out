# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=948909656, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=948909656, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.5227925777435303, acc=0.04744444414973259, loss=3.5227925777435303
test: epoch 1, loss 3.4284932613372803, acc=0.04444444552063942, loss=3.4284932613372803
train: epoch 2, loss 3.4620954990386963, acc=0.05094444379210472, loss=3.4620954990386963
test: epoch 2, loss 3.1367347240448, acc=0.07222222536802292, loss=3.1367347240448
train: epoch 3, loss 3.269029378890991, acc=0.06861110776662827, loss=3.269029378890991
test: epoch 3, loss 4.9400553703308105, acc=0.05277777835726738, loss=4.9400553703308105
train: epoch 4, loss 2.886094331741333, acc=0.11655555665493011, loss=2.886094331741333
test: epoch 4, loss 5.7394633293151855, acc=0.04722222313284874, loss=5.7394633293151855
train: epoch 5, loss 2.68502140045166, acc=0.13438889384269714, loss=2.68502140045166
test: epoch 5, loss 5.766911506652832, acc=0.05833333358168602, loss=5.766911506652832
train: epoch 6, loss 2.554762601852417, acc=0.15683333575725555, loss=2.554762601852417
test: epoch 6, loss 5.513606071472168, acc=0.05833333358168602, loss=5.513606071472168
train: epoch 7, loss 2.474743604660034, acc=0.1727222204208374, loss=2.474743604660034
test: epoch 7, loss 5.342428684234619, acc=0.06388889253139496, loss=5.342428684234619
train: epoch 8, loss 2.416651964187622, acc=0.18405555188655853, loss=2.416651964187622
test: epoch 8, loss 5.4039177894592285, acc=0.05000000074505806, loss=5.4039177894592285
train: epoch 9, loss 2.3724071979522705, acc=0.19244444370269775, loss=2.3724071979522705
test: epoch 9, loss 5.329658508300781, acc=0.05833333358168602, loss=5.329658508300781
train: epoch 10, loss 2.322983741760254, acc=0.2038888931274414, loss=2.322983741760254
test: epoch 10, loss 5.093923568725586, acc=0.06388889253139496, loss=5.093923568725586
train: epoch 11, loss 2.285325765609741, acc=0.20844444632530212, loss=2.285325765609741
test: epoch 11, loss 5.145865440368652, acc=0.05833333358168602, loss=5.145865440368652
train: epoch 12, loss 2.2779288291931152, acc=0.2160000056028366, loss=2.2779288291931152
test: epoch 12, loss 5.0171051025390625, acc=0.05277777835726738, loss=5.0171051025390625
train: epoch 13, loss 2.2471015453338623, acc=0.22272221744060516, loss=2.2471015453338623
test: epoch 13, loss 5.110540390014648, acc=0.06111111119389534, loss=5.110540390014648
train: epoch 14, loss 2.2319376468658447, acc=0.22683332860469818, loss=2.2319376468658447
test: epoch 14, loss 4.9300537109375, acc=0.0555555559694767, loss=4.9300537109375
train: epoch 15, loss 2.2213802337646484, acc=0.22972221672534943, loss=2.2213802337646484
test: epoch 15, loss 4.759509563446045, acc=0.05833333358168602, loss=4.759509563446045
train: epoch 16, loss 2.1921157836914062, acc=0.236277773976326, loss=2.1921157836914062
test: epoch 16, loss 4.645388603210449, acc=0.06111111119389534, loss=4.645388603210449
train: epoch 17, loss 2.185579299926758, acc=0.23888888955116272, loss=2.185579299926758
test: epoch 17, loss 4.6223015785217285, acc=0.06666667014360428, loss=4.6223015785217285
train: epoch 18, loss 2.1756441593170166, acc=0.2470555603504181, loss=2.1756441593170166
test: epoch 18, loss 4.520822525024414, acc=0.07222222536802292, loss=4.520822525024414
train: epoch 19, loss 2.1636571884155273, acc=0.25011110305786133, loss=2.1636571884155273
test: epoch 19, loss 4.338655948638916, acc=0.0694444477558136, loss=4.338655948638916
train: epoch 20, loss 2.147981882095337, acc=0.2528333365917206, loss=2.147981882095337
test: epoch 20, loss 4.3765082359313965, acc=0.06111111119389534, loss=4.3765082359313965
train: epoch 21, loss 2.123673439025879, acc=0.2531111240386963, loss=2.123673439025879
test: epoch 21, loss 4.285591125488281, acc=0.07777778059244156, loss=4.285591125488281
train: epoch 22, loss 2.13167142868042, acc=0.2641666531562805, loss=2.13167142868042
test: epoch 22, loss 4.236945152282715, acc=0.08611111342906952, loss=4.236945152282715
train: epoch 23, loss 2.1016244888305664, acc=0.26600000262260437, loss=2.1016244888305664
test: epoch 23, loss 4.106784343719482, acc=0.08055555820465088, loss=4.106784343719482
train: epoch 24, loss 2.093337297439575, acc=0.27266666293144226, loss=2.093337297439575
test: epoch 24, loss 3.917412042617798, acc=0.08055555820465088, loss=3.917412042617798
train: epoch 25, loss 2.0742170810699463, acc=0.2706666588783264, loss=2.0742170810699463
test: epoch 25, loss 3.889040231704712, acc=0.0972222238779068, loss=3.889040231704712
train: epoch 26, loss 2.072690010070801, acc=0.28038889169692993, loss=2.072690010070801
test: epoch 26, loss 3.7939155101776123, acc=0.09444444626569748, loss=3.7939155101776123
train: epoch 27, loss 2.05473256111145, acc=0.2809999883174896, loss=2.05473256111145
test: epoch 27, loss 3.599170446395874, acc=0.10555555671453476, loss=3.599170446395874
train: epoch 28, loss 2.058501720428467, acc=0.28288888931274414, loss=2.058501720428467
test: epoch 28, loss 3.5536422729492188, acc=0.10000000149011612, loss=3.5536422729492188
train: epoch 29, loss 2.054497718811035, acc=0.281166672706604, loss=2.054497718811035
test: epoch 29, loss 3.3517351150512695, acc=0.08888889104127884, loss=3.3517351150512695
train: epoch 30, loss 2.045368194580078, acc=0.28922221064567566, loss=2.045368194580078
test: epoch 30, loss 3.2909364700317383, acc=0.10833333432674408, loss=3.2909364700317383
train: epoch 31, loss 2.0265626907348633, acc=0.28905555605888367, loss=2.0265626907348633
test: epoch 31, loss 3.2447152137756348, acc=0.1111111119389534, loss=3.2447152137756348
train: epoch 32, loss 2.009340286254883, acc=0.2972777783870697, loss=2.009340286254883
test: epoch 32, loss 3.139101982116699, acc=0.125, loss=3.139101982116699
train: epoch 33, loss 2.0174665451049805, acc=0.2986111044883728, loss=2.0174665451049805
test: epoch 33, loss 3.0835700035095215, acc=0.11388888955116272, loss=3.0835700035095215
train: epoch 34, loss 1.984568476676941, acc=0.3050000071525574, loss=1.984568476676941
test: epoch 34, loss 3.0832557678222656, acc=0.11944444477558136, loss=3.0832557678222656
train: epoch 35, loss 1.9916737079620361, acc=0.3079444468021393, loss=1.9916737079620361
test: epoch 35, loss 2.901465892791748, acc=0.13333334028720856, loss=2.901465892791748
train: epoch 36, loss 1.9649419784545898, acc=0.3207777738571167, loss=1.9649419784545898
test: epoch 36, loss 2.8946452140808105, acc=0.12777778506278992, loss=2.8946452140808105
train: epoch 37, loss 1.952133297920227, acc=0.3184444308280945, loss=1.952133297920227
test: epoch 37, loss 2.783952236175537, acc=0.14166666567325592, loss=2.783952236175537
train: epoch 38, loss 1.9495832920074463, acc=0.33000001311302185, loss=1.9495832920074463
test: epoch 38, loss 2.727949857711792, acc=0.13611111044883728, loss=2.727949857711792
train: epoch 39, loss 1.9278799295425415, acc=0.3336666524410248, loss=1.9278799295425415
test: epoch 39, loss 2.787086009979248, acc=0.14722222089767456, loss=2.787086009979248
train: epoch 40, loss 1.9176138639450073, acc=0.3351111114025116, loss=1.9176138639450073
test: epoch 40, loss 2.641174793243408, acc=0.14722222089767456, loss=2.641174793243408
train: epoch 41, loss 1.9235775470733643, acc=0.33677777647972107, loss=1.9235775470733643
test: epoch 41, loss 2.6095476150512695, acc=0.15555556118488312, loss=2.6095476150512695
train: epoch 42, loss 1.903530240058899, acc=0.3464444577693939, loss=1.903530240058899
test: epoch 42, loss 2.5618298053741455, acc=0.17222222685813904, loss=2.5618298053741455
train: epoch 43, loss 1.864460825920105, acc=0.35277777910232544, loss=1.864460825920105
test: epoch 43, loss 2.5448055267333984, acc=0.1805555522441864, loss=2.5448055267333984
train: epoch 44, loss 1.8799049854278564, acc=0.3555000126361847, loss=1.8799049854278564
test: epoch 44, loss 2.515249252319336, acc=0.2083333283662796, loss=2.515249252319336
train: epoch 45, loss 1.8429057598114014, acc=0.36888888478279114, loss=1.8429057598114014
test: epoch 45, loss 2.441551446914673, acc=0.19166666269302368, loss=2.441551446914673
train: epoch 46, loss 1.8553354740142822, acc=0.36738887429237366, loss=1.8553354740142822
test: epoch 46, loss 2.3940680027008057, acc=0.18611110746860504, loss=2.3940680027008057
train: epoch 47, loss 1.8418275117874146, acc=0.37299999594688416, loss=1.8418275117874146
test: epoch 47, loss 2.403304100036621, acc=0.20555555820465088, loss=2.403304100036621
train: epoch 48, loss 1.808471918106079, acc=0.3795555531978607, loss=1.808471918106079
test: epoch 48, loss 2.3598673343658447, acc=0.20277777314186096, loss=2.3598673343658447
train: epoch 49, loss 1.7939623594284058, acc=0.3883333206176758, loss=1.7939623594284058
test: epoch 49, loss 2.3371074199676514, acc=0.1944444477558136, loss=2.3371074199676514
train: epoch 50, loss 1.8009850978851318, acc=0.39311110973358154, loss=1.8009850978851318
test: epoch 50, loss 2.2429633140563965, acc=0.18333333730697632, loss=2.2429633140563965
train: epoch 51, loss 1.7735141515731812, acc=0.40227776765823364, loss=1.7735141515731812
test: epoch 51, loss 2.2055628299713135, acc=0.21666666865348816, loss=2.2055628299713135
train: epoch 52, loss 1.7407033443450928, acc=0.4087222218513489, loss=1.7407033443450928
test: epoch 52, loss 2.2205498218536377, acc=0.21666666865348816, loss=2.2205498218536377
train: epoch 53, loss 1.7367180585861206, acc=0.4134444296360016, loss=1.7367180585861206
test: epoch 53, loss 2.1926887035369873, acc=0.22777777910232544, loss=2.1926887035369873
train: epoch 54, loss 1.730713963508606, acc=0.4189999997615814, loss=1.730713963508606
test: epoch 54, loss 2.1534745693206787, acc=0.21944443881511688, loss=2.1534745693206787
train: epoch 55, loss 1.7172393798828125, acc=0.4202222228050232, loss=1.7172393798828125
test: epoch 55, loss 2.1214396953582764, acc=0.22777777910232544, loss=2.1214396953582764
train: epoch 56, loss 1.712499737739563, acc=0.42355555295944214, loss=1.712499737739563
test: epoch 56, loss 2.0978574752807617, acc=0.2361111044883728, loss=2.0978574752807617
train: epoch 57, loss 1.6922030448913574, acc=0.43666666746139526, loss=1.6922030448913574
test: epoch 57, loss 2.038999080657959, acc=0.25555557012557983, loss=2.038999080657959
train: epoch 58, loss 1.6821191310882568, acc=0.4353888928890228, loss=1.6821191310882568
test: epoch 58, loss 2.0277597904205322, acc=0.23333333432674408, loss=2.0277597904205322
train: epoch 59, loss 1.6843416690826416, acc=0.4383888840675354, loss=1.6843416690826416
test: epoch 59, loss 1.989262580871582, acc=0.2666666805744171, loss=1.989262580871582
train: epoch 60, loss 1.643506407737732, acc=0.445166677236557, loss=1.643506407737732
test: epoch 60, loss 1.9888297319412231, acc=0.2638888955116272, loss=1.9888297319412231
train: epoch 61, loss 1.6079950332641602, acc=0.45516666769981384, loss=1.6079950332641602
test: epoch 61, loss 1.9573391675949097, acc=0.25833332538604736, loss=1.9573391675949097
train: epoch 62, loss 1.6148749589920044, acc=0.4589444398880005, loss=1.6148749589920044
test: epoch 62, loss 1.933005690574646, acc=0.2750000059604645, loss=1.933005690574646
train: epoch 63, loss 1.5945851802825928, acc=0.46461111307144165, loss=1.5945851802825928
test: epoch 63, loss 1.8872843980789185, acc=0.26944443583488464, loss=1.8872843980789185
train: epoch 64, loss 1.5985831022262573, acc=0.46772223711013794, loss=1.5985831022262573
test: epoch 64, loss 1.8621058464050293, acc=0.2777777910232544, loss=1.8621058464050293
train: epoch 65, loss 1.5808582305908203, acc=0.46711111068725586, loss=1.5808582305908203
test: epoch 65, loss 1.8436411619186401, acc=0.2777777910232544, loss=1.8436411619186401
train: epoch 66, loss 1.559629201889038, acc=0.480611115694046, loss=1.559629201889038
test: epoch 66, loss 1.8463082313537598, acc=0.2750000059604645, loss=1.8463082313537598
train: epoch 67, loss 1.5630773305892944, acc=0.48605555295944214, loss=1.5630773305892944
test: epoch 67, loss 1.8231685161590576, acc=0.2916666567325592, loss=1.8231685161590576
train: epoch 68, loss 1.5425196886062622, acc=0.4841666519641876, loss=1.5425196886062622
test: epoch 68, loss 1.8105828762054443, acc=0.3055555522441864, loss=1.8105828762054443
train: epoch 69, loss 1.5153100490570068, acc=0.4942222237586975, loss=1.5153100490570068
test: epoch 69, loss 1.798092007637024, acc=0.31111112236976624, loss=1.798092007637024
train: epoch 70, loss 1.5139211416244507, acc=0.4940555691719055, loss=1.5139211416244507
test: epoch 70, loss 1.7747279405593872, acc=0.3055555522441864, loss=1.7747279405593872
train: epoch 71, loss 1.5068351030349731, acc=0.5014444589614868, loss=1.5068351030349731
test: epoch 71, loss 1.7635245323181152, acc=0.2944444417953491, loss=1.7635245323181152
train: epoch 72, loss 1.4750806093215942, acc=0.5088889002799988, loss=1.4750806093215942
test: epoch 72, loss 1.735671043395996, acc=0.31388887763023376, loss=1.735671043395996
train: epoch 73, loss 1.47469162940979, acc=0.5114444494247437, loss=1.47469162940979
test: epoch 73, loss 1.7132329940795898, acc=0.3166666626930237, loss=1.7132329940795898
train: epoch 74, loss 1.4587754011154175, acc=0.5235555768013, loss=1.4587754011154175
test: epoch 74, loss 1.7065455913543701, acc=0.32499998807907104, loss=1.7065455913543701
train: epoch 75, loss 1.4409033060073853, acc=0.5230000019073486, loss=1.4409033060073853
test: epoch 75, loss 1.7094019651412964, acc=0.3083333373069763, loss=1.7094019651412964
train: epoch 76, loss 1.4305342435836792, acc=0.5298333168029785, loss=1.4305342435836792
test: epoch 76, loss 1.6868306398391724, acc=0.32499998807907104, loss=1.6868306398391724
train: epoch 77, loss 1.4015841484069824, acc=0.5378333330154419, loss=1.4015841484069824
test: epoch 77, loss 1.6621426343917847, acc=0.31388887763023376, loss=1.6621426343917847
train: epoch 78, loss 1.3945595026016235, acc=0.5421666502952576, loss=1.3945595026016235
test: epoch 78, loss 1.6705682277679443, acc=0.31388887763023376, loss=1.6705682277679443
train: epoch 79, loss 1.3665950298309326, acc=0.5460555553436279, loss=1.3665950298309326
test: epoch 79, loss 1.6658060550689697, acc=0.3222222328186035, loss=1.6658060550689697
train: epoch 80, loss 1.3593236207962036, acc=0.5555555820465088, loss=1.3593236207962036
test: epoch 80, loss 1.6331851482391357, acc=0.32777777314186096, loss=1.6331851482391357
train: epoch 81, loss 1.3472753763198853, acc=0.5637778043746948, loss=1.3472753763198853
test: epoch 81, loss 1.6222246885299683, acc=0.3361110985279083, loss=1.6222246885299683
train: epoch 82, loss 1.321826457977295, acc=0.5656111240386963, loss=1.321826457977295
test: epoch 82, loss 1.607485294342041, acc=0.3361110985279083, loss=1.607485294342041
train: epoch 83, loss 1.301468849182129, acc=0.570277750492096, loss=1.301468849182129
test: epoch 83, loss 1.5690503120422363, acc=0.33888888359069824, loss=1.5690503120422363
train: epoch 84, loss 1.3032079935073853, acc=0.5729444622993469, loss=1.3032079935073853
test: epoch 84, loss 1.5706217288970947, acc=0.3472222089767456, loss=1.5706217288970947
train: epoch 85, loss 1.280495524406433, acc=0.5792222023010254, loss=1.280495524406433
test: epoch 85, loss 1.558166742324829, acc=0.3583333194255829, loss=1.558166742324829
train: epoch 86, loss 1.2705234289169312, acc=0.5885000228881836, loss=1.2705234289169312
test: epoch 86, loss 1.5533596277236938, acc=0.3361110985279083, loss=1.5533596277236938
train: epoch 87, loss 1.2600232362747192, acc=0.5891110897064209, loss=1.2600232362747192
test: epoch 87, loss 1.5383853912353516, acc=0.3499999940395355, loss=1.5383853912353516
train: epoch 88, loss 1.250434160232544, acc=0.6008333563804626, loss=1.250434160232544
test: epoch 88, loss 1.5415948629379272, acc=0.3499999940395355, loss=1.5415948629379272
train: epoch 89, loss 1.2250471115112305, acc=0.6062777638435364, loss=1.2250471115112305
test: epoch 89, loss 1.59103262424469, acc=0.35555556416511536, loss=1.59103262424469
train: epoch 90, loss 1.2182927131652832, acc=0.6077777743339539, loss=1.2182927131652832
test: epoch 90, loss 1.5422953367233276, acc=0.3444444537162781, loss=1.5422953367233276
train: epoch 91, loss 1.2139614820480347, acc=0.6071110963821411, loss=1.2139614820480347
test: epoch 91, loss 1.5250505208969116, acc=0.35555556416511536, loss=1.5250505208969116
train: epoch 92, loss 1.1938316822052002, acc=0.6137222051620483, loss=1.1938316822052002
test: epoch 92, loss 1.5094550848007202, acc=0.36944442987442017, loss=1.5094550848007202
train: epoch 93, loss 1.189394474029541, acc=0.6221666932106018, loss=1.189394474029541
test: epoch 93, loss 1.5187879800796509, acc=0.35555556416511536, loss=1.5187879800796509
train: epoch 94, loss 1.1610033512115479, acc=0.6297222375869751, loss=1.1610033512115479
test: epoch 94, loss 1.5010285377502441, acc=0.3722222149372101, loss=1.5010285377502441
train: epoch 95, loss 1.1508891582489014, acc=0.6320000290870667, loss=1.1508891582489014
test: epoch 95, loss 1.4991204738616943, acc=0.3611111044883728, loss=1.4991204738616943
train: epoch 96, loss 1.1484190225601196, acc=0.6300555467605591, loss=1.1484190225601196
test: epoch 96, loss 1.4934226274490356, acc=0.3722222149372101, loss=1.4934226274490356
train: epoch 97, loss 1.1354013681411743, acc=0.6399999856948853, loss=1.1354013681411743
test: epoch 97, loss 1.4860316514968872, acc=0.36944442987442017, loss=1.4860316514968872
train: epoch 98, loss 1.1136940717697144, acc=0.6416110992431641, loss=1.1136940717697144
test: epoch 98, loss 1.4754464626312256, acc=0.36666667461395264, loss=1.4754464626312256
train: epoch 99, loss 1.099890112876892, acc=0.6460555791854858, loss=1.099890112876892
test: epoch 99, loss 1.4870063066482544, acc=0.3722222149372101, loss=1.4870063066482544
train: epoch 100, loss 1.1208813190460205, acc=0.6450555324554443, loss=1.1208813190460205
test: epoch 100, loss 1.4847800731658936, acc=0.3777777850627899, loss=1.4847800731658936
train: epoch 101, loss 1.1018750667572021, acc=0.6543333530426025, loss=1.1018750667572021
test: epoch 101, loss 1.4745527505874634, acc=0.36944442987442017, loss=1.4745527505874634
train: epoch 102, loss 1.0667792558670044, acc=0.6598333120346069, loss=1.0667792558670044
test: epoch 102, loss 1.4363594055175781, acc=0.3777777850627899, loss=1.4363594055175781
train: epoch 103, loss 1.070508360862732, acc=0.6570555567741394, loss=1.070508360862732
test: epoch 103, loss 1.4417972564697266, acc=0.3888888955116272, loss=1.4417972564697266
train: epoch 104, loss 1.0574613809585571, acc=0.6605555415153503, loss=1.0574613809585571
test: epoch 104, loss 1.4225937128067017, acc=0.3916666805744171, loss=1.4225937128067017
train: epoch 105, loss 1.0475329160690308, acc=0.6724444627761841, loss=1.0475329160690308
test: epoch 105, loss 1.4222908020019531, acc=0.3888888955116272, loss=1.4222908020019531
train: epoch 106, loss 1.0333826541900635, acc=0.6748889088630676, loss=1.0333826541900635
test: epoch 106, loss 1.430215835571289, acc=0.375, loss=1.430215835571289
train: epoch 107, loss 1.034058690071106, acc=0.6739444732666016, loss=1.034058690071106
test: epoch 107, loss 1.4180392026901245, acc=0.38333332538604736, loss=1.4180392026901245
train: epoch 108, loss 1.0158299207687378, acc=0.6775000095367432, loss=1.0158299207687378
test: epoch 108, loss 1.450503945350647, acc=0.3888888955116272, loss=1.450503945350647
train: epoch 109, loss 1.0054066181182861, acc=0.6817222237586975, loss=1.0054066181182861
test: epoch 109, loss 1.4190423488616943, acc=0.3916666805744171, loss=1.4190423488616943
train: epoch 110, loss 0.985582172870636, acc=0.6863333582878113, loss=0.985582172870636
test: epoch 110, loss 1.4197216033935547, acc=0.3916666805744171, loss=1.4197216033935547
train: epoch 111, loss 0.9951130747795105, acc=0.690500020980835, loss=0.9951130747795105
test: epoch 111, loss 1.3901704549789429, acc=0.38333332538604736, loss=1.3901704549789429
train: epoch 112, loss 0.9885584712028503, acc=0.6934444308280945, loss=0.9885584712028503
test: epoch 112, loss 1.3906683921813965, acc=0.39444443583488464, loss=1.3906683921813965
train: epoch 113, loss 0.9635342955589294, acc=0.6988333463668823, loss=0.9635342955589294
test: epoch 113, loss 1.3748385906219482, acc=0.4000000059604645, loss=1.3748385906219482
train: epoch 114, loss 0.975407600402832, acc=0.6969444155693054, loss=0.975407600402832
test: epoch 114, loss 1.3979781866073608, acc=0.3861111104488373, loss=1.3979781866073608
train: epoch 115, loss 0.9406976103782654, acc=0.70333331823349, loss=0.9406976103782654
test: epoch 115, loss 1.3970714807510376, acc=0.3861111104488373, loss=1.3970714807510376
train: epoch 116, loss 0.9545518159866333, acc=0.7038333415985107, loss=0.9545518159866333
test: epoch 116, loss 1.3761385679244995, acc=0.39444443583488464, loss=1.3761385679244995
train: epoch 117, loss 0.9563657641410828, acc=0.7055555582046509, loss=0.9563657641410828
test: epoch 117, loss 1.3640542030334473, acc=0.4000000059604645, loss=1.3640542030334473
train: epoch 118, loss 0.9381114840507507, acc=0.7085555791854858, loss=0.9381114840507507
test: epoch 118, loss 1.3646317720413208, acc=0.39444443583488464, loss=1.3646317720413208
train: epoch 119, loss 0.9212838411331177, acc=0.7120555639266968, loss=0.9212838411331177
test: epoch 119, loss 1.3542920351028442, acc=0.4027777910232544, loss=1.3542920351028442
train: epoch 120, loss 0.931506872177124, acc=0.7124444246292114, loss=0.931506872177124
test: epoch 120, loss 1.3477938175201416, acc=0.4000000059604645, loss=1.3477938175201416
train: epoch 121, loss 0.906187117099762, acc=0.7201666831970215, loss=0.906187117099762
test: epoch 121, loss 1.3414647579193115, acc=0.4055555462837219, loss=1.3414647579193115
train: epoch 122, loss 0.9121954441070557, acc=0.7193333506584167, loss=0.9121954441070557
test: epoch 122, loss 1.3321423530578613, acc=0.40833333134651184, loss=1.3321423530578613
train: epoch 123, loss 0.8909110426902771, acc=0.7278888821601868, loss=0.8909110426902771
test: epoch 123, loss 1.3277778625488281, acc=0.4055555462837219, loss=1.3277778625488281
train: epoch 124, loss 0.9003744125366211, acc=0.7248888611793518, loss=0.9003744125366211
test: epoch 124, loss 1.3398613929748535, acc=0.4027777910232544, loss=1.3398613929748535
train: epoch 125, loss 0.876836359500885, acc=0.7285555601119995, loss=0.876836359500885
test: epoch 125, loss 1.3345043659210205, acc=0.4138889014720917, loss=1.3345043659210205
train: epoch 126, loss 0.8686928749084473, acc=0.7329444289207458, loss=0.8686928749084473
test: epoch 126, loss 1.3539421558380127, acc=0.4055555462837219, loss=1.3539421558380127
train: epoch 127, loss 0.8662376999855042, acc=0.7320555448532104, loss=0.8662376999855042
test: epoch 127, loss 1.3509814739227295, acc=0.41111111640930176, loss=1.3509814739227295
train: epoch 128, loss 0.8598193526268005, acc=0.7356111407279968, loss=0.8598193526268005
test: epoch 128, loss 1.345060110092163, acc=0.3916666805744171, loss=1.345060110092163
train: epoch 129, loss 0.8438480496406555, acc=0.7338888645172119, loss=0.8438480496406555
test: epoch 129, loss 1.3420318365097046, acc=0.41111111640930176, loss=1.3420318365097046
train: epoch 130, loss 0.8598117232322693, acc=0.7419999837875366, loss=0.8598117232322693
test: epoch 130, loss 1.3215185403823853, acc=0.4138889014720917, loss=1.3215185403823853
train: epoch 131, loss 0.8480355143547058, acc=0.7366666793823242, loss=0.8480355143547058
test: epoch 131, loss 1.3417145013809204, acc=0.41111111640930176, loss=1.3417145013809204
train: epoch 132, loss 0.8368067741394043, acc=0.7468888759613037, loss=0.8368067741394043
test: epoch 132, loss 1.3464704751968384, acc=0.4277777671813965, loss=1.3464704751968384
train: epoch 133, loss 0.8419069051742554, acc=0.7419999837875366, loss=0.8419069051742554
test: epoch 133, loss 1.3146836757659912, acc=0.4166666567325592, loss=1.3146836757659912
train: epoch 134, loss 0.8244880437850952, acc=0.7479444742202759, loss=0.8244880437850952
test: epoch 134, loss 1.3473092317581177, acc=0.4333333373069763, loss=1.3473092317581177
train: epoch 135, loss 0.8162166476249695, acc=0.753166675567627, loss=0.8162166476249695
test: epoch 135, loss 1.3049561977386475, acc=0.4305555522441864, loss=1.3049561977386475
train: epoch 136, loss 0.804706871509552, acc=0.7520555257797241, loss=0.804706871509552
test: epoch 136, loss 1.3286337852478027, acc=0.4138889014720917, loss=1.3286337852478027
train: epoch 137, loss 0.8197237253189087, acc=0.7565000057220459, loss=0.8197237253189087
test: epoch 137, loss 1.3133623600006104, acc=0.42500001192092896, loss=1.3133623600006104
train: epoch 138, loss 0.8007457852363586, acc=0.7591666579246521, loss=0.8007457852363586
test: epoch 138, loss 1.323749303817749, acc=0.4138889014720917, loss=1.323749303817749
train: epoch 139, loss 0.7916569709777832, acc=0.7591111063957214, loss=0.7916569709777832
test: epoch 139, loss 1.3318276405334473, acc=0.40833333134651184, loss=1.3318276405334473
train: epoch 140, loss 0.8015750050544739, acc=0.7578333616256714, loss=0.8015750050544739
test: epoch 140, loss 1.3174248933792114, acc=0.4277777671813965, loss=1.3174248933792114
train: epoch 141, loss 0.7968111634254456, acc=0.7594444155693054, loss=0.7968111634254456
test: epoch 141, loss 1.3284156322479248, acc=0.4166666567325592, loss=1.3284156322479248
train: epoch 142, loss 0.8047077655792236, acc=0.7599999904632568, loss=0.8047077655792236
test: epoch 142, loss 1.3157862424850464, acc=0.4194444417953491, loss=1.3157862424850464
train: epoch 143, loss 0.7762435674667358, acc=0.768833339214325, loss=0.7762435674667358
test: epoch 143, loss 1.3074496984481812, acc=0.4138889014720917, loss=1.3074496984481812
train: epoch 144, loss 0.7709557414054871, acc=0.7672222256660461, loss=0.7709557414054871
test: epoch 144, loss 1.31435227394104, acc=0.4333333373069763, loss=1.31435227394104
train: epoch 145, loss 0.7721430659294128, acc=0.7716666460037231, loss=0.7721430659294128
test: epoch 145, loss 1.3167670965194702, acc=0.42222222685813904, loss=1.3167670965194702
train: epoch 146, loss 0.7722393870353699, acc=0.7702777981758118, loss=0.7722393870353699
test: epoch 146, loss 1.335022211074829, acc=0.4277777671813965, loss=1.335022211074829
train: epoch 147, loss 0.7489335536956787, acc=0.7712222337722778, loss=0.7489335536956787
test: epoch 147, loss 1.3339526653289795, acc=0.4277777671813965, loss=1.3339526653289795
train: epoch 148, loss 0.7534874677658081, acc=0.7753333449363708, loss=0.7534874677658081
test: epoch 148, loss 1.3188765048980713, acc=0.4138889014720917, loss=1.3188765048980713
train: epoch 149, loss 0.731073796749115, acc=0.777388870716095, loss=0.731073796749115
test: epoch 149, loss 1.2987481355667114, acc=0.4416666626930237, loss=1.2987481355667114
train: epoch 150, loss 0.7402868866920471, acc=0.781166672706604, loss=0.7402868866920471
test: epoch 150, loss 1.2996879816055298, acc=0.4305555522441864, loss=1.2996879816055298
