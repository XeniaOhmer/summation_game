# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=2.0", "--temp_decay=1", "--one_hot=1", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=2134111715, receiver_embed_dim=128, save_run=0, temp_decay=1.0, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=2134111715, receiver_embed_dim=128, save_run=False, temp_decay=1.0, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.6876296997070312, acc=0.11988888680934906, loss=2.6876296997070312
test: epoch 1, loss 4.790482044219971, acc=0.08611111342906952, loss=4.790482044219971
train: epoch 2, loss 1.476318120956421, acc=0.4152222275733948, loss=1.476318120956421
test: epoch 2, loss 5.823479652404785, acc=0.0972222238779068, loss=5.823479652404785
train: epoch 3, loss 0.8770567774772644, acc=0.6548333168029785, loss=0.8770567774772644
test: epoch 3, loss 4.108793258666992, acc=0.1944444477558136, loss=4.108793258666992
train: epoch 4, loss 0.604177713394165, acc=0.7657777667045593, loss=0.604177713394165
test: epoch 4, loss 4.136173725128174, acc=0.16111111640930176, loss=4.136173725128174
train: epoch 5, loss 0.4723290205001831, acc=0.8160555362701416, loss=0.4723290205001831
test: epoch 5, loss 2.5416371822357178, acc=0.3083333373069763, loss=2.5416371822357178
train: epoch 6, loss 0.3738359808921814, acc=0.8551666736602783, loss=0.3738359808921814
test: epoch 6, loss 3.094958543777466, acc=0.3472222089767456, loss=3.094958543777466
train: epoch 7, loss 0.30854904651641846, acc=0.8821666836738586, loss=0.30854904651641846
test: epoch 7, loss 3.569103717803955, acc=0.2888889014720917, loss=3.569103717803955
train: epoch 8, loss 0.2881433665752411, acc=0.8945000171661377, loss=0.2881433665752411
test: epoch 8, loss 3.0994644165039062, acc=0.30000001192092896, loss=3.0994644165039062
train: epoch 9, loss 0.24056880176067352, acc=0.9121111035346985, loss=0.24056880176067352
test: epoch 9, loss 3.1380066871643066, acc=0.3361110985279083, loss=3.1380066871643066
train: epoch 10, loss 0.21192871034145355, acc=0.9281666874885559, loss=0.21192871034145355
test: epoch 10, loss 2.3564693927764893, acc=0.33888888359069824, loss=2.3564693927764893
train: epoch 11, loss 0.2081521898508072, acc=0.9292222261428833, loss=0.2081521898508072
test: epoch 11, loss 2.60518741607666, acc=0.3333333432674408, loss=2.60518741607666
train: epoch 12, loss 0.1738150268793106, acc=0.9400555491447449, loss=0.1738150268793106
test: epoch 12, loss 2.5828239917755127, acc=0.3611111044883728, loss=2.5828239917755127
train: epoch 13, loss 0.17351391911506653, acc=0.9431666731834412, loss=0.17351391911506653
test: epoch 13, loss 2.3623173236846924, acc=0.3638888895511627, loss=2.3623173236846924
train: epoch 14, loss 0.17267873883247375, acc=0.9433333277702332, loss=0.17267873883247375
test: epoch 14, loss 2.2400991916656494, acc=0.375, loss=2.2400991916656494
train: epoch 15, loss 0.15468668937683105, acc=0.9470000267028809, loss=0.15468668937683105
test: epoch 15, loss 2.8477683067321777, acc=0.3305555582046509, loss=2.8477683067321777
train: epoch 16, loss 0.1330336332321167, acc=0.9555555582046509, loss=0.1330336332321167
test: epoch 16, loss 2.1344492435455322, acc=0.4611110985279083, loss=2.1344492435455322
train: epoch 17, loss 0.1411782056093216, acc=0.9536111354827881, loss=0.1411782056093216
test: epoch 17, loss 2.486501693725586, acc=0.4444444477558136, loss=2.486501693725586
train: epoch 18, loss 0.13372185826301575, acc=0.9570000171661377, loss=0.13372185826301575
test: epoch 18, loss 3.005957841873169, acc=0.32499998807907104, loss=3.005957841873169
train: epoch 19, loss 0.12781043350696564, acc=0.9611111283302307, loss=0.12781043350696564
test: epoch 19, loss 2.9163501262664795, acc=0.3638888895511627, loss=2.9163501262664795
train: epoch 20, loss 0.10846452414989471, acc=0.9637777805328369, loss=0.10846452414989471
test: epoch 20, loss 2.1216959953308105, acc=0.4749999940395355, loss=2.1216959953308105
train: epoch 21, loss 0.11496575176715851, acc=0.9631111025810242, loss=0.11496575176715851
test: epoch 21, loss 2.5803773403167725, acc=0.3888888955116272, loss=2.5803773403167725
train: epoch 22, loss 0.09750340133905411, acc=0.9704999923706055, loss=0.09750340133905411
test: epoch 22, loss 2.628894329071045, acc=0.46666666865348816, loss=2.628894329071045
train: epoch 23, loss 0.09466957300901413, acc=0.9714444279670715, loss=0.09466957300901413
test: epoch 23, loss 2.711660861968994, acc=0.42500001192092896, loss=2.711660861968994
train: epoch 24, loss 0.10141423344612122, acc=0.9687777757644653, loss=0.10141423344612122
test: epoch 24, loss 2.6486024856567383, acc=0.46388888359069824, loss=2.6486024856567383
train: epoch 25, loss 0.09797468781471252, acc=0.971833348274231, loss=0.09797468781471252
test: epoch 25, loss 2.633176326751709, acc=0.4277777671813965, loss=2.633176326751709
train: epoch 26, loss 0.09033501148223877, acc=0.9723888635635376, loss=0.09033501148223877
test: epoch 26, loss 2.4866514205932617, acc=0.43888887763023376, loss=2.4866514205932617
train: epoch 27, loss 0.0818915143609047, acc=0.9753333330154419, loss=0.0818915143609047
test: epoch 27, loss 2.967261791229248, acc=0.4194444417953491, loss=2.967261791229248
train: epoch 28, loss 0.07764653861522675, acc=0.975944459438324, loss=0.07764653861522675
test: epoch 28, loss 2.175956964492798, acc=0.46666666865348816, loss=2.175956964492798
train: epoch 29, loss 0.07427655160427094, acc=0.9766666889190674, loss=0.07427655160427094
test: epoch 29, loss 1.8924885988235474, acc=0.45277777314186096, loss=1.8924885988235474
train: epoch 30, loss 0.07913831621408463, acc=0.9763333201408386, loss=0.07913831621408463
test: epoch 30, loss 2.444343090057373, acc=0.45277777314186096, loss=2.444343090057373
train: epoch 31, loss 0.07606017589569092, acc=0.976111114025116, loss=0.07606017589569092
test: epoch 31, loss 1.919389247894287, acc=0.5611110925674438, loss=1.919389247894287
train: epoch 32, loss 0.06470251828432083, acc=0.9801111221313477, loss=0.06470251828432083
test: epoch 32, loss 2.6854989528656006, acc=0.43611112236976624, loss=2.6854989528656006
train: epoch 33, loss 0.06650623679161072, acc=0.9785555601119995, loss=0.06650623679161072
test: epoch 33, loss 2.2566616535186768, acc=0.5166666507720947, loss=2.2566616535186768
train: epoch 34, loss 0.06890327483415604, acc=0.9791666865348816, loss=0.06890327483415604
test: epoch 34, loss 1.8793569803237915, acc=0.519444465637207, loss=1.8793569803237915
train: epoch 35, loss 0.051815375685691833, acc=0.9858333468437195, loss=0.051815375685691833
test: epoch 35, loss 1.9297163486480713, acc=0.5361111164093018, loss=1.9297163486480713
train: epoch 36, loss 0.057994499802589417, acc=0.9820555448532104, loss=0.057994499802589417
test: epoch 36, loss 2.8816919326782227, acc=0.5055555701255798, loss=2.8816919326782227
train: epoch 37, loss 0.05546177178621292, acc=0.9841111302375793, loss=0.05546177178621292
test: epoch 37, loss 2.7837228775024414, acc=0.519444465637207, loss=2.7837228775024414
train: epoch 38, loss 0.07209568470716476, acc=0.9798333048820496, loss=0.07209568470716476
test: epoch 38, loss 1.8931201696395874, acc=0.5555555820465088, loss=1.8931201696395874
train: epoch 39, loss 0.04061359539628029, acc=0.9878333210945129, loss=0.04061359539628029
test: epoch 39, loss 2.093508005142212, acc=0.5722222328186035, loss=2.093508005142212
train: epoch 40, loss 0.057353027164936066, acc=0.9826111197471619, loss=0.057353027164936066
test: epoch 40, loss 1.979089379310608, acc=0.5305555462837219, loss=1.979089379310608
train: epoch 41, loss 0.03987798094749451, acc=0.9890000224113464, loss=0.03987798094749451
test: epoch 41, loss 2.358407974243164, acc=0.5888888835906982, loss=2.358407974243164
train: epoch 42, loss 0.050462450832128525, acc=0.9853333234786987, loss=0.050462450832128525
test: epoch 42, loss 2.0968375205993652, acc=0.5805555582046509, loss=2.0968375205993652
train: epoch 43, loss 0.04894338175654411, acc=0.9867777824401855, loss=0.04894338175654411
test: epoch 43, loss 1.140428900718689, acc=0.6611111164093018, loss=1.140428900718689
train: epoch 44, loss 0.05938504636287689, acc=0.9839444160461426, loss=0.05938504636287689
test: epoch 44, loss 2.202702045440674, acc=0.644444465637207, loss=2.202702045440674
train: epoch 45, loss 0.035224027931690216, acc=0.9900555610656738, loss=0.035224027931690216
test: epoch 45, loss 2.0179355144500732, acc=0.5777778029441833, loss=2.0179355144500732
train: epoch 46, loss 0.0424988716840744, acc=0.987666666507721, loss=0.0424988716840744
test: epoch 46, loss 2.2216298580169678, acc=0.6194444298744202, loss=2.2216298580169678
train: epoch 47, loss 0.041595373302698135, acc=0.9893333315849304, loss=0.041595373302698135
test: epoch 47, loss 1.3704367876052856, acc=0.6944444179534912, loss=1.3704367876052856
train: epoch 48, loss 0.039494507014751434, acc=0.9891666769981384, loss=0.039494507014751434
test: epoch 48, loss 1.8954015970230103, acc=0.6583333611488342, loss=1.8954015970230103
train: epoch 49, loss 0.041152749210596085, acc=0.9880555272102356, loss=0.041152749210596085
test: epoch 49, loss 2.1291189193725586, acc=0.6111111044883728, loss=2.1291189193725586
train: epoch 50, loss 0.04366516321897507, acc=0.9871666431427002, loss=0.04366516321897507
test: epoch 50, loss 1.4254276752471924, acc=0.7138888835906982, loss=1.4254276752471924
train: epoch 51, loss 0.036440856754779816, acc=0.9896110892295837, loss=0.036440856754779816
test: epoch 51, loss 1.1165568828582764, acc=0.7083333134651184, loss=1.1165568828582764
train: epoch 52, loss 0.03208938613533974, acc=0.9920555353164673, loss=0.03208938613533974
test: epoch 52, loss 1.528712272644043, acc=0.7166666388511658, loss=1.528712272644043
train: epoch 53, loss 0.04045553877949715, acc=0.9891111254692078, loss=0.04045553877949715
test: epoch 53, loss 1.6773234605789185, acc=0.7111111283302307, loss=1.6773234605789185
train: epoch 54, loss 0.0321846641600132, acc=0.99144446849823, loss=0.0321846641600132
test: epoch 54, loss 1.5292290449142456, acc=0.7361111044883728, loss=1.5292290449142456
train: epoch 55, loss 0.025765562430024147, acc=0.9920555353164673, loss=0.025765562430024147
test: epoch 55, loss 1.2746531963348389, acc=0.7333333492279053, loss=1.2746531963348389
train: epoch 56, loss 0.04084322601556778, acc=0.9890000224113464, loss=0.04084322601556778
test: epoch 56, loss 2.189312219619751, acc=0.6722221970558167, loss=2.189312219619751
train: epoch 57, loss 0.04213384538888931, acc=0.9882222414016724, loss=0.04213384538888931
test: epoch 57, loss 0.8754249215126038, acc=0.769444465637207, loss=0.8754249215126038
train: epoch 58, loss 0.031624432653188705, acc=0.9908333420753479, loss=0.031624432653188705
test: epoch 58, loss 1.3792234659194946, acc=0.7388888597488403, loss=1.3792234659194946
train: epoch 59, loss 0.025025494396686554, acc=0.9928333163261414, loss=0.025025494396686554
test: epoch 59, loss 1.1950942277908325, acc=0.7944444417953491, loss=1.1950942277908325
train: epoch 60, loss 0.036996349692344666, acc=0.9904999732971191, loss=0.036996349692344666
test: epoch 60, loss 1.242763638496399, acc=0.7749999761581421, loss=1.242763638496399
train: epoch 61, loss 0.026252998039126396, acc=0.9927777647972107, loss=0.026252998039126396
test: epoch 61, loss 0.9934806227684021, acc=0.8055555820465088, loss=0.9934806227684021
train: epoch 62, loss 0.026358582079410553, acc=0.9929999709129333, loss=0.026358582079410553
test: epoch 62, loss 1.339971661567688, acc=0.8111110925674438, loss=1.339971661567688
train: epoch 63, loss 0.022422170266509056, acc=0.9945555329322815, loss=0.022422170266509056
test: epoch 63, loss 1.3331917524337769, acc=0.7861111164093018, loss=1.3331917524337769
train: epoch 64, loss 0.02671358920633793, acc=0.9926111102104187, loss=0.02671358920633793
test: epoch 64, loss 1.001543402671814, acc=0.855555534362793, loss=1.001543402671814
train: epoch 65, loss 0.026552531868219376, acc=0.9936666488647461, loss=0.026552531868219376
test: epoch 65, loss 1.0396757125854492, acc=0.7777777910232544, loss=1.0396757125854492
train: epoch 66, loss 0.03006928414106369, acc=0.9922778010368347, loss=0.03006928414106369
test: epoch 66, loss 0.7151091694831848, acc=0.855555534362793, loss=0.7151091694831848
train: epoch 67, loss 0.018149662762880325, acc=0.9956666827201843, loss=0.018149662762880325
test: epoch 67, loss 0.6067630648612976, acc=0.8972222208976746, loss=0.6067630648612976
train: epoch 68, loss 0.023599261417984962, acc=0.9933888912200928, loss=0.023599261417984962
test: epoch 68, loss 0.5959942936897278, acc=0.8999999761581421, loss=0.5959942936897278
train: epoch 69, loss 0.02305147424340248, acc=0.9942222237586975, loss=0.02305147424340248
test: epoch 69, loss 0.5884160399436951, acc=0.9194444417953491, loss=0.5884160399436951
train: epoch 70, loss 0.019687874242663383, acc=0.9941111207008362, loss=0.019687874242663383
test: epoch 70, loss 0.4857509434223175, acc=0.9194444417953491, loss=0.4857509434223175
train: epoch 71, loss 0.019947774708271027, acc=0.9947222471237183, loss=0.019947774708271027
test: epoch 71, loss 0.38940998911857605, acc=0.9361110925674438, loss=0.38940998911857605
train: epoch 72, loss 0.016697941347956657, acc=0.9961110949516296, loss=0.016697941347956657
test: epoch 72, loss 0.3307475447654724, acc=0.9444444179534912, loss=0.3307475447654724
train: epoch 73, loss 0.023982863873243332, acc=0.9942777752876282, loss=0.023982863873243332
test: epoch 73, loss 0.4052931070327759, acc=0.9444444179534912, loss=0.4052931070327759
train: epoch 74, loss 0.018451565876603127, acc=0.9959444403648376, loss=0.018451565876603127
test: epoch 74, loss 0.589798629283905, acc=0.875, loss=0.589798629283905
train: epoch 75, loss 0.023170998319983482, acc=0.9946110844612122, loss=0.023170998319983482
test: epoch 75, loss 0.3730495572090149, acc=0.9194444417953491, loss=0.3730495572090149
train: epoch 76, loss 0.016443053260445595, acc=0.9961666464805603, loss=0.016443053260445595
test: epoch 76, loss 0.5459251403808594, acc=0.8972222208976746, loss=0.5459251403808594
train: epoch 77, loss 0.015580904670059681, acc=0.9961110949516296, loss=0.015580904670059681
test: epoch 77, loss 0.7588309645652771, acc=0.8972222208976746, loss=0.7588309645652771
train: epoch 78, loss 0.017199594527482986, acc=0.996055543422699, loss=0.017199594527482986
test: epoch 78, loss 0.38658449053764343, acc=0.925000011920929, loss=0.38658449053764343
train: epoch 79, loss 0.022672878578305244, acc=0.995888888835907, loss=0.022672878578305244
test: epoch 79, loss 0.28478139638900757, acc=0.9611111283302307, loss=0.28478139638900757
train: epoch 80, loss 0.012765108607709408, acc=0.9966111183166504, loss=0.012765108607709408
test: epoch 80, loss 0.2689494490623474, acc=0.9555555582046509, loss=0.2689494490623474
train: epoch 81, loss 0.02350347861647606, acc=0.9947777986526489, loss=0.02350347861647606
test: epoch 81, loss 0.24313956499099731, acc=0.9555555582046509, loss=0.24313956499099731
train: epoch 82, loss 0.013302688486874104, acc=0.9973888993263245, loss=0.013302688486874104
test: epoch 82, loss 0.26640239357948303, acc=0.9305555820465088, loss=0.26640239357948303
train: epoch 83, loss 0.018193624913692474, acc=0.996666669845581, loss=0.018193624913692474
test: epoch 83, loss 0.27093270421028137, acc=0.9305555820465088, loss=0.27093270421028137
train: epoch 84, loss 0.017676163464784622, acc=0.9964444637298584, loss=0.017676163464784622
test: epoch 84, loss 0.28193041682243347, acc=0.9611111283302307, loss=0.28193041682243347
train: epoch 85, loss 0.01211434043943882, acc=0.9973333477973938, loss=0.01211434043943882
test: epoch 85, loss 0.19201715290546417, acc=0.9666666388511658, loss=0.19201715290546417
train: epoch 86, loss 0.014747501350939274, acc=0.9969444274902344, loss=0.014747501350939274
test: epoch 86, loss 0.19724225997924805, acc=0.9611111283302307, loss=0.19724225997924805
train: epoch 87, loss 0.023417862132191658, acc=0.9961110949516296, loss=0.023417862132191658
test: epoch 87, loss 0.27704715728759766, acc=0.949999988079071, loss=0.27704715728759766
train: epoch 88, loss 0.006312001496553421, acc=0.9986110925674438, loss=0.006312001496553421
test: epoch 88, loss 0.25324416160583496, acc=0.9694444537162781, loss=0.25324416160583496
train: epoch 89, loss 0.010880297049880028, acc=0.9978333115577698, loss=0.010880297049880028
test: epoch 89, loss 0.2991255819797516, acc=0.9694444537162781, loss=0.2991255819797516
train: epoch 90, loss 0.014674519188702106, acc=0.9963333606719971, loss=0.014674519188702106
test: epoch 90, loss 0.20021039247512817, acc=0.9722222089767456, loss=0.20021039247512817
train: epoch 91, loss 0.01081456895917654, acc=0.9978333115577698, loss=0.01081456895917654
test: epoch 91, loss 0.2127836048603058, acc=0.9750000238418579, loss=0.2127836048603058
train: epoch 92, loss 0.00911273155361414, acc=0.9984444379806519, loss=0.00911273155361414
test: epoch 92, loss 0.30307626724243164, acc=0.9722222089767456, loss=0.30307626724243164
train: epoch 93, loss 0.01241355575621128, acc=0.9975000023841858, loss=0.01241355575621128
test: epoch 93, loss 0.16496749222278595, acc=0.9750000238418579, loss=0.16496749222278595
train: epoch 94, loss 0.011816169135272503, acc=0.9979444742202759, loss=0.011816169135272503
test: epoch 94, loss 0.18668785691261292, acc=0.9750000238418579, loss=0.18668785691261292
train: epoch 95, loss 0.011356589384377003, acc=0.9975555539131165, loss=0.011356589384377003
test: epoch 95, loss 0.2490616738796234, acc=0.9583333134651184, loss=0.2490616738796234
train: epoch 96, loss 0.012555252760648727, acc=0.9976111054420471, loss=0.012555252760648727
test: epoch 96, loss 0.19658660888671875, acc=0.9722222089767456, loss=0.19658660888671875
train: epoch 97, loss 0.0077744098380208015, acc=0.9987778067588806, loss=0.0077744098380208015
test: epoch 97, loss 0.17331936955451965, acc=0.9750000238418579, loss=0.17331936955451965
train: epoch 98, loss 0.02138564921915531, acc=0.996999979019165, loss=0.02138564921915531
test: epoch 98, loss 0.2128424197435379, acc=0.9666666388511658, loss=0.2128424197435379
train: epoch 99, loss 0.009630140848457813, acc=0.9982222318649292, loss=0.009630140848457813
test: epoch 99, loss 0.1603575050830841, acc=0.9750000238418579, loss=0.1603575050830841
train: epoch 100, loss 0.009906871244311333, acc=0.9979444742202759, loss=0.009906871244311333
test: epoch 100, loss 0.19360440969467163, acc=0.9750000238418579, loss=0.19360440969467163
train: epoch 101, loss 0.01676463522017002, acc=0.9965000152587891, loss=0.01676463522017002
test: epoch 101, loss 0.15607254207134247, acc=0.9750000238418579, loss=0.15607254207134247
train: epoch 102, loss 0.016827024519443512, acc=0.9976111054420471, loss=0.016827024519443512
test: epoch 102, loss 0.17196574807167053, acc=0.9722222089767456, loss=0.17196574807167053
train: epoch 103, loss 0.004957573022693396, acc=0.9991666674613953, loss=0.004957573022693396
test: epoch 103, loss 0.23902501165866852, acc=0.9722222089767456, loss=0.23902501165866852
train: epoch 104, loss 0.004839845933020115, acc=0.9990555644035339, loss=0.004839845933020115
test: epoch 104, loss 0.15059727430343628, acc=0.9750000238418579, loss=0.15059727430343628
train: epoch 105, loss 0.005336026661098003, acc=0.9988889098167419, loss=0.005336026661098003
test: epoch 105, loss 0.1699773371219635, acc=0.9750000238418579, loss=0.1699773371219635
train: epoch 106, loss 0.022848760709166527, acc=0.9967222213745117, loss=0.022848760709166527
test: epoch 106, loss 0.12359952926635742, acc=0.9750000238418579, loss=0.12359952926635742
train: epoch 107, loss 0.005849288310855627, acc=0.9990555644035339, loss=0.005849288310855627
test: epoch 107, loss 0.1435052752494812, acc=0.9750000238418579, loss=0.1435052752494812
train: epoch 108, loss 0.010428348556160927, acc=0.9983333349227905, loss=0.010428348556160927
test: epoch 108, loss 0.19787347316741943, acc=0.9722222089767456, loss=0.19787347316741943
train: epoch 109, loss 0.009187123738229275, acc=0.9983888864517212, loss=0.009187123738229275
test: epoch 109, loss 0.18918466567993164, acc=0.9750000238418579, loss=0.18918466567993164
train: epoch 110, loss 0.002480416325852275, acc=0.9993888735771179, loss=0.002480416325852275
test: epoch 110, loss 0.259805291891098, acc=0.9722222089767456, loss=0.259805291891098
train: epoch 111, loss 0.008104013279080391, acc=0.9983333349227905, loss=0.008104013279080391
test: epoch 111, loss 0.17702339589595795, acc=0.9722222089767456, loss=0.17702339589595795
train: epoch 112, loss 0.010114405304193497, acc=0.9988333582878113, loss=0.010114405304193497
test: epoch 112, loss 0.2696739137172699, acc=0.9750000238418579, loss=0.2696739137172699
train: epoch 113, loss 0.005695969797670841, acc=0.9990000128746033, loss=0.005695969797670841
test: epoch 113, loss 0.20949257910251617, acc=0.9750000238418579, loss=0.20949257910251617
train: epoch 114, loss 0.008999492973089218, acc=0.9983888864517212, loss=0.008999492973089218
test: epoch 114, loss 0.13471165299415588, acc=0.9750000238418579, loss=0.13471165299415588
train: epoch 115, loss 0.008236623369157314, acc=0.9988333582878113, loss=0.008236623369157314
test: epoch 115, loss 0.21521928906440735, acc=0.9750000238418579, loss=0.21521928906440735
train: epoch 116, loss 0.004615250043570995, acc=0.9991666674613953, loss=0.004615250043570995
test: epoch 116, loss 0.19368451833724976, acc=0.9750000238418579, loss=0.19368451833724976
train: epoch 117, loss 0.01245267502963543, acc=0.9981666803359985, loss=0.01245267502963543
test: epoch 117, loss 0.1613617241382599, acc=0.9750000238418579, loss=0.1613617241382599
train: epoch 118, loss 0.0034124800004065037, acc=0.9993333220481873, loss=0.0034124800004065037
test: epoch 118, loss 0.2164469063282013, acc=0.9722222089767456, loss=0.2164469063282013
train: epoch 119, loss 0.0031819322612136602, acc=0.9992777705192566, loss=0.0031819322612136602
test: epoch 119, loss 0.12502126395702362, acc=0.9750000238418579, loss=0.12502126395702362
train: epoch 120, loss 0.014217620715498924, acc=0.9987778067588806, loss=0.014217620715498924
test: epoch 120, loss 0.13595518469810486, acc=0.9722222089767456, loss=0.13595518469810486
train: epoch 121, loss 0.006932889111340046, acc=0.9990000128746033, loss=0.006932889111340046
test: epoch 121, loss 0.18084068596363068, acc=0.9750000238418579, loss=0.18084068596363068
train: epoch 122, loss 0.003713910235092044, acc=0.9992222189903259, loss=0.003713910235092044
test: epoch 122, loss 0.16368518769741058, acc=0.9750000238418579, loss=0.16368518769741058
train: epoch 123, loss 0.013332398608326912, acc=0.9983333349227905, loss=0.013332398608326912
test: epoch 123, loss 0.1747230440378189, acc=0.9750000238418579, loss=0.1747230440378189
train: epoch 124, loss 0.0007601092220284045, acc=0.999833345413208, loss=0.0007601092220284045
test: epoch 124, loss 0.2034025937318802, acc=0.9750000238418579, loss=0.2034025937318802
train: epoch 125, loss 0.007005594205111265, acc=0.9989444613456726, loss=0.007005594205111265
test: epoch 125, loss 0.36765578389167786, acc=0.9666666388511658, loss=0.36765578389167786
train: epoch 126, loss 0.0072137825191020966, acc=0.9985555410385132, loss=0.0072137825191020966
test: epoch 126, loss 0.2290564626455307, acc=0.9750000238418579, loss=0.2290564626455307
train: epoch 127, loss 0.002437310991808772, acc=0.9993888735771179, loss=0.002437310991808772
test: epoch 127, loss 0.2819640636444092, acc=0.9750000238418579, loss=0.2819640636444092
train: epoch 128, loss 0.0019014455610886216, acc=0.9997222423553467, loss=0.0019014455610886216
test: epoch 128, loss 0.17215262353420258, acc=0.9750000238418579, loss=0.17215262353420258
train: epoch 129, loss 0.00750426109880209, acc=0.9986666440963745, loss=0.00750426109880209
test: epoch 129, loss 0.18948771059513092, acc=0.9750000238418579, loss=0.18948771059513092
train: epoch 130, loss 0.002068024594336748, acc=0.999666690826416, loss=0.002068024594336748
test: epoch 130, loss 0.18419335782527924, acc=0.9750000238418579, loss=0.18419335782527924
train: epoch 131, loss 0.009864150546491146, acc=0.9983888864517212, loss=0.009864150546491146
test: epoch 131, loss 0.17550291121006012, acc=0.9750000238418579, loss=0.17550291121006012
train: epoch 132, loss 0.013663633726537228, acc=0.9978333115577698, loss=0.013663633726537228
test: epoch 132, loss 0.12104129046201706, acc=0.9750000238418579, loss=0.12104129046201706
train: epoch 133, loss 0.0010211719200015068, acc=0.9997777938842773, loss=0.0010211719200015068
test: epoch 133, loss 0.2026984989643097, acc=0.9750000238418579, loss=0.2026984989643097
train: epoch 134, loss 0.010359986685216427, acc=0.9987778067588806, loss=0.010359986685216427
test: epoch 134, loss 0.14669643342494965, acc=0.9722222089767456, loss=0.14669643342494965
train: epoch 135, loss 0.001036980887874961, acc=0.9997777938842773, loss=0.001036980887874961
test: epoch 135, loss 0.18983669579029083, acc=0.9750000238418579, loss=0.18983669579029083
train: epoch 136, loss 0.006270773243159056, acc=0.9992777705192566, loss=0.006270773243159056
test: epoch 136, loss 0.17246276140213013, acc=0.9750000238418579, loss=0.17246276140213013
train: epoch 137, loss 0.011193874292075634, acc=0.9983333349227905, loss=0.011193874292075634
test: epoch 137, loss 0.19062332808971405, acc=0.9750000238418579, loss=0.19062332808971405
train: epoch 138, loss 0.00575122470036149, acc=0.9994444251060486, loss=0.00575122470036149
test: epoch 138, loss 0.1997608095407486, acc=0.9750000238418579, loss=0.1997608095407486
train: epoch 139, loss 0.005291467532515526, acc=0.9991666674613953, loss=0.005291467532515526
test: epoch 139, loss 0.24134933948516846, acc=0.9750000238418579, loss=0.24134933948516846
train: epoch 140, loss 0.005624325480312109, acc=0.9991111159324646, loss=0.005624325480312109
test: epoch 140, loss 0.16879907250404358, acc=0.9750000238418579, loss=0.16879907250404358
train: epoch 141, loss 0.011277309618890285, acc=0.9992777705192566, loss=0.011277309618890285
test: epoch 141, loss 0.180470272898674, acc=0.9750000238418579, loss=0.180470272898674
train: epoch 142, loss 0.0036540087312459946, acc=0.9994444251060486, loss=0.0036540087312459946
test: epoch 142, loss 0.18830794095993042, acc=0.9750000238418579, loss=0.18830794095993042
train: epoch 143, loss 0.009678782895207405, acc=0.9984999895095825, loss=0.009678782895207405
test: epoch 143, loss 0.14963550865650177, acc=0.9750000238418579, loss=0.14963550865650177
train: epoch 144, loss 0.0012914377730339766, acc=0.9997222423553467, loss=0.0012914377730339766
test: epoch 144, loss 0.256204754114151, acc=0.9750000238418579, loss=0.256204754114151
train: epoch 145, loss 0.006183150690048933, acc=0.9993333220481873, loss=0.006183150690048933
test: epoch 145, loss 0.1760006695985794, acc=0.9750000238418579, loss=0.1760006695985794
train: epoch 146, loss 0.003327155252918601, acc=0.9994999766349792, loss=0.003327155252918601
test: epoch 146, loss 0.19957636296749115, acc=0.9750000238418579, loss=0.19957636296749115
train: epoch 147, loss 0.002115099923685193, acc=0.999666690826416, loss=0.002115099923685193
test: epoch 147, loss 0.18911612033843994, acc=0.9750000238418579, loss=0.18911612033843994
train: epoch 148, loss 0.005798546597361565, acc=0.9992777705192566, loss=0.005798546597361565
test: epoch 148, loss 0.17507785558700562, acc=0.9750000238418579, loss=0.17507785558700562
train: epoch 149, loss 0.002353954128921032, acc=0.9997222423553467, loss=0.002353954128921032
test: epoch 149, loss 0.22091767191886902, acc=0.9750000238418579, loss=0.22091767191886902
train: epoch 150, loss 0.0009392068022862077, acc=0.9997777938842773, loss=0.0009392068022862077
test: epoch 150, loss 0.17566728591918945, acc=0.9638888835906982, loss=0.17566728591918945
