# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=1", "--one_hot=1", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1543118901, receiver_embed_dim=64, save_run=0, temp_decay=1.0, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1543118901, receiver_embed_dim=64, save_run=False, temp_decay=1.0, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.180227518081665, acc=0.06505555659532547, loss=3.180227518081665
test: epoch 1, loss 3.0201830863952637, acc=0.08611111342906952, loss=3.0201830863952637
train: epoch 2, loss 2.289050579071045, acc=0.1702222228050232, loss=2.289050579071045
test: epoch 2, loss 2.3412840366363525, acc=0.17222222685813904, loss=2.3412840366363525
train: epoch 3, loss 1.7597215175628662, acc=0.3183888792991638, loss=1.7597215175628662
test: epoch 3, loss 1.833566665649414, acc=0.2527777850627899, loss=1.833566665649414
train: epoch 4, loss 1.404557466506958, acc=0.42016667127609253, loss=1.404557466506958
test: epoch 4, loss 1.8732202053070068, acc=0.24722221493721008, loss=1.8732202053070068
train: epoch 5, loss 1.2574498653411865, acc=0.46683332324028015, loss=1.2574498653411865
test: epoch 5, loss 1.8611701726913452, acc=0.24722221493721008, loss=1.8611701726913452
train: epoch 6, loss 1.1757292747497559, acc=0.4984999895095825, loss=1.1757292747497559
test: epoch 6, loss 1.7158831357955933, acc=0.27222222089767456, loss=1.7158831357955933
train: epoch 7, loss 1.0963916778564453, acc=0.5311111211776733, loss=1.0963916778564453
test: epoch 7, loss 1.8155850172042847, acc=0.2916666567325592, loss=1.8155850172042847
train: epoch 8, loss 1.0450843572616577, acc=0.5569444298744202, loss=1.0450843572616577
test: epoch 8, loss 1.9052680730819702, acc=0.3166666626930237, loss=1.9052680730819702
train: epoch 9, loss 0.9980946779251099, acc=0.5758333206176758, loss=0.9980946779251099
test: epoch 9, loss 1.9115676879882812, acc=0.32777777314186096, loss=1.9115676879882812
train: epoch 10, loss 0.945048451423645, acc=0.5926111340522766, loss=0.945048451423645
test: epoch 10, loss 1.8133835792541504, acc=0.3333333432674408, loss=1.8133835792541504
train: epoch 11, loss 0.9272083640098572, acc=0.5946666598320007, loss=0.9272083640098572
test: epoch 11, loss 1.9374281167984009, acc=0.31111112236976624, loss=1.9374281167984009
train: epoch 12, loss 0.8792644739151001, acc=0.613611102104187, loss=0.8792644739151001
test: epoch 12, loss 1.9893620014190674, acc=0.3333333432674408, loss=1.9893620014190674
train: epoch 13, loss 0.8716682195663452, acc=0.6172778010368347, loss=0.8716682195663452
test: epoch 13, loss 2.0105390548706055, acc=0.33888888359069824, loss=2.0105390548706055
train: epoch 14, loss 0.8571568131446838, acc=0.6181111335754395, loss=0.8571568131446838
test: epoch 14, loss 1.9251885414123535, acc=0.3361110985279083, loss=1.9251885414123535
train: epoch 15, loss 0.8253508806228638, acc=0.6302777528762817, loss=0.8253508806228638
test: epoch 15, loss 2.0636422634124756, acc=0.3222222328186035, loss=2.0636422634124756
train: epoch 16, loss 0.8216261863708496, acc=0.6311666369438171, loss=0.8216261863708496
test: epoch 16, loss 1.9688334465026855, acc=0.3583333194255829, loss=1.9688334465026855
train: epoch 17, loss 0.8140325546264648, acc=0.6300555467605591, loss=0.8140325546264648
test: epoch 17, loss 2.0415196418762207, acc=0.3305555582046509, loss=2.0415196418762207
train: epoch 18, loss 0.7981775999069214, acc=0.6361666917800903, loss=0.7981775999069214
test: epoch 18, loss 1.8342455625534058, acc=0.3472222089767456, loss=1.8342455625534058
train: epoch 19, loss 0.8159237504005432, acc=0.6220555305480957, loss=0.8159237504005432
test: epoch 19, loss 2.002639055252075, acc=0.35555556416511536, loss=2.002639055252075
train: epoch 20, loss 0.8028651475906372, acc=0.6320000290870667, loss=0.8028651475906372
test: epoch 20, loss 1.8652760982513428, acc=0.375, loss=1.8652760982513428
train: epoch 21, loss 0.7779839038848877, acc=0.6373888850212097, loss=0.7779839038848877
test: epoch 21, loss 1.8896121978759766, acc=0.3777777850627899, loss=1.8896121978759766
train: epoch 22, loss 0.7952582240104675, acc=0.6304444670677185, loss=0.7952582240104675
test: epoch 22, loss 2.032339572906494, acc=0.3583333194255829, loss=2.032339572906494
train: epoch 23, loss 0.7630417943000793, acc=0.6455000042915344, loss=0.7630417943000793
test: epoch 23, loss 1.9175282716751099, acc=0.375, loss=1.9175282716751099
train: epoch 24, loss 0.7845262289047241, acc=0.6341666579246521, loss=0.7845262289047241
test: epoch 24, loss 2.058800220489502, acc=0.3611111044883728, loss=2.058800220489502
train: epoch 25, loss 0.7691326141357422, acc=0.6383333206176758, loss=0.7691326141357422
test: epoch 25, loss 1.9439380168914795, acc=0.3638888895511627, loss=1.9439380168914795
train: epoch 26, loss 0.7558629512786865, acc=0.649222195148468, loss=0.7558629512786865
test: epoch 26, loss 1.834029197692871, acc=0.39444443583488464, loss=1.834029197692871
train: epoch 27, loss 0.7538508772850037, acc=0.649222195148468, loss=0.7538508772850037
test: epoch 27, loss 1.9236345291137695, acc=0.39722222089767456, loss=1.9236345291137695
train: epoch 28, loss 0.7520852088928223, acc=0.6549444198608398, loss=0.7520852088928223
test: epoch 28, loss 1.7472693920135498, acc=0.4027777910232544, loss=1.7472693920135498
train: epoch 29, loss 0.7563706636428833, acc=0.6504444479942322, loss=0.7563706636428833
test: epoch 29, loss 1.8998026847839355, acc=0.4027777910232544, loss=1.8998026847839355
train: epoch 30, loss 0.7338042855262756, acc=0.657444417476654, loss=0.7338042855262756
test: epoch 30, loss 1.9027419090270996, acc=0.40833333134651184, loss=1.9027419090270996
train: epoch 31, loss 0.7551191449165344, acc=0.6489999890327454, loss=0.7551191449165344
test: epoch 31, loss 2.0958032608032227, acc=0.4138889014720917, loss=2.0958032608032227
train: epoch 32, loss 0.7403656840324402, acc=0.649222195148468, loss=0.7403656840324402
test: epoch 32, loss 1.622178554534912, acc=0.4472222328186035, loss=1.622178554534912
train: epoch 33, loss 0.7399466633796692, acc=0.6556666493415833, loss=0.7399466633796692
test: epoch 33, loss 1.8285845518112183, acc=0.4027777910232544, loss=1.8285845518112183
train: epoch 34, loss 0.7307872772216797, acc=0.6538888812065125, loss=0.7307872772216797
test: epoch 34, loss 1.6617751121520996, acc=0.44999998807907104, loss=1.6617751121520996
train: epoch 35, loss 0.751349925994873, acc=0.6511666774749756, loss=0.751349925994873
test: epoch 35, loss 1.8677563667297363, acc=0.40833333134651184, loss=1.8677563667297363
train: epoch 36, loss 0.7329097986221313, acc=0.6556666493415833, loss=0.7329097986221313
test: epoch 36, loss 1.8027472496032715, acc=0.4055555462837219, loss=1.8027472496032715
train: epoch 37, loss 0.723397970199585, acc=0.6577222347259521, loss=0.723397970199585
test: epoch 37, loss 1.7438076734542847, acc=0.4305555522441864, loss=1.7438076734542847
train: epoch 38, loss 0.711064338684082, acc=0.6594444513320923, loss=0.711064338684082
test: epoch 38, loss 1.738626480102539, acc=0.43888887763023376, loss=1.738626480102539
train: epoch 39, loss 0.725816011428833, acc=0.6601666808128357, loss=0.725816011428833
test: epoch 39, loss 1.58746337890625, acc=0.45277777314186096, loss=1.58746337890625
train: epoch 40, loss 0.7260672450065613, acc=0.6571666598320007, loss=0.7260672450065613
test: epoch 40, loss 1.9570804834365845, acc=0.4305555522441864, loss=1.9570804834365845
train: epoch 41, loss 0.7023739814758301, acc=0.6676666736602783, loss=0.7023739814758301
test: epoch 41, loss 1.902957558631897, acc=0.42500001192092896, loss=1.902957558631897
train: epoch 42, loss 0.717010498046875, acc=0.6609444618225098, loss=0.717010498046875
test: epoch 42, loss 1.862247109413147, acc=0.40833333134651184, loss=1.862247109413147
train: epoch 43, loss 0.7063647508621216, acc=0.6656110882759094, loss=0.7063647508621216
test: epoch 43, loss 1.7966153621673584, acc=0.44999998807907104, loss=1.7966153621673584
train: epoch 44, loss 0.6868693828582764, acc=0.6710555553436279, loss=0.6868693828582764
test: epoch 44, loss 2.0088050365448, acc=0.4416666626930237, loss=2.0088050365448
train: epoch 45, loss 0.6999536752700806, acc=0.6713333129882812, loss=0.6999536752700806
test: epoch 45, loss 1.7005521059036255, acc=0.4555555582046509, loss=1.7005521059036255
train: epoch 46, loss 0.7214968204498291, acc=0.660444438457489, loss=0.7214968204498291
test: epoch 46, loss 1.7653712034225464, acc=0.44999998807907104, loss=1.7653712034225464
train: epoch 47, loss 0.7076053619384766, acc=0.6621666550636292, loss=0.7076053619384766
test: epoch 47, loss 1.7545758485794067, acc=0.44999998807907104, loss=1.7545758485794067
train: epoch 48, loss 0.6991947293281555, acc=0.6700555682182312, loss=0.6991947293281555
test: epoch 48, loss 1.927819013595581, acc=0.4416666626930237, loss=1.927819013595581
train: epoch 49, loss 0.6918423175811768, acc=0.6647777557373047, loss=0.6918423175811768
test: epoch 49, loss 1.840700626373291, acc=0.45277777314186096, loss=1.840700626373291
train: epoch 50, loss 0.6813589334487915, acc=0.6769444346427917, loss=0.6813589334487915
test: epoch 50, loss 1.8735932111740112, acc=0.4583333432674408, loss=1.8735932111740112
train: epoch 51, loss 0.6929030418395996, acc=0.6697777509689331, loss=0.6929030418395996
test: epoch 51, loss 1.8240582942962646, acc=0.4583333432674408, loss=1.8240582942962646
train: epoch 52, loss 0.6863921284675598, acc=0.6717222332954407, loss=0.6863921284675598
test: epoch 52, loss 1.784089207649231, acc=0.4611110985279083, loss=1.784089207649231
train: epoch 53, loss 0.6905062198638916, acc=0.6722777485847473, loss=0.6905062198638916
test: epoch 53, loss 1.7705836296081543, acc=0.4472222328186035, loss=1.7705836296081543
train: epoch 54, loss 0.6971661448478699, acc=0.6744444370269775, loss=0.6971661448478699
test: epoch 54, loss 1.699579119682312, acc=0.4583333432674408, loss=1.699579119682312
train: epoch 55, loss 0.6802389621734619, acc=0.6818333268165588, loss=0.6802389621734619
test: epoch 55, loss 1.7481958866119385, acc=0.4555555582046509, loss=1.7481958866119385
train: epoch 56, loss 0.6622282862663269, acc=0.6858888864517212, loss=0.6622282862663269
test: epoch 56, loss 1.4893487691879272, acc=0.4583333432674408, loss=1.4893487691879272
train: epoch 57, loss 0.6647039651870728, acc=0.6868888735771179, loss=0.6647039651870728
test: epoch 57, loss 1.8185486793518066, acc=0.4611110985279083, loss=1.8185486793518066
train: epoch 58, loss 0.6636603474617004, acc=0.6895555257797241, loss=0.6636603474617004
test: epoch 58, loss 1.8734261989593506, acc=0.4611110985279083, loss=1.8734261989593506
train: epoch 59, loss 0.6668801307678223, acc=0.6930000185966492, loss=0.6668801307678223
test: epoch 59, loss 1.706573247909546, acc=0.4555555582046509, loss=1.706573247909546
train: epoch 60, loss 0.6630061864852905, acc=0.6919999718666077, loss=0.6630061864852905
test: epoch 60, loss 1.5219415426254272, acc=0.4611110985279083, loss=1.5219415426254272
train: epoch 61, loss 0.6479835510253906, acc=0.6969444155693054, loss=0.6479835510253906
test: epoch 61, loss 1.7991043329238892, acc=0.4611110985279083, loss=1.7991043329238892
train: epoch 62, loss 0.6648935079574585, acc=0.6933333277702332, loss=0.6648935079574585
test: epoch 62, loss 1.7278056144714355, acc=0.4611110985279083, loss=1.7278056144714355
train: epoch 63, loss 0.6341509819030762, acc=0.7063888907432556, loss=0.6341509819030762
test: epoch 63, loss 1.7486164569854736, acc=0.4555555582046509, loss=1.7486164569854736
train: epoch 64, loss 0.6351670622825623, acc=0.7161111235618591, loss=0.6351670622825623
test: epoch 64, loss 1.8402824401855469, acc=0.4611110985279083, loss=1.8402824401855469
train: epoch 65, loss 0.6337283253669739, acc=0.7151111364364624, loss=0.6337283253669739
test: epoch 65, loss 1.6924381256103516, acc=0.4611110985279083, loss=1.6924381256103516
train: epoch 66, loss 0.6099427342414856, acc=0.726111114025116, loss=0.6099427342414856
test: epoch 66, loss 1.6497074365615845, acc=0.4611110985279083, loss=1.6497074365615845
train: epoch 67, loss 0.6178058385848999, acc=0.7168333530426025, loss=0.6178058385848999
test: epoch 67, loss 1.792702555656433, acc=0.4583333432674408, loss=1.792702555656433
train: epoch 68, loss 0.6226295828819275, acc=0.7230555415153503, loss=0.6226295828819275
test: epoch 68, loss 1.6707221269607544, acc=0.4583333432674408, loss=1.6707221269607544
train: epoch 69, loss 0.5900793671607971, acc=0.7315000295639038, loss=0.5900793671607971
test: epoch 69, loss 1.9504927396774292, acc=0.4611110985279083, loss=1.9504927396774292
train: epoch 70, loss 0.6111369729042053, acc=0.7285555601119995, loss=0.6111369729042053
test: epoch 70, loss 1.7743232250213623, acc=0.4611110985279083, loss=1.7743232250213623
train: epoch 71, loss 0.6004645824432373, acc=0.7316111326217651, loss=0.6004645824432373
test: epoch 71, loss 1.8792811632156372, acc=0.44999998807907104, loss=1.8792811632156372
train: epoch 72, loss 0.6094381809234619, acc=0.7260000109672546, loss=0.6094381809234619
test: epoch 72, loss 1.8754371404647827, acc=0.4611110985279083, loss=1.8754371404647827
train: epoch 73, loss 0.6130050420761108, acc=0.7246111035346985, loss=0.6130050420761108
test: epoch 73, loss 1.9062696695327759, acc=0.4611110985279083, loss=1.9062696695327759
train: epoch 74, loss 0.5904566049575806, acc=0.7354999780654907, loss=0.5904566049575806
test: epoch 74, loss 1.9232083559036255, acc=0.4611110985279083, loss=1.9232083559036255
train: epoch 75, loss 0.6039558053016663, acc=0.7317222356796265, loss=0.6039558053016663
test: epoch 75, loss 1.907828450202942, acc=0.4611110985279083, loss=1.907828450202942
train: epoch 76, loss 0.5988002419471741, acc=0.730222225189209, loss=0.5988002419471741
test: epoch 76, loss 1.9575889110565186, acc=0.4583333432674408, loss=1.9575889110565186
train: epoch 77, loss 0.6050019264221191, acc=0.7271666526794434, loss=0.6050019264221191
test: epoch 77, loss 1.9766582250595093, acc=0.4611110985279083, loss=1.9766582250595093
train: epoch 78, loss 0.5992501974105835, acc=0.7274444699287415, loss=0.5992501974105835
test: epoch 78, loss 1.8401081562042236, acc=0.4611110985279083, loss=1.8401081562042236
train: epoch 79, loss 0.6021589636802673, acc=0.7312222123146057, loss=0.6021589636802673
test: epoch 79, loss 1.9202731847763062, acc=0.4611110985279083, loss=1.9202731847763062
train: epoch 80, loss 0.6105942130088806, acc=0.7242777943611145, loss=0.6105942130088806
test: epoch 80, loss 1.9039415121078491, acc=0.4611110985279083, loss=1.9039415121078491
train: epoch 81, loss 0.6044996976852417, acc=0.7294444441795349, loss=0.6044996976852417
test: epoch 81, loss 1.8950692415237427, acc=0.46388888359069824, loss=1.8950692415237427
train: epoch 82, loss 0.5841024518013, acc=0.7362222075462341, loss=0.5841024518013
test: epoch 82, loss 1.7709324359893799, acc=0.4583333432674408, loss=1.7709324359893799
train: epoch 83, loss 0.5998900532722473, acc=0.734333336353302, loss=0.5998900532722473
test: epoch 83, loss 1.680964708328247, acc=0.4555555582046509, loss=1.680964708328247
train: epoch 84, loss 0.5980406999588013, acc=0.730055570602417, loss=0.5980406999588013
test: epoch 84, loss 1.9676198959350586, acc=0.4611110985279083, loss=1.9676198959350586
train: epoch 85, loss 0.5817487239837646, acc=0.7383888959884644, loss=0.5817487239837646
test: epoch 85, loss 1.7019801139831543, acc=0.4583333432674408, loss=1.7019801139831543
train: epoch 86, loss 0.5778641700744629, acc=0.738111138343811, loss=0.5778641700744629
test: epoch 86, loss 1.8367489576339722, acc=0.4611110985279083, loss=1.8367489576339722
train: epoch 87, loss 0.5802683234214783, acc=0.7352222204208374, loss=0.5802683234214783
test: epoch 87, loss 1.8637224435806274, acc=0.4611110985279083, loss=1.8637224435806274
train: epoch 88, loss 0.6135249137878418, acc=0.7232221961021423, loss=0.6135249137878418
test: epoch 88, loss 1.7554982900619507, acc=0.46388888359069824, loss=1.7554982900619507
train: epoch 89, loss 0.5788991451263428, acc=0.7358888983726501, loss=0.5788991451263428
test: epoch 89, loss 1.8605462312698364, acc=0.45277777314186096, loss=1.8605462312698364
train: epoch 90, loss 0.5923166871070862, acc=0.7362222075462341, loss=0.5923166871070862
test: epoch 90, loss 1.9618116617202759, acc=0.4611110985279083, loss=1.9618116617202759
train: epoch 91, loss 0.60270094871521, acc=0.7302777767181396, loss=0.60270094871521
test: epoch 91, loss 1.903574824333191, acc=0.45277777314186096, loss=1.903574824333191
train: epoch 92, loss 0.5922675132751465, acc=0.7352777719497681, loss=0.5922675132751465
test: epoch 92, loss 1.635272741317749, acc=0.4722222089767456, loss=1.635272741317749
train: epoch 93, loss 0.6016039848327637, acc=0.7328888773918152, loss=0.6016039848327637
test: epoch 93, loss 1.6701346635818481, acc=0.46388888359069824, loss=1.6701346635818481
train: epoch 94, loss 0.5853813290596008, acc=0.7367222309112549, loss=0.5853813290596008
test: epoch 94, loss 1.9177888631820679, acc=0.46388888359069824, loss=1.9177888631820679
train: epoch 95, loss 0.5820174813270569, acc=0.7402222156524658, loss=0.5820174813270569
test: epoch 95, loss 1.7635440826416016, acc=0.4611110985279083, loss=1.7635440826416016
train: epoch 96, loss 0.570103108882904, acc=0.7433888912200928, loss=0.570103108882904
test: epoch 96, loss 1.7614229917526245, acc=0.46666666865348816, loss=1.7614229917526245
train: epoch 97, loss 0.5802062749862671, acc=0.7407777905464172, loss=0.5802062749862671
test: epoch 97, loss 1.9356410503387451, acc=0.4694444537162781, loss=1.9356410503387451
train: epoch 98, loss 0.5929073691368103, acc=0.734499990940094, loss=0.5929073691368103
test: epoch 98, loss 1.6823469400405884, acc=0.4611110985279083, loss=1.6823469400405884
train: epoch 99, loss 0.569423258304596, acc=0.7422778010368347, loss=0.569423258304596
test: epoch 99, loss 1.7826745510101318, acc=0.46388888359069824, loss=1.7826745510101318
train: epoch 100, loss 0.5693917274475098, acc=0.7452222108840942, loss=0.5693917274475098
test: epoch 100, loss 1.7547017335891724, acc=0.4611110985279083, loss=1.7547017335891724
train: epoch 101, loss 0.570326566696167, acc=0.7422778010368347, loss=0.570326566696167
test: epoch 101, loss 1.7655261754989624, acc=0.4722222089767456, loss=1.7655261754989624
train: epoch 102, loss 0.5823152661323547, acc=0.7408888936042786, loss=0.5823152661323547
test: epoch 102, loss 1.8983534574508667, acc=0.4749999940395355, loss=1.8983534574508667
train: epoch 103, loss 0.5792627930641174, acc=0.7401666641235352, loss=0.5792627930641174
test: epoch 103, loss 1.6378679275512695, acc=0.47777777910232544, loss=1.6378679275512695
train: epoch 104, loss 0.562658965587616, acc=0.74272221326828, loss=0.562658965587616
test: epoch 104, loss 1.7442505359649658, acc=0.4749999940395355, loss=1.7442505359649658
train: epoch 105, loss 0.5660154819488525, acc=0.745722234249115, loss=0.5660154819488525
test: epoch 105, loss 1.6936135292053223, acc=0.47777777910232544, loss=1.6936135292053223
train: epoch 106, loss 0.5684140920639038, acc=0.7469444274902344, loss=0.5684140920639038
test: epoch 106, loss 1.7856794595718384, acc=0.46666666865348816, loss=1.7856794595718384
train: epoch 107, loss 0.5659406781196594, acc=0.7432777881622314, loss=0.5659406781196594
test: epoch 107, loss 1.7979425191879272, acc=0.47777777910232544, loss=1.7979425191879272
train: epoch 108, loss 0.5618283152580261, acc=0.7456666827201843, loss=0.5618283152580261
test: epoch 108, loss 1.6342322826385498, acc=0.4833333194255829, loss=1.6342322826385498
train: epoch 109, loss 0.5750367641448975, acc=0.742111086845398, loss=0.5750367641448975
test: epoch 109, loss 1.7537567615509033, acc=0.48055556416511536, loss=1.7537567615509033
train: epoch 110, loss 0.5564471483230591, acc=0.7483333349227905, loss=0.5564471483230591
test: epoch 110, loss 1.7598601579666138, acc=0.4833333194255829, loss=1.7598601579666138
train: epoch 111, loss 0.5565675497055054, acc=0.7462777495384216, loss=0.5565675497055054
test: epoch 111, loss 1.787778377532959, acc=0.47777777910232544, loss=1.787778377532959
train: epoch 112, loss 0.5450795888900757, acc=0.7541666626930237, loss=0.5450795888900757
test: epoch 112, loss 1.8048667907714844, acc=0.47777777910232544, loss=1.8048667907714844
train: epoch 113, loss 0.5501264333724976, acc=0.7568888664245605, loss=0.5501264333724976
test: epoch 113, loss 1.4972155094146729, acc=0.47777777910232544, loss=1.4972155094146729
train: epoch 114, loss 0.5283917188644409, acc=0.7708333134651184, loss=0.5283917188644409
test: epoch 114, loss 1.8049079179763794, acc=0.5166666507720947, loss=1.8049079179763794
train: epoch 115, loss 0.5303870439529419, acc=0.7764444351196289, loss=0.5303870439529419
test: epoch 115, loss 1.6358842849731445, acc=0.5166666507720947, loss=1.6358842849731445
train: epoch 116, loss 0.5106239914894104, acc=0.7826111316680908, loss=0.5106239914894104
test: epoch 116, loss 1.6201066970825195, acc=0.5166666507720947, loss=1.6201066970825195
train: epoch 117, loss 0.4920257031917572, acc=0.7866666913032532, loss=0.4920257031917572
test: epoch 117, loss 1.6297680139541626, acc=0.5166666507720947, loss=1.6297680139541626
train: epoch 118, loss 0.47466200590133667, acc=0.7933889031410217, loss=0.47466200590133667
test: epoch 118, loss 1.5557302236557007, acc=0.5166666507720947, loss=1.5557302236557007
train: epoch 119, loss 0.4816373288631439, acc=0.7908889055252075, loss=0.4816373288631439
test: epoch 119, loss 1.600924015045166, acc=0.5166666507720947, loss=1.600924015045166
train: epoch 120, loss 0.47471997141838074, acc=0.792555570602417, loss=0.47471997141838074
test: epoch 120, loss 1.7665481567382812, acc=0.5166666507720947, loss=1.7665481567382812
train: epoch 121, loss 0.48170703649520874, acc=0.7934444546699524, loss=0.48170703649520874
test: epoch 121, loss 1.7946281433105469, acc=0.5166666507720947, loss=1.7946281433105469
train: epoch 122, loss 0.4822259843349457, acc=0.792722225189209, loss=0.4822259843349457
test: epoch 122, loss 1.6640636920928955, acc=0.5111111402511597, loss=1.6640636920928955
train: epoch 123, loss 0.48044952750205994, acc=0.7929999828338623, loss=0.48044952750205994
test: epoch 123, loss 1.673270344734192, acc=0.5166666507720947, loss=1.673270344734192
train: epoch 124, loss 0.46326568722724915, acc=0.795722246170044, loss=0.46326568722724915
test: epoch 124, loss 1.6582845449447632, acc=0.5166666507720947, loss=1.6582845449447632
train: epoch 125, loss 0.48962467908859253, acc=0.7889444231987, loss=0.48962467908859253
test: epoch 125, loss 1.7515716552734375, acc=0.5166666507720947, loss=1.7515716552734375
train: epoch 126, loss 0.46070805191993713, acc=0.7976111173629761, loss=0.46070805191993713
test: epoch 126, loss 1.6348904371261597, acc=0.5166666507720947, loss=1.6348904371261597
train: epoch 127, loss 0.47430506348609924, acc=0.7940000295639038, loss=0.47430506348609924
test: epoch 127, loss 1.6954095363616943, acc=0.5166666507720947, loss=1.6954095363616943
train: epoch 128, loss 0.4763021171092987, acc=0.7927777767181396, loss=0.4763021171092987
test: epoch 128, loss 1.7004579305648804, acc=0.5138888955116272, loss=1.7004579305648804
train: epoch 129, loss 0.4853110909461975, acc=0.7881666421890259, loss=0.4853110909461975
test: epoch 129, loss 1.6185004711151123, acc=0.5138888955116272, loss=1.6185004711151123
train: epoch 130, loss 0.4698299467563629, acc=0.7944444417953491, loss=0.4698299467563629
test: epoch 130, loss 1.5972626209259033, acc=0.5138888955116272, loss=1.5972626209259033
train: epoch 131, loss 0.48048079013824463, acc=0.7910000085830688, loss=0.48048079013824463
test: epoch 131, loss 1.8060188293457031, acc=0.5138888955116272, loss=1.8060188293457031
train: epoch 132, loss 0.486839234828949, acc=0.7879999876022339, loss=0.486839234828949
test: epoch 132, loss 1.58439302444458, acc=0.5138888955116272, loss=1.58439302444458
train: epoch 133, loss 0.4561147689819336, acc=0.800777792930603, loss=0.4561147689819336
test: epoch 133, loss 1.850528359413147, acc=0.5138888955116272, loss=1.850528359413147
train: epoch 134, loss 0.46414270997047424, acc=0.7988888621330261, loss=0.46414270997047424
test: epoch 134, loss 1.732213020324707, acc=0.5138888955116272, loss=1.732213020324707
train: epoch 135, loss 0.4676694869995117, acc=0.79666668176651, loss=0.4676694869995117
test: epoch 135, loss 1.8011919260025024, acc=0.5083333253860474, loss=1.8011919260025024
train: epoch 136, loss 0.4493139088153839, acc=0.8038333058357239, loss=0.4493139088153839
test: epoch 136, loss 1.7360128164291382, acc=0.5166666507720947, loss=1.7360128164291382
train: epoch 137, loss 0.45980191230773926, acc=0.7994444370269775, loss=0.45980191230773926
test: epoch 137, loss 1.7474359273910522, acc=0.5138888955116272, loss=1.7474359273910522
train: epoch 138, loss 0.4637957513332367, acc=0.7986111044883728, loss=0.4637957513332367
test: epoch 138, loss 1.8347845077514648, acc=0.5166666507720947, loss=1.8347845077514648
train: epoch 139, loss 0.4465472102165222, acc=0.8061666488647461, loss=0.4465472102165222
test: epoch 139, loss 1.5913883447647095, acc=0.5166666507720947, loss=1.5913883447647095
train: epoch 140, loss 0.4576002061367035, acc=0.8015000224113464, loss=0.4576002061367035
test: epoch 140, loss 1.8485864400863647, acc=0.5166666507720947, loss=1.8485864400863647
train: epoch 141, loss 0.4464462101459503, acc=0.8032777905464172, loss=0.4464462101459503
test: epoch 141, loss 1.8371995687484741, acc=0.5166666507720947, loss=1.8371995687484741
train: epoch 142, loss 0.44963791966438293, acc=0.8025555610656738, loss=0.44963791966438293
test: epoch 142, loss 1.7372506856918335, acc=0.519444465637207, loss=1.7372506856918335
train: epoch 143, loss 0.45057064294815063, acc=0.8027222156524658, loss=0.45057064294815063
test: epoch 143, loss 1.6668120622634888, acc=0.5222222208976746, loss=1.6668120622634888
train: epoch 144, loss 0.4294486939907074, acc=0.8114444613456726, loss=0.4294486939907074
test: epoch 144, loss 1.837647557258606, acc=0.5166666507720947, loss=1.837647557258606
train: epoch 145, loss 0.44727519154548645, acc=0.8054999709129333, loss=0.44727519154548645
test: epoch 145, loss 1.7921699285507202, acc=0.5166666507720947, loss=1.7921699285507202
train: epoch 146, loss 0.43887192010879517, acc=0.8077222108840942, loss=0.43887192010879517
test: epoch 146, loss 1.6691495180130005, acc=0.519444465637207, loss=1.6691495180130005
train: epoch 147, loss 0.4350634813308716, acc=0.808222234249115, loss=0.4350634813308716
test: epoch 147, loss 1.9542901515960693, acc=0.5083333253860474, loss=1.9542901515960693
train: epoch 148, loss 0.45273181796073914, acc=0.8035555481910706, loss=0.45273181796073914
test: epoch 148, loss 1.6943236589431763, acc=0.5055555701255798, loss=1.6943236589431763
train: epoch 149, loss 0.4276120960712433, acc=0.8095555305480957, loss=0.4276120960712433
test: epoch 149, loss 1.7519872188568115, acc=0.5249999761581421, loss=1.7519872188568115
train: epoch 150, loss 0.4421374499797821, acc=0.8073333501815796, loss=0.4421374499797821
test: epoch 150, loss 1.673646092414856, acc=0.5222222208976746, loss=1.673646092414856
