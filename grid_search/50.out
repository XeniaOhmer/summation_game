# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=2.0", "--temp_decay=0.99", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=2141911190, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=2141911190, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.4629127979278564, acc=0.1738888919353485, loss=2.4629127979278564
test: epoch 1, loss 7.671994686126709, acc=0.06111111119389534, loss=7.671994686126709
train: epoch 2, loss 1.621284008026123, acc=0.3495555520057678, loss=1.621284008026123
test: epoch 2, loss 6.5886054039001465, acc=0.08055555820465088, loss=6.5886054039001465
train: epoch 3, loss 1.3678444623947144, acc=0.433555543422699, loss=1.3678444623947144
test: epoch 3, loss 7.5182204246521, acc=0.07222222536802292, loss=7.5182204246521
train: epoch 4, loss 1.2094897031784058, acc=0.5001111030578613, loss=1.2094897031784058
test: epoch 4, loss 5.217376708984375, acc=0.14722222089767456, loss=5.217376708984375
train: epoch 5, loss 1.1052757501602173, acc=0.5437777638435364, loss=1.1052757501602173
test: epoch 5, loss 5.418623924255371, acc=0.13055555522441864, loss=5.418623924255371
train: epoch 6, loss 1.0267887115478516, acc=0.5801110863685608, loss=1.0267887115478516
test: epoch 6, loss 4.932788848876953, acc=0.13611111044883728, loss=4.932788848876953
train: epoch 7, loss 0.959409236907959, acc=0.6157222390174866, loss=0.959409236907959
test: epoch 7, loss 4.259300708770752, acc=0.24722221493721008, loss=4.259300708770752
train: epoch 8, loss 0.8926224112510681, acc=0.6431111097335815, loss=0.8926224112510681
test: epoch 8, loss 3.321796417236328, acc=0.2638888955116272, loss=3.321796417236328
train: epoch 9, loss 0.8358480930328369, acc=0.6631666421890259, loss=0.8358480930328369
test: epoch 9, loss 3.0966274738311768, acc=0.2666666805744171, loss=3.0966274738311768
train: epoch 10, loss 0.8170284032821655, acc=0.6710555553436279, loss=0.8170284032821655
test: epoch 10, loss 3.0972352027893066, acc=0.25555557012557983, loss=3.0972352027893066
train: epoch 11, loss 0.7691658139228821, acc=0.6937222480773926, loss=0.7691658139228821
test: epoch 11, loss 3.1703391075134277, acc=0.2611111104488373, loss=3.1703391075134277
train: epoch 12, loss 0.7247679829597473, acc=0.714555561542511, loss=0.7247679829597473
test: epoch 12, loss 2.6887733936309814, acc=0.25555557012557983, loss=2.6887733936309814
train: epoch 13, loss 0.7214559316635132, acc=0.7206666469573975, loss=0.7214559316635132
test: epoch 13, loss 2.5414834022521973, acc=0.2944444417953491, loss=2.5414834022521973
train: epoch 14, loss 0.7048779726028442, acc=0.7276666760444641, loss=0.7048779726028442
test: epoch 14, loss 3.2913694381713867, acc=0.2944444417953491, loss=3.2913694381713867
train: epoch 15, loss 0.7010745406150818, acc=0.7236666679382324, loss=0.7010745406150818
test: epoch 15, loss 2.5460779666900635, acc=0.36944442987442017, loss=2.5460779666900635
train: epoch 16, loss 0.6504420638084412, acc=0.7471666932106018, loss=0.6504420638084412
test: epoch 16, loss 2.920063018798828, acc=0.31111112236976624, loss=2.920063018798828
train: epoch 17, loss 0.6367413401603699, acc=0.7523888945579529, loss=0.6367413401603699
test: epoch 17, loss 2.8167552947998047, acc=0.2666666805744171, loss=2.8167552947998047
train: epoch 18, loss 0.63935786485672, acc=0.7512778043746948, loss=0.63935786485672
test: epoch 18, loss 3.141566276550293, acc=0.32777777314186096, loss=3.141566276550293
train: epoch 19, loss 0.6026209592819214, acc=0.7638888955116272, loss=0.6026209592819214
test: epoch 19, loss 3.2426466941833496, acc=0.2750000059604645, loss=3.2426466941833496
train: epoch 20, loss 0.6136293411254883, acc=0.7586666941642761, loss=0.6136293411254883
test: epoch 20, loss 2.534832715988159, acc=0.30000001192092896, loss=2.534832715988159
train: epoch 21, loss 0.5888502597808838, acc=0.7749999761581421, loss=0.5888502597808838
test: epoch 21, loss 2.9707119464874268, acc=0.3027777671813965, loss=2.9707119464874268
train: epoch 22, loss 0.6189398765563965, acc=0.7627221941947937, loss=0.6189398765563965
test: epoch 22, loss 3.154499053955078, acc=0.2777777910232544, loss=3.154499053955078
train: epoch 23, loss 0.5965721607208252, acc=0.7706666588783264, loss=0.5965721607208252
test: epoch 23, loss 2.3477189540863037, acc=0.3027777671813965, loss=2.3477189540863037
train: epoch 24, loss 0.5738900303840637, acc=0.7839999794960022, loss=0.5738900303840637
test: epoch 24, loss 2.653834104537964, acc=0.3083333373069763, loss=2.653834104537964
train: epoch 25, loss 0.5957430601119995, acc=0.7711111307144165, loss=0.5957430601119995
test: epoch 25, loss 2.364805221557617, acc=0.2666666805744171, loss=2.364805221557617
train: epoch 26, loss 0.5899156928062439, acc=0.7812222242355347, loss=0.5899156928062439
test: epoch 26, loss 2.3009133338928223, acc=0.3166666626930237, loss=2.3009133338928223
train: epoch 27, loss 0.5985798239707947, acc=0.7702222466468811, loss=0.5985798239707947
test: epoch 27, loss 3.040940523147583, acc=0.24444444477558136, loss=3.040940523147583
train: epoch 28, loss 0.5793415904045105, acc=0.7792778015136719, loss=0.5793415904045105
test: epoch 28, loss 2.4459104537963867, acc=0.2944444417953491, loss=2.4459104537963867
train: epoch 29, loss 0.5749633312225342, acc=0.7797777652740479, loss=0.5749633312225342
test: epoch 29, loss 1.9275997877120972, acc=0.2750000059604645, loss=1.9275997877120972
train: epoch 30, loss 0.5615720748901367, acc=0.784333348274231, loss=0.5615720748901367
test: epoch 30, loss 2.3925936222076416, acc=0.25833332538604736, loss=2.3925936222076416
train: epoch 31, loss 0.5805256962776184, acc=0.7797777652740479, loss=0.5805256962776184
test: epoch 31, loss 2.6996452808380127, acc=0.3444444537162781, loss=2.6996452808380127
train: epoch 32, loss 0.5746988654136658, acc=0.7802222371101379, loss=0.5746988654136658
test: epoch 32, loss 2.333221197128296, acc=0.3361110985279083, loss=2.333221197128296
train: epoch 33, loss 0.6042181849479675, acc=0.7698333263397217, loss=0.6042181849479675
test: epoch 33, loss 2.3815627098083496, acc=0.23333333432674408, loss=2.3815627098083496
train: epoch 34, loss 0.5720376968383789, acc=0.7833333611488342, loss=0.5720376968383789
test: epoch 34, loss 2.2141129970550537, acc=0.3333333432674408, loss=2.2141129970550537
train: epoch 35, loss 0.562831699848175, acc=0.7860000133514404, loss=0.562831699848175
test: epoch 35, loss 2.0342397689819336, acc=0.32499998807907104, loss=2.0342397689819336
train: epoch 36, loss 0.5749108791351318, acc=0.7848333120346069, loss=0.5749108791351318
test: epoch 36, loss 2.5428407192230225, acc=0.3222222328186035, loss=2.5428407192230225
train: epoch 37, loss 0.619341254234314, acc=0.7623888850212097, loss=0.619341254234314
test: epoch 37, loss 2.051953077316284, acc=0.3722222149372101, loss=2.051953077316284
train: epoch 38, loss 0.5684130191802979, acc=0.7860000133514404, loss=0.5684130191802979
test: epoch 38, loss 1.9919453859329224, acc=0.3583333194255829, loss=1.9919453859329224
train: epoch 39, loss 0.5576032996177673, acc=0.7914444208145142, loss=0.5576032996177673
test: epoch 39, loss 2.04915189743042, acc=0.3916666805744171, loss=2.04915189743042
train: epoch 40, loss 0.5854371190071106, acc=0.7745000123977661, loss=0.5854371190071106
test: epoch 40, loss 1.9025447368621826, acc=0.36944442987442017, loss=1.9025447368621826
train: epoch 41, loss 0.574556827545166, acc=0.7836111187934875, loss=0.574556827545166
test: epoch 41, loss 1.8862407207489014, acc=0.38055557012557983, loss=1.8862407207489014
train: epoch 42, loss 0.5854374170303345, acc=0.7781111001968384, loss=0.5854374170303345
test: epoch 42, loss 2.421966552734375, acc=0.2944444417953491, loss=2.421966552734375
train: epoch 43, loss 0.5702404975891113, acc=0.7827777862548828, loss=0.5702404975891113
test: epoch 43, loss 1.910880208015442, acc=0.3583333194255829, loss=1.910880208015442
train: epoch 44, loss 0.565468430519104, acc=0.7843888998031616, loss=0.565468430519104
test: epoch 44, loss 2.33321213722229, acc=0.36944442987442017, loss=2.33321213722229
train: epoch 45, loss 0.5696658492088318, acc=0.7805555462837219, loss=0.5696658492088318
test: epoch 45, loss 1.8054945468902588, acc=0.3916666805744171, loss=1.8054945468902588
train: epoch 46, loss 0.5714223384857178, acc=0.7840555310249329, loss=0.5714223384857178
test: epoch 46, loss 2.363394021987915, acc=0.38333332538604736, loss=2.363394021987915
train: epoch 47, loss 0.5787485837936401, acc=0.7803333401679993, loss=0.5787485837936401
test: epoch 47, loss 1.5957167148590088, acc=0.41111111640930176, loss=1.5957167148590088
train: epoch 48, loss 0.5957615971565247, acc=0.7726110816001892, loss=0.5957615971565247
test: epoch 48, loss 1.5640161037445068, acc=0.39722222089767456, loss=1.5640161037445068
train: epoch 49, loss 0.5583133697509766, acc=0.7879999876022339, loss=0.5583133697509766
test: epoch 49, loss 1.624847412109375, acc=0.35555556416511536, loss=1.624847412109375
train: epoch 50, loss 0.573711097240448, acc=0.7798333168029785, loss=0.573711097240448
test: epoch 50, loss 1.5670654773712158, acc=0.375, loss=1.5670654773712158
train: epoch 51, loss 0.5535249710083008, acc=0.7902777791023254, loss=0.5535249710083008
test: epoch 51, loss 1.3973853588104248, acc=0.35277777910232544, loss=1.3973853588104248
train: epoch 52, loss 0.5635478496551514, acc=0.782444417476654, loss=0.5635478496551514
test: epoch 52, loss 1.9247151613235474, acc=0.40833333134651184, loss=1.9247151613235474
train: epoch 53, loss 0.5391584634780884, acc=0.7944999933242798, loss=0.5391584634780884
test: epoch 53, loss 1.572592854499817, acc=0.4194444417953491, loss=1.572592854499817
train: epoch 54, loss 0.5283454060554504, acc=0.7973889112472534, loss=0.5283454060554504
test: epoch 54, loss 1.2478933334350586, acc=0.42222222685813904, loss=1.2478933334350586
train: epoch 55, loss 0.5511736273765564, acc=0.7855555415153503, loss=0.5511736273765564
test: epoch 55, loss 1.9808592796325684, acc=0.3638888895511627, loss=1.9808592796325684
train: epoch 56, loss 0.5303270220756531, acc=0.7979444265365601, loss=0.5303270220756531
test: epoch 56, loss 1.5241409540176392, acc=0.4472222328186035, loss=1.5241409540176392
train: epoch 57, loss 0.5555041432380676, acc=0.7885555624961853, loss=0.5555041432380676
test: epoch 57, loss 1.6061469316482544, acc=0.39444443583488464, loss=1.6061469316482544
train: epoch 58, loss 0.539913535118103, acc=0.7894444465637207, loss=0.539913535118103
test: epoch 58, loss 1.4405758380889893, acc=0.4305555522441864, loss=1.4405758380889893
train: epoch 59, loss 0.513412594795227, acc=0.8033333420753479, loss=0.513412594795227
test: epoch 59, loss 1.3730672597885132, acc=0.5249999761581421, loss=1.3730672597885132
train: epoch 60, loss 0.5354619026184082, acc=0.7932222485542297, loss=0.5354619026184082
test: epoch 60, loss 1.3488365411758423, acc=0.44999998807907104, loss=1.3488365411758423
train: epoch 61, loss 0.515525221824646, acc=0.800611138343811, loss=0.515525221824646
test: epoch 61, loss 1.5647281408309937, acc=0.3916666805744171, loss=1.5647281408309937
train: epoch 62, loss 0.5062152743339539, acc=0.8073889017105103, loss=0.5062152743339539
test: epoch 62, loss 1.2814204692840576, acc=0.4861111044883728, loss=1.2814204692840576
train: epoch 63, loss 0.5221830010414124, acc=0.7994444370269775, loss=0.5221830010414124
test: epoch 63, loss 1.3111062049865723, acc=0.47777777910232544, loss=1.3111062049865723
train: epoch 64, loss 0.5285627841949463, acc=0.7924444675445557, loss=0.5285627841949463
test: epoch 64, loss 1.4012115001678467, acc=0.45277777314186096, loss=1.4012115001678467
train: epoch 65, loss 0.49237462878227234, acc=0.8102222084999084, loss=0.49237462878227234
test: epoch 65, loss 1.3066743612289429, acc=0.49166667461395264, loss=1.3066743612289429
train: epoch 66, loss 0.5133770108222961, acc=0.7998889088630676, loss=0.5133770108222961
test: epoch 66, loss 1.3185733556747437, acc=0.4305555522441864, loss=1.3185733556747437
train: epoch 67, loss 0.5290874242782593, acc=0.7994444370269775, loss=0.5290874242782593
test: epoch 67, loss 1.2608616352081299, acc=0.519444465637207, loss=1.2608616352081299
train: epoch 68, loss 0.530440628528595, acc=0.7961111068725586, loss=0.530440628528595
test: epoch 68, loss 1.2438687086105347, acc=0.45277777314186096, loss=1.2438687086105347
train: epoch 69, loss 0.5116052627563477, acc=0.804444432258606, loss=0.5116052627563477
test: epoch 69, loss 1.4585611820220947, acc=0.43611112236976624, loss=1.4585611820220947
train: epoch 70, loss 0.5185026526451111, acc=0.8026111125946045, loss=0.5185026526451111
test: epoch 70, loss 1.4865496158599854, acc=0.4333333373069763, loss=1.4865496158599854
train: epoch 71, loss 0.5136696100234985, acc=0.8021666407585144, loss=0.5136696100234985
test: epoch 71, loss 1.4699151515960693, acc=0.44999998807907104, loss=1.4699151515960693
train: epoch 72, loss 0.5110427141189575, acc=0.7996666431427002, loss=0.5110427141189575
test: epoch 72, loss 1.3137614727020264, acc=0.5, loss=1.3137614727020264
train: epoch 73, loss 0.5194125771522522, acc=0.7959444522857666, loss=0.5194125771522522
test: epoch 73, loss 1.397122859954834, acc=0.5111111402511597, loss=1.397122859954834
train: epoch 74, loss 0.5211532711982727, acc=0.7983333468437195, loss=0.5211532711982727
test: epoch 74, loss 1.37285315990448, acc=0.4749999940395355, loss=1.37285315990448
train: epoch 75, loss 0.5327777862548828, acc=0.7915555834770203, loss=0.5327777862548828
test: epoch 75, loss 1.2011548280715942, acc=0.49444442987442017, loss=1.2011548280715942
train: epoch 76, loss 0.5145536065101624, acc=0.8018333315849304, loss=0.5145536065101624
test: epoch 76, loss 1.1506354808807373, acc=0.5611110925674438, loss=1.1506354808807373
train: epoch 77, loss 0.5013661980628967, acc=0.8017777800559998, loss=0.5013661980628967
test: epoch 77, loss 1.2398122549057007, acc=0.4972222149372101, loss=1.2398122549057007
train: epoch 78, loss 0.5187795162200928, acc=0.7975000143051147, loss=0.5187795162200928
test: epoch 78, loss 1.5270096063613892, acc=0.4444444477558136, loss=1.5270096063613892
train: epoch 79, loss 0.5134186148643494, acc=0.7993888854980469, loss=0.5134186148643494
test: epoch 79, loss 1.2621499300003052, acc=0.4583333432674408, loss=1.2621499300003052
train: epoch 80, loss 0.5232067704200745, acc=0.7991111278533936, loss=0.5232067704200745
test: epoch 80, loss 1.2295414209365845, acc=0.5166666507720947, loss=1.2295414209365845
train: epoch 81, loss 0.5078394412994385, acc=0.8030555844306946, loss=0.5078394412994385
test: epoch 81, loss 1.1157019138336182, acc=0.5916666388511658, loss=1.1157019138336182
train: epoch 82, loss 0.5031201243400574, acc=0.8026111125946045, loss=0.5031201243400574
test: epoch 82, loss 1.1840375661849976, acc=0.4749999940395355, loss=1.1840375661849976
train: epoch 83, loss 0.5099877119064331, acc=0.7973889112472534, loss=0.5099877119064331
test: epoch 83, loss 1.016648530960083, acc=0.5833333134651184, loss=1.016648530960083
train: epoch 84, loss 0.49928921461105347, acc=0.8000555634498596, loss=0.49928921461105347
test: epoch 84, loss 0.9966275095939636, acc=0.6000000238418579, loss=0.9966275095939636
train: epoch 85, loss 0.5344371199607849, acc=0.784166693687439, loss=0.5344371199607849
test: epoch 85, loss 1.2691893577575684, acc=0.5833333134651184, loss=1.2691893577575684
train: epoch 86, loss 0.5260919332504272, acc=0.7900555729866028, loss=0.5260919332504272
test: epoch 86, loss 1.1372838020324707, acc=0.5805555582046509, loss=1.1372838020324707
train: epoch 87, loss 0.508354902267456, acc=0.7953888773918152, loss=0.508354902267456
test: epoch 87, loss 0.867518961429596, acc=0.6861110925674438, loss=0.867518961429596
train: epoch 88, loss 0.5334628224372864, acc=0.7863888740539551, loss=0.5334628224372864
test: epoch 88, loss 0.9935480952262878, acc=0.5833333134651184, loss=0.9935480952262878
train: epoch 89, loss 0.48526570200920105, acc=0.8029444217681885, loss=0.48526570200920105
test: epoch 89, loss 0.9775323271751404, acc=0.6277777552604675, loss=0.9775323271751404
train: epoch 90, loss 0.4984436631202698, acc=0.7949444651603699, loss=0.4984436631202698
test: epoch 90, loss 1.2345565557479858, acc=0.5444444417953491, loss=1.2345565557479858
train: epoch 91, loss 0.5290386080741882, acc=0.7899444699287415, loss=0.5290386080741882
test: epoch 91, loss 0.8361667990684509, acc=0.6722221970558167, loss=0.8361667990684509
train: epoch 92, loss 0.49559256434440613, acc=0.8006666898727417, loss=0.49559256434440613
test: epoch 92, loss 0.9569532871246338, acc=0.5805555582046509, loss=0.9569532871246338
train: epoch 93, loss 0.5390242338180542, acc=0.7847777605056763, loss=0.5390242338180542
test: epoch 93, loss 0.8179844617843628, acc=0.6166666746139526, loss=0.8179844617843628
train: epoch 94, loss 0.5018584728240967, acc=0.7974444627761841, loss=0.5018584728240967
test: epoch 94, loss 0.9210256338119507, acc=0.6194444298744202, loss=0.9210256338119507
train: epoch 95, loss 0.48336148262023926, acc=0.8038889169692993, loss=0.48336148262023926
test: epoch 95, loss 1.0092841386795044, acc=0.5916666388511658, loss=1.0092841386795044
train: epoch 96, loss 0.5317285060882568, acc=0.7833889126777649, loss=0.5317285060882568
test: epoch 96, loss 0.9673833250999451, acc=0.605555534362793, loss=0.9673833250999451
train: epoch 97, loss 0.49983370304107666, acc=0.796999990940094, loss=0.49983370304107666
test: epoch 97, loss 0.8240084648132324, acc=0.6277777552604675, loss=0.8240084648132324
train: epoch 98, loss 0.5184658169746399, acc=0.7900555729866028, loss=0.5184658169746399
test: epoch 98, loss 0.8905295729637146, acc=0.6472222208976746, loss=0.8905295729637146
train: epoch 99, loss 0.4803909659385681, acc=0.8056666851043701, loss=0.4803909659385681
test: epoch 99, loss 0.6982297301292419, acc=0.6694444417953491, loss=0.6982297301292419
train: epoch 100, loss 0.5340918898582458, acc=0.7878333330154419, loss=0.5340918898582458
test: epoch 100, loss 0.8010354042053223, acc=0.6694444417953491, loss=0.8010354042053223
train: epoch 101, loss 0.5079956650733948, acc=0.793666660785675, loss=0.5079956650733948
test: epoch 101, loss 0.904640793800354, acc=0.6194444298744202, loss=0.904640793800354
train: epoch 102, loss 0.4849329888820648, acc=0.7989444732666016, loss=0.4849329888820648
test: epoch 102, loss 1.0423139333724976, acc=0.5611110925674438, loss=1.0423139333724976
train: epoch 103, loss 0.48249512910842896, acc=0.8019999861717224, loss=0.48249512910842896
test: epoch 103, loss 0.7628291845321655, acc=0.6694444417953491, loss=0.7628291845321655
train: epoch 104, loss 0.47084417939186096, acc=0.8069444298744202, loss=0.47084417939186096
test: epoch 104, loss 0.8184480667114258, acc=0.6666666865348816, loss=0.8184480667114258
train: epoch 105, loss 0.47859352827072144, acc=0.8052777647972107, loss=0.47859352827072144
test: epoch 105, loss 0.8641840815544128, acc=0.6638888716697693, loss=0.8641840815544128
train: epoch 106, loss 0.47761133313179016, acc=0.804611086845398, loss=0.47761133313179016
test: epoch 106, loss 0.8115163445472717, acc=0.6638888716697693, loss=0.8115163445472717
train: epoch 107, loss 0.47566112875938416, acc=0.8075555562973022, loss=0.47566112875938416
test: epoch 107, loss 0.8234742283821106, acc=0.6666666865348816, loss=0.8234742283821106
train: epoch 108, loss 0.4855290949344635, acc=0.8059444427490234, loss=0.4855290949344635
test: epoch 108, loss 0.7612673044204712, acc=0.6861110925674438, loss=0.7612673044204712
train: epoch 109, loss 0.4317358434200287, acc=0.8236666917800903, loss=0.4317358434200287
test: epoch 109, loss 0.8064906001091003, acc=0.6638888716697693, loss=0.8064906001091003
train: epoch 110, loss 0.4915198087692261, acc=0.8037222027778625, loss=0.4915198087692261
test: epoch 110, loss 0.7842847108840942, acc=0.6638888716697693, loss=0.7842847108840942
train: epoch 111, loss 0.47138747572898865, acc=0.809333324432373, loss=0.47138747572898865
test: epoch 111, loss 1.1916781663894653, acc=0.5694444179534912, loss=1.1916781663894653
train: epoch 112, loss 0.44954249262809753, acc=0.8227777481079102, loss=0.44954249262809753
test: epoch 112, loss 0.7560153603553772, acc=0.6333333253860474, loss=0.7560153603553772
train: epoch 113, loss 0.42469003796577454, acc=0.8301666378974915, loss=0.42469003796577454
test: epoch 113, loss 0.8822980523109436, acc=0.6222222447395325, loss=0.8822980523109436
train: epoch 114, loss 0.46857622265815735, acc=0.8147777915000916, loss=0.46857622265815735
test: epoch 114, loss 1.2059059143066406, acc=0.574999988079071, loss=1.2059059143066406
train: epoch 115, loss 0.4352208971977234, acc=0.8253889083862305, loss=0.4352208971977234
test: epoch 115, loss 0.6188355684280396, acc=0.699999988079071, loss=0.6188355684280396
train: epoch 116, loss 0.44444236159324646, acc=0.8226666450500488, loss=0.44444236159324646
test: epoch 116, loss 0.920195460319519, acc=0.6666666865348816, loss=0.920195460319519
train: epoch 117, loss 0.4365933835506439, acc=0.8272777795791626, loss=0.4365933835506439
test: epoch 117, loss 0.9050807952880859, acc=0.6583333611488342, loss=0.9050807952880859
train: epoch 118, loss 0.4137958288192749, acc=0.8353888988494873, loss=0.4137958288192749
test: epoch 118, loss 0.8821524381637573, acc=0.6638888716697693, loss=0.8821524381637573
train: epoch 119, loss 0.44688844680786133, acc=0.8232777714729309, loss=0.44688844680786133
test: epoch 119, loss 0.8389747142791748, acc=0.7055555582046509, loss=0.8389747142791748
train: epoch 120, loss 0.42345160245895386, acc=0.8288333415985107, loss=0.42345160245895386
test: epoch 120, loss 0.863393247127533, acc=0.6611111164093018, loss=0.863393247127533
train: epoch 121, loss 0.4114285409450531, acc=0.8396666646003723, loss=0.4114285409450531
test: epoch 121, loss 0.793159008026123, acc=0.6972222328186035, loss=0.793159008026123
train: epoch 122, loss 0.4073939621448517, acc=0.8379999995231628, loss=0.4073939621448517
test: epoch 122, loss 0.8159757256507874, acc=0.6666666865348816, loss=0.8159757256507874
train: epoch 123, loss 0.42434242367744446, acc=0.8296111226081848, loss=0.42434242367744446
test: epoch 123, loss 0.7331595420837402, acc=0.7055555582046509, loss=0.7331595420837402
train: epoch 124, loss 0.4189544916152954, acc=0.8333333134651184, loss=0.4189544916152954
test: epoch 124, loss 0.9465817213058472, acc=0.644444465637207, loss=0.9465817213058472
train: epoch 125, loss 0.39938390254974365, acc=0.8439444303512573, loss=0.39938390254974365
test: epoch 125, loss 0.8508111834526062, acc=0.6694444417953491, loss=0.8508111834526062
train: epoch 126, loss 0.44313815236091614, acc=0.8286666870117188, loss=0.44313815236091614
test: epoch 126, loss 0.9254603385925293, acc=0.6000000238418579, loss=0.9254603385925293
train: epoch 127, loss 0.39972957968711853, acc=0.8405555486679077, loss=0.39972957968711853
test: epoch 127, loss 0.9326251745223999, acc=0.6111111044883728, loss=0.9326251745223999
train: epoch 128, loss 0.45995914936065674, acc=0.8221111297607422, loss=0.45995914936065674
test: epoch 128, loss 0.9420119524002075, acc=0.6722221970558167, loss=0.9420119524002075
train: epoch 129, loss 0.4148361086845398, acc=0.8419444561004639, loss=0.4148361086845398
test: epoch 129, loss 0.8105288147926331, acc=0.6694444417953491, loss=0.8105288147926331
train: epoch 130, loss 0.40495565533638, acc=0.8408889174461365, loss=0.40495565533638
test: epoch 130, loss 0.8157564401626587, acc=0.6805555820465088, loss=0.8157564401626587
train: epoch 131, loss 0.3799128830432892, acc=0.8535555601119995, loss=0.3799128830432892
test: epoch 131, loss 0.7685679197311401, acc=0.7166666388511658, loss=0.7685679197311401
train: epoch 132, loss 0.3735288679599762, acc=0.8580555319786072, loss=0.3735288679599762
test: epoch 132, loss 1.0790027379989624, acc=0.6527777910232544, loss=1.0790027379989624
train: epoch 133, loss 0.39188510179519653, acc=0.8497222065925598, loss=0.39188510179519653
test: epoch 133, loss 0.8213028311729431, acc=0.7194444537162781, loss=0.8213028311729431
train: epoch 134, loss 0.3980666995048523, acc=0.8447777628898621, loss=0.3980666995048523
test: epoch 134, loss 0.7396659255027771, acc=0.7166666388511658, loss=0.7396659255027771
train: epoch 135, loss 0.36917760968208313, acc=0.8586111068725586, loss=0.36917760968208313
test: epoch 135, loss 0.8567673563957214, acc=0.6944444179534912, loss=0.8567673563957214
train: epoch 136, loss 0.37611785531044006, acc=0.8528333306312561, loss=0.37611785531044006
test: epoch 136, loss 0.767604410648346, acc=0.7083333134651184, loss=0.767604410648346
train: epoch 137, loss 0.38368362188339233, acc=0.8542777895927429, loss=0.38368362188339233
test: epoch 137, loss 0.9449257850646973, acc=0.7027778029441833, loss=0.9449257850646973
train: epoch 138, loss 0.3867482542991638, acc=0.8496666550636292, loss=0.3867482542991638
test: epoch 138, loss 0.7631449699401855, acc=0.7138888835906982, loss=0.7631449699401855
train: epoch 139, loss 0.35994279384613037, acc=0.8575555682182312, loss=0.35994279384613037
test: epoch 139, loss 0.668144941329956, acc=0.7194444537162781, loss=0.668144941329956
train: epoch 140, loss 0.37145963311195374, acc=0.8541111350059509, loss=0.37145963311195374
test: epoch 140, loss 0.8186571002006531, acc=0.6916666626930237, loss=0.8186571002006531
train: epoch 141, loss 0.3856602609157562, acc=0.8498333096504211, loss=0.3856602609157562
test: epoch 141, loss 0.6528750061988831, acc=0.7166666388511658, loss=0.6528750061988831
train: epoch 142, loss 0.3720855116844177, acc=0.8553333282470703, loss=0.3720855116844177
test: epoch 142, loss 0.9857759475708008, acc=0.6861110925674438, loss=0.9857759475708008
train: epoch 143, loss 0.3836471736431122, acc=0.8480555415153503, loss=0.3836471736431122
test: epoch 143, loss 0.7445675730705261, acc=0.7194444537162781, loss=0.7445675730705261
train: epoch 144, loss 0.3581809997558594, acc=0.8576666712760925, loss=0.3581809997558594
test: epoch 144, loss 0.812976598739624, acc=0.7138888835906982, loss=0.812976598739624
train: epoch 145, loss 0.36357828974723816, acc=0.8583333492279053, loss=0.36357828974723816
test: epoch 145, loss 0.8090699315071106, acc=0.7138888835906982, loss=0.8090699315071106
train: epoch 146, loss 0.361695259809494, acc=0.8575000166893005, loss=0.361695259809494
test: epoch 146, loss 0.7149093151092529, acc=0.7111111283302307, loss=0.7149093151092529
train: epoch 147, loss 0.3489607870578766, acc=0.8655555844306946, loss=0.3489607870578766
test: epoch 147, loss 0.6820676326751709, acc=0.7166666388511658, loss=0.6820676326751709
train: epoch 148, loss 0.358929842710495, acc=0.8595555424690247, loss=0.358929842710495
test: epoch 148, loss 0.8951849341392517, acc=0.7027778029441833, loss=0.8951849341392517
train: epoch 149, loss 0.40299034118652344, acc=0.8426666855812073, loss=0.40299034118652344
test: epoch 149, loss 0.7266716957092285, acc=0.7166666388511658, loss=0.7266716957092285
train: epoch 150, loss 0.3727761507034302, acc=0.8572221994400024, loss=0.3727761507034302
test: epoch 150, loss 0.8619418740272522, acc=0.7111111283302307, loss=0.8619418740272522
