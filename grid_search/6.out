# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.995", "--one_hot=0", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1946226923, receiver_embed_dim=32, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1946226923, receiver_embed_dim=32, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.3542628288269043, acc=0.05788888782262802, loss=3.3542628288269043
test: epoch 1, loss 3.673959732055664, acc=0.04722222313284874, loss=3.673959732055664
train: epoch 2, loss 3.02532696723938, acc=0.08994444459676743, loss=3.02532696723938
test: epoch 2, loss 3.765011787414551, acc=0.05277777835726738, loss=3.765011787414551
train: epoch 3, loss 2.8852438926696777, acc=0.10766666382551193, loss=2.8852438926696777
test: epoch 3, loss 3.874354362487793, acc=0.05277777835726738, loss=3.874354362487793
train: epoch 4, loss 2.8029112815856934, acc=0.11544444411993027, loss=2.8029112815856934
test: epoch 4, loss 3.955416679382324, acc=0.0555555559694767, loss=3.955416679382324
train: epoch 5, loss 2.759493827819824, acc=0.12066666781902313, loss=2.759493827819824
test: epoch 5, loss 3.9952471256256104, acc=0.05000000074505806, loss=3.9952471256256104
train: epoch 6, loss 2.7160143852233887, acc=0.12866666913032532, loss=2.7160143852233887
test: epoch 6, loss 4.131088733673096, acc=0.0555555559694767, loss=4.131088733673096
train: epoch 7, loss 2.689910411834717, acc=0.12861111760139465, loss=2.689910411834717
test: epoch 7, loss 4.020631790161133, acc=0.05833333358168602, loss=4.020631790161133
train: epoch 8, loss 2.6761133670806885, acc=0.13383333384990692, loss=2.6761133670806885
test: epoch 8, loss 4.146299362182617, acc=0.05277777835726738, loss=4.146299362182617
train: epoch 9, loss 2.6467440128326416, acc=0.1312222182750702, loss=2.6467440128326416
test: epoch 9, loss 3.9749715328216553, acc=0.0555555559694767, loss=3.9749715328216553
train: epoch 10, loss 2.6348235607147217, acc=0.1386111080646515, loss=2.6348235607147217
test: epoch 10, loss 3.995455741882324, acc=0.0555555559694767, loss=3.995455741882324
train: epoch 11, loss 2.620513677597046, acc=0.14461110532283783, loss=2.620513677597046
test: epoch 11, loss 3.9065163135528564, acc=0.06666667014360428, loss=3.9065163135528564
train: epoch 12, loss 2.615253448486328, acc=0.14438888430595398, loss=2.615253448486328
test: epoch 12, loss 3.845980644226074, acc=0.07777778059244156, loss=3.845980644226074
train: epoch 13, loss 2.599250316619873, acc=0.14577777683734894, loss=2.599250316619873
test: epoch 13, loss 3.879714012145996, acc=0.05833333358168602, loss=3.879714012145996
train: epoch 14, loss 2.5843348503112793, acc=0.14749999344348907, loss=2.5843348503112793
test: epoch 14, loss 3.9802606105804443, acc=0.0555555559694767, loss=3.9802606105804443
train: epoch 15, loss 2.5641701221466064, acc=0.1478888839483261, loss=2.5641701221466064
test: epoch 15, loss 3.9071860313415527, acc=0.05833333358168602, loss=3.9071860313415527
train: epoch 16, loss 2.557739019393921, acc=0.1501111090183258, loss=2.557739019393921
test: epoch 16, loss 3.9354727268218994, acc=0.06388889253139496, loss=3.9354727268218994
train: epoch 17, loss 2.552652359008789, acc=0.15538889169692993, loss=2.552652359008789
test: epoch 17, loss 3.961906671524048, acc=0.05833333358168602, loss=3.961906671524048
train: epoch 18, loss 2.5510289669036865, acc=0.15316666662693024, loss=2.5510289669036865
test: epoch 18, loss 3.897320032119751, acc=0.05833333358168602, loss=3.897320032119751
train: epoch 19, loss 2.539921998977661, acc=0.1550000011920929, loss=2.539921998977661
test: epoch 19, loss 3.818988084793091, acc=0.05833333358168602, loss=3.818988084793091
train: epoch 20, loss 2.528914213180542, acc=0.15800000727176666, loss=2.528914213180542
test: epoch 20, loss 3.8698534965515137, acc=0.05833333358168602, loss=3.8698534965515137
train: epoch 21, loss 2.520888566970825, acc=0.1548333317041397, loss=2.520888566970825
test: epoch 21, loss 3.899348258972168, acc=0.05277777835726738, loss=3.899348258972168
train: epoch 22, loss 2.5221405029296875, acc=0.15566666424274445, loss=2.5221405029296875
test: epoch 22, loss 3.713287115097046, acc=0.05277777835726738, loss=3.713287115097046
train: epoch 23, loss 2.50746488571167, acc=0.1619444489479065, loss=2.50746488571167
test: epoch 23, loss 3.935671091079712, acc=0.05277777835726738, loss=3.935671091079712
train: epoch 24, loss 2.515791893005371, acc=0.16011111438274384, loss=2.515791893005371
test: epoch 24, loss 3.84728741645813, acc=0.0555555559694767, loss=3.84728741645813
train: epoch 25, loss 2.506153106689453, acc=0.1601666659116745, loss=2.506153106689453
test: epoch 25, loss 3.7952542304992676, acc=0.0555555559694767, loss=3.7952542304992676
train: epoch 26, loss 2.5135395526885986, acc=0.1617777794599533, loss=2.5135395526885986
test: epoch 26, loss 3.7330169677734375, acc=0.05833333358168602, loss=3.7330169677734375
train: epoch 27, loss 2.497790575027466, acc=0.16011111438274384, loss=2.497790575027466
test: epoch 27, loss 3.6762378215789795, acc=0.0555555559694767, loss=3.6762378215789795
train: epoch 28, loss 2.502514600753784, acc=0.15488888323307037, loss=2.502514600753784
test: epoch 28, loss 3.7543764114379883, acc=0.0694444477558136, loss=3.7543764114379883
train: epoch 29, loss 2.487492322921753, acc=0.15872222185134888, loss=2.487492322921753
test: epoch 29, loss 3.9770419597625732, acc=0.06388889253139496, loss=3.9770419597625732
train: epoch 30, loss 2.491716146469116, acc=0.16172222793102264, loss=2.491716146469116
test: epoch 30, loss 3.8404107093811035, acc=0.07222222536802292, loss=3.8404107093811035
train: epoch 31, loss 2.4876744747161865, acc=0.16200000047683716, loss=2.4876744747161865
test: epoch 31, loss 3.7224366664886475, acc=0.06111111119389534, loss=3.7224366664886475
train: epoch 32, loss 2.472747802734375, acc=0.15950000286102295, loss=2.472747802734375
test: epoch 32, loss 3.736314296722412, acc=0.06388889253139496, loss=3.736314296722412
train: epoch 33, loss 2.4881203174591064, acc=0.16027778387069702, loss=2.4881203174591064
test: epoch 33, loss 3.5719170570373535, acc=0.0833333358168602, loss=3.5719170570373535
train: epoch 34, loss 2.4791808128356934, acc=0.1588333398103714, loss=2.4791808128356934
test: epoch 34, loss 3.622999906539917, acc=0.08611111342906952, loss=3.622999906539917
train: epoch 35, loss 2.4827349185943604, acc=0.1657777726650238, loss=2.4827349185943604
test: epoch 35, loss 3.5486745834350586, acc=0.06666667014360428, loss=3.5486745834350586
train: epoch 36, loss 2.4732658863067627, acc=0.1663888841867447, loss=2.4732658863067627
test: epoch 36, loss 3.6517539024353027, acc=0.05833333358168602, loss=3.6517539024353027
train: epoch 37, loss 2.479240655899048, acc=0.1655000001192093, loss=2.479240655899048
test: epoch 37, loss 3.553269863128662, acc=0.06666667014360428, loss=3.553269863128662
train: epoch 38, loss 2.4556024074554443, acc=0.16555555164813995, loss=2.4556024074554443
test: epoch 38, loss 3.6980178356170654, acc=0.0694444477558136, loss=3.6980178356170654
train: epoch 39, loss 2.4664018154144287, acc=0.162222221493721, loss=2.4664018154144287
test: epoch 39, loss 3.4337058067321777, acc=0.07777778059244156, loss=3.4337058067321777
train: epoch 40, loss 2.463414430618286, acc=0.1635555624961853, loss=2.463414430618286
test: epoch 40, loss 3.4381606578826904, acc=0.06666667014360428, loss=3.4381606578826904
train: epoch 41, loss 2.4617245197296143, acc=0.1631111055612564, loss=2.4617245197296143
test: epoch 41, loss 3.463347911834717, acc=0.08055555820465088, loss=3.463347911834717
train: epoch 42, loss 2.4560232162475586, acc=0.16633333265781403, loss=2.4560232162475586
test: epoch 42, loss 3.4684438705444336, acc=0.06388889253139496, loss=3.4684438705444336
train: epoch 43, loss 2.459972858428955, acc=0.162222221493721, loss=2.459972858428955
test: epoch 43, loss 3.5043702125549316, acc=0.06666667014360428, loss=3.5043702125549316
train: epoch 44, loss 2.4534027576446533, acc=0.16633333265781403, loss=2.4534027576446533
test: epoch 44, loss 3.433457612991333, acc=0.07500000298023224, loss=3.433457612991333
train: epoch 45, loss 2.450061321258545, acc=0.1650555580854416, loss=2.450061321258545
test: epoch 45, loss 3.3638646602630615, acc=0.07777778059244156, loss=3.3638646602630615
train: epoch 46, loss 2.4563968181610107, acc=0.1660555601119995, loss=2.4563968181610107
test: epoch 46, loss 3.455775737762451, acc=0.07222222536802292, loss=3.455775737762451
train: epoch 47, loss 2.448387622833252, acc=0.16833333671092987, loss=2.448387622833252
test: epoch 47, loss 3.4080886840820312, acc=0.0833333358168602, loss=3.4080886840820312
train: epoch 48, loss 2.44180965423584, acc=0.1684444397687912, loss=2.44180965423584
test: epoch 48, loss 3.4774022102355957, acc=0.0833333358168602, loss=3.4774022102355957
train: epoch 49, loss 2.4499099254608154, acc=0.16261111199855804, loss=2.4499099254608154
test: epoch 49, loss 3.3462107181549072, acc=0.08055555820465088, loss=3.3462107181549072
train: epoch 50, loss 2.4587550163269043, acc=0.16611111164093018, loss=2.4587550163269043
test: epoch 50, loss 3.2828288078308105, acc=0.08888889104127884, loss=3.2828288078308105
train: epoch 51, loss 2.453016757965088, acc=0.16388888657093048, loss=2.453016757965088
test: epoch 51, loss 3.2490458488464355, acc=0.06666667014360428, loss=3.2490458488464355
train: epoch 52, loss 2.450934886932373, acc=0.1684444397687912, loss=2.450934886932373
test: epoch 52, loss 3.3231544494628906, acc=0.05833333358168602, loss=3.3231544494628906
train: epoch 53, loss 2.439946174621582, acc=0.1660555601119995, loss=2.439946174621582
test: epoch 53, loss 3.288379669189453, acc=0.08611111342906952, loss=3.288379669189453
train: epoch 54, loss 2.4551095962524414, acc=0.16261111199855804, loss=2.4551095962524414
test: epoch 54, loss 3.2260098457336426, acc=0.09444444626569748, loss=3.2260098457336426
train: epoch 55, loss 2.4410643577575684, acc=0.1675555557012558, loss=2.4410643577575684
test: epoch 55, loss 3.20208740234375, acc=0.09444444626569748, loss=3.20208740234375
train: epoch 56, loss 2.436044692993164, acc=0.1678333282470703, loss=2.436044692993164
test: epoch 56, loss 3.3240058422088623, acc=0.06388889253139496, loss=3.3240058422088623
train: epoch 57, loss 2.4424612522125244, acc=0.16922222077846527, loss=2.4424612522125244
test: epoch 57, loss 3.261806011199951, acc=0.07777778059244156, loss=3.261806011199951
train: epoch 58, loss 2.439490556716919, acc=0.16544444859027863, loss=2.439490556716919
test: epoch 58, loss 3.252368927001953, acc=0.0833333358168602, loss=3.252368927001953
train: epoch 59, loss 2.443882942199707, acc=0.16744443774223328, loss=2.443882942199707
test: epoch 59, loss 3.22042179107666, acc=0.0833333358168602, loss=3.22042179107666
train: epoch 60, loss 2.4345624446868896, acc=0.16588889062404633, loss=2.4345624446868896
test: epoch 60, loss 3.2882513999938965, acc=0.07500000298023224, loss=3.2882513999938965
train: epoch 61, loss 2.4231109619140625, acc=0.1657777726650238, loss=2.4231109619140625
test: epoch 61, loss 3.2151730060577393, acc=0.08888889104127884, loss=3.2151730060577393
train: epoch 62, loss 2.419711112976074, acc=0.17016667127609253, loss=2.419711112976074
test: epoch 62, loss 3.3136940002441406, acc=0.08055555820465088, loss=3.3136940002441406
train: epoch 63, loss 2.418546676635742, acc=0.16949999332427979, loss=2.418546676635742
test: epoch 63, loss 3.1870644092559814, acc=0.0833333358168602, loss=3.1870644092559814
train: epoch 64, loss 2.426391363143921, acc=0.1728888899087906, loss=2.426391363143921
test: epoch 64, loss 3.2738261222839355, acc=0.08055555820465088, loss=3.2738261222839355
train: epoch 65, loss 2.4249017238616943, acc=0.17061111330986023, loss=2.4249017238616943
test: epoch 65, loss 3.2301366329193115, acc=0.06388889253139496, loss=3.2301366329193115
train: epoch 66, loss 2.4216415882110596, acc=0.17133332788944244, loss=2.4216415882110596
test: epoch 66, loss 3.1818692684173584, acc=0.08888889104127884, loss=3.1818692684173584
train: epoch 67, loss 2.41847825050354, acc=0.17222222685813904, loss=2.41847825050354
test: epoch 67, loss 3.1830852031707764, acc=0.08888889104127884, loss=3.1830852031707764
train: epoch 68, loss 2.4191386699676514, acc=0.16861110925674438, loss=2.4191386699676514
test: epoch 68, loss 3.2854669094085693, acc=0.08611111342906952, loss=3.2854669094085693
train: epoch 69, loss 2.4118001461029053, acc=0.17033334076404572, loss=2.4118001461029053
test: epoch 69, loss 3.2441163063049316, acc=0.0833333358168602, loss=3.2441163063049316
train: epoch 70, loss 2.3975162506103516, acc=0.1720000058412552, loss=2.3975162506103516
test: epoch 70, loss 3.2149407863616943, acc=0.08055555820465088, loss=3.2149407863616943
train: epoch 71, loss 2.402127504348755, acc=0.17727777361869812, loss=2.402127504348755
test: epoch 71, loss 3.1943140029907227, acc=0.07500000298023224, loss=3.1943140029907227
train: epoch 72, loss 2.4003851413726807, acc=0.17550000548362732, loss=2.4003851413726807
test: epoch 72, loss 3.209259033203125, acc=0.08611111342906952, loss=3.209259033203125
train: epoch 73, loss 2.39713716506958, acc=0.17294444143772125, loss=2.39713716506958
test: epoch 73, loss 3.1514105796813965, acc=0.0833333358168602, loss=3.1514105796813965
train: epoch 74, loss 2.404362916946411, acc=0.17305555939674377, loss=2.404362916946411
test: epoch 74, loss 3.289628267288208, acc=0.0833333358168602, loss=3.289628267288208
train: epoch 75, loss 2.378537178039551, acc=0.18033333122730255, loss=2.378537178039551
test: epoch 75, loss 3.234283447265625, acc=0.0833333358168602, loss=3.234283447265625
train: epoch 76, loss 2.3776051998138428, acc=0.1752222180366516, loss=2.3776051998138428
test: epoch 76, loss 3.3112733364105225, acc=0.06388889253139496, loss=3.3112733364105225
train: epoch 77, loss 2.393083333969116, acc=0.17483332753181458, loss=2.393083333969116
test: epoch 77, loss 3.1670572757720947, acc=0.08055555820465088, loss=3.1670572757720947
train: epoch 78, loss 2.3793156147003174, acc=0.17511111497879028, loss=2.3793156147003174
test: epoch 78, loss 3.2016067504882812, acc=0.07222222536802292, loss=3.2016067504882812
train: epoch 79, loss 2.379563808441162, acc=0.17727777361869812, loss=2.379563808441162
test: epoch 79, loss 3.2679858207702637, acc=0.08055555820465088, loss=3.2679858207702637
train: epoch 80, loss 2.3769195079803467, acc=0.17911110818386078, loss=2.3769195079803467
test: epoch 80, loss 3.15905499458313, acc=0.09444444626569748, loss=3.15905499458313
train: epoch 81, loss 2.3717334270477295, acc=0.17544443905353546, loss=2.3717334270477295
test: epoch 81, loss 3.1051666736602783, acc=0.0833333358168602, loss=3.1051666736602783
train: epoch 82, loss 2.3684961795806885, acc=0.1805555522441864, loss=2.3684961795806885
test: epoch 82, loss 3.1818559169769287, acc=0.0833333358168602, loss=3.1818559169769287
train: epoch 83, loss 2.3846070766448975, acc=0.1745000034570694, loss=2.3846070766448975
test: epoch 83, loss 3.18977952003479, acc=0.07777778059244156, loss=3.18977952003479
train: epoch 84, loss 2.357562303543091, acc=0.17783333361148834, loss=2.357562303543091
test: epoch 84, loss 3.226001739501953, acc=0.07777778059244156, loss=3.226001739501953
train: epoch 85, loss 2.361337184906006, acc=0.17527778446674347, loss=2.361337184906006
test: epoch 85, loss 3.192986488342285, acc=0.07222222536802292, loss=3.192986488342285
train: epoch 86, loss 2.357398509979248, acc=0.1841111183166504, loss=2.357398509979248
test: epoch 86, loss 3.2712090015411377, acc=0.0833333358168602, loss=3.2712090015411377
train: epoch 87, loss 2.364467144012451, acc=0.17866666615009308, loss=2.364467144012451
test: epoch 87, loss 3.1067566871643066, acc=0.07222222536802292, loss=3.1067566871643066
train: epoch 88, loss 2.357360363006592, acc=0.1793888956308365, loss=2.357360363006592
test: epoch 88, loss 3.1425387859344482, acc=0.08611111342906952, loss=3.1425387859344482
train: epoch 89, loss 2.3602685928344727, acc=0.17883333563804626, loss=2.3602685928344727
test: epoch 89, loss 3.2871763706207275, acc=0.08611111342906952, loss=3.2871763706207275
train: epoch 90, loss 2.362287759780884, acc=0.17977777123451233, loss=2.362287759780884
test: epoch 90, loss 3.2541146278381348, acc=0.07500000298023224, loss=3.2541146278381348
train: epoch 91, loss 2.3434767723083496, acc=0.18405555188655853, loss=2.3434767723083496
test: epoch 91, loss 3.173473596572876, acc=0.07777778059244156, loss=3.173473596572876
train: epoch 92, loss 2.340869426727295, acc=0.18488888442516327, loss=2.340869426727295
test: epoch 92, loss 3.1807191371917725, acc=0.08055555820465088, loss=3.1807191371917725
train: epoch 93, loss 2.337632417678833, acc=0.18549999594688416, loss=2.337632417678833
test: epoch 93, loss 3.2376351356506348, acc=0.07777778059244156, loss=3.2376351356506348
train: epoch 94, loss 2.33882212638855, acc=0.18505555391311646, loss=2.33882212638855
test: epoch 94, loss 3.281435251235962, acc=0.07500000298023224, loss=3.281435251235962
train: epoch 95, loss 2.3415541648864746, acc=0.1811666637659073, loss=2.3415541648864746
test: epoch 95, loss 3.1346182823181152, acc=0.07222222536802292, loss=3.1346182823181152
train: epoch 96, loss 2.330009698867798, acc=0.18222221732139587, loss=2.330009698867798
test: epoch 96, loss 3.240187406539917, acc=0.07500000298023224, loss=3.240187406539917
train: epoch 97, loss 2.3352012634277344, acc=0.18511110544204712, loss=2.3352012634277344
test: epoch 97, loss 3.1819417476654053, acc=0.08055555820465088, loss=3.1819417476654053
train: epoch 98, loss 2.3323137760162354, acc=0.1802777796983719, loss=2.3323137760162354
test: epoch 98, loss 3.153890609741211, acc=0.0833333358168602, loss=3.153890609741211
train: epoch 99, loss 2.331437349319458, acc=0.1860000044107437, loss=2.331437349319458
test: epoch 99, loss 3.1386420726776123, acc=0.07500000298023224, loss=3.1386420726776123
train: epoch 100, loss 2.3257179260253906, acc=0.18766666948795319, loss=2.3257179260253906
test: epoch 100, loss 3.3637852668762207, acc=0.05833333358168602, loss=3.3637852668762207
train: epoch 101, loss 2.327726364135742, acc=0.18205556273460388, loss=2.327726364135742
test: epoch 101, loss 3.217508316040039, acc=0.08055555820465088, loss=3.217508316040039
train: epoch 102, loss 2.322193145751953, acc=0.18272222578525543, loss=2.322193145751953
test: epoch 102, loss 3.2968227863311768, acc=0.08888889104127884, loss=3.2968227863311768
train: epoch 103, loss 2.325254440307617, acc=0.18183332681655884, loss=2.325254440307617
test: epoch 103, loss 3.192901372909546, acc=0.07777778059244156, loss=3.192901372909546
train: epoch 104, loss 2.324157476425171, acc=0.18622222542762756, loss=2.324157476425171
test: epoch 104, loss 3.1653053760528564, acc=0.08611111342906952, loss=3.1653053760528564
train: epoch 105, loss 2.3099725246429443, acc=0.19099999964237213, loss=2.3099725246429443
test: epoch 105, loss 3.1887869834899902, acc=0.06388889253139496, loss=3.1887869834899902
train: epoch 106, loss 2.3155264854431152, acc=0.19216667115688324, loss=2.3155264854431152
test: epoch 106, loss 3.157769203186035, acc=0.07777778059244156, loss=3.157769203186035
train: epoch 107, loss 2.3243231773376465, acc=0.1908888816833496, loss=2.3243231773376465
test: epoch 107, loss 3.177212715148926, acc=0.07777778059244156, loss=3.177212715148926
train: epoch 108, loss 2.322112798690796, acc=0.1922222226858139, loss=2.322112798690796
test: epoch 108, loss 3.1846413612365723, acc=0.07500000298023224, loss=3.1846413612365723
train: epoch 109, loss 2.295801877975464, acc=0.18994444608688354, loss=2.295801877975464
test: epoch 109, loss 3.0794484615325928, acc=0.07222222536802292, loss=3.0794484615325928
train: epoch 110, loss 2.313164710998535, acc=0.18766666948795319, loss=2.313164710998535
test: epoch 110, loss 3.225180149078369, acc=0.08888889104127884, loss=3.225180149078369
train: epoch 111, loss 2.293954372406006, acc=0.189444437623024, loss=2.293954372406006
test: epoch 111, loss 3.1477363109588623, acc=0.0833333358168602, loss=3.1477363109588623
train: epoch 112, loss 2.3012142181396484, acc=0.19361111521720886, loss=2.3012142181396484
test: epoch 112, loss 3.174577474594116, acc=0.0833333358168602, loss=3.174577474594116
train: epoch 113, loss 2.300344228744507, acc=0.19366666674613953, loss=2.300344228744507
test: epoch 113, loss 3.0895137786865234, acc=0.07777778059244156, loss=3.0895137786865234
train: epoch 114, loss 2.2891175746917725, acc=0.1938333362340927, loss=2.2891175746917725
test: epoch 114, loss 3.1842072010040283, acc=0.08611111342906952, loss=3.1842072010040283
train: epoch 115, loss 2.2887818813323975, acc=0.19883333146572113, loss=2.2887818813323975
test: epoch 115, loss 3.1097893714904785, acc=0.07777778059244156, loss=3.1097893714904785
train: epoch 116, loss 2.293206214904785, acc=0.19616666436195374, loss=2.293206214904785
test: epoch 116, loss 3.1880569458007812, acc=0.07222222536802292, loss=3.1880569458007812
train: epoch 117, loss 2.2742083072662354, acc=0.1969444453716278, loss=2.2742083072662354
test: epoch 117, loss 3.217879295349121, acc=0.0694444477558136, loss=3.217879295349121
train: epoch 118, loss 2.2712409496307373, acc=0.20061111450195312, loss=2.2712409496307373
test: epoch 118, loss 3.225905656814575, acc=0.08055555820465088, loss=3.225905656814575
train: epoch 119, loss 2.2906711101531982, acc=0.19750000536441803, loss=2.2906711101531982
test: epoch 119, loss 3.119924545288086, acc=0.0555555559694767, loss=3.119924545288086
train: epoch 120, loss 2.2691664695739746, acc=0.19555555284023285, loss=2.2691664695739746
test: epoch 120, loss 3.2206192016601562, acc=0.07500000298023224, loss=3.2206192016601562
train: epoch 121, loss 2.2702324390411377, acc=0.19822221994400024, loss=2.2702324390411377
test: epoch 121, loss 3.1143717765808105, acc=0.07500000298023224, loss=3.1143717765808105
train: epoch 122, loss 2.28254771232605, acc=0.20144444704055786, loss=2.28254771232605
test: epoch 122, loss 3.1335737705230713, acc=0.07777778059244156, loss=3.1335737705230713
train: epoch 123, loss 2.263470411300659, acc=0.1984444409608841, loss=2.263470411300659
test: epoch 123, loss 3.141533136367798, acc=0.07500000298023224, loss=3.141533136367798
train: epoch 124, loss 2.2652032375335693, acc=0.20338888466358185, loss=2.2652032375335693
test: epoch 124, loss 3.166224956512451, acc=0.07500000298023224, loss=3.166224956512451
train: epoch 125, loss 2.268618106842041, acc=0.20116665959358215, loss=2.268618106842041
test: epoch 125, loss 3.2267937660217285, acc=0.08611111342906952, loss=3.2267937660217285
train: epoch 126, loss 2.2581746578216553, acc=0.1938333362340927, loss=2.2581746578216553
test: epoch 126, loss 3.060601234436035, acc=0.07500000298023224, loss=3.060601234436035
train: epoch 127, loss 2.2650890350341797, acc=0.20555555820465088, loss=2.2650890350341797
test: epoch 127, loss 3.1937718391418457, acc=0.0694444477558136, loss=3.1937718391418457
train: epoch 128, loss 2.251924514770508, acc=0.20283333957195282, loss=2.251924514770508
test: epoch 128, loss 3.2592461109161377, acc=0.08055555820465088, loss=3.2592461109161377
train: epoch 129, loss 2.2505548000335693, acc=0.2058333307504654, loss=2.2505548000335693
test: epoch 129, loss 3.1460795402526855, acc=0.07222222536802292, loss=3.1460795402526855
train: epoch 130, loss 2.2732737064361572, acc=0.20333333313465118, loss=2.2732737064361572
test: epoch 130, loss 3.0526857376098633, acc=0.07777778059244156, loss=3.0526857376098633
train: epoch 131, loss 2.2588565349578857, acc=0.2043333351612091, loss=2.2588565349578857
test: epoch 131, loss 3.2652699947357178, acc=0.08055555820465088, loss=3.2652699947357178
train: epoch 132, loss 2.2634124755859375, acc=0.19966666400432587, loss=2.2634124755859375
test: epoch 132, loss 3.1044392585754395, acc=0.08611111342906952, loss=3.1044392585754395
train: epoch 133, loss 2.256800651550293, acc=0.20550000667572021, loss=2.256800651550293
test: epoch 133, loss 3.1461479663848877, acc=0.07500000298023224, loss=3.1461479663848877
train: epoch 134, loss 2.252380132675171, acc=0.20000000298023224, loss=2.252380132675171
test: epoch 134, loss 3.0870399475097656, acc=0.0833333358168602, loss=3.0870399475097656
train: epoch 135, loss 2.2437427043914795, acc=0.20177777111530304, loss=2.2437427043914795
test: epoch 135, loss 3.090822458267212, acc=0.08055555820465088, loss=3.090822458267212
train: epoch 136, loss 2.2235374450683594, acc=0.2029999941587448, loss=2.2235374450683594
test: epoch 136, loss 3.1097939014434814, acc=0.09166666865348816, loss=3.1097939014434814
train: epoch 137, loss 2.2460596561431885, acc=0.2092222273349762, loss=2.2460596561431885
test: epoch 137, loss 3.125844955444336, acc=0.09444444626569748, loss=3.125844955444336
train: epoch 138, loss 2.235917091369629, acc=0.20677778124809265, loss=2.235917091369629
test: epoch 138, loss 3.0678508281707764, acc=0.08611111342906952, loss=3.0678508281707764
train: epoch 139, loss 2.237610101699829, acc=0.20305556058883667, loss=2.237610101699829
test: epoch 139, loss 3.1444621086120605, acc=0.0833333358168602, loss=3.1444621086120605
train: epoch 140, loss 2.22660493850708, acc=0.20483332872390747, loss=2.22660493850708
test: epoch 140, loss 3.1623241901397705, acc=0.0833333358168602, loss=3.1623241901397705
train: epoch 141, loss 2.2433722019195557, acc=0.20661111176013947, loss=2.2433722019195557
test: epoch 141, loss 3.1742422580718994, acc=0.08611111342906952, loss=3.1742422580718994
train: epoch 142, loss 2.2277896404266357, acc=0.21022222936153412, loss=2.2277896404266357
test: epoch 142, loss 3.2422173023223877, acc=0.08888889104127884, loss=3.2422173023223877
train: epoch 143, loss 2.236278772354126, acc=0.2084999978542328, loss=2.236278772354126
test: epoch 143, loss 3.152387857437134, acc=0.08611111342906952, loss=3.152387857437134
train: epoch 144, loss 2.234860420227051, acc=0.2076111137866974, loss=2.234860420227051
test: epoch 144, loss 3.131868839263916, acc=0.0694444477558136, loss=3.131868839263916
train: epoch 145, loss 2.236879587173462, acc=0.21355555951595306, loss=2.236879587173462
test: epoch 145, loss 3.1767337322235107, acc=0.08888889104127884, loss=3.1767337322235107
train: epoch 146, loss 2.225259780883789, acc=0.21050000190734863, loss=2.225259780883789
test: epoch 146, loss 3.2112584114074707, acc=0.06111111119389534, loss=3.2112584114074707
train: epoch 147, loss 2.200014352798462, acc=0.21166667342185974, loss=2.200014352798462
test: epoch 147, loss 3.2448794841766357, acc=0.0833333358168602, loss=3.2448794841766357
train: epoch 148, loss 2.215090036392212, acc=0.21061110496520996, loss=2.215090036392212
test: epoch 148, loss 3.2794759273529053, acc=0.08888889104127884, loss=3.2794759273529053
train: epoch 149, loss 2.2225520610809326, acc=0.2117222249507904, loss=2.2225520610809326
test: epoch 149, loss 3.3438022136688232, acc=0.08611111342906952, loss=3.3438022136688232
train: epoch 150, loss 2.2297720909118652, acc=0.21138888597488403, loss=2.2297720909118652
test: epoch 150, loss 3.1493303775787354, acc=0.0833333358168602, loss=3.1493303775787354
