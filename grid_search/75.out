# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1.5", "--temp_decay=1", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=31466834, receiver_embed_dim=64, save_run=0, temp_decay=1.0, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=31466834, receiver_embed_dim=64, save_run=False, temp_decay=1.0, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.539033889770508, acc=0.05050000175833702, loss=3.539033889770508
test: epoch 1, loss 3.4973292350769043, acc=0.06388889253139496, loss=3.4973292350769043
train: epoch 2, loss 3.4702394008636475, acc=0.053611110895872116, loss=3.4702394008636475
test: epoch 2, loss 3.253514289855957, acc=0.05277777835726738, loss=3.253514289855957
train: epoch 3, loss 3.3797028064727783, acc=0.057055555284023285, loss=3.3797028064727783
test: epoch 3, loss 3.462059497833252, acc=0.07777778059244156, loss=3.462059497833252
train: epoch 4, loss 3.027066230773926, acc=0.0981111079454422, loss=3.027066230773926
test: epoch 4, loss 4.897979259490967, acc=0.06388889253139496, loss=4.897979259490967
train: epoch 5, loss 2.7217202186584473, acc=0.13994444906711578, loss=2.7217202186584473
test: epoch 5, loss 5.626567840576172, acc=0.06388889253139496, loss=5.626567840576172
train: epoch 6, loss 2.5602784156799316, acc=0.15994444489479065, loss=2.5602784156799316
test: epoch 6, loss 5.778829097747803, acc=0.03888889029622078, loss=5.778829097747803
train: epoch 7, loss 2.4598941802978516, acc=0.17883333563804626, loss=2.4598941802978516
test: epoch 7, loss 5.981732368469238, acc=0.0416666679084301, loss=5.981732368469238
train: epoch 8, loss 2.3897483348846436, acc=0.18994444608688354, loss=2.3897483348846436
test: epoch 8, loss 5.841760158538818, acc=0.05000000074505806, loss=5.841760158538818
train: epoch 9, loss 2.327101469039917, acc=0.2040555626153946, loss=2.327101469039917
test: epoch 9, loss 5.815331935882568, acc=0.05277777835726738, loss=5.815331935882568
train: epoch 10, loss 2.2823524475097656, acc=0.2158888876438141, loss=2.2823524475097656
test: epoch 10, loss 5.885664939880371, acc=0.04722222313284874, loss=5.885664939880371
train: epoch 11, loss 2.2382326126098633, acc=0.2244444489479065, loss=2.2382326126098633
test: epoch 11, loss 5.932034015655518, acc=0.0555555559694767, loss=5.932034015655518
train: epoch 12, loss 2.194779396057129, acc=0.2345000058412552, loss=2.194779396057129
test: epoch 12, loss 5.887688636779785, acc=0.0555555559694767, loss=5.887688636779785
train: epoch 13, loss 2.152254104614258, acc=0.248055562376976, loss=2.152254104614258
test: epoch 13, loss 5.756507396697998, acc=0.05000000074505806, loss=5.756507396697998
train: epoch 14, loss 2.1313581466674805, acc=0.2516666650772095, loss=2.1313581466674805
test: epoch 14, loss 5.741308212280273, acc=0.04722222313284874, loss=5.741308212280273
train: epoch 15, loss 2.095137596130371, acc=0.26572221517562866, loss=2.095137596130371
test: epoch 15, loss 5.619330883026123, acc=0.0555555559694767, loss=5.619330883026123
train: epoch 16, loss 2.07951021194458, acc=0.27355554699897766, loss=2.07951021194458
test: epoch 16, loss 5.370813369750977, acc=0.07222222536802292, loss=5.370813369750977
train: epoch 17, loss 2.021271228790283, acc=0.2840000092983246, loss=2.021271228790283
test: epoch 17, loss 5.2842254638671875, acc=0.0694444477558136, loss=5.2842254638671875
train: epoch 18, loss 2.0111372470855713, acc=0.2943333387374878, loss=2.0111372470855713
test: epoch 18, loss 5.118933200836182, acc=0.07222222536802292, loss=5.118933200836182
train: epoch 19, loss 1.9811385869979858, acc=0.29677778482437134, loss=1.9811385869979858
test: epoch 19, loss 4.947318077087402, acc=0.08611111342906952, loss=4.947318077087402
train: epoch 20, loss 1.9591890573501587, acc=0.30327779054641724, loss=1.9591890573501587
test: epoch 20, loss 4.788921356201172, acc=0.0833333358168602, loss=4.788921356201172
train: epoch 21, loss 1.9292140007019043, acc=0.312666654586792, loss=1.9292140007019043
test: epoch 21, loss 4.569418907165527, acc=0.0972222238779068, loss=4.569418907165527
train: epoch 22, loss 1.9012956619262695, acc=0.31933334469795227, loss=1.9012956619262695
test: epoch 22, loss 4.719384670257568, acc=0.08055555820465088, loss=4.719384670257568
train: epoch 23, loss 1.8757888078689575, acc=0.32677778601646423, loss=1.8757888078689575
test: epoch 23, loss 4.548941612243652, acc=0.08888889104127884, loss=4.548941612243652
train: epoch 24, loss 1.8423817157745361, acc=0.33772221207618713, loss=1.8423817157745361
test: epoch 24, loss 4.514786243438721, acc=0.0972222238779068, loss=4.514786243438721
train: epoch 25, loss 1.8180180788040161, acc=0.3431110978126526, loss=1.8180180788040161
test: epoch 25, loss 4.4430084228515625, acc=0.0972222238779068, loss=4.4430084228515625
train: epoch 26, loss 1.787543773651123, acc=0.3563888967037201, loss=1.787543773651123
test: epoch 26, loss 4.3957319259643555, acc=0.0972222238779068, loss=4.3957319259643555
train: epoch 27, loss 1.782899022102356, acc=0.35883334279060364, loss=1.782899022102356
test: epoch 27, loss 4.261511325836182, acc=0.0972222238779068, loss=4.261511325836182
train: epoch 28, loss 1.7581342458724976, acc=0.3738333284854889, loss=1.7581342458724976
test: epoch 28, loss 4.16672420501709, acc=0.10833333432674408, loss=4.16672420501709
train: epoch 29, loss 1.7130519151687622, acc=0.37950000166893005, loss=1.7130519151687622
test: epoch 29, loss 4.166012287139893, acc=0.09444444626569748, loss=4.166012287139893
train: epoch 30, loss 1.7097151279449463, acc=0.38127776980400085, loss=1.7097151279449463
test: epoch 30, loss 3.94036602973938, acc=0.11944444477558136, loss=3.94036602973938
train: epoch 31, loss 1.6839505434036255, acc=0.3963888883590698, loss=1.6839505434036255
test: epoch 31, loss 3.789926290512085, acc=0.12777778506278992, loss=3.789926290512085
train: epoch 32, loss 1.6437501907348633, acc=0.4051666557788849, loss=1.6437501907348633
test: epoch 32, loss 3.6361353397369385, acc=0.13055555522441864, loss=3.6361353397369385
train: epoch 33, loss 1.6213023662567139, acc=0.41527777910232544, loss=1.6213023662567139
test: epoch 33, loss 3.6753971576690674, acc=0.12777778506278992, loss=3.6753971576690674
train: epoch 34, loss 1.614995002746582, acc=0.41538888216018677, loss=1.614995002746582
test: epoch 34, loss 3.6091344356536865, acc=0.12777778506278992, loss=3.6091344356536865
train: epoch 35, loss 1.5867218971252441, acc=0.4267222285270691, loss=1.5867218971252441
test: epoch 35, loss 3.544553279876709, acc=0.13611111044883728, loss=3.544553279876709
train: epoch 36, loss 1.5604501962661743, acc=0.44038888812065125, loss=1.5604501962661743
test: epoch 36, loss 3.3542985916137695, acc=0.14166666567325592, loss=3.3542985916137695
train: epoch 37, loss 1.5389432907104492, acc=0.4463889002799988, loss=1.5389432907104492
test: epoch 37, loss 3.2952094078063965, acc=0.14722222089767456, loss=3.2952094078063965
train: epoch 38, loss 1.5159515142440796, acc=0.46166667342185974, loss=1.5159515142440796
test: epoch 38, loss 3.268389940261841, acc=0.16111111640930176, loss=3.268389940261841
train: epoch 39, loss 1.5008925199508667, acc=0.46016666293144226, loss=1.5008925199508667
test: epoch 39, loss 3.2832608222961426, acc=0.15555556118488312, loss=3.2832608222961426
train: epoch 40, loss 1.4737268686294556, acc=0.47749999165534973, loss=1.4737268686294556
test: epoch 40, loss 3.2182343006134033, acc=0.14166666567325592, loss=3.2182343006134033
train: epoch 41, loss 1.4444100856781006, acc=0.4850555658340454, loss=1.4444100856781006
test: epoch 41, loss 3.142956256866455, acc=0.15555556118488312, loss=3.142956256866455
train: epoch 42, loss 1.4420702457427979, acc=0.49105554819107056, loss=1.4420702457427979
test: epoch 42, loss 3.1586906909942627, acc=0.16388888657093048, loss=3.1586906909942627
train: epoch 43, loss 1.4147220849990845, acc=0.5090555548667908, loss=1.4147220849990845
test: epoch 43, loss 3.065969944000244, acc=0.1805555522441864, loss=3.065969944000244
train: epoch 44, loss 1.3917124271392822, acc=0.512333333492279, loss=1.3917124271392822
test: epoch 44, loss 3.0516321659088135, acc=0.18611110746860504, loss=3.0516321659088135
train: epoch 45, loss 1.3693311214447021, acc=0.5251666903495789, loss=1.3693311214447021
test: epoch 45, loss 2.995204448699951, acc=0.18611110746860504, loss=2.995204448699951
train: epoch 46, loss 1.3569549322128296, acc=0.5302222371101379, loss=1.3569549322128296
test: epoch 46, loss 2.9925484657287598, acc=0.18888889253139496, loss=2.9925484657287598
train: epoch 47, loss 1.3342665433883667, acc=0.5423333048820496, loss=1.3342665433883667
test: epoch 47, loss 2.952075958251953, acc=0.1944444477558136, loss=2.952075958251953
train: epoch 48, loss 1.3162931203842163, acc=0.5521666407585144, loss=1.3162931203842163
test: epoch 48, loss 2.909508228302002, acc=0.21388888359069824, loss=2.909508228302002
train: epoch 49, loss 1.2770133018493652, acc=0.5640555620193481, loss=1.2770133018493652
test: epoch 49, loss 2.9114160537719727, acc=0.2083333283662796, loss=2.9114160537719727
train: epoch 50, loss 1.2469277381896973, acc=0.5843333601951599, loss=1.2469277381896973
test: epoch 50, loss 2.782381296157837, acc=0.23333333432674408, loss=2.782381296157837
train: epoch 51, loss 1.2291713953018188, acc=0.589722216129303, loss=1.2291713953018188
test: epoch 51, loss 2.7512195110321045, acc=0.23055554926395416, loss=2.7512195110321045
train: epoch 52, loss 1.2015119791030884, acc=0.6032778024673462, loss=1.2015119791030884
test: epoch 52, loss 2.7544479370117188, acc=0.25, loss=2.7544479370117188
train: epoch 53, loss 1.191318392753601, acc=0.6079999804496765, loss=1.191318392753601
test: epoch 53, loss 2.6798794269561768, acc=0.2527777850627899, loss=2.6798794269561768
train: epoch 54, loss 1.153588056564331, acc=0.6174444556236267, loss=1.153588056564331
test: epoch 54, loss 2.6303629875183105, acc=0.2361111044883728, loss=2.6303629875183105
train: epoch 55, loss 1.1305699348449707, acc=0.6309999823570251, loss=1.1305699348449707
test: epoch 55, loss 2.6547303199768066, acc=0.25, loss=2.6547303199768066
train: epoch 56, loss 1.107378363609314, acc=0.6464444398880005, loss=1.107378363609314
test: epoch 56, loss 2.641195774078369, acc=0.2527777850627899, loss=2.641195774078369
train: epoch 57, loss 1.0768139362335205, acc=0.6478888988494873, loss=1.0768139362335205
test: epoch 57, loss 2.5622470378875732, acc=0.2611111104488373, loss=2.5622470378875732
train: epoch 58, loss 1.0515084266662598, acc=0.6667222380638123, loss=1.0515084266662598
test: epoch 58, loss 2.483093023300171, acc=0.2638888955116272, loss=2.483093023300171
train: epoch 59, loss 1.015088677406311, acc=0.6728888750076294, loss=1.015088677406311
test: epoch 59, loss 2.449575424194336, acc=0.2666666805744171, loss=2.449575424194336
train: epoch 60, loss 1.001183032989502, acc=0.6868888735771179, loss=1.001183032989502
test: epoch 60, loss 2.4796135425567627, acc=0.26944443583488464, loss=2.4796135425567627
train: epoch 61, loss 0.9662750959396362, acc=0.7010555267333984, loss=0.9662750959396362
test: epoch 61, loss 2.473055601119995, acc=0.2916666567325592, loss=2.473055601119995
train: epoch 62, loss 0.958198606967926, acc=0.7063888907432556, loss=0.958198606967926
test: epoch 62, loss 2.465937376022339, acc=0.29722222685813904, loss=2.465937376022339
train: epoch 63, loss 0.9240204691886902, acc=0.7208333611488342, loss=0.9240204691886902
test: epoch 63, loss 2.463421106338501, acc=0.2944444417953491, loss=2.463421106338501
train: epoch 64, loss 0.8934472799301147, acc=0.7316666841506958, loss=0.8934472799301147
test: epoch 64, loss 2.4865756034851074, acc=0.28333333134651184, loss=2.4865756034851074
train: epoch 65, loss 0.8652797341346741, acc=0.7432222366333008, loss=0.8652797341346741
test: epoch 65, loss 2.4330496788024902, acc=0.30000001192092896, loss=2.4330496788024902
train: epoch 66, loss 0.8366841077804565, acc=0.7541666626930237, loss=0.8366841077804565
test: epoch 66, loss 2.3621182441711426, acc=0.3055555522441864, loss=2.3621182441711426
train: epoch 67, loss 0.8097177147865295, acc=0.7692221999168396, loss=0.8097177147865295
test: epoch 67, loss 2.3459231853485107, acc=0.31111112236976624, loss=2.3459231853485107
train: epoch 68, loss 0.782958447933197, acc=0.7749999761581421, loss=0.782958447933197
test: epoch 68, loss 2.2993967533111572, acc=0.3027777671813965, loss=2.2993967533111572
train: epoch 69, loss 0.7660449743270874, acc=0.781166672706604, loss=0.7660449743270874
test: epoch 69, loss 2.2375903129577637, acc=0.3027777671813965, loss=2.2375903129577637
train: epoch 70, loss 0.7450768351554871, acc=0.7906110882759094, loss=0.7450768351554871
test: epoch 70, loss 2.2491226196289062, acc=0.3055555522441864, loss=2.2491226196289062
train: epoch 71, loss 0.7299264669418335, acc=0.7985555529594421, loss=0.7299264669418335
test: epoch 71, loss 2.2175920009613037, acc=0.30000001192092896, loss=2.2175920009613037
train: epoch 72, loss 0.6926088333129883, acc=0.8133333325386047, loss=0.6926088333129883
test: epoch 72, loss 2.178408145904541, acc=0.30000001192092896, loss=2.178408145904541
train: epoch 73, loss 0.6595684885978699, acc=0.8236111402511597, loss=0.6595684885978699
test: epoch 73, loss 2.1441335678100586, acc=0.31111112236976624, loss=2.1441335678100586
train: epoch 74, loss 0.6218547224998474, acc=0.8338888883590698, loss=0.6218547224998474
test: epoch 74, loss 2.182141065597534, acc=0.3166666626930237, loss=2.182141065597534
train: epoch 75, loss 0.6227986812591553, acc=0.8355555534362793, loss=0.6227986812591553
test: epoch 75, loss 2.2000741958618164, acc=0.3027777671813965, loss=2.2000741958618164
train: epoch 76, loss 0.5768904089927673, acc=0.8483889102935791, loss=0.5768904089927673
test: epoch 76, loss 2.1616835594177246, acc=0.31111112236976624, loss=2.1616835594177246
train: epoch 77, loss 0.563361644744873, acc=0.855388879776001, loss=0.563361644744873
test: epoch 77, loss 2.12687611579895, acc=0.30000001192092896, loss=2.12687611579895
train: epoch 78, loss 0.5488429069519043, acc=0.8608333468437195, loss=0.5488429069519043
test: epoch 78, loss 2.0976991653442383, acc=0.3166666626930237, loss=2.0976991653442383
train: epoch 79, loss 0.5391061902046204, acc=0.8633333444595337, loss=0.5391061902046204
test: epoch 79, loss 2.098217725753784, acc=0.3166666626930237, loss=2.098217725753784
train: epoch 80, loss 0.5078506469726562, acc=0.8741111159324646, loss=0.5078506469726562
test: epoch 80, loss 2.0576188564300537, acc=0.3194444477558136, loss=2.0576188564300537
train: epoch 81, loss 0.49686628580093384, acc=0.8744444251060486, loss=0.49686628580093384
test: epoch 81, loss 2.089730739593506, acc=0.3194444477558136, loss=2.089730739593506
train: epoch 82, loss 0.475425660610199, acc=0.8814444541931152, loss=0.475425660610199
test: epoch 82, loss 2.023566484451294, acc=0.3194444477558136, loss=2.023566484451294
train: epoch 83, loss 0.47242704033851624, acc=0.8843333125114441, loss=0.47242704033851624
test: epoch 83, loss 1.977352261543274, acc=0.3222222328186035, loss=1.977352261543274
train: epoch 84, loss 0.461393266916275, acc=0.8899444341659546, loss=0.461393266916275
test: epoch 84, loss 1.9887938499450684, acc=0.32499998807907104, loss=1.9887938499450684
train: epoch 85, loss 0.4574059247970581, acc=0.894444465637207, loss=0.4574059247970581
test: epoch 85, loss 1.9471172094345093, acc=0.3361110985279083, loss=1.9471172094345093
train: epoch 86, loss 0.4194825291633606, acc=0.8992778062820435, loss=0.4194825291633606
test: epoch 86, loss 1.9350444078445435, acc=0.32499998807907104, loss=1.9350444078445435
train: epoch 87, loss 0.4141692519187927, acc=0.9028333425521851, loss=0.4141692519187927
test: epoch 87, loss 1.9444756507873535, acc=0.3194444477558136, loss=1.9444756507873535
train: epoch 88, loss 0.3964295983314514, acc=0.9068333506584167, loss=0.3964295983314514
test: epoch 88, loss 1.9606798887252808, acc=0.32777777314186096, loss=1.9606798887252808
train: epoch 89, loss 0.403948038816452, acc=0.9097777605056763, loss=0.403948038816452
test: epoch 89, loss 1.9437150955200195, acc=0.3361110985279083, loss=1.9437150955200195
train: epoch 90, loss 0.38504523038864136, acc=0.9152222275733948, loss=0.38504523038864136
test: epoch 90, loss 1.9547102451324463, acc=0.3166666626930237, loss=1.9547102451324463
train: epoch 91, loss 0.35940903425216675, acc=0.9198889136314392, loss=0.35940903425216675
test: epoch 91, loss 1.950905442237854, acc=0.32777777314186096, loss=1.950905442237854
train: epoch 92, loss 0.3567046821117401, acc=0.9196666479110718, loss=0.3567046821117401
test: epoch 92, loss 1.9546953439712524, acc=0.32777777314186096, loss=1.9546953439712524
train: epoch 93, loss 0.3498474359512329, acc=0.9210000038146973, loss=0.3498474359512329
test: epoch 93, loss 1.8899048566818237, acc=0.32777777314186096, loss=1.8899048566818237
train: epoch 94, loss 0.34663963317871094, acc=0.9242222309112549, loss=0.34663963317871094
test: epoch 94, loss 1.9180679321289062, acc=0.33888888359069824, loss=1.9180679321289062
train: epoch 95, loss 0.31903597712516785, acc=0.9308333396911621, loss=0.31903597712516785
test: epoch 95, loss 1.9456654787063599, acc=0.32499998807907104, loss=1.9456654787063599
train: epoch 96, loss 0.32626456022262573, acc=0.9283888936042786, loss=0.32626456022262573
test: epoch 96, loss 1.8761495351791382, acc=0.3361110985279083, loss=1.8761495351791382
train: epoch 97, loss 0.3128034770488739, acc=0.9311110973358154, loss=0.3128034770488739
test: epoch 97, loss 1.854966640472412, acc=0.3444444537162781, loss=1.854966640472412
train: epoch 98, loss 0.31299468874931335, acc=0.9300000071525574, loss=0.31299468874931335
test: epoch 98, loss 1.8642957210540771, acc=0.32499998807907104, loss=1.8642957210540771
train: epoch 99, loss 0.29303210973739624, acc=0.9385555386543274, loss=0.29303210973739624
test: epoch 99, loss 1.840003490447998, acc=0.34166666865348816, loss=1.840003490447998
train: epoch 100, loss 0.29557523131370544, acc=0.9367777705192566, loss=0.29557523131370544
test: epoch 100, loss 1.858132004737854, acc=0.32777777314186096, loss=1.858132004737854
train: epoch 101, loss 0.28889739513397217, acc=0.9396666884422302, loss=0.28889739513397217
test: epoch 101, loss 1.8233212232589722, acc=0.34166666865348816, loss=1.8233212232589722
train: epoch 102, loss 0.2687861919403076, acc=0.9420555830001831, loss=0.2687861919403076
test: epoch 102, loss 1.8643513917922974, acc=0.3361110985279083, loss=1.8643513917922974
train: epoch 103, loss 0.2784174680709839, acc=0.940666675567627, loss=0.2784174680709839
test: epoch 103, loss 1.8365482091903687, acc=0.33888888359069824, loss=1.8365482091903687
train: epoch 104, loss 0.2621631622314453, acc=0.945722222328186, loss=0.2621631622314453
test: epoch 104, loss 1.8149878978729248, acc=0.3638888895511627, loss=1.8149878978729248
train: epoch 105, loss 0.2719273269176483, acc=0.9450555443763733, loss=0.2719273269176483
test: epoch 105, loss 1.8273212909698486, acc=0.3611111044883728, loss=1.8273212909698486
train: epoch 106, loss 0.2579301595687866, acc=0.945555567741394, loss=0.2579301595687866
test: epoch 106, loss 1.803837776184082, acc=0.3444444537162781, loss=1.803837776184082
train: epoch 107, loss 0.24531929194927216, acc=0.9478333592414856, loss=0.24531929194927216
test: epoch 107, loss 1.8238393068313599, acc=0.3472222089767456, loss=1.8238393068313599
train: epoch 108, loss 0.24019411206245422, acc=0.9474444389343262, loss=0.24019411206245422
test: epoch 108, loss 1.7955689430236816, acc=0.3472222089767456, loss=1.7955689430236816
train: epoch 109, loss 0.24127346277236938, acc=0.9522777795791626, loss=0.24127346277236938
test: epoch 109, loss 1.7921799421310425, acc=0.3583333194255829, loss=1.7921799421310425
train: epoch 110, loss 0.23463326692581177, acc=0.9503333568572998, loss=0.23463326692581177
test: epoch 110, loss 1.8221361637115479, acc=0.32499998807907104, loss=1.8221361637115479
train: epoch 111, loss 0.22696100175380707, acc=0.952833354473114, loss=0.22696100175380707
test: epoch 111, loss 1.8097026348114014, acc=0.35277777910232544, loss=1.8097026348114014
train: epoch 112, loss 0.23055583238601685, acc=0.9536666870117188, loss=0.23055583238601685
test: epoch 112, loss 1.8200277090072632, acc=0.3638888895511627, loss=1.8200277090072632
train: epoch 113, loss 0.2102365791797638, acc=0.9566110968589783, loss=0.2102365791797638
test: epoch 113, loss 1.7539615631103516, acc=0.3361110985279083, loss=1.7539615631103516
train: epoch 114, loss 0.21736231446266174, acc=0.957277774810791, loss=0.21736231446266174
test: epoch 114, loss 1.7204457521438599, acc=0.35555556416511536, loss=1.7204457521438599
train: epoch 115, loss 0.20436424016952515, acc=0.9589444398880005, loss=0.20436424016952515
test: epoch 115, loss 1.7356679439544678, acc=0.3611111044883728, loss=1.7356679439544678
train: epoch 116, loss 0.19645453989505768, acc=0.9570555686950684, loss=0.19645453989505768
test: epoch 116, loss 1.7060706615447998, acc=0.3472222089767456, loss=1.7060706615447998
train: epoch 117, loss 0.20867611467838287, acc=0.9578889012336731, loss=0.20867611467838287
test: epoch 117, loss 1.6918331384658813, acc=0.3722222149372101, loss=1.6918331384658813
train: epoch 118, loss 0.18383567035198212, acc=0.9613333344459534, loss=0.18383567035198212
test: epoch 118, loss 1.7006107568740845, acc=0.3777777850627899, loss=1.7006107568740845
train: epoch 119, loss 0.1815251260995865, acc=0.9623888731002808, loss=0.1815251260995865
test: epoch 119, loss 1.6720343828201294, acc=0.3638888895511627, loss=1.6720343828201294
train: epoch 120, loss 0.19940049946308136, acc=0.9595555663108826, loss=0.19940049946308136
test: epoch 120, loss 1.6825592517852783, acc=0.3499999940395355, loss=1.6825592517852783
train: epoch 121, loss 0.17449359595775604, acc=0.9651111364364624, loss=0.17449359595775604
test: epoch 121, loss 1.6983184814453125, acc=0.3499999940395355, loss=1.6983184814453125
train: epoch 122, loss 0.1765671670436859, acc=0.965499997138977, loss=0.1765671670436859
test: epoch 122, loss 1.662203311920166, acc=0.35277777910232544, loss=1.662203311920166
train: epoch 123, loss 0.18325971066951752, acc=0.9627777934074402, loss=0.18325971066951752
test: epoch 123, loss 1.6747270822525024, acc=0.3611111044883728, loss=1.6747270822525024
train: epoch 124, loss 0.1938052773475647, acc=0.9612777829170227, loss=0.1938052773475647
test: epoch 124, loss 1.6818397045135498, acc=0.375, loss=1.6818397045135498
train: epoch 125, loss 0.1759522408246994, acc=0.9649999737739563, loss=0.1759522408246994
test: epoch 125, loss 1.6794710159301758, acc=0.3638888895511627, loss=1.6794710159301758
train: epoch 126, loss 0.16569648683071136, acc=0.9664999842643738, loss=0.16569648683071136
test: epoch 126, loss 1.6360995769500732, acc=0.3638888895511627, loss=1.6360995769500732
train: epoch 127, loss 0.16532506048679352, acc=0.9650555849075317, loss=0.16532506048679352
test: epoch 127, loss 1.648201584815979, acc=0.38055557012557983, loss=1.648201584815979
train: epoch 128, loss 0.1627044379711151, acc=0.9677222371101379, loss=0.1627044379711151
test: epoch 128, loss 1.6707344055175781, acc=0.36944442987442017, loss=1.6707344055175781
train: epoch 129, loss 0.1626332849264145, acc=0.9672777652740479, loss=0.1626332849264145
test: epoch 129, loss 1.6517333984375, acc=0.3861111104488373, loss=1.6517333984375
train: epoch 130, loss 0.16648556292057037, acc=0.9636666774749756, loss=0.16648556292057037
test: epoch 130, loss 1.6418001651763916, acc=0.38055557012557983, loss=1.6418001651763916
train: epoch 131, loss 0.14209198951721191, acc=0.9692777991294861, loss=0.14209198951721191
test: epoch 131, loss 1.63142991065979, acc=0.375, loss=1.63142991065979
train: epoch 132, loss 0.15738074481487274, acc=0.9695555567741394, loss=0.15738074481487274
test: epoch 132, loss 1.657454252243042, acc=0.3638888895511627, loss=1.657454252243042
train: epoch 133, loss 0.14977644383907318, acc=0.9702777862548828, loss=0.14977644383907318
test: epoch 133, loss 1.5943350791931152, acc=0.3722222149372101, loss=1.5943350791931152
train: epoch 134, loss 0.16615940630435944, acc=0.9681666493415833, loss=0.16615940630435944
test: epoch 134, loss 1.6449857950210571, acc=0.36666667461395264, loss=1.6449857950210571
train: epoch 135, loss 0.142124742269516, acc=0.9707221984863281, loss=0.142124742269516
test: epoch 135, loss 1.6432228088378906, acc=0.375, loss=1.6432228088378906
train: epoch 136, loss 0.14382420480251312, acc=0.9713333249092102, loss=0.14382420480251312
test: epoch 136, loss 1.6804691553115845, acc=0.38055557012557983, loss=1.6804691553115845
train: epoch 137, loss 0.15036161243915558, acc=0.9705555438995361, loss=0.15036161243915558
test: epoch 137, loss 1.6493659019470215, acc=0.36666667461395264, loss=1.6493659019470215
train: epoch 138, loss 0.13682521879673004, acc=0.9726666808128357, loss=0.13682521879673004
test: epoch 138, loss 1.648779034614563, acc=0.3722222149372101, loss=1.648779034614563
train: epoch 139, loss 0.1421501338481903, acc=0.9713333249092102, loss=0.1421501338481903
test: epoch 139, loss 1.6454638242721558, acc=0.3777777850627899, loss=1.6454638242721558
train: epoch 140, loss 0.13923610746860504, acc=0.9709444642066956, loss=0.13923610746860504
test: epoch 140, loss 1.5917091369628906, acc=0.38055557012557983, loss=1.5917091369628906
train: epoch 141, loss 0.14170324802398682, acc=0.9720555543899536, loss=0.14170324802398682
test: epoch 141, loss 1.6406852006912231, acc=0.3638888895511627, loss=1.6406852006912231
train: epoch 142, loss 0.1218123808503151, acc=0.9756110906600952, loss=0.1218123808503151
test: epoch 142, loss 1.5999990701675415, acc=0.3916666805744171, loss=1.5999990701675415
train: epoch 143, loss 0.12626004219055176, acc=0.9758889079093933, loss=0.12626004219055176
test: epoch 143, loss 1.5848603248596191, acc=0.38333332538604736, loss=1.5848603248596191
train: epoch 144, loss 0.1348465234041214, acc=0.9746666550636292, loss=0.1348465234041214
test: epoch 144, loss 1.6056140661239624, acc=0.3777777850627899, loss=1.6056140661239624
train: epoch 145, loss 0.14280377328395844, acc=0.9735555648803711, loss=0.14280377328395844
test: epoch 145, loss 1.607445478439331, acc=0.4027777910232544, loss=1.607445478439331
train: epoch 146, loss 0.12935717403888702, acc=0.975777804851532, loss=0.12935717403888702
test: epoch 146, loss 1.5359487533569336, acc=0.38055557012557983, loss=1.5359487533569336
train: epoch 147, loss 0.1295909732580185, acc=0.9754999876022339, loss=0.1295909732580185
test: epoch 147, loss 1.5492528676986694, acc=0.4055555462837219, loss=1.5492528676986694
train: epoch 148, loss 0.12111379206180573, acc=0.9745555520057678, loss=0.12111379206180573
test: epoch 148, loss 1.5914585590362549, acc=0.39722222089767456, loss=1.5914585590362549
train: epoch 149, loss 0.12913809716701508, acc=0.9753333330154419, loss=0.12913809716701508
test: epoch 149, loss 1.6468957662582397, acc=0.3861111104488373, loss=1.6468957662582397
train: epoch 150, loss 0.12369686365127563, acc=0.9750555753707886, loss=0.12369686365127563
test: epoch 150, loss 1.6073496341705322, acc=0.3722222149372101, loss=1.6073496341705322
