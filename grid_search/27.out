# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1.5", "--temp_decay=0.995", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=54906345, receiver_embed_dim=32, save_run=0, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=54906345, receiver_embed_dim=32, save_run=False, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.56304931640625, acc=0.04838888719677925, loss=3.56304931640625
test: epoch 1, loss 3.5470571517944336, acc=0.06111111119389534, loss=3.5470571517944336
train: epoch 2, loss 3.47031307220459, acc=0.05294444411993027, loss=3.47031307220459
test: epoch 2, loss 3.235445261001587, acc=0.0833333358168602, loss=3.235445261001587
train: epoch 3, loss 3.3702313899993896, acc=0.05716666579246521, loss=3.3702313899993896
test: epoch 3, loss 3.2399916648864746, acc=0.07222222536802292, loss=3.2399916648864746
train: epoch 4, loss 3.0535409450531006, acc=0.09694444388151169, loss=3.0535409450531006
test: epoch 4, loss 4.322713851928711, acc=0.0694444477558136, loss=4.322713851928711
train: epoch 5, loss 2.7729175090789795, acc=0.13011111319065094, loss=2.7729175090789795
test: epoch 5, loss 4.8000335693359375, acc=0.06111111119389534, loss=4.8000335693359375
train: epoch 6, loss 2.6131815910339355, acc=0.15299999713897705, loss=2.6131815910339355
test: epoch 6, loss 4.8403120040893555, acc=0.06666667014360428, loss=4.8403120040893555
train: epoch 7, loss 2.516920804977417, acc=0.1691666692495346, loss=2.516920804977417
test: epoch 7, loss 5.117550373077393, acc=0.06388889253139496, loss=5.117550373077393
train: epoch 8, loss 2.442173719406128, acc=0.17738889157772064, loss=2.442173719406128
test: epoch 8, loss 5.169028282165527, acc=0.06111111119389534, loss=5.169028282165527
train: epoch 9, loss 2.393174886703491, acc=0.19411110877990723, loss=2.393174886703491
test: epoch 9, loss 5.196335792541504, acc=0.05833333358168602, loss=5.196335792541504
train: epoch 10, loss 2.3333206176757812, acc=0.20677778124809265, loss=2.3333206176757812
test: epoch 10, loss 5.261752128601074, acc=0.05833333358168602, loss=5.261752128601074
train: epoch 11, loss 2.3056514263153076, acc=0.21311111748218536, loss=2.3056514263153076
test: epoch 11, loss 5.29693603515625, acc=0.05277777835726738, loss=5.29693603515625
train: epoch 12, loss 2.2680466175079346, acc=0.2207222282886505, loss=2.2680466175079346
test: epoch 12, loss 5.115355968475342, acc=0.06111111119389534, loss=5.115355968475342
train: epoch 13, loss 2.2363333702087402, acc=0.22866666316986084, loss=2.2363333702087402
test: epoch 13, loss 5.142416000366211, acc=0.0694444477558136, loss=5.142416000366211
train: epoch 14, loss 2.2199361324310303, acc=0.23333333432674408, loss=2.2199361324310303
test: epoch 14, loss 5.161576271057129, acc=0.06666667014360428, loss=5.161576271057129
train: epoch 15, loss 2.197713613510132, acc=0.23488888144493103, loss=2.197713613510132
test: epoch 15, loss 5.063614845275879, acc=0.07222222536802292, loss=5.063614845275879
train: epoch 16, loss 2.1635496616363525, acc=0.2507777810096741, loss=2.1635496616363525
test: epoch 16, loss 5.112272262573242, acc=0.06111111119389534, loss=5.112272262573242
train: epoch 17, loss 2.14672589302063, acc=0.2516111135482788, loss=2.14672589302063
test: epoch 17, loss 5.074434757232666, acc=0.05833333358168602, loss=5.074434757232666
train: epoch 18, loss 2.123070001602173, acc=0.2573888897895813, loss=2.123070001602173
test: epoch 18, loss 5.083559989929199, acc=0.06111111119389534, loss=5.083559989929199
train: epoch 19, loss 2.096418857574463, acc=0.26100000739097595, loss=2.096418857574463
test: epoch 19, loss 4.6956939697265625, acc=0.06111111119389534, loss=4.6956939697265625
train: epoch 20, loss 2.075610637664795, acc=0.27000001072883606, loss=2.075610637664795
test: epoch 20, loss 4.4950690269470215, acc=0.0694444477558136, loss=4.4950690269470215
train: epoch 21, loss 2.0630064010620117, acc=0.2746666669845581, loss=2.0630064010620117
test: epoch 21, loss 4.279194355010986, acc=0.07777778059244156, loss=4.279194355010986
train: epoch 22, loss 2.0456719398498535, acc=0.2711111009120941, loss=2.0456719398498535
test: epoch 22, loss 4.151896953582764, acc=0.06388889253139496, loss=4.151896953582764
train: epoch 23, loss 2.023146629333496, acc=0.2853333353996277, loss=2.023146629333496
test: epoch 23, loss 3.9452319145202637, acc=0.08055555820465088, loss=3.9452319145202637
train: epoch 24, loss 2.0101053714752197, acc=0.29427778720855713, loss=2.0101053714752197
test: epoch 24, loss 3.8428163528442383, acc=0.08055555820465088, loss=3.8428163528442383
train: epoch 25, loss 1.9906892776489258, acc=0.29100000858306885, loss=1.9906892776489258
test: epoch 25, loss 3.7099406719207764, acc=0.07500000298023224, loss=3.7099406719207764
train: epoch 26, loss 1.9541517496109009, acc=0.3034999966621399, loss=1.9541517496109009
test: epoch 26, loss 3.5922586917877197, acc=0.07500000298023224, loss=3.5922586917877197
train: epoch 27, loss 1.9477810859680176, acc=0.3095000088214874, loss=1.9477810859680176
test: epoch 27, loss 3.5401134490966797, acc=0.06666667014360428, loss=3.5401134490966797
train: epoch 28, loss 1.9248474836349487, acc=0.3182222247123718, loss=1.9248474836349487
test: epoch 28, loss 3.4402289390563965, acc=0.07777778059244156, loss=3.4402289390563965
train: epoch 29, loss 1.90793776512146, acc=0.316222220659256, loss=1.90793776512146
test: epoch 29, loss 3.3244669437408447, acc=0.08888889104127884, loss=3.3244669437408447
train: epoch 30, loss 1.889496088027954, acc=0.328000009059906, loss=1.889496088027954
test: epoch 30, loss 3.2501466274261475, acc=0.08611111342906952, loss=3.2501466274261475
train: epoch 31, loss 1.8894333839416504, acc=0.3230000138282776, loss=1.8894333839416504
test: epoch 31, loss 3.177294969558716, acc=0.08055555820465088, loss=3.177294969558716
train: epoch 32, loss 1.8619635105133057, acc=0.3421666622161865, loss=1.8619635105133057
test: epoch 32, loss 3.056344985961914, acc=0.10555555671453476, loss=3.056344985961914
train: epoch 33, loss 1.8567349910736084, acc=0.33622223138809204, loss=1.8567349910736084
test: epoch 33, loss 2.993429660797119, acc=0.09444444626569748, loss=2.993429660797119
train: epoch 34, loss 1.828721523284912, acc=0.33727777004241943, loss=1.828721523284912
test: epoch 34, loss 2.998340606689453, acc=0.0972222238779068, loss=2.998340606689453
train: epoch 35, loss 1.8187106847763062, acc=0.34583333134651184, loss=1.8187106847763062
test: epoch 35, loss 2.841341257095337, acc=0.11666666716337204, loss=2.841341257095337
train: epoch 36, loss 1.7969739437103271, acc=0.3601111173629761, loss=1.7969739437103271
test: epoch 36, loss 2.8009514808654785, acc=0.12222222238779068, loss=2.8009514808654785
train: epoch 37, loss 1.7969833612442017, acc=0.3561111092567444, loss=1.7969833612442017
test: epoch 37, loss 2.7107818126678467, acc=0.12777778506278992, loss=2.7107818126678467
train: epoch 38, loss 1.7813706398010254, acc=0.3651111125946045, loss=1.7813706398010254
test: epoch 38, loss 2.646254301071167, acc=0.14444445073604584, loss=2.646254301071167
train: epoch 39, loss 1.7657657861709595, acc=0.3677777647972107, loss=1.7657657861709595
test: epoch 39, loss 2.596830368041992, acc=0.15555556118488312, loss=2.596830368041992
train: epoch 40, loss 1.7506486177444458, acc=0.3738333284854889, loss=1.7506486177444458
test: epoch 40, loss 2.595076322555542, acc=0.15833333134651184, loss=2.595076322555542
train: epoch 41, loss 1.7313721179962158, acc=0.3784444332122803, loss=1.7313721179962158
test: epoch 41, loss 2.5735485553741455, acc=0.15833333134651184, loss=2.5735485553741455
train: epoch 42, loss 1.7272186279296875, acc=0.3824999928474426, loss=1.7272186279296875
test: epoch 42, loss 2.562779188156128, acc=0.1527777761220932, loss=2.562779188156128
train: epoch 43, loss 1.717527151107788, acc=0.386555552482605, loss=1.717527151107788
test: epoch 43, loss 2.546875476837158, acc=0.16944444179534912, loss=2.546875476837158
train: epoch 44, loss 1.7027231454849243, acc=0.39500001072883606, loss=1.7027231454849243
test: epoch 44, loss 2.4715590476989746, acc=0.1805555522441864, loss=2.4715590476989746
train: epoch 45, loss 1.6860296726226807, acc=0.4004444479942322, loss=1.6860296726226807
test: epoch 45, loss 2.5085363388061523, acc=0.16944444179534912, loss=2.5085363388061523
train: epoch 46, loss 1.6715573072433472, acc=0.40672221779823303, loss=1.6715573072433472
test: epoch 46, loss 2.4889564514160156, acc=0.1805555522441864, loss=2.4889564514160156
train: epoch 47, loss 1.6723713874816895, acc=0.40255555510520935, loss=1.6723713874816895
test: epoch 47, loss 2.4576473236083984, acc=0.18333333730697632, loss=2.4576473236083984
train: epoch 48, loss 1.6640558242797852, acc=0.4147222340106964, loss=1.6640558242797852
test: epoch 48, loss 2.4111993312835693, acc=0.17222222685813904, loss=2.4111993312835693
train: epoch 49, loss 1.637267827987671, acc=0.41922223567962646, loss=1.637267827987671
test: epoch 49, loss 2.394399642944336, acc=0.17499999701976776, loss=2.394399642944336
train: epoch 50, loss 1.6471205949783325, acc=0.4098333418369293, loss=1.6471205949783325
test: epoch 50, loss 2.313175916671753, acc=0.18611110746860504, loss=2.313175916671753
train: epoch 51, loss 1.6168018579483032, acc=0.4258333444595337, loss=1.6168018579483032
test: epoch 51, loss 2.28912091255188, acc=0.18611110746860504, loss=2.28912091255188
train: epoch 52, loss 1.613578200340271, acc=0.4343888759613037, loss=1.613578200340271
test: epoch 52, loss 2.3141090869903564, acc=0.16944444179534912, loss=2.3141090869903564
train: epoch 53, loss 1.5933308601379395, acc=0.433611124753952, loss=1.5933308601379395
test: epoch 53, loss 2.2299423217773438, acc=0.21388888359069824, loss=2.2299423217773438
train: epoch 54, loss 1.5896432399749756, acc=0.43966665863990784, loss=1.5896432399749756
test: epoch 54, loss 2.20750093460083, acc=0.20000000298023224, loss=2.20750093460083
train: epoch 55, loss 1.5858536958694458, acc=0.44655555486679077, loss=1.5858536958694458
test: epoch 55, loss 2.2105350494384766, acc=0.21944443881511688, loss=2.2105350494384766
train: epoch 56, loss 1.5667461156845093, acc=0.44894444942474365, loss=1.5667461156845093
test: epoch 56, loss 2.1850969791412354, acc=0.20000000298023224, loss=2.1850969791412354
train: epoch 57, loss 1.563048005104065, acc=0.45294445753097534, loss=1.563048005104065
test: epoch 57, loss 2.144136905670166, acc=0.20000000298023224, loss=2.144136905670166
train: epoch 58, loss 1.5492159128189087, acc=0.45811110734939575, loss=1.5492159128189087
test: epoch 58, loss 2.15295672416687, acc=0.20555555820465088, loss=2.15295672416687
train: epoch 59, loss 1.531617283821106, acc=0.47022223472595215, loss=1.531617283821106
test: epoch 59, loss 2.1195764541625977, acc=0.21944443881511688, loss=2.1195764541625977
train: epoch 60, loss 1.5050878524780273, acc=0.4707222282886505, loss=1.5050878524780273
test: epoch 60, loss 2.110380172729492, acc=0.21111111342906952, loss=2.110380172729492
train: epoch 61, loss 1.5141386985778809, acc=0.4690000116825104, loss=1.5141386985778809
test: epoch 61, loss 2.0404603481292725, acc=0.22777777910232544, loss=2.0404603481292725
train: epoch 62, loss 1.5024315118789673, acc=0.48172223567962646, loss=1.5024315118789673
test: epoch 62, loss 2.040639877319336, acc=0.21666666865348816, loss=2.040639877319336
train: epoch 63, loss 1.48790442943573, acc=0.49133333563804626, loss=1.48790442943573
test: epoch 63, loss 2.005962610244751, acc=0.23333333432674408, loss=2.005962610244751
train: epoch 64, loss 1.4803248643875122, acc=0.48622220754623413, loss=1.4803248643875122
test: epoch 64, loss 2.024817943572998, acc=0.21944443881511688, loss=2.024817943572998
train: epoch 65, loss 1.4713826179504395, acc=0.4963333308696747, loss=1.4713826179504395
test: epoch 65, loss 2.00081205368042, acc=0.22499999403953552, loss=2.00081205368042
train: epoch 66, loss 1.4443937540054321, acc=0.5018333196640015, loss=1.4443937540054321
test: epoch 66, loss 2.007533550262451, acc=0.23888888955116272, loss=2.007533550262451
train: epoch 67, loss 1.4407066106796265, acc=0.5026111006736755, loss=1.4407066106796265
test: epoch 67, loss 1.9864885807037354, acc=0.23333333432674408, loss=1.9864885807037354
train: epoch 68, loss 1.4125359058380127, acc=0.5191110968589783, loss=1.4125359058380127
test: epoch 68, loss 1.9462521076202393, acc=0.23333333432674408, loss=1.9462521076202393
train: epoch 69, loss 1.4057269096374512, acc=0.5177778005599976, loss=1.4057269096374512
test: epoch 69, loss 1.95217764377594, acc=0.24444444477558136, loss=1.95217764377594
train: epoch 70, loss 1.396066665649414, acc=0.5216666460037231, loss=1.396066665649414
test: epoch 70, loss 1.9051504135131836, acc=0.24722221493721008, loss=1.9051504135131836
train: epoch 71, loss 1.383048415184021, acc=0.5287222266197205, loss=1.383048415184021
test: epoch 71, loss 1.8890259265899658, acc=0.2527777850627899, loss=1.8890259265899658
train: epoch 72, loss 1.3678735494613647, acc=0.5316110849380493, loss=1.3678735494613647
test: epoch 72, loss 1.87958824634552, acc=0.25, loss=1.87958824634552
train: epoch 73, loss 1.3544739484786987, acc=0.5347777605056763, loss=1.3544739484786987
test: epoch 73, loss 1.8512241840362549, acc=0.2611111104488373, loss=1.8512241840362549
train: epoch 74, loss 1.3399264812469482, acc=0.5436111092567444, loss=1.3399264812469482
test: epoch 74, loss 1.8473552465438843, acc=0.26944443583488464, loss=1.8473552465438843
train: epoch 75, loss 1.334912657737732, acc=0.5458333492279053, loss=1.334912657737732
test: epoch 75, loss 1.8599317073822021, acc=0.26944443583488464, loss=1.8599317073822021
train: epoch 76, loss 1.3496274948120117, acc=0.5516111254692078, loss=1.3496274948120117
test: epoch 76, loss 1.8278477191925049, acc=0.2750000059604645, loss=1.8278477191925049
train: epoch 77, loss 1.3249006271362305, acc=0.5544999837875366, loss=1.3249006271362305
test: epoch 77, loss 1.78311026096344, acc=0.2916666567325592, loss=1.78311026096344
train: epoch 78, loss 1.2883214950561523, acc=0.5627222061157227, loss=1.2883214950561523
test: epoch 78, loss 1.805511474609375, acc=0.2888889014720917, loss=1.805511474609375
train: epoch 79, loss 1.2842216491699219, acc=0.569944441318512, loss=1.2842216491699219
test: epoch 79, loss 1.7688301801681519, acc=0.2916666567325592, loss=1.7688301801681519
train: epoch 80, loss 1.272554636001587, acc=0.5777222514152527, loss=1.272554636001587
test: epoch 80, loss 1.7732594013214111, acc=0.29722222685813904, loss=1.7732594013214111
train: epoch 81, loss 1.2716484069824219, acc=0.5809444189071655, loss=1.2716484069824219
test: epoch 81, loss 1.7558890581130981, acc=0.2805555462837219, loss=1.7558890581130981
train: epoch 82, loss 1.2478805780410767, acc=0.5920000076293945, loss=1.2478805780410767
test: epoch 82, loss 1.7473946809768677, acc=0.2888889014720917, loss=1.7473946809768677
train: epoch 83, loss 1.239907145500183, acc=0.5866666436195374, loss=1.239907145500183
test: epoch 83, loss 1.7386778593063354, acc=0.28333333134651184, loss=1.7386778593063354
train: epoch 84, loss 1.2174984216690063, acc=0.5947777628898621, loss=1.2174984216690063
test: epoch 84, loss 1.7341973781585693, acc=0.3055555522441864, loss=1.7341973781585693
train: epoch 85, loss 1.2087173461914062, acc=0.5974444150924683, loss=1.2087173461914062
test: epoch 85, loss 1.7276033163070679, acc=0.2888889014720917, loss=1.7276033163070679
train: epoch 86, loss 1.2147650718688965, acc=0.6043888926506042, loss=1.2147650718688965
test: epoch 86, loss 1.7259045839309692, acc=0.3083333373069763, loss=1.7259045839309692
train: epoch 87, loss 1.197320818901062, acc=0.6111666560173035, loss=1.197320818901062
test: epoch 87, loss 1.722169041633606, acc=0.30000001192092896, loss=1.722169041633606
train: epoch 88, loss 1.179460048675537, acc=0.6177777647972107, loss=1.179460048675537
test: epoch 88, loss 1.6949697732925415, acc=0.3305555582046509, loss=1.6949697732925415
train: epoch 89, loss 1.1707197427749634, acc=0.6238333582878113, loss=1.1707197427749634
test: epoch 89, loss 1.671937108039856, acc=0.3222222328186035, loss=1.671937108039856
train: epoch 90, loss 1.1692767143249512, acc=0.6311666369438171, loss=1.1692767143249512
test: epoch 90, loss 1.6827539205551147, acc=0.3083333373069763, loss=1.6827539205551147
train: epoch 91, loss 1.1549763679504395, acc=0.6298333406448364, loss=1.1549763679504395
test: epoch 91, loss 1.6579848527908325, acc=0.3305555582046509, loss=1.6579848527908325
train: epoch 92, loss 1.145116925239563, acc=0.6359444260597229, loss=1.145116925239563
test: epoch 92, loss 1.6395065784454346, acc=0.32499998807907104, loss=1.6395065784454346
train: epoch 93, loss 1.1241116523742676, acc=0.6432777643203735, loss=1.1241116523742676
test: epoch 93, loss 1.6266449689865112, acc=0.3333333432674408, loss=1.6266449689865112
train: epoch 94, loss 1.099631428718567, acc=0.6507222056388855, loss=1.099631428718567
test: epoch 94, loss 1.6096816062927246, acc=0.3305555582046509, loss=1.6096816062927246
train: epoch 95, loss 1.0959622859954834, acc=0.6485555768013, loss=1.0959622859954834
test: epoch 95, loss 1.6075565814971924, acc=0.3361110985279083, loss=1.6075565814971924
train: epoch 96, loss 1.0905424356460571, acc=0.6552777886390686, loss=1.0905424356460571
test: epoch 96, loss 1.5914784669876099, acc=0.3194444477558136, loss=1.5914784669876099
train: epoch 97, loss 1.0786279439926147, acc=0.6595555543899536, loss=1.0786279439926147
test: epoch 97, loss 1.5924087762832642, acc=0.3333333432674408, loss=1.5924087762832642
train: epoch 98, loss 1.082327961921692, acc=0.6593888998031616, loss=1.082327961921692
test: epoch 98, loss 1.5840882062911987, acc=0.32499998807907104, loss=1.5840882062911987
train: epoch 99, loss 1.0816245079040527, acc=0.6621111035346985, loss=1.0816245079040527
test: epoch 99, loss 1.5596805810928345, acc=0.33888888359069824, loss=1.5596805810928345
train: epoch 100, loss 1.0749276876449585, acc=0.6699444651603699, loss=1.0749276876449585
test: epoch 100, loss 1.5556211471557617, acc=0.3583333194255829, loss=1.5556211471557617
train: epoch 101, loss 1.0434645414352417, acc=0.6778333187103271, loss=1.0434645414352417
test: epoch 101, loss 1.5392451286315918, acc=0.3611111044883728, loss=1.5392451286315918
train: epoch 102, loss 1.027304768562317, acc=0.6767777800559998, loss=1.027304768562317
test: epoch 102, loss 1.5257929563522339, acc=0.35555556416511536, loss=1.5257929563522339
train: epoch 103, loss 1.033995509147644, acc=0.6831666827201843, loss=1.033995509147644
test: epoch 103, loss 1.537428379058838, acc=0.3583333194255829, loss=1.537428379058838
train: epoch 104, loss 1.0385247468948364, acc=0.6872777938842773, loss=1.0385247468948364
test: epoch 104, loss 1.4966566562652588, acc=0.3583333194255829, loss=1.4966566562652588
train: epoch 105, loss 1.021953821182251, acc=0.6873888969421387, loss=1.021953821182251
test: epoch 105, loss 1.4834825992584229, acc=0.3583333194255829, loss=1.4834825992584229
train: epoch 106, loss 0.9897459149360657, acc=0.6953333616256714, loss=0.9897459149360657
test: epoch 106, loss 1.4765218496322632, acc=0.375, loss=1.4765218496322632
train: epoch 107, loss 1.0127370357513428, acc=0.6946666836738586, loss=1.0127370357513428
test: epoch 107, loss 1.4744185209274292, acc=0.3722222149372101, loss=1.4744185209274292
train: epoch 108, loss 0.9903523325920105, acc=0.6967777609825134, loss=0.9903523325920105
test: epoch 108, loss 1.4582515954971313, acc=0.3777777850627899, loss=1.4582515954971313
train: epoch 109, loss 0.9864338636398315, acc=0.6997777819633484, loss=0.9864338636398315
test: epoch 109, loss 1.4502521753311157, acc=0.36666667461395264, loss=1.4502521753311157
train: epoch 110, loss 0.9802795052528381, acc=0.7037222385406494, loss=0.9802795052528381
test: epoch 110, loss 1.4220330715179443, acc=0.38333332538604736, loss=1.4220330715179443
train: epoch 111, loss 0.9607343673706055, acc=0.7108333110809326, loss=0.9607343673706055
test: epoch 111, loss 1.4391952753067017, acc=0.36666667461395264, loss=1.4391952753067017
train: epoch 112, loss 0.9587544798851013, acc=0.7076666951179504, loss=0.9587544798851013
test: epoch 112, loss 1.4358371496200562, acc=0.3777777850627899, loss=1.4358371496200562
train: epoch 113, loss 0.9337618350982666, acc=0.7180555462837219, loss=0.9337618350982666
test: epoch 113, loss 1.4266706705093384, acc=0.3777777850627899, loss=1.4266706705093384
train: epoch 114, loss 0.9388189315795898, acc=0.7155555486679077, loss=0.9388189315795898
test: epoch 114, loss 1.4176273345947266, acc=0.3916666805744171, loss=1.4176273345947266
train: epoch 115, loss 0.9399607181549072, acc=0.7235555648803711, loss=0.9399607181549072
test: epoch 115, loss 1.367749571800232, acc=0.39444443583488464, loss=1.367749571800232
train: epoch 116, loss 0.9443262815475464, acc=0.7189444303512573, loss=0.9443262815475464
test: epoch 116, loss 1.4014592170715332, acc=0.38055557012557983, loss=1.4014592170715332
train: epoch 117, loss 0.9366587400436401, acc=0.7261666655540466, loss=0.9366587400436401
test: epoch 117, loss 1.3903639316558838, acc=0.3888888955116272, loss=1.3903639316558838
train: epoch 118, loss 0.9095086455345154, acc=0.7297222018241882, loss=0.9095086455345154
test: epoch 118, loss 1.3818875551223755, acc=0.3888888955116272, loss=1.3818875551223755
train: epoch 119, loss 0.9026710987091064, acc=0.7332777976989746, loss=0.9026710987091064
test: epoch 119, loss 1.368167757987976, acc=0.41111111640930176, loss=1.368167757987976
train: epoch 120, loss 0.9162126183509827, acc=0.7319444417953491, loss=0.9162126183509827
test: epoch 120, loss 1.3765257596969604, acc=0.39722222089767456, loss=1.3765257596969604
train: epoch 121, loss 0.8709679841995239, acc=0.7387222051620483, loss=0.8709679841995239
test: epoch 121, loss 1.363467812538147, acc=0.4027777910232544, loss=1.363467812538147
train: epoch 122, loss 0.8968182802200317, acc=0.7383888959884644, loss=0.8968182802200317
test: epoch 122, loss 1.348476529121399, acc=0.4138889014720917, loss=1.348476529121399
train: epoch 123, loss 0.8758435845375061, acc=0.7426666617393494, loss=0.8758435845375061
test: epoch 123, loss 1.3216893672943115, acc=0.4138889014720917, loss=1.3216893672943115
train: epoch 124, loss 0.8983775973320007, acc=0.7360555529594421, loss=0.8983775973320007
test: epoch 124, loss 1.3386120796203613, acc=0.4194444417953491, loss=1.3386120796203613
train: epoch 125, loss 0.8834176659584045, acc=0.7423333525657654, loss=0.8834176659584045
test: epoch 125, loss 1.3303163051605225, acc=0.4138889014720917, loss=1.3303163051605225
train: epoch 126, loss 0.8721898794174194, acc=0.7488333582878113, loss=0.8721898794174194
test: epoch 126, loss 1.317166805267334, acc=0.4027777910232544, loss=1.317166805267334
train: epoch 127, loss 0.8838579654693604, acc=0.7465555667877197, loss=0.8838579654693604
test: epoch 127, loss 1.320401906967163, acc=0.4138889014720917, loss=1.320401906967163
train: epoch 128, loss 0.865349531173706, acc=0.7485555410385132, loss=0.865349531173706
test: epoch 128, loss 1.3143882751464844, acc=0.4055555462837219, loss=1.3143882751464844
train: epoch 129, loss 0.8314700722694397, acc=0.749833345413208, loss=0.8314700722694397
test: epoch 129, loss 1.2872138023376465, acc=0.4194444417953491, loss=1.2872138023376465
train: epoch 130, loss 0.8444828987121582, acc=0.7482222318649292, loss=0.8444828987121582
test: epoch 130, loss 1.2976793050765991, acc=0.4305555522441864, loss=1.2976793050765991
train: epoch 131, loss 0.8470293879508972, acc=0.7506111264228821, loss=0.8470293879508972
test: epoch 131, loss 1.2686208486557007, acc=0.4416666626930237, loss=1.2686208486557007
train: epoch 132, loss 0.8363438844680786, acc=0.7530555725097656, loss=0.8363438844680786
test: epoch 132, loss 1.2871135473251343, acc=0.4277777671813965, loss=1.2871135473251343
train: epoch 133, loss 0.833849310874939, acc=0.7557222247123718, loss=0.833849310874939
test: epoch 133, loss 1.2730841636657715, acc=0.43888887763023376, loss=1.2730841636657715
train: epoch 134, loss 0.8159191012382507, acc=0.7591666579246521, loss=0.8159191012382507
test: epoch 134, loss 1.2632900476455688, acc=0.43611112236976624, loss=1.2632900476455688
train: epoch 135, loss 0.8261864185333252, acc=0.7631666660308838, loss=0.8261864185333252
test: epoch 135, loss 1.2571648359298706, acc=0.43888887763023376, loss=1.2571648359298706
train: epoch 136, loss 0.8220707774162292, acc=0.7622777819633484, loss=0.8220707774162292
test: epoch 136, loss 1.263164758682251, acc=0.4416666626930237, loss=1.263164758682251
train: epoch 137, loss 0.8289355635643005, acc=0.7601666450500488, loss=0.8289355635643005
test: epoch 137, loss 1.2617688179016113, acc=0.4277777671813965, loss=1.2617688179016113
train: epoch 138, loss 0.8107063174247742, acc=0.7616666555404663, loss=0.8107063174247742
test: epoch 138, loss 1.2479395866394043, acc=0.43611112236976624, loss=1.2479395866394043
train: epoch 139, loss 0.8207459449768066, acc=0.765500009059906, loss=0.8207459449768066
test: epoch 139, loss 1.2512189149856567, acc=0.43888887763023376, loss=1.2512189149856567
train: epoch 140, loss 0.8227689266204834, acc=0.7639999985694885, loss=0.8227689266204834
test: epoch 140, loss 1.2378066778182983, acc=0.4333333373069763, loss=1.2378066778182983
train: epoch 141, loss 0.7984922528266907, acc=0.7678889036178589, loss=0.7984922528266907
test: epoch 141, loss 1.2564924955368042, acc=0.4305555522441864, loss=1.2564924955368042
train: epoch 142, loss 0.7819673418998718, acc=0.7712777853012085, loss=0.7819673418998718
test: epoch 142, loss 1.2416523694992065, acc=0.4472222328186035, loss=1.2416523694992065
train: epoch 143, loss 0.8160430788993835, acc=0.7679444551467896, loss=0.8160430788993835
test: epoch 143, loss 1.2387667894363403, acc=0.44999998807907104, loss=1.2387667894363403
train: epoch 144, loss 0.7881326675415039, acc=0.7738333344459534, loss=0.7881326675415039
test: epoch 144, loss 1.218072533607483, acc=0.4555555582046509, loss=1.218072533607483
train: epoch 145, loss 0.7686105370521545, acc=0.7747777700424194, loss=0.7686105370521545
test: epoch 145, loss 1.2159919738769531, acc=0.46388888359069824, loss=1.2159919738769531
train: epoch 146, loss 0.7809569239616394, acc=0.777388870716095, loss=0.7809569239616394
test: epoch 146, loss 1.2068160772323608, acc=0.44999998807907104, loss=1.2068160772323608
train: epoch 147, loss 0.76137775182724, acc=0.7827222347259521, loss=0.76137775182724
test: epoch 147, loss 1.2227979898452759, acc=0.4416666626930237, loss=1.2227979898452759
train: epoch 148, loss 0.7432756423950195, acc=0.7827222347259521, loss=0.7432756423950195
test: epoch 148, loss 1.188302755355835, acc=0.46666666865348816, loss=1.188302755355835
train: epoch 149, loss 0.7534630298614502, acc=0.7832777500152588, loss=0.7534630298614502
test: epoch 149, loss 1.1781623363494873, acc=0.47777777910232544, loss=1.1781623363494873
train: epoch 150, loss 0.7594254016876221, acc=0.7841110825538635, loss=0.7594254016876221
test: epoch 150, loss 1.1912041902542114, acc=0.4583333432674408, loss=1.1912041902542114
