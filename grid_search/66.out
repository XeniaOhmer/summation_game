# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.99", "--one_hot=0", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1560988755, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1560988755, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.251786708831787, acc=0.06599999964237213, loss=3.251786708831787
test: epoch 1, loss 3.907681703567505, acc=0.0416666679084301, loss=3.907681703567505
train: epoch 2, loss 2.9289965629577637, acc=0.09905555844306946, loss=2.9289965629577637
test: epoch 2, loss 3.886807918548584, acc=0.05000000074505806, loss=3.886807918548584
train: epoch 3, loss 2.804875373840332, acc=0.11277777701616287, loss=2.804875373840332
test: epoch 3, loss 4.056790351867676, acc=0.05000000074505806, loss=4.056790351867676
train: epoch 4, loss 2.7475247383117676, acc=0.11916666477918625, loss=2.7475247383117676
test: epoch 4, loss 4.077602863311768, acc=0.05000000074505806, loss=4.077602863311768
train: epoch 5, loss 2.7155144214630127, acc=0.12688888609409332, loss=2.7155144214630127
test: epoch 5, loss 3.9470622539520264, acc=0.05277777835726738, loss=3.9470622539520264
train: epoch 6, loss 2.6896636486053467, acc=0.12950000166893005, loss=2.6896636486053467
test: epoch 6, loss 4.1047892570495605, acc=0.05277777835726738, loss=4.1047892570495605
train: epoch 7, loss 2.6743171215057373, acc=0.13655555248260498, loss=2.6743171215057373
test: epoch 7, loss 3.9997289180755615, acc=0.05277777835726738, loss=3.9997289180755615
train: epoch 8, loss 2.651707172393799, acc=0.13555555045604706, loss=2.651707172393799
test: epoch 8, loss 4.012582778930664, acc=0.05277777835726738, loss=4.012582778930664
train: epoch 9, loss 2.6328725814819336, acc=0.13972222805023193, loss=2.6328725814819336
test: epoch 9, loss 3.9226174354553223, acc=0.05000000074505806, loss=3.9226174354553223
train: epoch 10, loss 2.624403476715088, acc=0.13794444501399994, loss=2.624403476715088
test: epoch 10, loss 4.013150691986084, acc=0.05277777835726738, loss=4.013150691986084
train: epoch 11, loss 2.6174569129943848, acc=0.138722226023674, loss=2.6174569129943848
test: epoch 11, loss 3.9498448371887207, acc=0.05833333358168602, loss=3.9498448371887207
train: epoch 12, loss 2.602344512939453, acc=0.14233332872390747, loss=2.602344512939453
test: epoch 12, loss 3.855485439300537, acc=0.05000000074505806, loss=3.855485439300537
train: epoch 13, loss 2.586399555206299, acc=0.1418333351612091, loss=2.586399555206299
test: epoch 13, loss 3.8062620162963867, acc=0.0555555559694767, loss=3.8062620162963867
train: epoch 14, loss 2.592783212661743, acc=0.14561110734939575, loss=2.592783212661743
test: epoch 14, loss 3.7427978515625, acc=0.0555555559694767, loss=3.7427978515625
train: epoch 15, loss 2.573223352432251, acc=0.14838889241218567, loss=2.573223352432251
test: epoch 15, loss 3.764348030090332, acc=0.05000000074505806, loss=3.764348030090332
train: epoch 16, loss 2.576413631439209, acc=0.14650000631809235, loss=2.576413631439209
test: epoch 16, loss 3.65938663482666, acc=0.06111111119389534, loss=3.65938663482666
train: epoch 17, loss 2.586226463317871, acc=0.14705555140972137, loss=2.586226463317871
test: epoch 17, loss 3.6698927879333496, acc=0.06111111119389534, loss=3.6698927879333496
train: epoch 18, loss 2.577988862991333, acc=0.14399999380111694, loss=2.577988862991333
test: epoch 18, loss 3.695528507232666, acc=0.05833333358168602, loss=3.695528507232666
train: epoch 19, loss 2.5697994232177734, acc=0.1468888819217682, loss=2.5697994232177734
test: epoch 19, loss 3.6798415184020996, acc=0.05833333358168602, loss=3.6798415184020996
train: epoch 20, loss 2.540344476699829, acc=0.15333333611488342, loss=2.540344476699829
test: epoch 20, loss 3.643665313720703, acc=0.05277777835726738, loss=3.643665313720703
train: epoch 21, loss 2.566439390182495, acc=0.14427778124809265, loss=2.566439390182495
test: epoch 21, loss 3.6297144889831543, acc=0.06666667014360428, loss=3.6297144889831543
train: epoch 22, loss 2.5643439292907715, acc=0.14755555987358093, loss=2.5643439292907715
test: epoch 22, loss 3.6434106826782227, acc=0.04722222313284874, loss=3.6434106826782227
train: epoch 23, loss 2.5605533123016357, acc=0.14772222936153412, loss=2.5605533123016357
test: epoch 23, loss 3.5823240280151367, acc=0.05833333358168602, loss=3.5823240280151367
train: epoch 24, loss 2.5418105125427246, acc=0.1516111046075821, loss=2.5418105125427246
test: epoch 24, loss 3.4500794410705566, acc=0.05833333358168602, loss=3.4500794410705566
train: epoch 25, loss 2.559994697570801, acc=0.14399999380111694, loss=2.559994697570801
test: epoch 25, loss 3.5635478496551514, acc=0.04722222313284874, loss=3.5635478496551514
train: epoch 26, loss 2.5429489612579346, acc=0.1487777829170227, loss=2.5429489612579346
test: epoch 26, loss 3.5406901836395264, acc=0.06111111119389534, loss=3.5406901836395264
train: epoch 27, loss 2.5588972568511963, acc=0.1493888944387436, loss=2.5588972568511963
test: epoch 27, loss 3.5837178230285645, acc=0.0555555559694767, loss=3.5837178230285645
train: epoch 28, loss 2.554694652557373, acc=0.1440555602312088, loss=2.554694652557373
test: epoch 28, loss 3.3980443477630615, acc=0.05833333358168602, loss=3.3980443477630615
train: epoch 29, loss 2.554536819458008, acc=0.1474444419145584, loss=2.554536819458008
test: epoch 29, loss 3.3877177238464355, acc=0.0555555559694767, loss=3.3877177238464355
train: epoch 30, loss 2.5525717735290527, acc=0.14772222936153412, loss=2.5525717735290527
test: epoch 30, loss 3.3574209213256836, acc=0.0555555559694767, loss=3.3574209213256836
train: epoch 31, loss 2.541757583618164, acc=0.14772222936153412, loss=2.541757583618164
test: epoch 31, loss 3.4284844398498535, acc=0.0555555559694767, loss=3.4284844398498535
train: epoch 32, loss 2.54013729095459, acc=0.15016666054725647, loss=2.54013729095459
test: epoch 32, loss 3.491297483444214, acc=0.0555555559694767, loss=3.491297483444214
train: epoch 33, loss 2.5365729331970215, acc=0.14861111342906952, loss=2.5365729331970215
test: epoch 33, loss 3.414019823074341, acc=0.06388889253139496, loss=3.414019823074341
train: epoch 34, loss 2.5385560989379883, acc=0.14638888835906982, loss=2.5385560989379883
test: epoch 34, loss 3.3175570964813232, acc=0.07500000298023224, loss=3.3175570964813232
train: epoch 35, loss 2.5259389877319336, acc=0.1509999930858612, loss=2.5259389877319336
test: epoch 35, loss 3.4675042629241943, acc=0.06111111119389534, loss=3.4675042629241943
train: epoch 36, loss 2.5249319076538086, acc=0.1514444500207901, loss=2.5249319076538086
test: epoch 36, loss 3.317387580871582, acc=0.06388889253139496, loss=3.317387580871582
train: epoch 37, loss 2.5199973583221436, acc=0.14888888597488403, loss=2.5199973583221436
test: epoch 37, loss 3.281679630279541, acc=0.06388889253139496, loss=3.281679630279541
train: epoch 38, loss 2.517902135848999, acc=0.15538889169692993, loss=2.517902135848999
test: epoch 38, loss 3.3352041244506836, acc=0.0555555559694767, loss=3.3352041244506836
train: epoch 39, loss 2.5223238468170166, acc=0.15461111068725586, loss=2.5223238468170166
test: epoch 39, loss 3.3737549781799316, acc=0.07777778059244156, loss=3.3737549781799316
train: epoch 40, loss 2.515329599380493, acc=0.15877777338027954, loss=2.515329599380493
test: epoch 40, loss 3.2724952697753906, acc=0.0694444477558136, loss=3.2724952697753906
train: epoch 41, loss 2.4951789379119873, acc=0.15216666460037231, loss=2.4951789379119873
test: epoch 41, loss 3.3100898265838623, acc=0.0694444477558136, loss=3.3100898265838623
train: epoch 42, loss 2.49424147605896, acc=0.15138888359069824, loss=2.49424147605896
test: epoch 42, loss 3.2534241676330566, acc=0.08611111342906952, loss=3.2534241676330566
train: epoch 43, loss 2.490661382675171, acc=0.15594445168972015, loss=2.490661382675171
test: epoch 43, loss 3.264887809753418, acc=0.08055555820465088, loss=3.264887809753418
train: epoch 44, loss 2.495502471923828, acc=0.1523333340883255, loss=2.495502471923828
test: epoch 44, loss 3.326845645904541, acc=0.06666667014360428, loss=3.326845645904541
train: epoch 45, loss 2.4799299240112305, acc=0.15772221982479095, loss=2.4799299240112305
test: epoch 45, loss 3.2842068672180176, acc=0.08888889104127884, loss=3.2842068672180176
train: epoch 46, loss 2.483729600906372, acc=0.1548333317041397, loss=2.483729600906372
test: epoch 46, loss 3.169532060623169, acc=0.08888889104127884, loss=3.169532060623169
train: epoch 47, loss 2.475259304046631, acc=0.1586666703224182, loss=2.475259304046631
test: epoch 47, loss 3.287522077560425, acc=0.06111111119389534, loss=3.287522077560425
train: epoch 48, loss 2.4666223526000977, acc=0.15738889575004578, loss=2.4666223526000977
test: epoch 48, loss 3.136312246322632, acc=0.08888889104127884, loss=3.136312246322632
train: epoch 49, loss 2.461240530014038, acc=0.15600000321865082, loss=2.461240530014038
test: epoch 49, loss 3.310019016265869, acc=0.07222222536802292, loss=3.310019016265869
train: epoch 50, loss 2.4595091342926025, acc=0.15744444727897644, loss=2.4595091342926025
test: epoch 50, loss 3.1887025833129883, acc=0.0833333358168602, loss=3.1887025833129883
train: epoch 51, loss 2.4789841175079346, acc=0.15688888728618622, loss=2.4789841175079346
test: epoch 51, loss 3.2099552154541016, acc=0.07777778059244156, loss=3.2099552154541016
train: epoch 52, loss 2.4627020359039307, acc=0.16272221505641937, loss=2.4627020359039307
test: epoch 52, loss 3.4008049964904785, acc=0.0555555559694767, loss=3.4008049964904785
train: epoch 53, loss 2.4595279693603516, acc=0.15683333575725555, loss=2.4595279693603516
test: epoch 53, loss 3.2186777591705322, acc=0.06388889253139496, loss=3.2186777591705322
train: epoch 54, loss 2.4560399055480957, acc=0.16011111438274384, loss=2.4560399055480957
test: epoch 54, loss 3.212053060531616, acc=0.06111111119389534, loss=3.212053060531616
train: epoch 55, loss 2.4549574851989746, acc=0.16050000488758087, loss=2.4549574851989746
test: epoch 55, loss 3.1638259887695312, acc=0.08611111342906952, loss=3.1638259887695312
train: epoch 56, loss 2.461421489715576, acc=0.15833333134651184, loss=2.461421489715576
test: epoch 56, loss 3.226257801055908, acc=0.08611111342906952, loss=3.226257801055908
train: epoch 57, loss 2.4356255531311035, acc=0.16116666793823242, loss=2.4356255531311035
test: epoch 57, loss 3.1553704738616943, acc=0.07500000298023224, loss=3.1553704738616943
train: epoch 58, loss 2.41756534576416, acc=0.16288888454437256, loss=2.41756534576416
test: epoch 58, loss 3.1677098274230957, acc=0.08888889104127884, loss=3.1677098274230957
train: epoch 59, loss 2.4403796195983887, acc=0.15983332693576813, loss=2.4403796195983887
test: epoch 59, loss 3.183831214904785, acc=0.09166666865348816, loss=3.183831214904785
train: epoch 60, loss 2.4328112602233887, acc=0.16988888382911682, loss=2.4328112602233887
test: epoch 60, loss 3.19646954536438, acc=0.07777778059244156, loss=3.19646954536438
train: epoch 61, loss 2.437295913696289, acc=0.16527777910232544, loss=2.437295913696289
test: epoch 61, loss 3.1769707202911377, acc=0.0833333358168602, loss=3.1769707202911377
train: epoch 62, loss 2.4299306869506836, acc=0.16894444823265076, loss=2.4299306869506836
test: epoch 62, loss 3.205899238586426, acc=0.09166666865348816, loss=3.205899238586426
train: epoch 63, loss 2.4246861934661865, acc=0.164000004529953, loss=2.4246861934661865
test: epoch 63, loss 3.1539700031280518, acc=0.0833333358168602, loss=3.1539700031280518
train: epoch 64, loss 2.4210546016693115, acc=0.1635555624961853, loss=2.4210546016693115
test: epoch 64, loss 3.158484697341919, acc=0.08055555820465088, loss=3.158484697341919
train: epoch 65, loss 2.421499729156494, acc=0.1696111112833023, loss=2.421499729156494
test: epoch 65, loss 3.255528450012207, acc=0.06388889253139496, loss=3.255528450012207
train: epoch 66, loss 2.4083938598632812, acc=0.1684444397687912, loss=2.4083938598632812
test: epoch 66, loss 3.2198879718780518, acc=0.0972222238779068, loss=3.2198879718780518
train: epoch 67, loss 2.4030051231384277, acc=0.17149999737739563, loss=2.4030051231384277
test: epoch 67, loss 3.1729493141174316, acc=0.08611111342906952, loss=3.1729493141174316
train: epoch 68, loss 2.411773681640625, acc=0.1682777851819992, loss=2.411773681640625
test: epoch 68, loss 3.2456061840057373, acc=0.06388889253139496, loss=3.2456061840057373
train: epoch 69, loss 2.414020538330078, acc=0.1682777851819992, loss=2.414020538330078
test: epoch 69, loss 3.2304329872131348, acc=0.08055555820465088, loss=3.2304329872131348
train: epoch 70, loss 2.4080095291137695, acc=0.16966666281223297, loss=2.4080095291137695
test: epoch 70, loss 3.158754825592041, acc=0.0833333358168602, loss=3.158754825592041
train: epoch 71, loss 2.404383420944214, acc=0.16588889062404633, loss=2.404383420944214
test: epoch 71, loss 3.0196220874786377, acc=0.08611111342906952, loss=3.0196220874786377
train: epoch 72, loss 2.3918185234069824, acc=0.16966666281223297, loss=2.3918185234069824
test: epoch 72, loss 3.2233786582946777, acc=0.07777778059244156, loss=3.2233786582946777
train: epoch 73, loss 2.4239273071289062, acc=0.17444443702697754, loss=2.4239273071289062
test: epoch 73, loss 3.20185923576355, acc=0.07500000298023224, loss=3.20185923576355
train: epoch 74, loss 2.401278257369995, acc=0.16988888382911682, loss=2.401278257369995
test: epoch 74, loss 3.2491142749786377, acc=0.07777778059244156, loss=3.2491142749786377
train: epoch 75, loss 2.40372896194458, acc=0.16633333265781403, loss=2.40372896194458
test: epoch 75, loss 3.10918927192688, acc=0.07500000298023224, loss=3.10918927192688
train: epoch 76, loss 2.3807952404022217, acc=0.16927777230739594, loss=2.3807952404022217
test: epoch 76, loss 3.1714465618133545, acc=0.0694444477558136, loss=3.1714465618133545
train: epoch 77, loss 2.3873450756073, acc=0.17088888585567474, loss=2.3873450756073
test: epoch 77, loss 3.1315009593963623, acc=0.07777778059244156, loss=3.1315009593963623
train: epoch 78, loss 2.3764450550079346, acc=0.17561110854148865, loss=2.3764450550079346
test: epoch 78, loss 3.1981167793273926, acc=0.0833333358168602, loss=3.1981167793273926
train: epoch 79, loss 2.377101421356201, acc=0.17438888549804688, loss=2.377101421356201
test: epoch 79, loss 3.110462188720703, acc=0.05000000074505806, loss=3.110462188720703
train: epoch 80, loss 2.388561964035034, acc=0.171833336353302, loss=2.388561964035034
test: epoch 80, loss 3.1816108226776123, acc=0.07500000298023224, loss=3.1816108226776123
train: epoch 81, loss 2.3871841430664062, acc=0.1704999953508377, loss=2.3871841430664062
test: epoch 81, loss 3.141435384750366, acc=0.07777778059244156, loss=3.141435384750366
train: epoch 82, loss 2.3742618560791016, acc=0.17416666448116302, loss=2.3742618560791016
test: epoch 82, loss 3.1516263484954834, acc=0.0833333358168602, loss=3.1516263484954834
train: epoch 83, loss 2.3779501914978027, acc=0.1728888899087906, loss=2.3779501914978027
test: epoch 83, loss 3.1194920539855957, acc=0.07777778059244156, loss=3.1194920539855957
train: epoch 84, loss 2.3518452644348145, acc=0.17755556106567383, loss=2.3518452644348145
test: epoch 84, loss 3.0764689445495605, acc=0.0833333358168602, loss=3.0764689445495605
train: epoch 85, loss 2.369302988052368, acc=0.17411111295223236, loss=2.369302988052368
test: epoch 85, loss 2.992248296737671, acc=0.08888889104127884, loss=2.992248296737671
train: epoch 86, loss 2.367823600769043, acc=0.17749999463558197, loss=2.367823600769043
test: epoch 86, loss 3.102073907852173, acc=0.10277777910232544, loss=3.102073907852173
train: epoch 87, loss 2.361905574798584, acc=0.1783333271741867, loss=2.361905574798584
test: epoch 87, loss 3.1169474124908447, acc=0.08055555820465088, loss=3.1169474124908447
train: epoch 88, loss 2.362908363342285, acc=0.17544443905353546, loss=2.362908363342285
test: epoch 88, loss 3.0427258014678955, acc=0.0833333358168602, loss=3.0427258014678955
train: epoch 89, loss 2.3598968982696533, acc=0.18150000274181366, loss=2.3598968982696533
test: epoch 89, loss 3.0942251682281494, acc=0.07222222536802292, loss=3.0942251682281494
train: epoch 90, loss 2.3628416061401367, acc=0.17616666853427887, loss=2.3628416061401367
test: epoch 90, loss 3.0710158348083496, acc=0.07500000298023224, loss=3.0710158348083496
train: epoch 91, loss 2.3495633602142334, acc=0.17661111056804657, loss=2.3495633602142334
test: epoch 91, loss 3.068472385406494, acc=0.07777778059244156, loss=3.068472385406494
train: epoch 92, loss 2.3551206588745117, acc=0.17988888919353485, loss=2.3551206588745117
test: epoch 92, loss 3.115896701812744, acc=0.07222222536802292, loss=3.115896701812744
train: epoch 93, loss 2.33211350440979, acc=0.18066667020320892, loss=2.33211350440979
test: epoch 93, loss 3.142317056655884, acc=0.0833333358168602, loss=3.142317056655884
train: epoch 94, loss 2.341585874557495, acc=0.18111111223697662, loss=2.341585874557495
test: epoch 94, loss 3.0957086086273193, acc=0.07500000298023224, loss=3.0957086086273193
train: epoch 95, loss 2.327730655670166, acc=0.1843888908624649, loss=2.327730655670166
test: epoch 95, loss 3.047884941101074, acc=0.08055555820465088, loss=3.047884941101074
train: epoch 96, loss 2.36173677444458, acc=0.17927777767181396, loss=2.36173677444458
test: epoch 96, loss 3.1303417682647705, acc=0.08611111342906952, loss=3.1303417682647705
train: epoch 97, loss 2.342569351196289, acc=0.1774444431066513, loss=2.342569351196289
test: epoch 97, loss 3.021426200866699, acc=0.08888889104127884, loss=3.021426200866699
train: epoch 98, loss 2.3365156650543213, acc=0.1811666637659073, loss=2.3365156650543213
test: epoch 98, loss 3.1224260330200195, acc=0.0972222238779068, loss=3.1224260330200195
train: epoch 99, loss 2.3229382038116455, acc=0.1835000067949295, loss=2.3229382038116455
test: epoch 99, loss 3.036015510559082, acc=0.09166666865348816, loss=3.036015510559082
train: epoch 100, loss 2.3343684673309326, acc=0.18316666781902313, loss=2.3343684673309326
test: epoch 100, loss 3.0307252407073975, acc=0.07500000298023224, loss=3.0307252407073975
train: epoch 101, loss 2.3221285343170166, acc=0.18472221493721008, loss=2.3221285343170166
test: epoch 101, loss 3.041416883468628, acc=0.10555555671453476, loss=3.041416883468628
train: epoch 102, loss 2.328364610671997, acc=0.185722216963768, loss=2.328364610671997
test: epoch 102, loss 3.006460189819336, acc=0.08055555820465088, loss=3.006460189819336
train: epoch 103, loss 2.31003475189209, acc=0.1886666715145111, loss=2.31003475189209
test: epoch 103, loss 3.0921530723571777, acc=0.05833333358168602, loss=3.0921530723571777
train: epoch 104, loss 2.3238396644592285, acc=0.18477778136730194, loss=2.3238396644592285
test: epoch 104, loss 3.1510682106018066, acc=0.07777778059244156, loss=3.1510682106018066
train: epoch 105, loss 2.3294620513916016, acc=0.1821666657924652, loss=2.3294620513916016
test: epoch 105, loss 3.1080005168914795, acc=0.07777778059244156, loss=3.1080005168914795
train: epoch 106, loss 2.3216233253479004, acc=0.18577778339385986, loss=2.3216233253479004
test: epoch 106, loss 3.0460827350616455, acc=0.07500000298023224, loss=3.0460827350616455
train: epoch 107, loss 2.319251537322998, acc=0.18361110985279083, loss=2.319251537322998
test: epoch 107, loss 3.069873809814453, acc=0.07500000298023224, loss=3.069873809814453
train: epoch 108, loss 2.3194022178649902, acc=0.18522222340106964, loss=2.3194022178649902
test: epoch 108, loss 3.089693307876587, acc=0.08888889104127884, loss=3.089693307876587
train: epoch 109, loss 2.323547840118408, acc=0.1823333352804184, loss=2.323547840118408
test: epoch 109, loss 3.050128936767578, acc=0.08888889104127884, loss=3.050128936767578
train: epoch 110, loss 2.2937698364257812, acc=0.19011111557483673, loss=2.2937698364257812
test: epoch 110, loss 3.0350210666656494, acc=0.08611111342906952, loss=3.0350210666656494
train: epoch 111, loss 2.304110050201416, acc=0.1841111183166504, loss=2.304110050201416
test: epoch 111, loss 3.108415126800537, acc=0.06666667014360428, loss=3.108415126800537
train: epoch 112, loss 2.2990705966949463, acc=0.18700000643730164, loss=2.2990705966949463
test: epoch 112, loss 2.893345594406128, acc=0.08611111342906952, loss=2.893345594406128
train: epoch 113, loss 2.2962288856506348, acc=0.18777777254581451, loss=2.2962288856506348
test: epoch 113, loss 3.0895586013793945, acc=0.0972222238779068, loss=3.0895586013793945
train: epoch 114, loss 2.3033528327941895, acc=0.18761111795902252, loss=2.3033528327941895
test: epoch 114, loss 3.004326105117798, acc=0.0694444477558136, loss=3.004326105117798
train: epoch 115, loss 2.3003122806549072, acc=0.19033333659172058, loss=2.3003122806549072
test: epoch 115, loss 3.0585598945617676, acc=0.07222222536802292, loss=3.0585598945617676
train: epoch 116, loss 2.29679536819458, acc=0.1904444396495819, loss=2.29679536819458
test: epoch 116, loss 3.109581232070923, acc=0.07500000298023224, loss=3.109581232070923
train: epoch 117, loss 2.298706293106079, acc=0.18783333897590637, loss=2.298706293106079
test: epoch 117, loss 3.0444228649139404, acc=0.09166666865348816, loss=3.0444228649139404
train: epoch 118, loss 2.289058208465576, acc=0.18761111795902252, loss=2.289058208465576
test: epoch 118, loss 3.1032097339630127, acc=0.07500000298023224, loss=3.1032097339630127
train: epoch 119, loss 2.2934257984161377, acc=0.1914999932050705, loss=2.2934257984161377
test: epoch 119, loss 3.172834634780884, acc=0.0833333358168602, loss=3.172834634780884
train: epoch 120, loss 2.268610954284668, acc=0.19699999690055847, loss=2.268610954284668
test: epoch 120, loss 3.1503968238830566, acc=0.0694444477558136, loss=3.1503968238830566
train: epoch 121, loss 2.289325475692749, acc=0.19477777183055878, loss=2.289325475692749
test: epoch 121, loss 3.125772476196289, acc=0.09444444626569748, loss=3.125772476196289
train: epoch 122, loss 2.278233528137207, acc=0.19677777588367462, loss=2.278233528137207
test: epoch 122, loss 2.9589715003967285, acc=0.08055555820465088, loss=2.9589715003967285
train: epoch 123, loss 2.2859692573547363, acc=0.19394443929195404, loss=2.2859692573547363
test: epoch 123, loss 3.0159590244293213, acc=0.07777778059244156, loss=3.0159590244293213
train: epoch 124, loss 2.2767386436462402, acc=0.19033333659172058, loss=2.2767386436462402
test: epoch 124, loss 3.0856785774230957, acc=0.07777778059244156, loss=3.0856785774230957
train: epoch 125, loss 2.2779808044433594, acc=0.19611111283302307, loss=2.2779808044433594
test: epoch 125, loss 2.9459328651428223, acc=0.07777778059244156, loss=2.9459328651428223
train: epoch 126, loss 2.276266098022461, acc=0.19466666877269745, loss=2.276266098022461
test: epoch 126, loss 3.1020426750183105, acc=0.07777778059244156, loss=3.1020426750183105
train: epoch 127, loss 2.2593302726745605, acc=0.1984444409608841, loss=2.2593302726745605
test: epoch 127, loss 3.107074499130249, acc=0.07500000298023224, loss=3.107074499130249
train: epoch 128, loss 2.2649645805358887, acc=0.1997777819633484, loss=2.2649645805358887
test: epoch 128, loss 3.0559232234954834, acc=0.07777778059244156, loss=3.0559232234954834
train: epoch 129, loss 2.268188238143921, acc=0.1944444477558136, loss=2.268188238143921
test: epoch 129, loss 2.9463088512420654, acc=0.07777778059244156, loss=2.9463088512420654
train: epoch 130, loss 2.2696564197540283, acc=0.1951666623353958, loss=2.2696564197540283
test: epoch 130, loss 3.0415711402893066, acc=0.07777778059244156, loss=3.0415711402893066
train: epoch 131, loss 2.276285409927368, acc=0.20000000298023224, loss=2.276285409927368
test: epoch 131, loss 3.0926589965820312, acc=0.07777778059244156, loss=3.0926589965820312
train: epoch 132, loss 2.272597074508667, acc=0.1960555613040924, loss=2.272597074508667
test: epoch 132, loss 3.0710132122039795, acc=0.08611111342906952, loss=3.0710132122039795
train: epoch 133, loss 2.2701616287231445, acc=0.20011110603809357, loss=2.2701616287231445
test: epoch 133, loss 2.980957508087158, acc=0.08611111342906952, loss=2.980957508087158
train: epoch 134, loss 2.245352268218994, acc=0.20466665923595428, loss=2.245352268218994
test: epoch 134, loss 3.0118422508239746, acc=0.0694444477558136, loss=3.0118422508239746
train: epoch 135, loss 2.238308906555176, acc=0.2034444510936737, loss=2.238308906555176
test: epoch 135, loss 3.08341383934021, acc=0.10000000149011612, loss=3.08341383934021
train: epoch 136, loss 2.246609687805176, acc=0.19733333587646484, loss=2.246609687805176
test: epoch 136, loss 2.9921529293060303, acc=0.08055555820465088, loss=2.9921529293060303
train: epoch 137, loss 2.239718437194824, acc=0.20383332669734955, loss=2.239718437194824
test: epoch 137, loss 3.0860061645507812, acc=0.06111111119389534, loss=3.0860061645507812
train: epoch 138, loss 2.252686023712158, acc=0.19927777349948883, loss=2.252686023712158
test: epoch 138, loss 3.0217161178588867, acc=0.06666667014360428, loss=3.0217161178588867
train: epoch 139, loss 2.2458391189575195, acc=0.20172221958637238, loss=2.2458391189575195
test: epoch 139, loss 3.036829948425293, acc=0.07222222536802292, loss=3.036829948425293
train: epoch 140, loss 2.230532169342041, acc=0.20577777922153473, loss=2.230532169342041
test: epoch 140, loss 3.0163321495056152, acc=0.07777778059244156, loss=3.0163321495056152
train: epoch 141, loss 2.2492849826812744, acc=0.20072221755981445, loss=2.2492849826812744
test: epoch 141, loss 3.0218801498413086, acc=0.08611111342906952, loss=3.0218801498413086
train: epoch 142, loss 2.2370002269744873, acc=0.20266667008399963, loss=2.2370002269744873
test: epoch 142, loss 3.0055434703826904, acc=0.0833333358168602, loss=3.0055434703826904
train: epoch 143, loss 2.236492156982422, acc=0.20783333480358124, loss=2.236492156982422
test: epoch 143, loss 3.0042691230773926, acc=0.06666667014360428, loss=3.0042691230773926
train: epoch 144, loss 2.234609842300415, acc=0.2076111137866974, loss=2.234609842300415
test: epoch 144, loss 3.0253734588623047, acc=0.0833333358168602, loss=3.0253734588623047
train: epoch 145, loss 2.242955207824707, acc=0.20733332633972168, loss=2.242955207824707
test: epoch 145, loss 3.079641342163086, acc=0.07500000298023224, loss=3.079641342163086
train: epoch 146, loss 2.2387354373931885, acc=0.20561110973358154, loss=2.2387354373931885
test: epoch 146, loss 2.984257698059082, acc=0.08055555820465088, loss=2.984257698059082
train: epoch 147, loss 2.224884510040283, acc=0.20694445073604584, loss=2.224884510040283
test: epoch 147, loss 3.0163497924804688, acc=0.08055555820465088, loss=3.0163497924804688
train: epoch 148, loss 2.237236261367798, acc=0.20505554974079132, loss=2.237236261367798
test: epoch 148, loss 2.9609227180480957, acc=0.0833333358168602, loss=2.9609227180480957
train: epoch 149, loss 2.2415449619293213, acc=0.20055556297302246, loss=2.2415449619293213
test: epoch 149, loss 2.976545572280884, acc=0.08055555820465088, loss=2.976545572280884
train: epoch 150, loss 2.224135160446167, acc=0.20555555820465088, loss=2.224135160446167
test: epoch 150, loss 2.9480316638946533, acc=0.08888889104127884, loss=2.9480316638946533
