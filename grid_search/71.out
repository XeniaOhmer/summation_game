# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.99", "--one_hot=1", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=139343780, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=139343780, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.1045405864715576, acc=0.07450000196695328, loss=3.1045405864715576
test: epoch 1, loss 3.134859323501587, acc=0.09166666865348816, loss=3.134859323501587
train: epoch 2, loss 2.2520689964294434, acc=0.18427777290344238, loss=2.2520689964294434
test: epoch 2, loss 2.6808526515960693, acc=0.12222222238779068, loss=2.6808526515960693
train: epoch 3, loss 1.8352673053741455, acc=0.2967222332954407, loss=1.8352673053741455
test: epoch 3, loss 2.2311432361602783, acc=0.17499999701976776, loss=2.2311432361602783
train: epoch 4, loss 1.5638974905014038, acc=0.3857777714729309, loss=1.5638974905014038
test: epoch 4, loss 2.0547144412994385, acc=0.20555555820465088, loss=2.0547144412994385
train: epoch 5, loss 1.4098187685012817, acc=0.4320000112056732, loss=1.4098187685012817
test: epoch 5, loss 1.9913914203643799, acc=0.2611111104488373, loss=1.9913914203643799
train: epoch 6, loss 1.279874324798584, acc=0.4736666679382324, loss=1.279874324798584
test: epoch 6, loss 1.767183780670166, acc=0.2888889014720917, loss=1.767183780670166
train: epoch 7, loss 1.196939468383789, acc=0.5146111249923706, loss=1.196939468383789
test: epoch 7, loss 1.6564955711364746, acc=0.3027777671813965, loss=1.6564955711364746
train: epoch 8, loss 1.1347613334655762, acc=0.5306666493415833, loss=1.1347613334655762
test: epoch 8, loss 1.6708444356918335, acc=0.35555556416511536, loss=1.6708444356918335
train: epoch 9, loss 1.0278159379959106, acc=0.5742777585983276, loss=1.0278159379959106
test: epoch 9, loss 1.6110872030258179, acc=0.36666667461395264, loss=1.6110872030258179
train: epoch 10, loss 0.9895134568214417, acc=0.5852222442626953, loss=0.9895134568214417
test: epoch 10, loss 1.4753755331039429, acc=0.3861111104488373, loss=1.4753755331039429
train: epoch 11, loss 0.926564633846283, acc=0.6133888959884644, loss=0.926564633846283
test: epoch 11, loss 1.541978120803833, acc=0.3722222149372101, loss=1.541978120803833
train: epoch 12, loss 0.8902645707130432, acc=0.6259444355964661, loss=0.8902645707130432
test: epoch 12, loss 1.6330926418304443, acc=0.3888888955116272, loss=1.6330926418304443
train: epoch 13, loss 0.8446232080459595, acc=0.6460000276565552, loss=0.8446232080459595
test: epoch 13, loss 1.5191038846969604, acc=0.4277777671813965, loss=1.5191038846969604
train: epoch 14, loss 0.811689555644989, acc=0.6649444699287415, loss=0.811689555644989
test: epoch 14, loss 1.4621083736419678, acc=0.3916666805744171, loss=1.4621083736419678
train: epoch 15, loss 0.7684627175331116, acc=0.6768888831138611, loss=0.7684627175331116
test: epoch 15, loss 1.3656188249588013, acc=0.46666666865348816, loss=1.3656188249588013
train: epoch 16, loss 0.7545746564865112, acc=0.6772778034210205, loss=0.7545746564865112
test: epoch 16, loss 1.343686819076538, acc=0.4583333432674408, loss=1.343686819076538
train: epoch 17, loss 0.7159804701805115, acc=0.6973333358764648, loss=0.7159804701805115
test: epoch 17, loss 1.3974058628082275, acc=0.4444444477558136, loss=1.3974058628082275
train: epoch 18, loss 0.6996214985847473, acc=0.7005555629730225, loss=0.6996214985847473
test: epoch 18, loss 1.1958630084991455, acc=0.5333333611488342, loss=1.1958630084991455
train: epoch 19, loss 0.6599658727645874, acc=0.7213333249092102, loss=0.6599658727645874
test: epoch 19, loss 1.399628758430481, acc=0.5055555701255798, loss=1.399628758430481
train: epoch 20, loss 0.6514633893966675, acc=0.7250000238418579, loss=0.6514633893966675
test: epoch 20, loss 1.329605221748352, acc=0.4722222089767456, loss=1.329605221748352
train: epoch 21, loss 0.6331599950790405, acc=0.7347221970558167, loss=0.6331599950790405
test: epoch 21, loss 1.218348503112793, acc=0.5027777552604675, loss=1.218348503112793
train: epoch 22, loss 0.608982264995575, acc=0.7453888654708862, loss=0.608982264995575
test: epoch 22, loss 1.1594369411468506, acc=0.5472221970558167, loss=1.1594369411468506
train: epoch 23, loss 0.5993123650550842, acc=0.7476666569709778, loss=0.5993123650550842
test: epoch 23, loss 1.342073917388916, acc=0.5388888716697693, loss=1.342073917388916
train: epoch 24, loss 0.5928670167922974, acc=0.7503888607025146, loss=0.5928670167922974
test: epoch 24, loss 1.131134033203125, acc=0.5694444179534912, loss=1.131134033203125
train: epoch 25, loss 0.5816590189933777, acc=0.7553889155387878, loss=0.5816590189933777
test: epoch 25, loss 1.3105428218841553, acc=0.5444444417953491, loss=1.3105428218841553
train: epoch 26, loss 0.5871699452400208, acc=0.7494444251060486, loss=0.5871699452400208
test: epoch 26, loss 1.162955641746521, acc=0.5222222208976746, loss=1.162955641746521
train: epoch 27, loss 0.5651107430458069, acc=0.7663888931274414, loss=0.5651107430458069
test: epoch 27, loss 1.2378093004226685, acc=0.5472221970558167, loss=1.2378093004226685
train: epoch 28, loss 0.574711263179779, acc=0.7599999904632568, loss=0.574711263179779
test: epoch 28, loss 1.152915596961975, acc=0.5388888716697693, loss=1.152915596961975
train: epoch 29, loss 0.5609314441680908, acc=0.7629444599151611, loss=0.5609314441680908
test: epoch 29, loss 1.2619074583053589, acc=0.5611110925674438, loss=1.2619074583053589
train: epoch 30, loss 0.5585212111473083, acc=0.7642777562141418, loss=0.5585212111473083
test: epoch 30, loss 1.142521858215332, acc=0.5555555820465088, loss=1.142521858215332
train: epoch 31, loss 0.5448306202888489, acc=0.7715555429458618, loss=0.5448306202888489
test: epoch 31, loss 1.1509023904800415, acc=0.5666666626930237, loss=1.1509023904800415
train: epoch 32, loss 0.5504967570304871, acc=0.7674999833106995, loss=0.5504967570304871
test: epoch 32, loss 1.311645746231079, acc=0.550000011920929, loss=1.311645746231079
train: epoch 33, loss 0.5438356399536133, acc=0.7695555686950684, loss=0.5438356399536133
test: epoch 33, loss 1.1976944208145142, acc=0.5638889074325562, loss=1.1976944208145142
train: epoch 34, loss 0.5443108081817627, acc=0.7682777643203735, loss=0.5443108081817627
test: epoch 34, loss 1.1981556415557861, acc=0.5694444179534912, loss=1.1981556415557861
train: epoch 35, loss 0.5223007798194885, acc=0.7792778015136719, loss=0.5223007798194885
test: epoch 35, loss 1.1940733194351196, acc=0.5416666865348816, loss=1.1940733194351196
train: epoch 36, loss 0.5290800929069519, acc=0.7730555534362793, loss=0.5290800929069519
test: epoch 36, loss 1.152571439743042, acc=0.5694444179534912, loss=1.152571439743042
train: epoch 37, loss 0.5282644629478455, acc=0.7730555534362793, loss=0.5282644629478455
test: epoch 37, loss 1.226737141609192, acc=0.5555555820465088, loss=1.226737141609192
train: epoch 38, loss 0.5027384757995605, acc=0.7807222008705139, loss=0.5027384757995605
test: epoch 38, loss 1.179943323135376, acc=0.5638889074325562, loss=1.179943323135376
train: epoch 39, loss 0.5091090202331543, acc=0.780055582523346, loss=0.5091090202331543
test: epoch 39, loss 1.2480560541152954, acc=0.5611110925674438, loss=1.2480560541152954
train: epoch 40, loss 0.5169402956962585, acc=0.7756111025810242, loss=0.5169402956962585
test: epoch 40, loss 1.2480806112289429, acc=0.5694444179534912, loss=1.2480806112289429
train: epoch 41, loss 0.491413414478302, acc=0.785611093044281, loss=0.491413414478302
test: epoch 41, loss 1.134144902229309, acc=0.5722222328186035, loss=1.134144902229309
train: epoch 42, loss 0.5109137296676636, acc=0.7791666388511658, loss=0.5109137296676636
test: epoch 42, loss 1.0812690258026123, acc=0.5722222328186035, loss=1.0812690258026123
train: epoch 43, loss 0.5054684281349182, acc=0.7815555334091187, loss=0.5054684281349182
test: epoch 43, loss 1.141857385635376, acc=0.5638889074325562, loss=1.141857385635376
train: epoch 44, loss 0.5011339783668518, acc=0.7853888869285583, loss=0.5011339783668518
test: epoch 44, loss 1.1196303367614746, acc=0.5666666626930237, loss=1.1196303367614746
train: epoch 45, loss 0.4946533739566803, acc=0.7872777581214905, loss=0.4946533739566803
test: epoch 45, loss 1.2409493923187256, acc=0.5722222328186035, loss=1.2409493923187256
train: epoch 46, loss 0.4961310625076294, acc=0.7860000133514404, loss=0.4961310625076294
test: epoch 46, loss 1.1475449800491333, acc=0.574999988079071, loss=1.1475449800491333
train: epoch 47, loss 0.4899258017539978, acc=0.7909444570541382, loss=0.4899258017539978
test: epoch 47, loss 1.105039358139038, acc=0.5722222328186035, loss=1.105039358139038
train: epoch 48, loss 0.4888008236885071, acc=0.78938889503479, loss=0.4888008236885071
test: epoch 48, loss 1.20731782913208, acc=0.5694444179534912, loss=1.20731782913208
train: epoch 49, loss 0.4750538170337677, acc=0.7906666398048401, loss=0.4750538170337677
test: epoch 49, loss 1.33951735496521, acc=0.5694444179534912, loss=1.33951735496521
train: epoch 50, loss 0.4872830808162689, acc=0.7906110882759094, loss=0.4872830808162689
test: epoch 50, loss 1.2934671640396118, acc=0.5333333611488342, loss=1.2934671640396118
train: epoch 51, loss 0.4955028295516968, acc=0.7856666445732117, loss=0.4955028295516968
test: epoch 51, loss 1.2905631065368652, acc=0.550000011920929, loss=1.2905631065368652
train: epoch 52, loss 0.4805371165275574, acc=0.792888879776001, loss=0.4805371165275574
test: epoch 52, loss 1.1338814496994019, acc=0.574999988079071, loss=1.1338814496994019
train: epoch 53, loss 0.48942476511001587, acc=0.7923333048820496, loss=0.48942476511001587
test: epoch 53, loss 1.0665152072906494, acc=0.5805555582046509, loss=1.0665152072906494
train: epoch 54, loss 0.48058605194091797, acc=0.7926666736602783, loss=0.48058605194091797
test: epoch 54, loss 1.1633758544921875, acc=0.574999988079071, loss=1.1633758544921875
train: epoch 55, loss 0.4773351550102234, acc=0.7920555472373962, loss=0.4773351550102234
test: epoch 55, loss 1.1455062627792358, acc=0.5833333134651184, loss=1.1455062627792358
train: epoch 56, loss 0.45280691981315613, acc=0.8009999990463257, loss=0.45280691981315613
test: epoch 56, loss 1.198154091835022, acc=0.5777778029441833, loss=1.198154091835022
train: epoch 57, loss 0.46630433201789856, acc=0.801277756690979, loss=0.46630433201789856
test: epoch 57, loss 1.154034972190857, acc=0.574999988079071, loss=1.154034972190857
train: epoch 58, loss 0.46231991052627563, acc=0.8025000095367432, loss=0.46231991052627563
test: epoch 58, loss 1.1867915391921997, acc=0.5805555582046509, loss=1.1867915391921997
train: epoch 59, loss 0.4605311453342438, acc=0.800611138343811, loss=0.4605311453342438
test: epoch 59, loss 1.1593940258026123, acc=0.5805555582046509, loss=1.1593940258026123
train: epoch 60, loss 0.46465012431144714, acc=0.8028888702392578, loss=0.46465012431144714
test: epoch 60, loss 1.231938362121582, acc=0.5777778029441833, loss=1.231938362121582
train: epoch 61, loss 0.4683298170566559, acc=0.7970555424690247, loss=0.4683298170566559
test: epoch 61, loss 1.1544142961502075, acc=0.574999988079071, loss=1.1544142961502075
train: epoch 62, loss 0.44884970784187317, acc=0.8065000176429749, loss=0.44884970784187317
test: epoch 62, loss 1.189711570739746, acc=0.5833333134651184, loss=1.189711570739746
train: epoch 63, loss 0.4537155330181122, acc=0.8037777543067932, loss=0.4537155330181122
test: epoch 63, loss 1.245137095451355, acc=0.5833333134651184, loss=1.245137095451355
train: epoch 64, loss 0.46297308802604675, acc=0.8051111102104187, loss=0.46297308802604675
test: epoch 64, loss 1.1453965902328491, acc=0.5805555582046509, loss=1.1453965902328491
train: epoch 65, loss 0.4609546959400177, acc=0.8040000200271606, loss=0.4609546959400177
test: epoch 65, loss 1.2108895778656006, acc=0.5833333134651184, loss=1.2108895778656006
train: epoch 66, loss 0.45916804671287537, acc=0.801277756690979, loss=0.45916804671287537
test: epoch 66, loss 1.1331764459609985, acc=0.5805555582046509, loss=1.1331764459609985
train: epoch 67, loss 0.4613457918167114, acc=0.8014444708824158, loss=0.4613457918167114
test: epoch 67, loss 1.0014357566833496, acc=0.5805555582046509, loss=1.0014357566833496
train: epoch 68, loss 0.45305541157722473, acc=0.8078888654708862, loss=0.45305541157722473
test: epoch 68, loss 1.1552181243896484, acc=0.5805555582046509, loss=1.1552181243896484
train: epoch 69, loss 0.43445855379104614, acc=0.8148333430290222, loss=0.43445855379104614
test: epoch 69, loss 1.4137136936187744, acc=0.5722222328186035, loss=1.4137136936187744
train: epoch 70, loss 0.4543910622596741, acc=0.8077222108840942, loss=0.4543910622596741
test: epoch 70, loss 1.170526146888733, acc=0.5666666626930237, loss=1.170526146888733
train: epoch 71, loss 0.4411366283893585, acc=0.8101111054420471, loss=0.4411366283893585
test: epoch 71, loss 1.1215934753417969, acc=0.5777778029441833, loss=1.1215934753417969
train: epoch 72, loss 0.4620618224143982, acc=0.804111123085022, loss=0.4620618224143982
test: epoch 72, loss 1.1674561500549316, acc=0.5833333134651184, loss=1.1674561500549316
train: epoch 73, loss 0.450066477060318, acc=0.8067222237586975, loss=0.450066477060318
test: epoch 73, loss 1.2003740072250366, acc=0.5805555582046509, loss=1.2003740072250366
train: epoch 74, loss 0.46220120787620544, acc=0.8043333292007446, loss=0.46220120787620544
test: epoch 74, loss 1.1881864070892334, acc=0.5805555582046509, loss=1.1881864070892334
train: epoch 75, loss 0.46230754256248474, acc=0.8025555610656738, loss=0.46230754256248474
test: epoch 75, loss 1.3347405195236206, acc=0.5833333134651184, loss=1.3347405195236206
train: epoch 76, loss 0.43128323554992676, acc=0.8140555620193481, loss=0.43128323554992676
test: epoch 76, loss 1.262876272201538, acc=0.5805555582046509, loss=1.262876272201538
train: epoch 77, loss 0.43891990184783936, acc=0.8114444613456726, loss=0.43891990184783936
test: epoch 77, loss 1.280498743057251, acc=0.5833333134651184, loss=1.280498743057251
train: epoch 78, loss 0.44121474027633667, acc=0.8134999871253967, loss=0.44121474027633667
test: epoch 78, loss 1.079521656036377, acc=0.5777778029441833, loss=1.079521656036377
train: epoch 79, loss 0.46215078234672546, acc=0.8059444427490234, loss=0.46215078234672546
test: epoch 79, loss 1.16849684715271, acc=0.5861111283302307, loss=1.16849684715271
train: epoch 80, loss 0.46239563822746277, acc=0.8061110973358154, loss=0.46239563822746277
test: epoch 80, loss 1.1302167177200317, acc=0.5833333134651184, loss=1.1302167177200317
train: epoch 81, loss 0.44083818793296814, acc=0.8075555562973022, loss=0.44083818793296814
test: epoch 81, loss 1.1298823356628418, acc=0.5722222328186035, loss=1.1298823356628418
train: epoch 82, loss 0.4548369348049164, acc=0.8047778010368347, loss=0.4548369348049164
test: epoch 82, loss 1.142684817314148, acc=0.5833333134651184, loss=1.142684817314148
train: epoch 83, loss 0.44439852237701416, acc=0.8080000281333923, loss=0.44439852237701416
test: epoch 83, loss 1.0797808170318604, acc=0.5833333134651184, loss=1.0797808170318604
train: epoch 84, loss 0.4169364869594574, acc=0.8180000185966492, loss=0.4169364869594574
test: epoch 84, loss 1.3062299489974976, acc=0.5777778029441833, loss=1.3062299489974976
train: epoch 85, loss 0.42938563227653503, acc=0.8116666674613953, loss=0.42938563227653503
test: epoch 85, loss 1.237682819366455, acc=0.5833333134651184, loss=1.237682819366455
train: epoch 86, loss 0.4574854373931885, acc=0.8047778010368347, loss=0.4574854373931885
test: epoch 86, loss 1.2780659198760986, acc=0.5805555582046509, loss=1.2780659198760986
train: epoch 87, loss 0.4337504506111145, acc=0.8101111054420471, loss=0.4337504506111145
test: epoch 87, loss 1.2172614336013794, acc=0.5833333134651184, loss=1.2172614336013794
train: epoch 88, loss 0.42411765456199646, acc=0.8171666860580444, loss=0.42411765456199646
test: epoch 88, loss 1.2941278219223022, acc=0.5833333134651184, loss=1.2941278219223022
train: epoch 89, loss 0.4426816999912262, acc=0.8089444637298584, loss=0.4426816999912262
test: epoch 89, loss 1.203447699546814, acc=0.5833333134651184, loss=1.203447699546814
train: epoch 90, loss 0.43828216195106506, acc=0.8113889098167419, loss=0.43828216195106506
test: epoch 90, loss 1.2937822341918945, acc=0.5555555820465088, loss=1.2937822341918945
train: epoch 91, loss 0.42179566621780396, acc=0.8174444437026978, loss=0.42179566621780396
test: epoch 91, loss 1.264934778213501, acc=0.5833333134651184, loss=1.264934778213501
train: epoch 92, loss 0.4191318154335022, acc=0.816611111164093, loss=0.4191318154335022
test: epoch 92, loss 1.166014313697815, acc=0.5722222328186035, loss=1.166014313697815
train: epoch 93, loss 0.4328114092350006, acc=0.8130555748939514, loss=0.4328114092350006
test: epoch 93, loss 1.3015426397323608, acc=0.5805555582046509, loss=1.3015426397323608
train: epoch 94, loss 0.43173477053642273, acc=0.8171111345291138, loss=0.43173477053642273
test: epoch 94, loss 1.2287660837173462, acc=0.5861111283302307, loss=1.2287660837173462
train: epoch 95, loss 0.43453869223594666, acc=0.8156111240386963, loss=0.43453869223594666
test: epoch 95, loss 1.2573878765106201, acc=0.5833333134651184, loss=1.2573878765106201
train: epoch 96, loss 0.41815316677093506, acc=0.819611132144928, loss=0.41815316677093506
test: epoch 96, loss 1.301480770111084, acc=0.5583333373069763, loss=1.301480770111084
train: epoch 97, loss 0.43801161646842957, acc=0.8105000257492065, loss=0.43801161646842957
test: epoch 97, loss 1.2461230754852295, acc=0.5833333134651184, loss=1.2461230754852295
train: epoch 98, loss 0.4195088744163513, acc=0.8192222118377686, loss=0.4195088744163513
test: epoch 98, loss 1.2754316329956055, acc=0.5805555582046509, loss=1.2754316329956055
train: epoch 99, loss 0.43084239959716797, acc=0.8141111135482788, loss=0.43084239959716797
test: epoch 99, loss 1.0920205116271973, acc=0.5833333134651184, loss=1.0920205116271973
train: epoch 100, loss 0.4227541983127594, acc=0.8177777528762817, loss=0.4227541983127594
test: epoch 100, loss 1.059680461883545, acc=0.5861111283302307, loss=1.059680461883545
train: epoch 101, loss 0.43307605385780334, acc=0.8142777681350708, loss=0.43307605385780334
test: epoch 101, loss 1.2320700883865356, acc=0.5666666626930237, loss=1.2320700883865356
train: epoch 102, loss 0.42386293411254883, acc=0.8158888816833496, loss=0.42386293411254883
test: epoch 102, loss 1.233638882637024, acc=0.5777778029441833, loss=1.233638882637024
train: epoch 103, loss 0.4251035749912262, acc=0.8168888688087463, loss=0.4251035749912262
test: epoch 103, loss 1.183665156364441, acc=0.5833333134651184, loss=1.183665156364441
train: epoch 104, loss 0.4161796569824219, acc=0.8203333616256714, loss=0.4161796569824219
test: epoch 104, loss 1.2408323287963867, acc=0.574999988079071, loss=1.2408323287963867
train: epoch 105, loss 0.4174111783504486, acc=0.8182777762413025, loss=0.4174111783504486
test: epoch 105, loss 1.2299169301986694, acc=0.5916666388511658, loss=1.2299169301986694
train: epoch 106, loss 0.4403718411922455, acc=0.8141111135482788, loss=0.4403718411922455
test: epoch 106, loss 1.2874937057495117, acc=0.5861111283302307, loss=1.2874937057495117
train: epoch 107, loss 0.41455182433128357, acc=0.8216666579246521, loss=0.41455182433128357
test: epoch 107, loss 1.1762280464172363, acc=0.5861111283302307, loss=1.1762280464172363
train: epoch 108, loss 0.4399634897708893, acc=0.8109444379806519, loss=0.4399634897708893
test: epoch 108, loss 1.2431875467300415, acc=0.5833333134651184, loss=1.2431875467300415
train: epoch 109, loss 0.4076593518257141, acc=0.8205000162124634, loss=0.4076593518257141
test: epoch 109, loss 1.293898344039917, acc=0.5805555582046509, loss=1.293898344039917
train: epoch 110, loss 0.4208020567893982, acc=0.8201666474342346, loss=0.4208020567893982
test: epoch 110, loss 1.328499436378479, acc=0.5722222328186035, loss=1.328499436378479
train: epoch 111, loss 0.41491347551345825, acc=0.82105553150177, loss=0.41491347551345825
test: epoch 111, loss 1.2263860702514648, acc=0.5805555582046509, loss=1.2263860702514648
train: epoch 112, loss 0.4331153631210327, acc=0.8105555772781372, loss=0.4331153631210327
test: epoch 112, loss 1.3007574081420898, acc=0.5777778029441833, loss=1.3007574081420898
train: epoch 113, loss 0.4061092436313629, acc=0.8231666684150696, loss=0.4061092436313629
test: epoch 113, loss 1.2043691873550415, acc=0.5833333134651184, loss=1.2043691873550415
train: epoch 114, loss 0.4074594974517822, acc=0.8265555500984192, loss=0.4074594974517822
test: epoch 114, loss 1.070162057876587, acc=0.5888888835906982, loss=1.070162057876587
train: epoch 115, loss 0.41088271141052246, acc=0.8216111063957214, loss=0.41088271141052246
test: epoch 115, loss 1.1672736406326294, acc=0.5833333134651184, loss=1.1672736406326294
train: epoch 116, loss 0.4282478094100952, acc=0.816444456577301, loss=0.4282478094100952
test: epoch 116, loss 1.2862813472747803, acc=0.5805555582046509, loss=1.2862813472747803
train: epoch 117, loss 0.41065654158592224, acc=0.8225555419921875, loss=0.41065654158592224
test: epoch 117, loss 1.2336363792419434, acc=0.5833333134651184, loss=1.2336363792419434
train: epoch 118, loss 0.41261714696884155, acc=0.8239444494247437, loss=0.41261714696884155
test: epoch 118, loss 1.1481695175170898, acc=0.5861111283302307, loss=1.1481695175170898
train: epoch 119, loss 0.4073188602924347, acc=0.8227777481079102, loss=0.4073188602924347
test: epoch 119, loss 1.1410914659500122, acc=0.5833333134651184, loss=1.1410914659500122
train: epoch 120, loss 0.4120364189147949, acc=0.8234999775886536, loss=0.4120364189147949
test: epoch 120, loss 1.2494981288909912, acc=0.574999988079071, loss=1.2494981288909912
train: epoch 121, loss 0.40661829710006714, acc=0.8248888850212097, loss=0.40661829710006714
test: epoch 121, loss 1.2490969896316528, acc=0.5805555582046509, loss=1.2490969896316528
train: epoch 122, loss 0.4169774055480957, acc=0.8201666474342346, loss=0.4169774055480957
test: epoch 122, loss 1.1744484901428223, acc=0.5833333134651184, loss=1.1744484901428223
train: epoch 123, loss 0.4097171723842621, acc=0.8207777738571167, loss=0.4097171723842621
test: epoch 123, loss 1.3155187368392944, acc=0.5833333134651184, loss=1.3155187368392944
train: epoch 124, loss 0.3958488702774048, acc=0.8286111354827881, loss=0.3958488702774048
test: epoch 124, loss 1.2100279331207275, acc=0.5861111283302307, loss=1.2100279331207275
train: epoch 125, loss 0.4066277742385864, acc=0.820388913154602, loss=0.4066277742385864
test: epoch 125, loss 1.3168346881866455, acc=0.5888888835906982, loss=1.3168346881866455
train: epoch 126, loss 0.4196934401988983, acc=0.8177777528762817, loss=0.4196934401988983
test: epoch 126, loss 1.3484188318252563, acc=0.5888888835906982, loss=1.3484188318252563
train: epoch 127, loss 0.3861117959022522, acc=0.8297777771949768, loss=0.3861117959022522
test: epoch 127, loss 1.298173427581787, acc=0.5916666388511658, loss=1.298173427581787
train: epoch 128, loss 0.3961141109466553, acc=0.8286666870117188, loss=0.3961141109466553
test: epoch 128, loss 1.3232321739196777, acc=0.574999988079071, loss=1.3232321739196777
train: epoch 129, loss 0.4095301628112793, acc=0.8224999904632568, loss=0.4095301628112793
test: epoch 129, loss 1.2659474611282349, acc=0.5861111283302307, loss=1.2659474611282349
train: epoch 130, loss 0.4122878313064575, acc=0.8201666474342346, loss=0.4122878313064575
test: epoch 130, loss 1.2532358169555664, acc=0.5888888835906982, loss=1.2532358169555664
train: epoch 131, loss 0.4104185998439789, acc=0.8220555782318115, loss=0.4104185998439789
test: epoch 131, loss 1.2155344486236572, acc=0.5833333134651184, loss=1.2155344486236572
train: epoch 132, loss 0.4211643934249878, acc=0.8215000033378601, loss=0.4211643934249878
test: epoch 132, loss 1.2430899143218994, acc=0.5833333134651184, loss=1.2430899143218994
train: epoch 133, loss 0.40810853242874146, acc=0.8221111297607422, loss=0.40810853242874146
test: epoch 133, loss 1.128948450088501, acc=0.5888888835906982, loss=1.128948450088501
train: epoch 134, loss 0.40273115038871765, acc=0.8276110887527466, loss=0.40273115038871765
test: epoch 134, loss 1.221933126449585, acc=0.5722222328186035, loss=1.221933126449585
train: epoch 135, loss 0.3931416869163513, acc=0.8282777667045593, loss=0.3931416869163513
test: epoch 135, loss 1.311486005783081, acc=0.5888888835906982, loss=1.311486005783081
train: epoch 136, loss 0.398746520280838, acc=0.8270000219345093, loss=0.398746520280838
test: epoch 136, loss 1.2467948198318481, acc=0.5888888835906982, loss=1.2467948198318481
train: epoch 137, loss 0.39486071467399597, acc=0.8295555710792542, loss=0.39486071467399597
test: epoch 137, loss 1.2065576314926147, acc=0.5722222328186035, loss=1.2065576314926147
train: epoch 138, loss 0.4123322069644928, acc=0.8230555653572083, loss=0.4123322069644928
test: epoch 138, loss 1.2315561771392822, acc=0.5916666388511658, loss=1.2315561771392822
train: epoch 139, loss 0.42595338821411133, acc=0.8188889026641846, loss=0.42595338821411133
test: epoch 139, loss 1.2145906686782837, acc=0.5861111283302307, loss=1.2145906686782837
train: epoch 140, loss 0.39452847838401794, acc=0.8285555839538574, loss=0.39452847838401794
test: epoch 140, loss 1.2072299718856812, acc=0.5888888835906982, loss=1.2072299718856812
train: epoch 141, loss 0.4097035527229309, acc=0.8241666555404663, loss=0.4097035527229309
test: epoch 141, loss 1.1924268007278442, acc=0.5861111283302307, loss=1.1924268007278442
train: epoch 142, loss 0.4008633494377136, acc=0.8277222514152527, loss=0.4008633494377136
test: epoch 142, loss 1.1669365167617798, acc=0.5833333134651184, loss=1.1669365167617798
train: epoch 143, loss 0.3953714668750763, acc=0.8295555710792542, loss=0.3953714668750763
test: epoch 143, loss 1.3194454908370972, acc=0.5888888835906982, loss=1.3194454908370972
train: epoch 144, loss 0.382508784532547, acc=0.8344444632530212, loss=0.382508784532547
test: epoch 144, loss 1.2456157207489014, acc=0.5888888835906982, loss=1.2456157207489014
train: epoch 145, loss 0.3855132460594177, acc=0.8331666588783264, loss=0.3855132460594177
test: epoch 145, loss 1.25736403465271, acc=0.5888888835906982, loss=1.25736403465271
train: epoch 146, loss 0.38053441047668457, acc=0.8364444375038147, loss=0.38053441047668457
test: epoch 146, loss 1.2909168004989624, acc=0.5638889074325562, loss=1.2909168004989624
train: epoch 147, loss 0.3941875696182251, acc=0.8300555348396301, loss=0.3941875696182251
test: epoch 147, loss 1.284839153289795, acc=0.5916666388511658, loss=1.284839153289795
train: epoch 148, loss 0.3984575867652893, acc=0.8312777876853943, loss=0.3984575867652893
test: epoch 148, loss 1.1900317668914795, acc=0.5888888835906982, loss=1.1900317668914795
train: epoch 149, loss 0.406180739402771, acc=0.8281111121177673, loss=0.406180739402771
test: epoch 149, loss 1.262528657913208, acc=0.5861111283302307, loss=1.262528657913208
train: epoch 150, loss 0.4023820757865906, acc=0.8269444704055786, loss=0.4023820757865906
test: epoch 150, loss 1.2636703252792358, acc=0.5888888835906982, loss=1.2636703252792358
