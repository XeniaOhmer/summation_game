# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.995", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=601393327, receiver_embed_dim=64, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=601393327, receiver_embed_dim=64, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.590733051300049, acc=0.03894444555044174, loss=3.590733051300049
test: epoch 1, loss 3.5492703914642334, acc=0.0555555559694767, loss=3.5492703914642334
train: epoch 2, loss 3.5032126903533936, acc=0.05133333429694176, loss=3.5032126903533936
test: epoch 2, loss 3.3861749172210693, acc=0.05277777835726738, loss=3.3861749172210693
train: epoch 3, loss 3.4811513423919678, acc=0.05299999937415123, loss=3.4811513423919678
test: epoch 3, loss 3.228125810623169, acc=0.06388889253139496, loss=3.228125810623169
train: epoch 4, loss 3.434974431991577, acc=0.04855555668473244, loss=3.434974431991577
test: epoch 4, loss 2.9865899085998535, acc=0.08888889104127884, loss=2.9865899085998535
train: epoch 5, loss 3.3085837364196777, acc=0.059388887137174606, loss=3.3085837364196777
test: epoch 5, loss 3.1940348148345947, acc=0.08055555820465088, loss=3.1940348148345947
train: epoch 6, loss 3.1137280464172363, acc=0.08344444632530212, loss=3.1137280464172363
test: epoch 6, loss 3.3472399711608887, acc=0.05833333358168602, loss=3.3472399711608887
train: epoch 7, loss 2.963655471801758, acc=0.09761111438274384, loss=2.963655471801758
test: epoch 7, loss 3.4200527667999268, acc=0.06111111119389534, loss=3.4200527667999268
train: epoch 8, loss 2.884512424468994, acc=0.10649999976158142, loss=2.884512424468994
test: epoch 8, loss 3.446995735168457, acc=0.0694444477558136, loss=3.446995735168457
train: epoch 9, loss 2.805910110473633, acc=0.11616666615009308, loss=2.805910110473633
test: epoch 9, loss 3.4412307739257812, acc=0.06111111119389534, loss=3.4412307739257812
train: epoch 10, loss 2.7471814155578613, acc=0.12488888949155807, loss=2.7471814155578613
test: epoch 10, loss 3.392620325088501, acc=0.06111111119389534, loss=3.392620325088501
train: epoch 11, loss 2.696653127670288, acc=0.13127778470516205, loss=2.696653127670288
test: epoch 11, loss 3.3843588829040527, acc=0.0694444477558136, loss=3.3843588829040527
train: epoch 12, loss 2.655184268951416, acc=0.1393333375453949, loss=2.655184268951416
test: epoch 12, loss 3.3306872844696045, acc=0.0833333358168602, loss=3.3306872844696045
train: epoch 13, loss 2.61753511428833, acc=0.14427778124809265, loss=2.61753511428833
test: epoch 13, loss 3.381009578704834, acc=0.0833333358168602, loss=3.381009578704834
train: epoch 14, loss 2.5849006175994873, acc=0.14811110496520996, loss=2.5849006175994873
test: epoch 14, loss 3.4109983444213867, acc=0.07222222536802292, loss=3.4109983444213867
train: epoch 15, loss 2.5673062801361084, acc=0.15222221612930298, loss=2.5673062801361084
test: epoch 15, loss 3.339463710784912, acc=0.07222222536802292, loss=3.339463710784912
train: epoch 16, loss 2.5430748462677, acc=0.15694443881511688, loss=2.5430748462677
test: epoch 16, loss 3.280595064163208, acc=0.0694444477558136, loss=3.280595064163208
train: epoch 17, loss 2.5189192295074463, acc=0.15888889133930206, loss=2.5189192295074463
test: epoch 17, loss 3.3069844245910645, acc=0.07222222536802292, loss=3.3069844245910645
train: epoch 18, loss 2.5127549171447754, acc=0.1641666740179062, loss=2.5127549171447754
test: epoch 18, loss 3.2844884395599365, acc=0.07500000298023224, loss=3.2844884395599365
train: epoch 19, loss 2.4927828311920166, acc=0.16500000655651093, loss=2.4927828311920166
test: epoch 19, loss 3.28151535987854, acc=0.07777778059244156, loss=3.28151535987854
train: epoch 20, loss 2.4869422912597656, acc=0.16661110520362854, loss=2.4869422912597656
test: epoch 20, loss 3.2117867469787598, acc=0.07500000298023224, loss=3.2117867469787598
train: epoch 21, loss 2.4575695991516113, acc=0.1711111068725586, loss=2.4575695991516113
test: epoch 21, loss 3.1850552558898926, acc=0.07777778059244156, loss=3.1850552558898926
train: epoch 22, loss 2.437274217605591, acc=0.1789444386959076, loss=2.437274217605591
test: epoch 22, loss 3.1780407428741455, acc=0.08055555820465088, loss=3.1780407428741455
train: epoch 23, loss 2.4435153007507324, acc=0.17927777767181396, loss=2.4435153007507324
test: epoch 23, loss 3.156782388687134, acc=0.09444444626569748, loss=3.156782388687134
train: epoch 24, loss 2.432480812072754, acc=0.17911110818386078, loss=2.432480812072754
test: epoch 24, loss 3.1390092372894287, acc=0.08611111342906952, loss=3.1390092372894287
train: epoch 25, loss 2.4108030796051025, acc=0.1826111078262329, loss=2.4108030796051025
test: epoch 25, loss 3.085690498352051, acc=0.08611111342906952, loss=3.085690498352051
train: epoch 26, loss 2.3872599601745605, acc=0.18444444239139557, loss=2.3872599601745605
test: epoch 26, loss 3.0507097244262695, acc=0.10000000149011612, loss=3.0507097244262695
train: epoch 27, loss 2.393331527709961, acc=0.18111111223697662, loss=2.393331527709961
test: epoch 27, loss 2.987454652786255, acc=0.09166666865348816, loss=2.987454652786255
train: epoch 28, loss 2.3662283420562744, acc=0.1850000023841858, loss=2.3662283420562744
test: epoch 28, loss 2.9939894676208496, acc=0.0972222238779068, loss=2.9939894676208496
train: epoch 29, loss 2.35642409324646, acc=0.19005554914474487, loss=2.35642409324646
test: epoch 29, loss 2.9225451946258545, acc=0.10555555671453476, loss=2.9225451946258545
train: epoch 30, loss 2.3393497467041016, acc=0.1937222182750702, loss=2.3393497467041016
test: epoch 30, loss 2.911099672317505, acc=0.10833333432674408, loss=2.911099672317505
train: epoch 31, loss 2.327623128890991, acc=0.19855555891990662, loss=2.327623128890991
test: epoch 31, loss 2.8379311561584473, acc=0.12222222238779068, loss=2.8379311561584473
train: epoch 32, loss 2.307143211364746, acc=0.20355555415153503, loss=2.307143211364746
test: epoch 32, loss 2.7340755462646484, acc=0.11388888955116272, loss=2.7340755462646484
train: epoch 33, loss 2.293536901473999, acc=0.20455555617809296, loss=2.293536901473999
test: epoch 33, loss 2.6539435386657715, acc=0.14166666567325592, loss=2.6539435386657715
train: epoch 34, loss 2.2663238048553467, acc=0.21427777409553528, loss=2.2663238048553467
test: epoch 34, loss 2.593069076538086, acc=0.1388888955116272, loss=2.593069076538086
train: epoch 35, loss 2.282423257827759, acc=0.21061110496520996, loss=2.282423257827759
test: epoch 35, loss 2.5546295642852783, acc=0.14166666567325592, loss=2.5546295642852783
train: epoch 36, loss 2.2360949516296387, acc=0.22083333134651184, loss=2.2360949516296387
test: epoch 36, loss 2.5232434272766113, acc=0.15000000596046448, loss=2.5232434272766113
train: epoch 37, loss 2.222381830215454, acc=0.21872222423553467, loss=2.222381830215454
test: epoch 37, loss 2.48642897605896, acc=0.1527777761220932, loss=2.48642897605896
train: epoch 38, loss 2.2242462635040283, acc=0.22300000488758087, loss=2.2242462635040283
test: epoch 38, loss 2.4791243076324463, acc=0.16944444179534912, loss=2.4791243076324463
train: epoch 39, loss 2.20072603225708, acc=0.22627778351306915, loss=2.20072603225708
test: epoch 39, loss 2.4527833461761475, acc=0.1666666716337204, loss=2.4527833461761475
train: epoch 40, loss 2.184648036956787, acc=0.2347777783870697, loss=2.184648036956787
test: epoch 40, loss 2.4498450756073, acc=0.1666666716337204, loss=2.4498450756073
train: epoch 41, loss 2.159637689590454, acc=0.2352222204208374, loss=2.159637689590454
test: epoch 41, loss 2.430389165878296, acc=0.17222222685813904, loss=2.430389165878296
train: epoch 42, loss 2.1406965255737305, acc=0.2387777715921402, loss=2.1406965255737305
test: epoch 42, loss 2.398418664932251, acc=0.17777778208255768, loss=2.398418664932251
train: epoch 43, loss 2.1283602714538574, acc=0.24022221565246582, loss=2.1283602714538574
test: epoch 43, loss 2.345569133758545, acc=0.1805555522441864, loss=2.345569133758545
train: epoch 44, loss 2.119516372680664, acc=0.24238888919353485, loss=2.119516372680664
test: epoch 44, loss 2.346620559692383, acc=0.17777778208255768, loss=2.346620559692383
train: epoch 45, loss 2.107121467590332, acc=0.24888889491558075, loss=2.107121467590332
test: epoch 45, loss 2.3402249813079834, acc=0.17777778208255768, loss=2.3402249813079834
train: epoch 46, loss 2.0969653129577637, acc=0.2516666650772095, loss=2.0969653129577637
test: epoch 46, loss 2.335441827774048, acc=0.18611110746860504, loss=2.335441827774048
train: epoch 47, loss 2.068279266357422, acc=0.25494444370269775, loss=2.068279266357422
test: epoch 47, loss 2.281175136566162, acc=0.18888889253139496, loss=2.281175136566162
train: epoch 48, loss 2.0645174980163574, acc=0.25716665387153625, loss=2.0645174980163574
test: epoch 48, loss 2.2367987632751465, acc=0.20277777314186096, loss=2.2367987632751465
train: epoch 49, loss 2.036954402923584, acc=0.26438888907432556, loss=2.036954402923584
test: epoch 49, loss 2.2354037761688232, acc=0.18611110746860504, loss=2.2354037761688232
train: epoch 50, loss 2.027510404586792, acc=0.269611120223999, loss=2.027510404586792
test: epoch 50, loss 2.2200968265533447, acc=0.20277777314186096, loss=2.2200968265533447
train: epoch 51, loss 1.9995137453079224, acc=0.2707222104072571, loss=1.9995137453079224
test: epoch 51, loss 2.1829285621643066, acc=0.20000000298023224, loss=2.1829285621643066
train: epoch 52, loss 2.0041663646698, acc=0.27166667580604553, loss=2.0041663646698
test: epoch 52, loss 2.122807502746582, acc=0.21111111342906952, loss=2.122807502746582
train: epoch 53, loss 1.993997573852539, acc=0.27844443917274475, loss=1.993997573852539
test: epoch 53, loss 2.1142399311065674, acc=0.21666666865348816, loss=2.1142399311065674
train: epoch 54, loss 1.967787742614746, acc=0.28299999237060547, loss=1.967787742614746
test: epoch 54, loss 2.0910654067993164, acc=0.2222222238779068, loss=2.0910654067993164
train: epoch 55, loss 1.953482747077942, acc=0.2810555696487427, loss=1.953482747077942
test: epoch 55, loss 2.100027561187744, acc=0.21111111342906952, loss=2.100027561187744
train: epoch 56, loss 1.942458987236023, acc=0.2916666567325592, loss=1.942458987236023
test: epoch 56, loss 2.0222222805023193, acc=0.21944443881511688, loss=2.0222222805023193
train: epoch 57, loss 1.9124120473861694, acc=0.2900555431842804, loss=1.9124120473861694
test: epoch 57, loss 2.0412790775299072, acc=0.2083333283662796, loss=2.0412790775299072
train: epoch 58, loss 1.920640468597412, acc=0.29516667127609253, loss=1.920640468597412
test: epoch 58, loss 2.0175814628601074, acc=0.21666666865348816, loss=2.0175814628601074
train: epoch 59, loss 1.8847242593765259, acc=0.3039444386959076, loss=1.8847242593765259
test: epoch 59, loss 1.9777436256408691, acc=0.22777777910232544, loss=1.9777436256408691
train: epoch 60, loss 1.8914029598236084, acc=0.29866665601730347, loss=1.8914029598236084
test: epoch 60, loss 1.9802396297454834, acc=0.2222222238779068, loss=1.9802396297454834
train: epoch 61, loss 1.8828370571136475, acc=0.3049444556236267, loss=1.8828370571136475
test: epoch 61, loss 1.994577407836914, acc=0.21666666865348816, loss=1.994577407836914
train: epoch 62, loss 1.8651946783065796, acc=0.31238889694213867, loss=1.8651946783065796
test: epoch 62, loss 1.9727641344070435, acc=0.23333333432674408, loss=1.9727641344070435
train: epoch 63, loss 1.849244475364685, acc=0.31344443559646606, loss=1.849244475364685
test: epoch 63, loss 1.9572237730026245, acc=0.22499999403953552, loss=1.9572237730026245
train: epoch 64, loss 1.8374700546264648, acc=0.3190000057220459, loss=1.8374700546264648
test: epoch 64, loss 1.9661273956298828, acc=0.2222222238779068, loss=1.9661273956298828
train: epoch 65, loss 1.8459398746490479, acc=0.3199999928474426, loss=1.8459398746490479
test: epoch 65, loss 1.9408011436462402, acc=0.23055554926395416, loss=1.9408011436462402
train: epoch 66, loss 1.8176052570343018, acc=0.32322221994400024, loss=1.8176052570343018
test: epoch 66, loss 1.9310417175292969, acc=0.2222222238779068, loss=1.9310417175292969
train: epoch 67, loss 1.7820497751235962, acc=0.3305000066757202, loss=1.7820497751235962
test: epoch 67, loss 1.9098951816558838, acc=0.22499999403953552, loss=1.9098951816558838
train: epoch 68, loss 1.7786180973052979, acc=0.33016666769981384, loss=1.7786180973052979
test: epoch 68, loss 1.902859091758728, acc=0.22777777910232544, loss=1.902859091758728
train: epoch 69, loss 1.7586838006973267, acc=0.32944443821907043, loss=1.7586838006973267
test: epoch 69, loss 1.8837904930114746, acc=0.23055554926395416, loss=1.8837904930114746
train: epoch 70, loss 1.771236538887024, acc=0.33605554699897766, loss=1.771236538887024
test: epoch 70, loss 1.8844001293182373, acc=0.22777777910232544, loss=1.8844001293182373
train: epoch 71, loss 1.7531119585037231, acc=0.34761109948158264, loss=1.7531119585037231
test: epoch 71, loss 1.8645167350769043, acc=0.22499999403953552, loss=1.8645167350769043
train: epoch 72, loss 1.7398402690887451, acc=0.34022220969200134, loss=1.7398402690887451
test: epoch 72, loss 1.8759291172027588, acc=0.23055554926395416, loss=1.8759291172027588
train: epoch 73, loss 1.7268017530441284, acc=0.34150001406669617, loss=1.7268017530441284
test: epoch 73, loss 1.8359826803207397, acc=0.23888888955116272, loss=1.8359826803207397
train: epoch 74, loss 1.72035813331604, acc=0.34744444489479065, loss=1.72035813331604
test: epoch 74, loss 1.850297451019287, acc=0.2361111044883728, loss=1.850297451019287
train: epoch 75, loss 1.7187378406524658, acc=0.34905555844306946, loss=1.7187378406524658
test: epoch 75, loss 1.825763463973999, acc=0.23888888955116272, loss=1.825763463973999
train: epoch 76, loss 1.7199541330337524, acc=0.3466666638851166, loss=1.7199541330337524
test: epoch 76, loss 1.829820990562439, acc=0.23333333432674408, loss=1.829820990562439
train: epoch 77, loss 1.6890255212783813, acc=0.3536111116409302, loss=1.6890255212783813
test: epoch 77, loss 1.8203386068344116, acc=0.2361111044883728, loss=1.8203386068344116
train: epoch 78, loss 1.6845076084136963, acc=0.35733333230018616, loss=1.6845076084136963
test: epoch 78, loss 1.807861089706421, acc=0.23055554926395416, loss=1.807861089706421
train: epoch 79, loss 1.675624132156372, acc=0.36411112546920776, loss=1.675624132156372
test: epoch 79, loss 1.8119703531265259, acc=0.2361111044883728, loss=1.8119703531265259
train: epoch 80, loss 1.6644078493118286, acc=0.36594444513320923, loss=1.6644078493118286
test: epoch 80, loss 1.8065426349639893, acc=0.2361111044883728, loss=1.8065426349639893
train: epoch 81, loss 1.6583702564239502, acc=0.37299999594688416, loss=1.6583702564239502
test: epoch 81, loss 1.7924730777740479, acc=0.23888888955116272, loss=1.7924730777740479
train: epoch 82, loss 1.6420707702636719, acc=0.3737777769565582, loss=1.6420707702636719
test: epoch 82, loss 1.8055506944656372, acc=0.24444444477558136, loss=1.8055506944656372
train: epoch 83, loss 1.634406328201294, acc=0.37966665625572205, loss=1.634406328201294
test: epoch 83, loss 1.7992618083953857, acc=0.23888888955116272, loss=1.7992618083953857
train: epoch 84, loss 1.6317659616470337, acc=0.3764444589614868, loss=1.6317659616470337
test: epoch 84, loss 1.7865045070648193, acc=0.23888888955116272, loss=1.7865045070648193
train: epoch 85, loss 1.6125056743621826, acc=0.38394445180892944, loss=1.6125056743621826
test: epoch 85, loss 1.7914378643035889, acc=0.2361111044883728, loss=1.7914378643035889
train: epoch 86, loss 1.6158210039138794, acc=0.3832777738571167, loss=1.6158210039138794
test: epoch 86, loss 1.7867341041564941, acc=0.24444444477558136, loss=1.7867341041564941
train: epoch 87, loss 1.6048580408096313, acc=0.3800555467605591, loss=1.6048580408096313
test: epoch 87, loss 1.7884140014648438, acc=0.24444444477558136, loss=1.7884140014648438
train: epoch 88, loss 1.6002357006072998, acc=0.3887222111225128, loss=1.6002357006072998
test: epoch 88, loss 1.7745541334152222, acc=0.24166665971279144, loss=1.7745541334152222
train: epoch 89, loss 1.5951117277145386, acc=0.39011111855506897, loss=1.5951117277145386
test: epoch 89, loss 1.769388198852539, acc=0.25, loss=1.769388198852539
train: epoch 90, loss 1.5857751369476318, acc=0.3916666805744171, loss=1.5857751369476318
test: epoch 90, loss 1.7724392414093018, acc=0.25, loss=1.7724392414093018
train: epoch 91, loss 1.5772987604141235, acc=0.39311110973358154, loss=1.5772987604141235
test: epoch 91, loss 1.7385444641113281, acc=0.24722221493721008, loss=1.7385444641113281
train: epoch 92, loss 1.5605288743972778, acc=0.4029444456100464, loss=1.5605288743972778
test: epoch 92, loss 1.7514716386795044, acc=0.25, loss=1.7514716386795044
train: epoch 93, loss 1.5565409660339355, acc=0.40533334016799927, loss=1.5565409660339355
test: epoch 93, loss 1.7579842805862427, acc=0.24444444477558136, loss=1.7579842805862427
train: epoch 94, loss 1.5407887697219849, acc=0.40627777576446533, loss=1.5407887697219849
test: epoch 94, loss 1.7189933061599731, acc=0.25833332538604736, loss=1.7189933061599731
train: epoch 95, loss 1.5481051206588745, acc=0.4072222113609314, loss=1.5481051206588745
test: epoch 95, loss 1.7197766304016113, acc=0.25833332538604736, loss=1.7197766304016113
train: epoch 96, loss 1.5407114028930664, acc=0.4123888909816742, loss=1.5407114028930664
test: epoch 96, loss 1.7109134197235107, acc=0.26944443583488464, loss=1.7109134197235107
train: epoch 97, loss 1.513594627380371, acc=0.4146111011505127, loss=1.513594627380371
test: epoch 97, loss 1.7161132097244263, acc=0.2638888955116272, loss=1.7161132097244263
train: epoch 98, loss 1.519901990890503, acc=0.41588887572288513, loss=1.519901990890503
test: epoch 98, loss 1.7186222076416016, acc=0.26944443583488464, loss=1.7186222076416016
train: epoch 99, loss 1.5335625410079956, acc=0.41411110758781433, loss=1.5335625410079956
test: epoch 99, loss 1.7098538875579834, acc=0.2638888955116272, loss=1.7098538875579834
train: epoch 100, loss 1.500701665878296, acc=0.421999990940094, loss=1.500701665878296
test: epoch 100, loss 1.6991479396820068, acc=0.26944443583488464, loss=1.6991479396820068
train: epoch 101, loss 1.5040063858032227, acc=0.41972222924232483, loss=1.5040063858032227
test: epoch 101, loss 1.7048795223236084, acc=0.2666666805744171, loss=1.7048795223236084
train: epoch 102, loss 1.5144505500793457, acc=0.4247777760028839, loss=1.5144505500793457
test: epoch 102, loss 1.6979587078094482, acc=0.2666666805744171, loss=1.6979587078094482
train: epoch 103, loss 1.4944430589675903, acc=0.42455556988716125, loss=1.4944430589675903
test: epoch 103, loss 1.6695607900619507, acc=0.2638888955116272, loss=1.6695607900619507
train: epoch 104, loss 1.479256510734558, acc=0.43805554509162903, loss=1.479256510734558
test: epoch 104, loss 1.681267499923706, acc=0.2638888955116272, loss=1.681267499923706
train: epoch 105, loss 1.4776804447174072, acc=0.43477776646614075, loss=1.4776804447174072
test: epoch 105, loss 1.6876626014709473, acc=0.2666666805744171, loss=1.6876626014709473
train: epoch 106, loss 1.4845635890960693, acc=0.4320000112056732, loss=1.4845635890960693
test: epoch 106, loss 1.6907501220703125, acc=0.2638888955116272, loss=1.6907501220703125
train: epoch 107, loss 1.4646506309509277, acc=0.4354444444179535, loss=1.4646506309509277
test: epoch 107, loss 1.6641013622283936, acc=0.26944443583488464, loss=1.6641013622283936
train: epoch 108, loss 1.461234211921692, acc=0.4345555603504181, loss=1.461234211921692
test: epoch 108, loss 1.6805888414382935, acc=0.2666666805744171, loss=1.6805888414382935
train: epoch 109, loss 1.4582334756851196, acc=0.43833333253860474, loss=1.4582334756851196
test: epoch 109, loss 1.6537920236587524, acc=0.27222222089767456, loss=1.6537920236587524
train: epoch 110, loss 1.4340767860412598, acc=0.44316667318344116, loss=1.4340767860412598
test: epoch 110, loss 1.6641334295272827, acc=0.2750000059604645, loss=1.6641334295272827
train: epoch 111, loss 1.4450304508209229, acc=0.4460555613040924, loss=1.4450304508209229
test: epoch 111, loss 1.6535820960998535, acc=0.2777777910232544, loss=1.6535820960998535
train: epoch 112, loss 1.414407730102539, acc=0.4577777683734894, loss=1.414407730102539
test: epoch 112, loss 1.651060700416565, acc=0.2777777910232544, loss=1.651060700416565
train: epoch 113, loss 1.4283623695373535, acc=0.4543333351612091, loss=1.4283623695373535
test: epoch 113, loss 1.6391472816467285, acc=0.2777777910232544, loss=1.6391472816467285
train: epoch 114, loss 1.4314073324203491, acc=0.45444443821907043, loss=1.4314073324203491
test: epoch 114, loss 1.6104485988616943, acc=0.28611111640930176, loss=1.6104485988616943
train: epoch 115, loss 1.4223395586013794, acc=0.4559444487094879, loss=1.4223395586013794
test: epoch 115, loss 1.6095240116119385, acc=0.2944444417953491, loss=1.6095240116119385
train: epoch 116, loss 1.4213730096817017, acc=0.45127779245376587, loss=1.4213730096817017
test: epoch 116, loss 1.6171228885650635, acc=0.2916666567325592, loss=1.6171228885650635
train: epoch 117, loss 1.4082950353622437, acc=0.457111120223999, loss=1.4082950353622437
test: epoch 117, loss 1.60161554813385, acc=0.29722222685813904, loss=1.60161554813385
train: epoch 118, loss 1.4032340049743652, acc=0.46133333444595337, loss=1.4032340049743652
test: epoch 118, loss 1.626381278038025, acc=0.2805555462837219, loss=1.626381278038025
train: epoch 119, loss 1.3935699462890625, acc=0.4667777717113495, loss=1.3935699462890625
test: epoch 119, loss 1.6044261455535889, acc=0.28611111640930176, loss=1.6044261455535889
train: epoch 120, loss 1.3760981559753418, acc=0.46922221779823303, loss=1.3760981559753418
test: epoch 120, loss 1.6074402332305908, acc=0.2944444417953491, loss=1.6074402332305908
train: epoch 121, loss 1.378302812576294, acc=0.4724999964237213, loss=1.378302812576294
test: epoch 121, loss 1.6215511560440063, acc=0.29722222685813904, loss=1.6215511560440063
train: epoch 122, loss 1.3837553262710571, acc=0.4667777717113495, loss=1.3837553262710571
test: epoch 122, loss 1.610307216644287, acc=0.29722222685813904, loss=1.610307216644287
train: epoch 123, loss 1.3907198905944824, acc=0.4676111042499542, loss=1.3907198905944824
test: epoch 123, loss 1.6057847738265991, acc=0.28611111640930176, loss=1.6057847738265991
train: epoch 124, loss 1.3555412292480469, acc=0.4758888781070709, loss=1.3555412292480469
test: epoch 124, loss 1.619653344154358, acc=0.2916666567325592, loss=1.619653344154358
train: epoch 125, loss 1.3512712717056274, acc=0.480611115694046, loss=1.3512712717056274
test: epoch 125, loss 1.6142059564590454, acc=0.2888889014720917, loss=1.6142059564590454
train: epoch 126, loss 1.3572328090667725, acc=0.4830000102519989, loss=1.3572328090667725
test: epoch 126, loss 1.6067091226577759, acc=0.3027777671813965, loss=1.6067091226577759
train: epoch 127, loss 1.3486590385437012, acc=0.4828333258628845, loss=1.3486590385437012
test: epoch 127, loss 1.6207326650619507, acc=0.3027777671813965, loss=1.6207326650619507
train: epoch 128, loss 1.3490936756134033, acc=0.48350000381469727, loss=1.3490936756134033
test: epoch 128, loss 1.6039880514144897, acc=0.3055555522441864, loss=1.6039880514144897
train: epoch 129, loss 1.3545337915420532, acc=0.4813888967037201, loss=1.3545337915420532
test: epoch 129, loss 1.6150720119476318, acc=0.28611111640930176, loss=1.6150720119476318
train: epoch 130, loss 1.338724970817566, acc=0.48533332347869873, loss=1.338724970817566
test: epoch 130, loss 1.607501745223999, acc=0.3055555522441864, loss=1.607501745223999
train: epoch 131, loss 1.3210495710372925, acc=0.4896666705608368, loss=1.3210495710372925
test: epoch 131, loss 1.6132419109344482, acc=0.2888889014720917, loss=1.6132419109344482
train: epoch 132, loss 1.3212890625, acc=0.4836111068725586, loss=1.3212890625
test: epoch 132, loss 1.594020962715149, acc=0.30000001192092896, loss=1.594020962715149
train: epoch 133, loss 1.31587815284729, acc=0.4942777752876282, loss=1.31587815284729
test: epoch 133, loss 1.5994434356689453, acc=0.30000001192092896, loss=1.5994434356689453
train: epoch 134, loss 1.322035551071167, acc=0.4930555522441864, loss=1.322035551071167
test: epoch 134, loss 1.6224898099899292, acc=0.2888889014720917, loss=1.6224898099899292
train: epoch 135, loss 1.2983615398406982, acc=0.5006666779518127, loss=1.2983615398406982
test: epoch 135, loss 1.6158922910690308, acc=0.2888889014720917, loss=1.6158922910690308
train: epoch 136, loss 1.3234782218933105, acc=0.49361109733581543, loss=1.3234782218933105
test: epoch 136, loss 1.5969747304916382, acc=0.3083333373069763, loss=1.5969747304916382
train: epoch 137, loss 1.3058971166610718, acc=0.4948333203792572, loss=1.3058971166610718
test: epoch 137, loss 1.586714744567871, acc=0.30000001192092896, loss=1.586714744567871
train: epoch 138, loss 1.2957191467285156, acc=0.5009444355964661, loss=1.2957191467285156
test: epoch 138, loss 1.583893895149231, acc=0.31388887763023376, loss=1.583893895149231
train: epoch 139, loss 1.3012701272964478, acc=0.5020555257797241, loss=1.3012701272964478
test: epoch 139, loss 1.5730255842208862, acc=0.31388887763023376, loss=1.5730255842208862
train: epoch 140, loss 1.2873761653900146, acc=0.5004444718360901, loss=1.2873761653900146
test: epoch 140, loss 1.5892670154571533, acc=0.3083333373069763, loss=1.5892670154571533
train: epoch 141, loss 1.2817872762680054, acc=0.5017777681350708, loss=1.2817872762680054
test: epoch 141, loss 1.61141037940979, acc=0.31388887763023376, loss=1.61141037940979
train: epoch 142, loss 1.2877254486083984, acc=0.5008333325386047, loss=1.2877254486083984
test: epoch 142, loss 1.5774973630905151, acc=0.30000001192092896, loss=1.5774973630905151
train: epoch 143, loss 1.2779057025909424, acc=0.5086666941642761, loss=1.2779057025909424
test: epoch 143, loss 1.5950299501419067, acc=0.3194444477558136, loss=1.5950299501419067
train: epoch 144, loss 1.2860618829727173, acc=0.5048333406448364, loss=1.2860618829727173
test: epoch 144, loss 1.577580451965332, acc=0.28611111640930176, loss=1.577580451965332
train: epoch 145, loss 1.269603967666626, acc=0.5138888955116272, loss=1.269603967666626
test: epoch 145, loss 1.585485577583313, acc=0.30000001192092896, loss=1.585485577583313
train: epoch 146, loss 1.2626969814300537, acc=0.5182222127914429, loss=1.2626969814300537
test: epoch 146, loss 1.5947265625, acc=0.31388887763023376, loss=1.5947265625
train: epoch 147, loss 1.269230842590332, acc=0.5181111097335815, loss=1.269230842590332
test: epoch 147, loss 1.5981491804122925, acc=0.2944444417953491, loss=1.5981491804122925
train: epoch 148, loss 1.2664096355438232, acc=0.5197222232818604, loss=1.2664096355438232
test: epoch 148, loss 1.5969103574752808, acc=0.2944444417953491, loss=1.5969103574752808
train: epoch 149, loss 1.243445873260498, acc=0.5211111307144165, loss=1.243445873260498
test: epoch 149, loss 1.5995055437088013, acc=0.3027777671813965, loss=1.5995055437088013
train: epoch 150, loss 1.2641581296920776, acc=0.5247222185134888, loss=1.2641581296920776
test: epoch 150, loss 1.5904868841171265, acc=0.3055555522441864, loss=1.5904868841171265
