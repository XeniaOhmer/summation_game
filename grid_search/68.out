# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.99", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1056146121, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1056146121, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.828612804412842, acc=0.10483333468437195, loss=2.828612804412842
test: epoch 1, loss 3.290217876434326, acc=0.07777778059244156, loss=3.290217876434326
train: epoch 2, loss 2.1072399616241455, acc=0.2076111137866974, loss=2.1072399616241455
test: epoch 2, loss 2.675795555114746, acc=0.1388888955116272, loss=2.675795555114746
train: epoch 3, loss 1.8353098630905151, acc=0.26738888025283813, loss=1.8353098630905151
test: epoch 3, loss 2.1891696453094482, acc=0.18611110746860504, loss=2.1891696453094482
train: epoch 4, loss 1.663432002067566, acc=0.32066667079925537, loss=1.663432002067566
test: epoch 4, loss 2.1243951320648193, acc=0.23888888955116272, loss=2.1243951320648193
train: epoch 5, loss 1.555993676185608, acc=0.36105555295944214, loss=1.555993676185608
test: epoch 5, loss 2.099156141281128, acc=0.23333333432674408, loss=2.099156141281128
train: epoch 6, loss 1.4743432998657227, acc=0.39016667008399963, loss=1.4743432998657227
test: epoch 6, loss 1.6701678037643433, acc=0.3472222089767456, loss=1.6701678037643433
train: epoch 7, loss 1.389192819595337, acc=0.4175555408000946, loss=1.389192819595337
test: epoch 7, loss 1.8243025541305542, acc=0.23333333432674408, loss=1.8243025541305542
train: epoch 8, loss 1.313905954360962, acc=0.445499986410141, loss=1.313905954360962
test: epoch 8, loss 1.7307919263839722, acc=0.2527777850627899, loss=1.7307919263839722
train: epoch 9, loss 1.2844321727752686, acc=0.4642222225666046, loss=1.2844321727752686
test: epoch 9, loss 1.608124852180481, acc=0.3027777671813965, loss=1.608124852180481
train: epoch 10, loss 1.2322453260421753, acc=0.4818333387374878, loss=1.2322453260421753
test: epoch 10, loss 1.713815689086914, acc=0.2916666567325592, loss=1.713815689086914
train: epoch 11, loss 1.2068525552749634, acc=0.492166668176651, loss=1.2068525552749634
test: epoch 11, loss 1.746924877166748, acc=0.2805555462837219, loss=1.746924877166748
train: epoch 12, loss 1.16103994846344, acc=0.5148888826370239, loss=1.16103994846344
test: epoch 12, loss 1.7057641744613647, acc=0.2666666805744171, loss=1.7057641744613647
train: epoch 13, loss 1.1322487592697144, acc=0.528333306312561, loss=1.1322487592697144
test: epoch 13, loss 1.6224297285079956, acc=0.3444444537162781, loss=1.6224297285079956
train: epoch 14, loss 1.120074987411499, acc=0.5320555567741394, loss=1.120074987411499
test: epoch 14, loss 1.6277191638946533, acc=0.3055555522441864, loss=1.6277191638946533
train: epoch 15, loss 1.0754963159561157, acc=0.5493333339691162, loss=1.0754963159561157
test: epoch 15, loss 1.609225869178772, acc=0.3361110985279083, loss=1.609225869178772
train: epoch 16, loss 1.0539944171905518, acc=0.5629444718360901, loss=1.0539944171905518
test: epoch 16, loss 1.5984708070755005, acc=0.29722222685813904, loss=1.5984708070755005
train: epoch 17, loss 1.0381067991256714, acc=0.5682222247123718, loss=1.0381067991256714
test: epoch 17, loss 1.5472948551177979, acc=0.39722222089767456, loss=1.5472948551177979
train: epoch 18, loss 0.9929237365722656, acc=0.5886666774749756, loss=0.9929237365722656
test: epoch 18, loss 1.612174153327942, acc=0.36944442987442017, loss=1.612174153327942
train: epoch 19, loss 1.011002540588379, acc=0.5784444212913513, loss=1.011002540588379
test: epoch 19, loss 1.529177188873291, acc=0.31111112236976624, loss=1.529177188873291
train: epoch 20, loss 0.9768630266189575, acc=0.5954999923706055, loss=0.9768630266189575
test: epoch 20, loss 1.551204800605774, acc=0.32499998807907104, loss=1.551204800605774
train: epoch 21, loss 0.9732191562652588, acc=0.5893333554267883, loss=0.9732191562652588
test: epoch 21, loss 1.4786319732666016, acc=0.3472222089767456, loss=1.4786319732666016
train: epoch 22, loss 0.9572240114212036, acc=0.6051111221313477, loss=0.9572240114212036
test: epoch 22, loss 1.5835204124450684, acc=0.35277777910232544, loss=1.5835204124450684
train: epoch 23, loss 0.9549682140350342, acc=0.6042777895927429, loss=0.9549682140350342
test: epoch 23, loss 1.2882591485977173, acc=0.4055555462837219, loss=1.2882591485977173
train: epoch 24, loss 0.9225781559944153, acc=0.6154444217681885, loss=0.9225781559944153
test: epoch 24, loss 1.5186123847961426, acc=0.3916666805744171, loss=1.5186123847961426
train: epoch 25, loss 0.9358870983123779, acc=0.6121666431427002, loss=0.9358870983123779
test: epoch 25, loss 1.4638932943344116, acc=0.38333332538604736, loss=1.4638932943344116
train: epoch 26, loss 0.9272107481956482, acc=0.6130555272102356, loss=0.9272107481956482
test: epoch 26, loss 1.5628515481948853, acc=0.38055557012557983, loss=1.5628515481948853
train: epoch 27, loss 0.9088314771652222, acc=0.6216111183166504, loss=0.9088314771652222
test: epoch 27, loss 1.774532437324524, acc=0.3611111044883728, loss=1.774532437324524
train: epoch 28, loss 0.9001361727714539, acc=0.6255000233650208, loss=0.9001361727714539
test: epoch 28, loss 1.3861547708511353, acc=0.41111111640930176, loss=1.3861547708511353
train: epoch 29, loss 0.8895983099937439, acc=0.6356666684150696, loss=0.8895983099937439
test: epoch 29, loss 1.5844171047210693, acc=0.4166666567325592, loss=1.5844171047210693
train: epoch 30, loss 0.8842139840126038, acc=0.6311110854148865, loss=0.8842139840126038
test: epoch 30, loss 1.5713776350021362, acc=0.3472222089767456, loss=1.5713776350021362
train: epoch 31, loss 0.8983725905418396, acc=0.6227222084999084, loss=0.8983725905418396
test: epoch 31, loss 1.3253308534622192, acc=0.4138889014720917, loss=1.3253308534622192
train: epoch 32, loss 0.8649342656135559, acc=0.6386111378669739, loss=0.8649342656135559
test: epoch 32, loss 1.3446015119552612, acc=0.4277777671813965, loss=1.3446015119552612
train: epoch 33, loss 0.8801232576370239, acc=0.6303889155387878, loss=0.8801232576370239
test: epoch 33, loss 1.3238580226898193, acc=0.4611110985279083, loss=1.3238580226898193
train: epoch 34, loss 0.8565405607223511, acc=0.6424444317817688, loss=0.8565405607223511
test: epoch 34, loss 1.37638521194458, acc=0.45277777314186096, loss=1.37638521194458
train: epoch 35, loss 0.8318474888801575, acc=0.6522777676582336, loss=0.8318474888801575
test: epoch 35, loss 1.298042893409729, acc=0.46388888359069824, loss=1.298042893409729
train: epoch 36, loss 0.8332275152206421, acc=0.6503888964653015, loss=0.8332275152206421
test: epoch 36, loss 1.3295317888259888, acc=0.4333333373069763, loss=1.3295317888259888
train: epoch 37, loss 0.8402961492538452, acc=0.6480555534362793, loss=0.8402961492538452
test: epoch 37, loss 1.2845574617385864, acc=0.4722222089767456, loss=1.2845574617385864
train: epoch 38, loss 0.8206766247749329, acc=0.6562777757644653, loss=0.8206766247749329
test: epoch 38, loss 1.3869059085845947, acc=0.43611112236976624, loss=1.3869059085845947
train: epoch 39, loss 0.8135576248168945, acc=0.6621666550636292, loss=0.8135576248168945
test: epoch 39, loss 1.300582766532898, acc=0.43888887763023376, loss=1.300582766532898
train: epoch 40, loss 0.826728343963623, acc=0.6583889126777649, loss=0.826728343963623
test: epoch 40, loss 1.502256989479065, acc=0.3611111044883728, loss=1.502256989479065
train: epoch 41, loss 0.8028179407119751, acc=0.6658889055252075, loss=0.8028179407119751
test: epoch 41, loss 1.307390570640564, acc=0.4305555522441864, loss=1.307390570640564
train: epoch 42, loss 0.7692713737487793, acc=0.6787777543067932, loss=0.7692713737487793
test: epoch 42, loss 1.3876032829284668, acc=0.4000000059604645, loss=1.3876032829284668
train: epoch 43, loss 0.7884759902954102, acc=0.6727222204208374, loss=0.7884759902954102
test: epoch 43, loss 1.3919973373413086, acc=0.4333333373069763, loss=1.3919973373413086
train: epoch 44, loss 0.7825819253921509, acc=0.6746666431427002, loss=0.7825819253921509
test: epoch 44, loss 1.4859213829040527, acc=0.45277777314186096, loss=1.4859213829040527
train: epoch 45, loss 0.7884596586227417, acc=0.6756666898727417, loss=0.7884596586227417
test: epoch 45, loss 1.3099099397659302, acc=0.4305555522441864, loss=1.3099099397659302
train: epoch 46, loss 0.7610073685646057, acc=0.6890555620193481, loss=0.7610073685646057
test: epoch 46, loss 1.3251713514328003, acc=0.4722222089767456, loss=1.3251713514328003
train: epoch 47, loss 0.7670931816101074, acc=0.6876111030578613, loss=0.7670931816101074
test: epoch 47, loss 1.4030065536499023, acc=0.4055555462837219, loss=1.4030065536499023
train: epoch 48, loss 0.7256880402565002, acc=0.7062222361564636, loss=0.7256880402565002
test: epoch 48, loss 1.4199841022491455, acc=0.4611110985279083, loss=1.4199841022491455
train: epoch 49, loss 0.7370039820671082, acc=0.6949999928474426, loss=0.7370039820671082
test: epoch 49, loss 1.2224372625350952, acc=0.46666666865348816, loss=1.2224372625350952
train: epoch 50, loss 0.7427907586097717, acc=0.6937222480773926, loss=0.7427907586097717
test: epoch 50, loss 1.2158193588256836, acc=0.43888887763023376, loss=1.2158193588256836
train: epoch 51, loss 0.7535573840141296, acc=0.6902222037315369, loss=0.7535573840141296
test: epoch 51, loss 1.2510490417480469, acc=0.4722222089767456, loss=1.2510490417480469
train: epoch 52, loss 0.7451835870742798, acc=0.6968333125114441, loss=0.7451835870742798
test: epoch 52, loss 1.4788089990615845, acc=0.46666666865348816, loss=1.4788089990615845
train: epoch 53, loss 0.7455742359161377, acc=0.6979444622993469, loss=0.7455742359161377
test: epoch 53, loss 1.3491188287734985, acc=0.4611110985279083, loss=1.3491188287734985
train: epoch 54, loss 0.7107947468757629, acc=0.7017777562141418, loss=0.7107947468757629
test: epoch 54, loss 1.5449646711349487, acc=0.4138889014720917, loss=1.5449646711349487
train: epoch 55, loss 0.7220727205276489, acc=0.6965555548667908, loss=0.7220727205276489
test: epoch 55, loss 1.1487635374069214, acc=0.49166667461395264, loss=1.1487635374069214
train: epoch 56, loss 0.7299209833145142, acc=0.6966666579246521, loss=0.7299209833145142
test: epoch 56, loss 1.1839720010757446, acc=0.5138888955116272, loss=1.1839720010757446
train: epoch 57, loss 0.7273793816566467, acc=0.7002221941947937, loss=0.7273793816566467
test: epoch 57, loss 1.2118791341781616, acc=0.49444442987442017, loss=1.2118791341781616
train: epoch 58, loss 0.6952451467514038, acc=0.7155555486679077, loss=0.6952451467514038
test: epoch 58, loss 1.317503571510315, acc=0.4611110985279083, loss=1.317503571510315
train: epoch 59, loss 0.6905930638313293, acc=0.7163333296775818, loss=0.6905930638313293
test: epoch 59, loss 1.2004649639129639, acc=0.47777777910232544, loss=1.2004649639129639
train: epoch 60, loss 0.6817442774772644, acc=0.7215555310249329, loss=0.6817442774772644
test: epoch 60, loss 1.2376359701156616, acc=0.4444444477558136, loss=1.2376359701156616
train: epoch 61, loss 0.69663405418396, acc=0.7166110873222351, loss=0.69663405418396
test: epoch 61, loss 1.2725512981414795, acc=0.5138888955116272, loss=1.2725512981414795
train: epoch 62, loss 0.6782143712043762, acc=0.7246666550636292, loss=0.6782143712043762
test: epoch 62, loss 1.4925246238708496, acc=0.46388888359069824, loss=1.4925246238708496
train: epoch 63, loss 0.6650188565254211, acc=0.7322221994400024, loss=0.6650188565254211
test: epoch 63, loss 1.2232877016067505, acc=0.4861111044883728, loss=1.2232877016067505
train: epoch 64, loss 0.6669941544532776, acc=0.7341111302375793, loss=0.6669941544532776
test: epoch 64, loss 1.3876971006393433, acc=0.42222222685813904, loss=1.3876971006393433
train: epoch 65, loss 0.6452064514160156, acc=0.742888867855072, loss=0.6452064514160156
test: epoch 65, loss 1.3830814361572266, acc=0.4416666626930237, loss=1.3830814361572266
train: epoch 66, loss 0.6676830649375916, acc=0.7370555400848389, loss=0.6676830649375916
test: epoch 66, loss 1.1705976724624634, acc=0.5138888955116272, loss=1.1705976724624634
train: epoch 67, loss 0.6418449878692627, acc=0.7453888654708862, loss=0.6418449878692627
test: epoch 67, loss 1.2626309394836426, acc=0.5305555462837219, loss=1.2626309394836426
train: epoch 68, loss 0.6727515459060669, acc=0.7323333621025085, loss=0.6727515459060669
test: epoch 68, loss 1.2193959951400757, acc=0.5027777552604675, loss=1.2193959951400757
train: epoch 69, loss 0.6575130224227905, acc=0.7403333187103271, loss=0.6575130224227905
test: epoch 69, loss 1.2615610361099243, acc=0.5055555701255798, loss=1.2615610361099243
train: epoch 70, loss 0.6483135223388672, acc=0.7411666512489319, loss=0.6483135223388672
test: epoch 70, loss 1.3911117315292358, acc=0.4194444417953491, loss=1.3911117315292358
train: epoch 71, loss 0.6562122702598572, acc=0.738277792930603, loss=0.6562122702598572
test: epoch 71, loss 1.490604043006897, acc=0.4583333432674408, loss=1.490604043006897
train: epoch 72, loss 0.6612843871116638, acc=0.7336666584014893, loss=0.6612843871116638
test: epoch 72, loss 1.1977856159210205, acc=0.5027777552604675, loss=1.1977856159210205
train: epoch 73, loss 0.6498031616210938, acc=0.7401666641235352, loss=0.6498031616210938
test: epoch 73, loss 1.2594208717346191, acc=0.47777777910232544, loss=1.2594208717346191
train: epoch 74, loss 0.6259339451789856, acc=0.746999979019165, loss=0.6259339451789856
test: epoch 74, loss 1.3403722047805786, acc=0.43611112236976624, loss=1.3403722047805786
train: epoch 75, loss 0.6275444626808167, acc=0.7512221932411194, loss=0.6275444626808167
test: epoch 75, loss 1.1655693054199219, acc=0.5, loss=1.1655693054199219
train: epoch 76, loss 0.6134672164916992, acc=0.7563333511352539, loss=0.6134672164916992
test: epoch 76, loss 1.198610782623291, acc=0.4972222149372101, loss=1.198610782623291
train: epoch 77, loss 0.6313256621360779, acc=0.7516111135482788, loss=0.6313256621360779
test: epoch 77, loss 1.3168847560882568, acc=0.5, loss=1.3168847560882568
train: epoch 78, loss 0.6270941495895386, acc=0.7514444589614868, loss=0.6270941495895386
test: epoch 78, loss 1.3733229637145996, acc=0.5027777552604675, loss=1.3733229637145996
train: epoch 79, loss 0.6254486441612244, acc=0.7514444589614868, loss=0.6254486441612244
test: epoch 79, loss 1.3448729515075684, acc=0.4861111044883728, loss=1.3448729515075684
train: epoch 80, loss 0.6381866931915283, acc=0.7483333349227905, loss=0.6381866931915283
test: epoch 80, loss 1.1091419458389282, acc=0.5138888955116272, loss=1.1091419458389282
train: epoch 81, loss 0.6231125593185425, acc=0.7553333044052124, loss=0.6231125593185425
test: epoch 81, loss 1.3216701745986938, acc=0.5, loss=1.3216701745986938
train: epoch 82, loss 0.6205248832702637, acc=0.7551666498184204, loss=0.6205248832702637
test: epoch 82, loss 1.3203186988830566, acc=0.47777777910232544, loss=1.3203186988830566
train: epoch 83, loss 0.5969573259353638, acc=0.7626110911369324, loss=0.5969573259353638
test: epoch 83, loss 1.5181310176849365, acc=0.4472222328186035, loss=1.5181310176849365
train: epoch 84, loss 0.6089369654655457, acc=0.7561666369438171, loss=0.6089369654655457
test: epoch 84, loss 1.2230796813964844, acc=0.5027777552604675, loss=1.2230796813964844
train: epoch 85, loss 0.6350311636924744, acc=0.7467222213745117, loss=0.6350311636924744
test: epoch 85, loss 1.1307928562164307, acc=0.4583333432674408, loss=1.1307928562164307
train: epoch 86, loss 0.6007115840911865, acc=0.7611111402511597, loss=0.6007115840911865
test: epoch 86, loss 1.21500563621521, acc=0.5055555701255798, loss=1.21500563621521
train: epoch 87, loss 0.6239776611328125, acc=0.7486666440963745, loss=0.6239776611328125
test: epoch 87, loss 1.2737778425216675, acc=0.5027777552604675, loss=1.2737778425216675
train: epoch 88, loss 0.6202395558357239, acc=0.7526111006736755, loss=0.6202395558357239
test: epoch 88, loss 1.1484882831573486, acc=0.5027777552604675, loss=1.1484882831573486
train: epoch 89, loss 0.6185171604156494, acc=0.7571666836738586, loss=0.6185171604156494
test: epoch 89, loss 1.2294042110443115, acc=0.5388888716697693, loss=1.2294042110443115
train: epoch 90, loss 0.647097110748291, acc=0.746055543422699, loss=0.647097110748291
test: epoch 90, loss 1.3073139190673828, acc=0.5055555701255798, loss=1.3073139190673828
train: epoch 91, loss 0.6014769673347473, acc=0.7639999985694885, loss=0.6014769673347473
test: epoch 91, loss 1.114448070526123, acc=0.5, loss=1.114448070526123
train: epoch 92, loss 0.5898035168647766, acc=0.7687777876853943, loss=0.5898035168647766
test: epoch 92, loss 1.238362193107605, acc=0.5055555701255798, loss=1.238362193107605
train: epoch 93, loss 0.587283730506897, acc=0.7680000066757202, loss=0.587283730506897
test: epoch 93, loss 1.1688276529312134, acc=0.5027777552604675, loss=1.1688276529312134
train: epoch 94, loss 0.6003433465957642, acc=0.7620000243186951, loss=0.6003433465957642
test: epoch 94, loss 1.2340565919876099, acc=0.5027777552604675, loss=1.2340565919876099
train: epoch 95, loss 0.5989903807640076, acc=0.7599999904632568, loss=0.5989903807640076
test: epoch 95, loss 1.2341657876968384, acc=0.5416666865348816, loss=1.2341657876968384
train: epoch 96, loss 0.6238954663276672, acc=0.7513333559036255, loss=0.6238954663276672
test: epoch 96, loss 1.2807010412216187, acc=0.5083333253860474, loss=1.2807010412216187
train: epoch 97, loss 0.5835253596305847, acc=0.7690555453300476, loss=0.5835253596305847
test: epoch 97, loss 1.2108198404312134, acc=0.5083333253860474, loss=1.2108198404312134
train: epoch 98, loss 0.5852962732315063, acc=0.7717777490615845, loss=0.5852962732315063
test: epoch 98, loss 1.3923345804214478, acc=0.5027777552604675, loss=1.3923345804214478
train: epoch 99, loss 0.5962205529212952, acc=0.7664444446563721, loss=0.5962205529212952
test: epoch 99, loss 1.222615361213684, acc=0.5277777910232544, loss=1.222615361213684
train: epoch 100, loss 0.6166208982467651, acc=0.7512778043746948, loss=0.6166208982467651
test: epoch 100, loss 1.125447392463684, acc=0.5027777552604675, loss=1.125447392463684
train: epoch 101, loss 0.6024104356765747, acc=0.7595000267028809, loss=0.6024104356765747
test: epoch 101, loss 1.1360200643539429, acc=0.5361111164093018, loss=1.1360200643539429
train: epoch 102, loss 0.6161853671073914, acc=0.7545555830001831, loss=0.6161853671073914
test: epoch 102, loss 1.1008554697036743, acc=0.5444444417953491, loss=1.1008554697036743
train: epoch 103, loss 0.5679298043251038, acc=0.7651666402816772, loss=0.5679298043251038
test: epoch 103, loss 1.1525285243988037, acc=0.5555555820465088, loss=1.1525285243988037
train: epoch 104, loss 0.5838207602500916, acc=0.7650555372238159, loss=0.5838207602500916
test: epoch 104, loss 1.2999508380889893, acc=0.4749999940395355, loss=1.2999508380889893
train: epoch 105, loss 0.5971368551254272, acc=0.75727778673172, loss=0.5971368551254272
test: epoch 105, loss 1.1493293046951294, acc=0.5027777552604675, loss=1.1493293046951294
train: epoch 106, loss 0.5910331606864929, acc=0.7650555372238159, loss=0.5910331606864929
test: epoch 106, loss 1.2034618854522705, acc=0.5555555820465088, loss=1.2034618854522705
train: epoch 107, loss 0.5814281105995178, acc=0.7655555605888367, loss=0.5814281105995178
test: epoch 107, loss 1.2720102071762085, acc=0.4555555582046509, loss=1.2720102071762085
train: epoch 108, loss 0.5617349147796631, acc=0.7719444632530212, loss=0.5617349147796631
test: epoch 108, loss 1.21333646774292, acc=0.5361111164093018, loss=1.21333646774292
train: epoch 109, loss 0.5900366902351379, acc=0.7676666378974915, loss=0.5900366902351379
test: epoch 109, loss 1.3799281120300293, acc=0.46666666865348816, loss=1.3799281120300293
train: epoch 110, loss 0.5711014270782471, acc=0.7755555510520935, loss=0.5711014270782471
test: epoch 110, loss 1.3159756660461426, acc=0.5166666507720947, loss=1.3159756660461426
train: epoch 111, loss 0.5739498734474182, acc=0.7702777981758118, loss=0.5739498734474182
test: epoch 111, loss 1.404632806777954, acc=0.49166667461395264, loss=1.404632806777954
train: epoch 112, loss 0.5567321181297302, acc=0.7801666855812073, loss=0.5567321181297302
test: epoch 112, loss 1.30626380443573, acc=0.47777777910232544, loss=1.30626380443573
train: epoch 113, loss 0.5807050466537476, acc=0.7705000042915344, loss=0.5807050466537476
test: epoch 113, loss 1.119848608970642, acc=0.5333333611488342, loss=1.119848608970642
train: epoch 114, loss 0.5735781788825989, acc=0.7707222104072571, loss=0.5735781788825989
test: epoch 114, loss 1.3683990240097046, acc=0.4055555462837219, loss=1.3683990240097046
train: epoch 115, loss 0.6025527119636536, acc=0.7616111040115356, loss=0.6025527119636536
test: epoch 115, loss 1.19936203956604, acc=0.5166666507720947, loss=1.19936203956604
train: epoch 116, loss 0.5709947347640991, acc=0.7735000252723694, loss=0.5709947347640991
test: epoch 116, loss 1.0627646446228027, acc=0.5527777671813965, loss=1.0627646446228027
train: epoch 117, loss 0.5461987853050232, acc=0.7836111187934875, loss=0.5461987853050232
test: epoch 117, loss 1.3278908729553223, acc=0.5, loss=1.3278908729553223
train: epoch 118, loss 0.5513473749160767, acc=0.7787777781486511, loss=0.5513473749160767
test: epoch 118, loss 1.2301439046859741, acc=0.5416666865348816, loss=1.2301439046859741
train: epoch 119, loss 0.5684269070625305, acc=0.7735000252723694, loss=0.5684269070625305
test: epoch 119, loss 1.3565483093261719, acc=0.5, loss=1.3565483093261719
train: epoch 120, loss 0.5743131041526794, acc=0.7720555663108826, loss=0.5743131041526794
test: epoch 120, loss 1.1266584396362305, acc=0.5555555820465088, loss=1.1266584396362305
train: epoch 121, loss 0.567589282989502, acc=0.7739444375038147, loss=0.567589282989502
test: epoch 121, loss 1.0192524194717407, acc=0.5027777552604675, loss=1.0192524194717407
train: epoch 122, loss 0.5920148491859436, acc=0.75727778673172, loss=0.5920148491859436
test: epoch 122, loss 1.1889735460281372, acc=0.5416666865348816, loss=1.1889735460281372
train: epoch 123, loss 0.5813814401626587, acc=0.7620555758476257, loss=0.5813814401626587
test: epoch 123, loss 1.19365656375885, acc=0.5611110925674438, loss=1.19365656375885
train: epoch 124, loss 0.5808267593383789, acc=0.7593888640403748, loss=0.5808267593383789
test: epoch 124, loss 1.1769379377365112, acc=0.5861111283302307, loss=1.1769379377365112
train: epoch 125, loss 0.5975614190101624, acc=0.7503888607025146, loss=0.5975614190101624
test: epoch 125, loss 1.109453558921814, acc=0.5888888835906982, loss=1.109453558921814
train: epoch 126, loss 0.5579006671905518, acc=0.7666666507720947, loss=0.5579006671905518
test: epoch 126, loss 1.22036874294281, acc=0.5444444417953491, loss=1.22036874294281
train: epoch 127, loss 0.5551590919494629, acc=0.7696666717529297, loss=0.5551590919494629
test: epoch 127, loss 0.9445972442626953, acc=0.5888888835906982, loss=0.9445972442626953
train: epoch 128, loss 0.5508338212966919, acc=0.7671111226081848, loss=0.5508338212966919
test: epoch 128, loss 1.1297847032546997, acc=0.5555555820465088, loss=1.1297847032546997
train: epoch 129, loss 0.5729308724403381, acc=0.761722207069397, loss=0.5729308724403381
test: epoch 129, loss 1.1661186218261719, acc=0.5583333373069763, loss=1.1661186218261719
train: epoch 130, loss 0.5680606365203857, acc=0.7650555372238159, loss=0.5680606365203857
test: epoch 130, loss 1.2786283493041992, acc=0.5138888955116272, loss=1.2786283493041992
train: epoch 131, loss 0.5952447652816772, acc=0.7563333511352539, loss=0.5952447652816772
test: epoch 131, loss 1.3774949312210083, acc=0.48055556416511536, loss=1.3774949312210083
train: epoch 132, loss 0.5589919090270996, acc=0.7660555839538574, loss=0.5589919090270996
test: epoch 132, loss 1.244832158088684, acc=0.4833333194255829, loss=1.244832158088684
train: epoch 133, loss 0.583314836025238, acc=0.7575555443763733, loss=0.583314836025238
test: epoch 133, loss 1.217344880104065, acc=0.5249999761581421, loss=1.217344880104065
train: epoch 134, loss 0.5725628733634949, acc=0.7601666450500488, loss=0.5725628733634949
test: epoch 134, loss 1.294826865196228, acc=0.5249999761581421, loss=1.294826865196228
train: epoch 135, loss 0.6288686394691467, acc=0.738111138343811, loss=0.6288686394691467
test: epoch 135, loss 1.1611247062683105, acc=0.5472221970558167, loss=1.1611247062683105
train: epoch 136, loss 0.5557539463043213, acc=0.7695555686950684, loss=0.5557539463043213
test: epoch 136, loss 1.2473983764648438, acc=0.5555555820465088, loss=1.2473983764648438
train: epoch 137, loss 0.5479865074157715, acc=0.7734444737434387, loss=0.5479865074157715
test: epoch 137, loss 1.1746840476989746, acc=0.5583333373069763, loss=1.1746840476989746
train: epoch 138, loss 0.5467559099197388, acc=0.7759444713592529, loss=0.5467559099197388
test: epoch 138, loss 1.2110875844955444, acc=0.5388888716697693, loss=1.2110875844955444
train: epoch 139, loss 0.5512456297874451, acc=0.769944429397583, loss=0.5512456297874451
test: epoch 139, loss 1.0164430141448975, acc=0.5555555820465088, loss=1.0164430141448975
train: epoch 140, loss 0.551422655582428, acc=0.7721666693687439, loss=0.551422655582428
test: epoch 140, loss 1.1980161666870117, acc=0.519444465637207, loss=1.1980161666870117
train: epoch 141, loss 0.5419694185256958, acc=0.7727222442626953, loss=0.5419694185256958
test: epoch 141, loss 1.0715038776397705, acc=0.5972222089767456, loss=1.0715038776397705
train: epoch 142, loss 0.5894176363945007, acc=0.7572222352027893, loss=0.5894176363945007
test: epoch 142, loss 1.362665057182312, acc=0.5694444179534912, loss=1.362665057182312
train: epoch 143, loss 0.5811284780502319, acc=0.7637222409248352, loss=0.5811284780502319
test: epoch 143, loss 1.3657306432724, acc=0.46388888359069824, loss=1.3657306432724
train: epoch 144, loss 0.5577027201652527, acc=0.7700555324554443, loss=0.5577027201652527
test: epoch 144, loss 1.1702684164047241, acc=0.5611110925674438, loss=1.1702684164047241
train: epoch 145, loss 0.5517972707748413, acc=0.7676110863685608, loss=0.5517972707748413
test: epoch 145, loss 1.2001630067825317, acc=0.5527777671813965, loss=1.2001630067825317
train: epoch 146, loss 0.5459621548652649, acc=0.7724444270133972, loss=0.5459621548652649
test: epoch 146, loss 1.4059370756149292, acc=0.4861111044883728, loss=1.4059370756149292
train: epoch 147, loss 0.5600762367248535, acc=0.7681666612625122, loss=0.5600762367248535
test: epoch 147, loss 1.1266883611679077, acc=0.5555555820465088, loss=1.1266883611679077
train: epoch 148, loss 0.5956647992134094, acc=0.7547222375869751, loss=0.5956647992134094
test: epoch 148, loss 1.3542442321777344, acc=0.5555555820465088, loss=1.3542442321777344
train: epoch 149, loss 0.5495690107345581, acc=0.7696666717529297, loss=0.5495690107345581
test: epoch 149, loss 1.2360883951187134, acc=0.5638889074325562, loss=1.2360883951187134
train: epoch 150, loss 0.5649356842041016, acc=0.765666663646698, loss=0.5649356842041016
test: epoch 150, loss 1.0833343267440796, acc=0.6000000238418579, loss=1.0833343267440796
