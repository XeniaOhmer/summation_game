# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1.5", "--temp_decay=0.995", "--one_hot=1", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=648779147, receiver_embed_dim=128, save_run=0, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=648779147, receiver_embed_dim=128, save_run=False, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.0492565631866455, acc=0.0976666659116745, loss=3.0492565631866455
test: epoch 1, loss 2.7595582008361816, acc=0.125, loss=2.7595582008361816
train: epoch 2, loss 1.6909164190292358, acc=0.3213333189487457, loss=1.6909164190292358
test: epoch 2, loss 2.506349802017212, acc=0.1666666716337204, loss=2.506349802017212
train: epoch 3, loss 1.346486210823059, acc=0.4486111104488373, loss=1.346486210823059
test: epoch 3, loss 2.699706554412842, acc=0.18333333730697632, loss=2.699706554412842
train: epoch 4, loss 1.1645147800445557, acc=0.5272777676582336, loss=1.1645147800445557
test: epoch 4, loss 2.3746159076690674, acc=0.23888888955116272, loss=2.3746159076690674
train: epoch 5, loss 1.0264207124710083, acc=0.5899999737739563, loss=1.0264207124710083
test: epoch 5, loss 2.3787283897399902, acc=0.21944443881511688, loss=2.3787283897399902
train: epoch 6, loss 0.9255008101463318, acc=0.6349444389343262, loss=0.9255008101463318
test: epoch 6, loss 2.0594887733459473, acc=0.28611111640930176, loss=2.0594887733459473
train: epoch 7, loss 0.8343848586082458, acc=0.6741111278533936, loss=0.8343848586082458
test: epoch 7, loss 2.054955005645752, acc=0.31111112236976624, loss=2.054955005645752
train: epoch 8, loss 0.7582322955131531, acc=0.7016111016273499, loss=0.7582322955131531
test: epoch 8, loss 2.152538537979126, acc=0.32499998807907104, loss=2.152538537979126
train: epoch 9, loss 0.7090632319450378, acc=0.7246111035346985, loss=0.7090632319450378
test: epoch 9, loss 2.1150996685028076, acc=0.3166666626930237, loss=2.1150996685028076
train: epoch 10, loss 0.6489354968070984, acc=0.7450555562973022, loss=0.6489354968070984
test: epoch 10, loss 2.0076663494110107, acc=0.3472222089767456, loss=2.0076663494110107
train: epoch 11, loss 0.6209194660186768, acc=0.7584444284439087, loss=0.6209194660186768
test: epoch 11, loss 1.9448012113571167, acc=0.3583333194255829, loss=1.9448012113571167
train: epoch 12, loss 0.5857819318771362, acc=0.7760000228881836, loss=0.5857819318771362
test: epoch 12, loss 1.7680039405822754, acc=0.3888888955116272, loss=1.7680039405822754
train: epoch 13, loss 0.5476361513137817, acc=0.7871111035346985, loss=0.5476361513137817
test: epoch 13, loss 1.8567756414413452, acc=0.39722222089767456, loss=1.8567756414413452
train: epoch 14, loss 0.5100452899932861, acc=0.8032777905464172, loss=0.5100452899932861
test: epoch 14, loss 1.7660771608352661, acc=0.375, loss=1.7660771608352661
train: epoch 15, loss 0.48884114623069763, acc=0.8156111240386963, loss=0.48884114623069763
test: epoch 15, loss 1.7878071069717407, acc=0.375, loss=1.7878071069717407
train: epoch 16, loss 0.4560193419456482, acc=0.8227777481079102, loss=0.4560193419456482
test: epoch 16, loss 1.8482661247253418, acc=0.41111111640930176, loss=1.8482661247253418
train: epoch 17, loss 0.4600415527820587, acc=0.8261111378669739, loss=0.4600415527820587
test: epoch 17, loss 1.8272258043289185, acc=0.39722222089767456, loss=1.8272258043289185
train: epoch 18, loss 0.4397888481616974, acc=0.83561110496521, loss=0.4397888481616974
test: epoch 18, loss 1.756540298461914, acc=0.39722222089767456, loss=1.756540298461914
train: epoch 19, loss 0.42454618215560913, acc=0.8443333506584167, loss=0.42454618215560913
test: epoch 19, loss 1.8555858135223389, acc=0.36944442987442017, loss=1.8555858135223389
train: epoch 20, loss 0.4082998037338257, acc=0.8463888764381409, loss=0.4082998037338257
test: epoch 20, loss 1.8468899726867676, acc=0.4472222328186035, loss=1.8468899726867676
train: epoch 21, loss 0.38340672850608826, acc=0.8557222485542297, loss=0.38340672850608826
test: epoch 21, loss 1.7660038471221924, acc=0.4472222328186035, loss=1.7660038471221924
train: epoch 22, loss 0.3856971263885498, acc=0.855388879776001, loss=0.3856971263885498
test: epoch 22, loss 1.6555172204971313, acc=0.43888887763023376, loss=1.6555172204971313
train: epoch 23, loss 0.36045992374420166, acc=0.8661666512489319, loss=0.36045992374420166
test: epoch 23, loss 1.863076090812683, acc=0.4333333373069763, loss=1.863076090812683
train: epoch 24, loss 0.3602696359157562, acc=0.8675000071525574, loss=0.3602696359157562
test: epoch 24, loss 1.7527179718017578, acc=0.46666666865348816, loss=1.7527179718017578
train: epoch 25, loss 0.3454116880893707, acc=0.871222198009491, loss=0.3454116880893707
test: epoch 25, loss 1.628144383430481, acc=0.44999998807907104, loss=1.628144383430481
train: epoch 26, loss 0.3446026146411896, acc=0.8733333349227905, loss=0.3446026146411896
test: epoch 26, loss 1.5307000875473022, acc=0.519444465637207, loss=1.5307000875473022
train: epoch 27, loss 0.31364408135414124, acc=0.8849999904632568, loss=0.31364408135414124
test: epoch 27, loss 1.5465725660324097, acc=0.4972222149372101, loss=1.5465725660324097
train: epoch 28, loss 0.30572667717933655, acc=0.887499988079071, loss=0.30572667717933655
test: epoch 28, loss 1.5769366025924683, acc=0.5249999761581421, loss=1.5769366025924683
train: epoch 29, loss 0.30669623613357544, acc=0.8865000009536743, loss=0.30669623613357544
test: epoch 29, loss 1.5748947858810425, acc=0.5138888955116272, loss=1.5748947858810425
train: epoch 30, loss 0.3079192340373993, acc=0.8892222046852112, loss=0.3079192340373993
test: epoch 30, loss 1.5329102277755737, acc=0.49166667461395264, loss=1.5329102277755737
train: epoch 31, loss 0.28872615098953247, acc=0.8956111073493958, loss=0.28872615098953247
test: epoch 31, loss 1.5123518705368042, acc=0.5694444179534912, loss=1.5123518705368042
train: epoch 32, loss 0.26630517840385437, acc=0.9027777910232544, loss=0.26630517840385437
test: epoch 32, loss 1.5542192459106445, acc=0.5277777910232544, loss=1.5542192459106445
train: epoch 33, loss 0.2645726203918457, acc=0.9065555334091187, loss=0.2645726203918457
test: epoch 33, loss 1.432708978652954, acc=0.5722222328186035, loss=1.432708978652954
train: epoch 34, loss 0.2500995993614197, acc=0.9081110954284668, loss=0.2500995993614197
test: epoch 34, loss 1.7724838256835938, acc=0.5, loss=1.7724838256835938
train: epoch 35, loss 0.2483666092157364, acc=0.9086111187934875, loss=0.2483666092157364
test: epoch 35, loss 1.6238332986831665, acc=0.5416666865348816, loss=1.6238332986831665
train: epoch 36, loss 0.2425989806652069, acc=0.9088888764381409, loss=0.2425989806652069
test: epoch 36, loss 1.3863413333892822, acc=0.5666666626930237, loss=1.3863413333892822
train: epoch 37, loss 0.2438809871673584, acc=0.9123333096504211, loss=0.2438809871673584
test: epoch 37, loss 1.6633986234664917, acc=0.5277777910232544, loss=1.6633986234664917
train: epoch 38, loss 0.22890444099903107, acc=0.9175000190734863, loss=0.22890444099903107
test: epoch 38, loss 1.383670687675476, acc=0.6000000238418579, loss=1.383670687675476
train: epoch 39, loss 0.21898002922534943, acc=0.9222777485847473, loss=0.21898002922534943
test: epoch 39, loss 1.4460445642471313, acc=0.5805555582046509, loss=1.4460445642471313
train: epoch 40, loss 0.22212573885917664, acc=0.9190555810928345, loss=0.22212573885917664
test: epoch 40, loss 1.4825245141983032, acc=0.5833333134651184, loss=1.4825245141983032
train: epoch 41, loss 0.21790193021297455, acc=0.9220555424690247, loss=0.21790193021297455
test: epoch 41, loss 1.4224945306777954, acc=0.5972222089767456, loss=1.4224945306777954
train: epoch 42, loss 0.20866242051124573, acc=0.9255555272102356, loss=0.20866242051124573
test: epoch 42, loss 1.249481439590454, acc=0.6472222208976746, loss=1.249481439590454
train: epoch 43, loss 0.20695184171199799, acc=0.9281111359596252, loss=0.20695184171199799
test: epoch 43, loss 1.756313443183899, acc=0.5527777671813965, loss=1.756313443183899
train: epoch 44, loss 0.1985660344362259, acc=0.926277756690979, loss=0.1985660344362259
test: epoch 44, loss 1.4936704635620117, acc=0.6166666746139526, loss=1.4936704635620117
train: epoch 45, loss 0.20185385644435883, acc=0.9293333292007446, loss=0.20185385644435883
test: epoch 45, loss 1.4529600143432617, acc=0.5777778029441833, loss=1.4529600143432617
train: epoch 46, loss 0.1886567622423172, acc=0.9326111078262329, loss=0.1886567622423172
test: epoch 46, loss 1.2504428625106812, acc=0.6222222447395325, loss=1.2504428625106812
train: epoch 47, loss 0.19174011051654816, acc=0.9342777729034424, loss=0.19174011051654816
test: epoch 47, loss 1.4694405794143677, acc=0.6361111402511597, loss=1.4694405794143677
train: epoch 48, loss 0.18530555069446564, acc=0.9343888759613037, loss=0.18530555069446564
test: epoch 48, loss 1.2870662212371826, acc=0.6583333611488342, loss=1.2870662212371826
train: epoch 49, loss 0.19066625833511353, acc=0.9339444637298584, loss=0.19066625833511353
test: epoch 49, loss 1.2610012292861938, acc=0.6805555820465088, loss=1.2610012292861938
train: epoch 50, loss 0.18093374371528625, acc=0.9358333349227905, loss=0.18093374371528625
test: epoch 50, loss 1.2498513460159302, acc=0.6611111164093018, loss=1.2498513460159302
train: epoch 51, loss 0.18014466762542725, acc=0.9362778067588806, loss=0.18014466762542725
test: epoch 51, loss 1.2500079870224, acc=0.6722221970558167, loss=1.2500079870224
train: epoch 52, loss 0.17121745645999908, acc=0.9398888945579529, loss=0.17121745645999908
test: epoch 52, loss 1.4090343713760376, acc=0.6611111164093018, loss=1.4090343713760376
train: epoch 53, loss 0.1736205369234085, acc=0.9401666522026062, loss=0.1736205369234085
test: epoch 53, loss 1.2513089179992676, acc=0.6833333373069763, loss=1.2513089179992676
train: epoch 54, loss 0.17055614292621613, acc=0.9393333196640015, loss=0.17055614292621613
test: epoch 54, loss 1.3015977144241333, acc=0.6861110925674438, loss=1.3015977144241333
train: epoch 55, loss 0.17277511954307556, acc=0.9372777938842773, loss=0.17277511954307556
test: epoch 55, loss 1.217488169670105, acc=0.7027778029441833, loss=1.217488169670105
train: epoch 56, loss 0.1639130711555481, acc=0.9427222013473511, loss=0.1639130711555481
test: epoch 56, loss 1.264850378036499, acc=0.6694444417953491, loss=1.264850378036499
train: epoch 57, loss 0.1710711419582367, acc=0.9402777552604675, loss=0.1710711419582367
test: epoch 57, loss 1.0564852952957153, acc=0.7194444537162781, loss=1.0564852952957153
train: epoch 58, loss 0.1684422343969345, acc=0.9399444460868835, loss=0.1684422343969345
test: epoch 58, loss 1.114067792892456, acc=0.6638888716697693, loss=1.114067792892456
train: epoch 59, loss 0.16812875866889954, acc=0.9399444460868835, loss=0.16812875866889954
test: epoch 59, loss 1.1284315586090088, acc=0.7277777791023254, loss=1.1284315586090088
train: epoch 60, loss 0.16763955354690552, acc=0.941277801990509, loss=0.16763955354690552
test: epoch 60, loss 1.0203368663787842, acc=0.7111111283302307, loss=1.0203368663787842
train: epoch 61, loss 0.1580645591020584, acc=0.9437777996063232, loss=0.1580645591020584
test: epoch 61, loss 1.1002522706985474, acc=0.7250000238418579, loss=1.1002522706985474
train: epoch 62, loss 0.16194888949394226, acc=0.941277801990509, loss=0.16194888949394226
test: epoch 62, loss 1.1368812322616577, acc=0.6972222328186035, loss=1.1368812322616577
train: epoch 63, loss 0.15270370244979858, acc=0.9438889026641846, loss=0.15270370244979858
test: epoch 63, loss 1.0302488803863525, acc=0.7472222447395325, loss=1.0302488803863525
train: epoch 64, loss 0.15845684707164764, acc=0.944611132144928, loss=0.15845684707164764
test: epoch 64, loss 1.0304906368255615, acc=0.7388888597488403, loss=1.0304906368255615
train: epoch 65, loss 0.1569685935974121, acc=0.9448333382606506, loss=0.1569685935974121
test: epoch 65, loss 0.8478886485099792, acc=0.7222222089767456, loss=0.8478886485099792
train: epoch 66, loss 0.16274085640907288, acc=0.9445000290870667, loss=0.16274085640907288
test: epoch 66, loss 1.1590834856033325, acc=0.7277777791023254, loss=1.1590834856033325
train: epoch 67, loss 0.1467067003250122, acc=0.9474444389343262, loss=0.1467067003250122
test: epoch 67, loss 0.998600959777832, acc=0.7361111044883728, loss=0.998600959777832
train: epoch 68, loss 0.15877364575862885, acc=0.9458333253860474, loss=0.15877364575862885
test: epoch 68, loss 1.0591509342193604, acc=0.7472222447395325, loss=1.0591509342193604
train: epoch 69, loss 0.15679959952831268, acc=0.9434999823570251, loss=0.15679959952831268
test: epoch 69, loss 0.978947639465332, acc=0.7527777552604675, loss=0.978947639465332
train: epoch 70, loss 0.1546681672334671, acc=0.945388913154602, loss=0.1546681672334671
test: epoch 70, loss 0.9095377922058105, acc=0.7527777552604675, loss=0.9095377922058105
train: epoch 71, loss 0.1533261239528656, acc=0.9448888897895813, loss=0.1533261239528656
test: epoch 71, loss 1.0529792308807373, acc=0.7527777552604675, loss=1.0529792308807373
train: epoch 72, loss 0.14788475632667542, acc=0.9469444155693054, loss=0.14788475632667542
test: epoch 72, loss 0.9204766154289246, acc=0.769444465637207, loss=0.9204766154289246
train: epoch 73, loss 0.1565316766500473, acc=0.9426110982894897, loss=0.1565316766500473
test: epoch 73, loss 0.7743349075317383, acc=0.769444465637207, loss=0.7743349075317383
train: epoch 74, loss 0.14935342967510223, acc=0.9465000033378601, loss=0.14935342967510223
test: epoch 74, loss 0.8822023272514343, acc=0.7722222208976746, loss=0.8822023272514343
train: epoch 75, loss 0.15004953742027283, acc=0.9468333125114441, loss=0.15004953742027283
test: epoch 75, loss 0.9080912470817566, acc=0.7749999761581421, loss=0.9080912470817566
train: epoch 76, loss 0.1578095555305481, acc=0.9436110854148865, loss=0.1578095555305481
test: epoch 76, loss 0.9378281831741333, acc=0.7666666507720947, loss=0.9378281831741333
train: epoch 77, loss 0.14476174116134644, acc=0.9472222328186035, loss=0.14476174116134644
test: epoch 77, loss 0.9521188139915466, acc=0.7666666507720947, loss=0.9521188139915466
train: epoch 78, loss 0.14806772768497467, acc=0.9462777972221375, loss=0.14806772768497467
test: epoch 78, loss 0.9739692211151123, acc=0.769444465637207, loss=0.9739692211151123
train: epoch 79, loss 0.14471198618412018, acc=0.9470555782318115, loss=0.14471198618412018
test: epoch 79, loss 0.9245900511741638, acc=0.7749999761581421, loss=0.9245900511741638
train: epoch 80, loss 0.14710767567157745, acc=0.945888876914978, loss=0.14710767567157745
test: epoch 80, loss 0.7978559732437134, acc=0.7861111164093018, loss=0.7978559732437134
train: epoch 81, loss 0.14890117943286896, acc=0.9471111297607422, loss=0.14890117943286896
test: epoch 81, loss 0.8755918741226196, acc=0.7777777910232544, loss=0.8755918741226196
train: epoch 82, loss 0.15265300869941711, acc=0.9473333358764648, loss=0.15265300869941711
test: epoch 82, loss 0.8368099927902222, acc=0.7722222208976746, loss=0.8368099927902222
train: epoch 83, loss 0.14457881450653076, acc=0.9480000138282776, loss=0.14457881450653076
test: epoch 83, loss 1.043078899383545, acc=0.7861111164093018, loss=1.043078899383545
train: epoch 84, loss 0.14611823856830597, acc=0.9468333125114441, loss=0.14611823856830597
test: epoch 84, loss 0.8838312029838562, acc=0.7833333611488342, loss=0.8838312029838562
train: epoch 85, loss 0.14816252887248993, acc=0.9459444284439087, loss=0.14816252887248993
test: epoch 85, loss 0.757696807384491, acc=0.7944444417953491, loss=0.757696807384491
train: epoch 86, loss 0.13331979513168335, acc=0.9513888955116272, loss=0.13331979513168335
test: epoch 86, loss 0.8981382846832275, acc=0.7888888716697693, loss=0.8981382846832275
train: epoch 87, loss 0.14242693781852722, acc=0.9497777819633484, loss=0.14242693781852722
test: epoch 87, loss 0.7491731643676758, acc=0.7944444417953491, loss=0.7491731643676758
train: epoch 88, loss 0.14219990372657776, acc=0.9491666555404663, loss=0.14219990372657776
test: epoch 88, loss 0.7367587089538574, acc=0.7916666865348816, loss=0.7367587089538574
train: epoch 89, loss 0.1435609608888626, acc=0.9458333253860474, loss=0.1435609608888626
test: epoch 89, loss 0.8559770584106445, acc=0.8055555820465088, loss=0.8559770584106445
train: epoch 90, loss 0.14192770421504974, acc=0.9461110830307007, loss=0.14192770421504974
test: epoch 90, loss 0.5972033143043518, acc=0.8111110925674438, loss=0.5972033143043518
train: epoch 91, loss 0.1553347408771515, acc=0.9431666731834412, loss=0.1553347408771515
test: epoch 91, loss 0.7288171052932739, acc=0.7833333611488342, loss=0.7288171052932739
train: epoch 92, loss 0.1464756578207016, acc=0.9469444155693054, loss=0.1464756578207016
test: epoch 92, loss 0.6961926221847534, acc=0.7888888716697693, loss=0.6961926221847534
train: epoch 93, loss 0.14357416331768036, acc=0.9467777609825134, loss=0.14357416331768036
test: epoch 93, loss 0.6927942037582397, acc=0.7972221970558167, loss=0.6927942037582397
train: epoch 94, loss 0.1518184095621109, acc=0.9427777528762817, loss=0.1518184095621109
test: epoch 94, loss 0.6854007244110107, acc=0.8083333373069763, loss=0.6854007244110107
train: epoch 95, loss 0.14451824128627777, acc=0.94605553150177, loss=0.14451824128627777
test: epoch 95, loss 0.6499525308609009, acc=0.8083333373069763, loss=0.6499525308609009
train: epoch 96, loss 0.13842977583408356, acc=0.9499444365501404, loss=0.13842977583408356
test: epoch 96, loss 0.7414605021476746, acc=0.8138889074325562, loss=0.7414605021476746
train: epoch 97, loss 0.1456579715013504, acc=0.9466111063957214, loss=0.1456579715013504
test: epoch 97, loss 0.7740384340286255, acc=0.8055555820465088, loss=0.7740384340286255
train: epoch 98, loss 0.13960407674312592, acc=0.9477221965789795, loss=0.13960407674312592
test: epoch 98, loss 0.7824454307556152, acc=0.7972221970558167, loss=0.7824454307556152
train: epoch 99, loss 0.14270591735839844, acc=0.9484444260597229, loss=0.14270591735839844
test: epoch 99, loss 0.766937255859375, acc=0.8055555820465088, loss=0.766937255859375
train: epoch 100, loss 0.13988478481769562, acc=0.9472222328186035, loss=0.13988478481769562
test: epoch 100, loss 0.9530960321426392, acc=0.7944444417953491, loss=0.9530960321426392
train: epoch 101, loss 0.14820222556591034, acc=0.9471666812896729, loss=0.14820222556591034
test: epoch 101, loss 0.57715904712677, acc=0.7888888716697693, loss=0.57715904712677
train: epoch 102, loss 0.14871421456336975, acc=0.9477221965789795, loss=0.14871421456336975
test: epoch 102, loss 0.7617565989494324, acc=0.8111110925674438, loss=0.7617565989494324
train: epoch 103, loss 0.14379510283470154, acc=0.9482777714729309, loss=0.14379510283470154
test: epoch 103, loss 0.7616324424743652, acc=0.8027777671813965, loss=0.7616324424743652
train: epoch 104, loss 0.14016035199165344, acc=0.9503333568572998, loss=0.14016035199165344
test: epoch 104, loss 0.7657938003540039, acc=0.7944444417953491, loss=0.7657938003540039
train: epoch 105, loss 0.1388094425201416, acc=0.9496111273765564, loss=0.1388094425201416
test: epoch 105, loss 0.6795728206634521, acc=0.8027777671813965, loss=0.6795728206634521
train: epoch 106, loss 0.14376462996006012, acc=0.9471666812896729, loss=0.14376462996006012
test: epoch 106, loss 0.7147919535636902, acc=0.8138889074325562, loss=0.7147919535636902
train: epoch 107, loss 0.13704752922058105, acc=0.9473333358764648, loss=0.13704752922058105
test: epoch 107, loss 0.654142439365387, acc=0.800000011920929, loss=0.654142439365387
train: epoch 108, loss 0.153191477060318, acc=0.9409444332122803, loss=0.153191477060318
test: epoch 108, loss 0.7921664714813232, acc=0.7944444417953491, loss=0.7921664714813232
train: epoch 109, loss 0.14851392805576324, acc=0.9432222247123718, loss=0.14851392805576324
test: epoch 109, loss 0.7529805898666382, acc=0.8027777671813965, loss=0.7529805898666382
train: epoch 110, loss 0.14736588299274445, acc=0.9420555830001831, loss=0.14736588299274445
test: epoch 110, loss 0.8750324249267578, acc=0.7972221970558167, loss=0.8750324249267578
train: epoch 111, loss 0.14237430691719055, acc=0.9448888897895813, loss=0.14237430691719055
test: epoch 111, loss 0.7232183814048767, acc=0.8111110925674438, loss=0.7232183814048767
train: epoch 112, loss 0.14861667156219482, acc=0.9448333382606506, loss=0.14861667156219482
test: epoch 112, loss 0.7232203483581543, acc=0.8055555820465088, loss=0.7232203483581543
train: epoch 113, loss 0.14050394296646118, acc=0.9463333487510681, loss=0.14050394296646118
test: epoch 113, loss 0.733079731464386, acc=0.8027777671813965, loss=0.733079731464386
train: epoch 114, loss 0.15181584656238556, acc=0.9436110854148865, loss=0.15181584656238556
test: epoch 114, loss 0.7049372792243958, acc=0.8055555820465088, loss=0.7049372792243958
train: epoch 115, loss 0.1454288810491562, acc=0.945555567741394, loss=0.1454288810491562
test: epoch 115, loss 0.5975514054298401, acc=0.8027777671813965, loss=0.5975514054298401
train: epoch 116, loss 0.14547044038772583, acc=0.9467222094535828, loss=0.14547044038772583
test: epoch 116, loss 0.5750396251678467, acc=0.8027777671813965, loss=0.5750396251678467
train: epoch 117, loss 0.1464080810546875, acc=0.9462777972221375, loss=0.1464080810546875
test: epoch 117, loss 0.6586748361587524, acc=0.8083333373069763, loss=0.6586748361587524
train: epoch 118, loss 0.14855505526065826, acc=0.9445555806159973, loss=0.14855505526065826
test: epoch 118, loss 0.700589656829834, acc=0.8083333373069763, loss=0.700589656829834
train: epoch 119, loss 0.14911775290966034, acc=0.9432222247123718, loss=0.14911775290966034
test: epoch 119, loss 0.6474465727806091, acc=0.8222222328186035, loss=0.6474465727806091
train: epoch 120, loss 0.14790549874305725, acc=0.945555567741394, loss=0.14790549874305725
test: epoch 120, loss 0.6535038948059082, acc=0.8027777671813965, loss=0.6535038948059082
train: epoch 121, loss 0.15007255971431732, acc=0.944944441318512, loss=0.15007255971431732
test: epoch 121, loss 0.6409281492233276, acc=0.8166666626930237, loss=0.6409281492233276
train: epoch 122, loss 0.1490401178598404, acc=0.9416666626930237, loss=0.1490401178598404
test: epoch 122, loss 0.6082949638366699, acc=0.8111110925674438, loss=0.6082949638366699
train: epoch 123, loss 0.14532767236232758, acc=0.9467777609825134, loss=0.14532767236232758
test: epoch 123, loss 0.6473962068557739, acc=0.8027777671813965, loss=0.6473962068557739
train: epoch 124, loss 0.15196934342384338, acc=0.9432222247123718, loss=0.15196934342384338
test: epoch 124, loss 0.6356890201568604, acc=0.8166666626930237, loss=0.6356890201568604
train: epoch 125, loss 0.14081260561943054, acc=0.9474999904632568, loss=0.14081260561943054
test: epoch 125, loss 0.6462115049362183, acc=0.8166666626930237, loss=0.6462115049362183
train: epoch 126, loss 0.14761261641979218, acc=0.9439444541931152, loss=0.14761261641979218
test: epoch 126, loss 0.6980314254760742, acc=0.8083333373069763, loss=0.6980314254760742
train: epoch 127, loss 0.14833270013332367, acc=0.9436110854148865, loss=0.14833270013332367
test: epoch 127, loss 0.7268479466438293, acc=0.8166666626930237, loss=0.7268479466438293
train: epoch 128, loss 0.1564035564661026, acc=0.9419999718666077, loss=0.1564035564661026
test: epoch 128, loss 0.750096321105957, acc=0.8083333373069763, loss=0.750096321105957
train: epoch 129, loss 0.15064910054206848, acc=0.9427222013473511, loss=0.15064910054206848
test: epoch 129, loss 0.675567626953125, acc=0.8055555820465088, loss=0.675567626953125
train: epoch 130, loss 0.1463094800710678, acc=0.9459999799728394, loss=0.1463094800710678
test: epoch 130, loss 0.5944791436195374, acc=0.8166666626930237, loss=0.5944791436195374
train: epoch 131, loss 0.1459590196609497, acc=0.9445000290870667, loss=0.1459590196609497
test: epoch 131, loss 0.7965724468231201, acc=0.8083333373069763, loss=0.7965724468231201
train: epoch 132, loss 0.15437614917755127, acc=0.9410555362701416, loss=0.15437614917755127
test: epoch 132, loss 0.7080787420272827, acc=0.8166666626930237, loss=0.7080787420272827
train: epoch 133, loss 0.16257114708423615, acc=0.9433888792991638, loss=0.16257114708423615
test: epoch 133, loss 0.6825026869773865, acc=0.8055555820465088, loss=0.6825026869773865
train: epoch 134, loss 0.15459437668323517, acc=0.9383888840675354, loss=0.15459437668323517
test: epoch 134, loss 0.6158772706985474, acc=0.800000011920929, loss=0.6158772706985474
train: epoch 135, loss 0.1494692713022232, acc=0.941777765750885, loss=0.1494692713022232
test: epoch 135, loss 0.6252956390380859, acc=0.8166666626930237, loss=0.6252956390380859
train: epoch 136, loss 0.15243621170520782, acc=0.9424444437026978, loss=0.15243621170520782
test: epoch 136, loss 0.6715072989463806, acc=0.8166666626930237, loss=0.6715072989463806
train: epoch 137, loss 0.15915408730506897, acc=0.9433333277702332, loss=0.15915408730506897
test: epoch 137, loss 0.5942201614379883, acc=0.8166666626930237, loss=0.5942201614379883
train: epoch 138, loss 0.15583869814872742, acc=0.9434444308280945, loss=0.15583869814872742
test: epoch 138, loss 0.6861360669136047, acc=0.8166666626930237, loss=0.6861360669136047
train: epoch 139, loss 0.17082466185092926, acc=0.9419999718666077, loss=0.17082466185092926
test: epoch 139, loss 0.6979483366012573, acc=0.8111110925674438, loss=0.6979483366012573
train: epoch 140, loss 0.15202464163303375, acc=0.9439444541931152, loss=0.15202464163303375
test: epoch 140, loss 0.6208300590515137, acc=0.8166666626930237, loss=0.6208300590515137
train: epoch 141, loss 0.15575279295444489, acc=0.9454444646835327, loss=0.15575279295444489
test: epoch 141, loss 0.5969487428665161, acc=0.8166666626930237, loss=0.5969487428665161
train: epoch 142, loss 0.15346871316432953, acc=0.9427222013473511, loss=0.15346871316432953
test: epoch 142, loss 0.5846686959266663, acc=0.8055555820465088, loss=0.5846686959266663
train: epoch 143, loss 0.16106711328029633, acc=0.940666675567627, loss=0.16106711328029633
test: epoch 143, loss 0.6094079613685608, acc=0.8166666626930237, loss=0.6094079613685608
train: epoch 144, loss 0.15080341696739197, acc=0.9443888664245605, loss=0.15080341696739197
test: epoch 144, loss 0.632662296295166, acc=0.8138889074325562, loss=0.632662296295166
train: epoch 145, loss 0.14995193481445312, acc=0.9458333253860474, loss=0.14995193481445312
test: epoch 145, loss 0.6157782077789307, acc=0.8166666626930237, loss=0.6157782077789307
train: epoch 146, loss 0.15353316068649292, acc=0.9437777996063232, loss=0.15353316068649292
test: epoch 146, loss 0.6133981347084045, acc=0.8166666626930237, loss=0.6133981347084045
train: epoch 147, loss 0.15037751197814941, acc=0.9450555443763733, loss=0.15037751197814941
test: epoch 147, loss 0.6337611675262451, acc=0.8166666626930237, loss=0.6337611675262451
train: epoch 148, loss 0.16212758421897888, acc=0.9415000081062317, loss=0.16212758421897888
test: epoch 148, loss 0.6505682468414307, acc=0.8166666626930237, loss=0.6505682468414307
train: epoch 149, loss 0.14851580560207367, acc=0.9431111216545105, loss=0.14851580560207367
test: epoch 149, loss 0.666931688785553, acc=0.8166666626930237, loss=0.666931688785553
train: epoch 150, loss 0.15276047587394714, acc=0.941611111164093, loss=0.15276047587394714
test: epoch 150, loss 0.593408465385437, acc=0.8222222328186035, loss=0.593408465385437
