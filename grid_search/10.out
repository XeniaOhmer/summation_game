# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.995", "--one_hot=1", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=365276648, receiver_embed_dim=32, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=365276648, receiver_embed_dim=32, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.4792227745056152, acc=0.050333332270383835, loss=3.4792227745056152
test: epoch 1, loss 3.7868731021881104, acc=0.05833333358168602, loss=3.7868731021881104
train: epoch 2, loss 3.008272409439087, acc=0.094722218811512, loss=3.008272409439087
test: epoch 2, loss 3.3110880851745605, acc=0.0833333358168602, loss=3.3110880851745605
train: epoch 3, loss 2.4399337768554688, acc=0.17638888955116272, loss=2.4399337768554688
test: epoch 3, loss 3.280360221862793, acc=0.08611111342906952, loss=3.280360221862793
train: epoch 4, loss 2.205465793609619, acc=0.21933333575725555, loss=2.205465793609619
test: epoch 4, loss 2.8919034004211426, acc=0.09444444626569748, loss=2.8919034004211426
train: epoch 5, loss 2.0398755073547363, acc=0.2598888874053955, loss=2.0398755073547363
test: epoch 5, loss 2.679516553878784, acc=0.16111111640930176, loss=2.679516553878784
train: epoch 6, loss 1.8856381177902222, acc=0.2860555648803711, loss=1.8856381177902222
test: epoch 6, loss 2.6512341499328613, acc=0.1666666716337204, loss=2.6512341499328613
train: epoch 7, loss 1.7968089580535889, acc=0.3106110990047455, loss=1.7968089580535889
test: epoch 7, loss 2.3694610595703125, acc=0.16388888657093048, loss=2.3694610595703125
train: epoch 8, loss 1.7233343124389648, acc=0.32766667008399963, loss=1.7233343124389648
test: epoch 8, loss 2.2133805751800537, acc=0.1805555522441864, loss=2.2133805751800537
train: epoch 9, loss 1.6626919507980347, acc=0.3461666703224182, loss=1.6626919507980347
test: epoch 9, loss 2.200634717941284, acc=0.20000000298023224, loss=2.200634717941284
train: epoch 10, loss 1.5929304361343384, acc=0.36222222447395325, loss=1.5929304361343384
test: epoch 10, loss 2.007293701171875, acc=0.19722221791744232, loss=2.007293701171875
train: epoch 11, loss 1.5417615175247192, acc=0.382666677236557, loss=1.5417615175247192
test: epoch 11, loss 1.9429059028625488, acc=0.20555555820465088, loss=1.9429059028625488
train: epoch 12, loss 1.5008708238601685, acc=0.3964444398880005, loss=1.5008708238601685
test: epoch 12, loss 1.914536476135254, acc=0.2083333283662796, loss=1.914536476135254
train: epoch 13, loss 1.4537973403930664, acc=0.40833333134651184, loss=1.4537973403930664
test: epoch 13, loss 1.9166488647460938, acc=0.23333333432674408, loss=1.9166488647460938
train: epoch 14, loss 1.4282647371292114, acc=0.426111102104187, loss=1.4282647371292114
test: epoch 14, loss 1.8797225952148438, acc=0.24722221493721008, loss=1.8797225952148438
train: epoch 15, loss 1.3816721439361572, acc=0.43833333253860474, loss=1.3816721439361572
test: epoch 15, loss 1.87296724319458, acc=0.24722221493721008, loss=1.87296724319458
train: epoch 16, loss 1.363090991973877, acc=0.44272223114967346, loss=1.363090991973877
test: epoch 16, loss 1.8622584342956543, acc=0.24722221493721008, loss=1.8622584342956543
train: epoch 17, loss 1.3332505226135254, acc=0.46005555987358093, loss=1.3332505226135254
test: epoch 17, loss 1.828190803527832, acc=0.2666666805744171, loss=1.828190803527832
train: epoch 18, loss 1.308213710784912, acc=0.460999995470047, loss=1.308213710784912
test: epoch 18, loss 1.757688045501709, acc=0.27222222089767456, loss=1.757688045501709
train: epoch 19, loss 1.2760413885116577, acc=0.4699999988079071, loss=1.2760413885116577
test: epoch 19, loss 1.7513489723205566, acc=0.2777777910232544, loss=1.7513489723205566
train: epoch 20, loss 1.2520509958267212, acc=0.49000000953674316, loss=1.2520509958267212
test: epoch 20, loss 1.7527313232421875, acc=0.2888889014720917, loss=1.7527313232421875
train: epoch 21, loss 1.2396070957183838, acc=0.48883333802223206, loss=1.2396070957183838
test: epoch 21, loss 1.6958720684051514, acc=0.2916666567325592, loss=1.6958720684051514
train: epoch 22, loss 1.2138415575027466, acc=0.5006666779518127, loss=1.2138415575027466
test: epoch 22, loss 1.7248839139938354, acc=0.28611111640930176, loss=1.7248839139938354
train: epoch 23, loss 1.2008635997772217, acc=0.5060555338859558, loss=1.2008635997772217
test: epoch 23, loss 1.627528429031372, acc=0.3055555522441864, loss=1.627528429031372
train: epoch 24, loss 1.181388258934021, acc=0.5137777924537659, loss=1.181388258934021
test: epoch 24, loss 1.6422563791275024, acc=0.2805555462837219, loss=1.6422563791275024
train: epoch 25, loss 1.1588760614395142, acc=0.5271666646003723, loss=1.1588760614395142
test: epoch 25, loss 1.6767405271530151, acc=0.2916666567325592, loss=1.6767405271530151
train: epoch 26, loss 1.1455903053283691, acc=0.5295000076293945, loss=1.1455903053283691
test: epoch 26, loss 1.6573137044906616, acc=0.3027777671813965, loss=1.6573137044906616
train: epoch 27, loss 1.1241449117660522, acc=0.5375000238418579, loss=1.1241449117660522
test: epoch 27, loss 1.6262054443359375, acc=0.3305555582046509, loss=1.6262054443359375
train: epoch 28, loss 1.115613579750061, acc=0.5455555319786072, loss=1.115613579750061
test: epoch 28, loss 1.6352964639663696, acc=0.3083333373069763, loss=1.6352964639663696
train: epoch 29, loss 1.1188349723815918, acc=0.5472221970558167, loss=1.1188349723815918
test: epoch 29, loss 1.628143310546875, acc=0.3361110985279083, loss=1.628143310546875
train: epoch 30, loss 1.0924619436264038, acc=0.5491111278533936, loss=1.0924619436264038
test: epoch 30, loss 1.6621754169464111, acc=0.3166666626930237, loss=1.6621754169464111
train: epoch 31, loss 1.0818145275115967, acc=0.554888904094696, loss=1.0818145275115967
test: epoch 31, loss 1.615315318107605, acc=0.34166666865348816, loss=1.615315318107605
train: epoch 32, loss 1.0658252239227295, acc=0.5603888630867004, loss=1.0658252239227295
test: epoch 32, loss 1.6817891597747803, acc=0.3472222089767456, loss=1.6817891597747803
train: epoch 33, loss 1.053368330001831, acc=0.5638333559036255, loss=1.053368330001831
test: epoch 33, loss 1.6130315065383911, acc=0.33888888359069824, loss=1.6130315065383911
train: epoch 34, loss 1.0433380603790283, acc=0.5613889098167419, loss=1.0433380603790283
test: epoch 34, loss 1.6023722887039185, acc=0.35555556416511536, loss=1.6023722887039185
train: epoch 35, loss 1.035443663597107, acc=0.5688889026641846, loss=1.035443663597107
test: epoch 35, loss 1.6367720365524292, acc=0.3472222089767456, loss=1.6367720365524292
train: epoch 36, loss 1.0155314207077026, acc=0.5728889107704163, loss=1.0155314207077026
test: epoch 36, loss 1.578356146812439, acc=0.35277777910232544, loss=1.578356146812439
train: epoch 37, loss 1.0119333267211914, acc=0.5654444694519043, loss=1.0119333267211914
test: epoch 37, loss 1.5007007122039795, acc=0.36944442987442017, loss=1.5007007122039795
train: epoch 38, loss 1.0038750171661377, acc=0.5757222175598145, loss=1.0038750171661377
test: epoch 38, loss 1.600713849067688, acc=0.35555556416511536, loss=1.600713849067688
train: epoch 39, loss 0.984611988067627, acc=0.5782222151756287, loss=0.984611988067627
test: epoch 39, loss 1.5864295959472656, acc=0.3638888895511627, loss=1.5864295959472656
train: epoch 40, loss 0.9871525168418884, acc=0.5851110816001892, loss=0.9871525168418884
test: epoch 40, loss 1.5128273963928223, acc=0.36666667461395264, loss=1.5128273963928223
train: epoch 41, loss 0.9706788659095764, acc=0.5817777514457703, loss=0.9706788659095764
test: epoch 41, loss 1.5266966819763184, acc=0.3638888895511627, loss=1.5266966819763184
train: epoch 42, loss 0.9566680788993835, acc=0.5923333168029785, loss=0.9566680788993835
test: epoch 42, loss 1.4886788129806519, acc=0.38333332538604736, loss=1.4886788129806519
train: epoch 43, loss 0.9603239297866821, acc=0.5976666808128357, loss=0.9603239297866821
test: epoch 43, loss 1.4501844644546509, acc=0.3611111044883728, loss=1.4501844644546509
train: epoch 44, loss 0.9434967637062073, acc=0.598111093044281, loss=0.9434967637062073
test: epoch 44, loss 1.4818412065505981, acc=0.3722222149372101, loss=1.4818412065505981
train: epoch 45, loss 0.9496320486068726, acc=0.5972777605056763, loss=0.9496320486068726
test: epoch 45, loss 1.4989250898361206, acc=0.3611111044883728, loss=1.4989250898361206
train: epoch 46, loss 0.9231486320495605, acc=0.605555534362793, loss=0.9231486320495605
test: epoch 46, loss 1.5013785362243652, acc=0.36944442987442017, loss=1.5013785362243652
train: epoch 47, loss 0.9125375151634216, acc=0.6107777953147888, loss=0.9125375151634216
test: epoch 47, loss 1.4934338331222534, acc=0.375, loss=1.4934338331222534
train: epoch 48, loss 0.9084240198135376, acc=0.6127222180366516, loss=0.9084240198135376
test: epoch 48, loss 1.463903784751892, acc=0.375, loss=1.463903784751892
train: epoch 49, loss 0.910068154335022, acc=0.6110000014305115, loss=0.910068154335022
test: epoch 49, loss 1.5591145753860474, acc=0.38333332538604736, loss=1.5591145753860474
train: epoch 50, loss 0.8982069492340088, acc=0.6188333630561829, loss=0.8982069492340088
test: epoch 50, loss 1.5331966876983643, acc=0.36944442987442017, loss=1.5331966876983643
train: epoch 51, loss 0.9033399820327759, acc=0.6201111078262329, loss=0.9033399820327759
test: epoch 51, loss 1.504535436630249, acc=0.3916666805744171, loss=1.504535436630249
train: epoch 52, loss 0.8913668990135193, acc=0.620722234249115, loss=0.8913668990135193
test: epoch 52, loss 1.5118483304977417, acc=0.39722222089767456, loss=1.5118483304977417
train: epoch 53, loss 0.8732688426971436, acc=0.6271666884422302, loss=0.8732688426971436
test: epoch 53, loss 1.5301569700241089, acc=0.4027777910232544, loss=1.5301569700241089
train: epoch 54, loss 0.8542855978012085, acc=0.632444441318512, loss=0.8542855978012085
test: epoch 54, loss 1.4732074737548828, acc=0.3861111104488373, loss=1.4732074737548828
train: epoch 55, loss 0.8605895042419434, acc=0.6330000162124634, loss=0.8605895042419434
test: epoch 55, loss 1.5350613594055176, acc=0.39444443583488464, loss=1.5350613594055176
train: epoch 56, loss 0.854694664478302, acc=0.6313333511352539, loss=0.854694664478302
test: epoch 56, loss 1.5586433410644531, acc=0.3888888955116272, loss=1.5586433410644531
train: epoch 57, loss 0.8388416767120361, acc=0.6426666378974915, loss=0.8388416767120361
test: epoch 57, loss 1.5359925031661987, acc=0.3888888955116272, loss=1.5359925031661987
train: epoch 58, loss 0.8310380578041077, acc=0.6412222385406494, loss=0.8310380578041077
test: epoch 58, loss 1.4572597742080688, acc=0.4027777910232544, loss=1.4572597742080688
train: epoch 59, loss 0.8279122710227966, acc=0.6433333158493042, loss=0.8279122710227966
test: epoch 59, loss 1.4846922159194946, acc=0.39722222089767456, loss=1.4846922159194946
train: epoch 60, loss 0.8193709850311279, acc=0.6438888907432556, loss=0.8193709850311279
test: epoch 60, loss 1.4382002353668213, acc=0.4166666567325592, loss=1.4382002353668213
train: epoch 61, loss 0.8083961606025696, acc=0.6452222466468811, loss=0.8083961606025696
test: epoch 61, loss 1.4471431970596313, acc=0.4055555462837219, loss=1.4471431970596313
train: epoch 62, loss 0.8125468492507935, acc=0.6478333473205566, loss=0.8125468492507935
test: epoch 62, loss 1.4871265888214111, acc=0.42222222685813904, loss=1.4871265888214111
train: epoch 63, loss 0.8000855445861816, acc=0.6517221927642822, loss=0.8000855445861816
test: epoch 63, loss 1.4913570880889893, acc=0.4055555462837219, loss=1.4913570880889893
train: epoch 64, loss 0.7993054986000061, acc=0.653333306312561, loss=0.7993054986000061
test: epoch 64, loss 1.4540168046951294, acc=0.42222222685813904, loss=1.4540168046951294
train: epoch 65, loss 0.787531316280365, acc=0.6535555720329285, loss=0.787531316280365
test: epoch 65, loss 1.3885365724563599, acc=0.42222222685813904, loss=1.3885365724563599
train: epoch 66, loss 0.7874298691749573, acc=0.6570555567741394, loss=0.7874298691749573
test: epoch 66, loss 1.529295563697815, acc=0.4277777671813965, loss=1.529295563697815
train: epoch 67, loss 0.7730215787887573, acc=0.6570555567741394, loss=0.7730215787887573
test: epoch 67, loss 1.496072769165039, acc=0.42500001192092896, loss=1.496072769165039
train: epoch 68, loss 0.7631675004959106, acc=0.6603333353996277, loss=0.7631675004959106
test: epoch 68, loss 1.4190165996551514, acc=0.4166666567325592, loss=1.4190165996551514
train: epoch 69, loss 0.759060800075531, acc=0.6619444489479065, loss=0.759060800075531
test: epoch 69, loss 1.4952454566955566, acc=0.41111111640930176, loss=1.4952454566955566
train: epoch 70, loss 0.757768988609314, acc=0.6572777628898621, loss=0.757768988609314
test: epoch 70, loss 1.4571664333343506, acc=0.4305555522441864, loss=1.4571664333343506
train: epoch 71, loss 0.7725465893745422, acc=0.6578333377838135, loss=0.7725465893745422
test: epoch 71, loss 1.4751396179199219, acc=0.43611112236976624, loss=1.4751396179199219
train: epoch 72, loss 0.7560070753097534, acc=0.6686111092567444, loss=0.7560070753097534
test: epoch 72, loss 1.5069804191589355, acc=0.42222222685813904, loss=1.5069804191589355
train: epoch 73, loss 0.7540072798728943, acc=0.6657222509384155, loss=0.7540072798728943
test: epoch 73, loss 1.4687323570251465, acc=0.4444444477558136, loss=1.4687323570251465
train: epoch 74, loss 0.7416243553161621, acc=0.6696110963821411, loss=0.7416243553161621
test: epoch 74, loss 1.509729027748108, acc=0.42500001192092896, loss=1.509729027748108
train: epoch 75, loss 0.7339277863502502, acc=0.672166645526886, loss=0.7339277863502502
test: epoch 75, loss 1.5454376935958862, acc=0.43611112236976624, loss=1.5454376935958862
train: epoch 76, loss 0.7436951994895935, acc=0.6696110963821411, loss=0.7436951994895935
test: epoch 76, loss 1.4479811191558838, acc=0.4305555522441864, loss=1.4479811191558838
train: epoch 77, loss 0.7373449206352234, acc=0.6689444184303284, loss=0.7373449206352234
test: epoch 77, loss 1.4896376132965088, acc=0.43888887763023376, loss=1.4896376132965088
train: epoch 78, loss 0.7314637303352356, acc=0.6753333210945129, loss=0.7314637303352356
test: epoch 78, loss 1.5288525819778442, acc=0.4305555522441864, loss=1.5288525819778442
train: epoch 79, loss 0.7292536497116089, acc=0.6707777976989746, loss=0.7292536497116089
test: epoch 79, loss 1.5260523557662964, acc=0.42222222685813904, loss=1.5260523557662964
train: epoch 80, loss 0.7264559864997864, acc=0.6751111149787903, loss=0.7264559864997864
test: epoch 80, loss 1.542868733406067, acc=0.4416666626930237, loss=1.542868733406067
train: epoch 81, loss 0.7227925658226013, acc=0.6757222414016724, loss=0.7227925658226013
test: epoch 81, loss 1.513089656829834, acc=0.4305555522441864, loss=1.513089656829834
train: epoch 82, loss 0.7304379343986511, acc=0.675166666507721, loss=0.7304379343986511
test: epoch 82, loss 1.5066323280334473, acc=0.4333333373069763, loss=1.5066323280334473
train: epoch 83, loss 0.7189670205116272, acc=0.6819999814033508, loss=0.7189670205116272
test: epoch 83, loss 1.5179121494293213, acc=0.4277777671813965, loss=1.5179121494293213
train: epoch 84, loss 0.7220810651779175, acc=0.6784999966621399, loss=0.7220810651779175
test: epoch 84, loss 1.4840810298919678, acc=0.43611112236976624, loss=1.4840810298919678
train: epoch 85, loss 0.7335197925567627, acc=0.6747778058052063, loss=0.7335197925567627
test: epoch 85, loss 1.4667209386825562, acc=0.42500001192092896, loss=1.4667209386825562
train: epoch 86, loss 0.704788327217102, acc=0.6790555715560913, loss=0.704788327217102
test: epoch 86, loss 1.4711239337921143, acc=0.4333333373069763, loss=1.4711239337921143
train: epoch 87, loss 0.7215133905410767, acc=0.6731666922569275, loss=0.7215133905410767
test: epoch 87, loss 1.3617987632751465, acc=0.4305555522441864, loss=1.3617987632751465
train: epoch 88, loss 0.7076115012168884, acc=0.6826111078262329, loss=0.7076115012168884
test: epoch 88, loss 1.5234653949737549, acc=0.4333333373069763, loss=1.5234653949737549
train: epoch 89, loss 0.7094912528991699, acc=0.6787222027778625, loss=0.7094912528991699
test: epoch 89, loss 1.5098609924316406, acc=0.4194444417953491, loss=1.5098609924316406
train: epoch 90, loss 0.7029713988304138, acc=0.6812222003936768, loss=0.7029713988304138
test: epoch 90, loss 1.5592690706253052, acc=0.4333333373069763, loss=1.5592690706253052
train: epoch 91, loss 0.7056335806846619, acc=0.6822777986526489, loss=0.7056335806846619
test: epoch 91, loss 1.5024373531341553, acc=0.4277777671813965, loss=1.5024373531341553
train: epoch 92, loss 0.6970043778419495, acc=0.6842222213745117, loss=0.6970043778419495
test: epoch 92, loss 1.548706293106079, acc=0.4333333373069763, loss=1.548706293106079
train: epoch 93, loss 0.6941602230072021, acc=0.6870555281639099, loss=0.6941602230072021
test: epoch 93, loss 1.5601108074188232, acc=0.4277777671813965, loss=1.5601108074188232
train: epoch 94, loss 0.7007412910461426, acc=0.6865555644035339, loss=0.7007412910461426
test: epoch 94, loss 1.5121443271636963, acc=0.43611112236976624, loss=1.5121443271636963
train: epoch 95, loss 0.691260576248169, acc=0.6894999742507935, loss=0.691260576248169
test: epoch 95, loss 1.522794485092163, acc=0.4305555522441864, loss=1.522794485092163
train: epoch 96, loss 0.7019019722938538, acc=0.6862221956253052, loss=0.7019019722938538
test: epoch 96, loss 1.519686222076416, acc=0.4333333373069763, loss=1.519686222076416
train: epoch 97, loss 0.6986722946166992, acc=0.6891111135482788, loss=0.6986722946166992
test: epoch 97, loss 1.4948493242263794, acc=0.4416666626930237, loss=1.4948493242263794
train: epoch 98, loss 0.6806392669677734, acc=0.6917222142219543, loss=0.6806392669677734
test: epoch 98, loss 1.5827869176864624, acc=0.4333333373069763, loss=1.5827869176864624
train: epoch 99, loss 0.6791656017303467, acc=0.6910555362701416, loss=0.6791656017303467
test: epoch 99, loss 1.5198042392730713, acc=0.4333333373069763, loss=1.5198042392730713
train: epoch 100, loss 0.6835340857505798, acc=0.6930555701255798, loss=0.6835340857505798
test: epoch 100, loss 1.5190024375915527, acc=0.4333333373069763, loss=1.5190024375915527
train: epoch 101, loss 0.6914291381835938, acc=0.6906111240386963, loss=0.6914291381835938
test: epoch 101, loss 1.530218243598938, acc=0.4416666626930237, loss=1.530218243598938
train: epoch 102, loss 0.6744053363800049, acc=0.6965555548667908, loss=0.6744053363800049
test: epoch 102, loss 1.569663166999817, acc=0.4416666626930237, loss=1.569663166999817
train: epoch 103, loss 0.6796868443489075, acc=0.6921666860580444, loss=0.6796868443489075
test: epoch 103, loss 1.584753155708313, acc=0.4416666626930237, loss=1.584753155708313
train: epoch 104, loss 0.6746622323989868, acc=0.6940000057220459, loss=0.6746622323989868
test: epoch 104, loss 1.520606517791748, acc=0.44999998807907104, loss=1.520606517791748
train: epoch 105, loss 0.677916944026947, acc=0.6970555782318115, loss=0.677916944026947
test: epoch 105, loss 1.4753106832504272, acc=0.43611112236976624, loss=1.4753106832504272
train: epoch 106, loss 0.6728490591049194, acc=0.6978333592414856, loss=0.6728490591049194
test: epoch 106, loss 1.6654812097549438, acc=0.4416666626930237, loss=1.6654812097549438
train: epoch 107, loss 0.6684070229530334, acc=0.6972222328186035, loss=0.6684070229530334
test: epoch 107, loss 1.5716214179992676, acc=0.43888887763023376, loss=1.5716214179992676
train: epoch 108, loss 0.6852284669876099, acc=0.6952221989631653, loss=0.6852284669876099
test: epoch 108, loss 1.5355896949768066, acc=0.4416666626930237, loss=1.5355896949768066
train: epoch 109, loss 0.6710895299911499, acc=0.6932222247123718, loss=0.6710895299911499
test: epoch 109, loss 1.6252186298370361, acc=0.43611112236976624, loss=1.6252186298370361
train: epoch 110, loss 0.6749527454376221, acc=0.6937222480773926, loss=0.6749527454376221
test: epoch 110, loss 1.5862549543380737, acc=0.4444444477558136, loss=1.5862549543380737
train: epoch 111, loss 0.6672948002815247, acc=0.6961110830307007, loss=0.6672948002815247
test: epoch 111, loss 1.420810580253601, acc=0.4305555522441864, loss=1.420810580253601
train: epoch 112, loss 0.6729102730751038, acc=0.6935555338859558, loss=0.6729102730751038
test: epoch 112, loss 1.5976532697677612, acc=0.4472222328186035, loss=1.5976532697677612
train: epoch 113, loss 0.6658568978309631, acc=0.6983333230018616, loss=0.6658568978309631
test: epoch 113, loss 1.5386247634887695, acc=0.4444444477558136, loss=1.5386247634887695
train: epoch 114, loss 0.6684030294418335, acc=0.6994444727897644, loss=0.6684030294418335
test: epoch 114, loss 1.5567145347595215, acc=0.4583333432674408, loss=1.5567145347595215
train: epoch 115, loss 0.654511034488678, acc=0.7016111016273499, loss=0.654511034488678
test: epoch 115, loss 1.6598460674285889, acc=0.43888887763023376, loss=1.6598460674285889
train: epoch 116, loss 0.6550194621086121, acc=0.703499972820282, loss=0.6550194621086121
test: epoch 116, loss 1.510865330696106, acc=0.4472222328186035, loss=1.510865330696106
train: epoch 117, loss 0.650597095489502, acc=0.7024999856948853, loss=0.650597095489502
test: epoch 117, loss 1.5984487533569336, acc=0.4472222328186035, loss=1.5984487533569336
train: epoch 118, loss 0.6546792387962341, acc=0.7026666402816772, loss=0.6546792387962341
test: epoch 118, loss 1.5458980798721313, acc=0.4472222328186035, loss=1.5458980798721313
train: epoch 119, loss 0.647754430770874, acc=0.7070000171661377, loss=0.647754430770874
test: epoch 119, loss 1.5828194618225098, acc=0.4472222328186035, loss=1.5828194618225098
train: epoch 120, loss 0.6549385786056519, acc=0.7030555605888367, loss=0.6549385786056519
test: epoch 120, loss 1.5773407220840454, acc=0.44999998807907104, loss=1.5773407220840454
train: epoch 121, loss 0.6489083170890808, acc=0.7059999704360962, loss=0.6489083170890808
test: epoch 121, loss 1.5120171308517456, acc=0.4555555582046509, loss=1.5120171308517456
train: epoch 122, loss 0.6580425500869751, acc=0.7045000195503235, loss=0.6580425500869751
test: epoch 122, loss 1.4765242338180542, acc=0.44999998807907104, loss=1.4765242338180542
train: epoch 123, loss 0.6430926322937012, acc=0.707277774810791, loss=0.6430926322937012
test: epoch 123, loss 1.54965078830719, acc=0.4555555582046509, loss=1.54965078830719
train: epoch 124, loss 0.6484189033508301, acc=0.7036111354827881, loss=0.6484189033508301
test: epoch 124, loss 1.5945767164230347, acc=0.4555555582046509, loss=1.5945767164230347
train: epoch 125, loss 0.6387581825256348, acc=0.7088888883590698, loss=0.6387581825256348
test: epoch 125, loss 1.5289462804794312, acc=0.46666666865348816, loss=1.5289462804794312
train: epoch 126, loss 0.6606533527374268, acc=0.7041666507720947, loss=0.6606533527374268
test: epoch 126, loss 1.601781964302063, acc=0.4583333432674408, loss=1.601781964302063
train: epoch 127, loss 0.6482955813407898, acc=0.7049444317817688, loss=0.6482955813407898
test: epoch 127, loss 1.5706288814544678, acc=0.4555555582046509, loss=1.5706288814544678
train: epoch 128, loss 0.6433220505714417, acc=0.7038333415985107, loss=0.6433220505714417
test: epoch 128, loss 1.480507254600525, acc=0.4611110985279083, loss=1.480507254600525
train: epoch 129, loss 0.6358036994934082, acc=0.7080555558204651, loss=0.6358036994934082
test: epoch 129, loss 1.5388587713241577, acc=0.4583333432674408, loss=1.5388587713241577
train: epoch 130, loss 0.6288414597511292, acc=0.7095000147819519, loss=0.6288414597511292
test: epoch 130, loss 1.5686798095703125, acc=0.4583333432674408, loss=1.5686798095703125
train: epoch 131, loss 0.6400414109230042, acc=0.7081666588783264, loss=0.6400414109230042
test: epoch 131, loss 1.4689579010009766, acc=0.4583333432674408, loss=1.4689579010009766
train: epoch 132, loss 0.634428083896637, acc=0.7108888626098633, loss=0.634428083896637
test: epoch 132, loss 1.566443920135498, acc=0.4611110985279083, loss=1.566443920135498
train: epoch 133, loss 0.6271510124206543, acc=0.71061110496521, loss=0.6271510124206543
test: epoch 133, loss 1.5943013429641724, acc=0.45277777314186096, loss=1.5943013429641724
train: epoch 134, loss 0.6358934044837952, acc=0.7103888988494873, loss=0.6358934044837952
test: epoch 134, loss 1.5416829586029053, acc=0.4611110985279083, loss=1.5416829586029053
train: epoch 135, loss 0.6279503703117371, acc=0.7135555744171143, loss=0.6279503703117371
test: epoch 135, loss 1.7026491165161133, acc=0.4611110985279083, loss=1.7026491165161133
train: epoch 136, loss 0.6327381730079651, acc=0.7120555639266968, loss=0.6327381730079651
test: epoch 136, loss 1.6735196113586426, acc=0.4749999940395355, loss=1.6735196113586426
train: epoch 137, loss 0.6321871280670166, acc=0.7093333601951599, loss=0.6321871280670166
test: epoch 137, loss 1.5586020946502686, acc=0.46388888359069824, loss=1.5586020946502686
train: epoch 138, loss 0.626724898815155, acc=0.7103888988494873, loss=0.626724898815155
test: epoch 138, loss 1.598416805267334, acc=0.4611110985279083, loss=1.598416805267334
train: epoch 139, loss 0.6185557246208191, acc=0.7121666669845581, loss=0.6185557246208191
test: epoch 139, loss 1.4994703531265259, acc=0.4611110985279083, loss=1.4994703531265259
train: epoch 140, loss 0.6321350932121277, acc=0.7111666798591614, loss=0.6321350932121277
test: epoch 140, loss 1.569334626197815, acc=0.4611110985279083, loss=1.569334626197815
train: epoch 141, loss 0.6440251469612122, acc=0.7065555453300476, loss=0.6440251469612122
test: epoch 141, loss 1.6344118118286133, acc=0.4611110985279083, loss=1.6344118118286133
train: epoch 142, loss 0.6233222484588623, acc=0.7133333086967468, loss=0.6233222484588623
test: epoch 142, loss 1.6763092279434204, acc=0.4583333432674408, loss=1.6763092279434204
train: epoch 143, loss 0.6203913688659668, acc=0.7162222266197205, loss=0.6203913688659668
test: epoch 143, loss 1.57847261428833, acc=0.46388888359069824, loss=1.57847261428833
train: epoch 144, loss 0.6163232922554016, acc=0.7171111106872559, loss=0.6163232922554016
test: epoch 144, loss 1.6200380325317383, acc=0.4749999940395355, loss=1.6200380325317383
train: epoch 145, loss 0.6193249225616455, acc=0.7136111259460449, loss=0.6193249225616455
test: epoch 145, loss 1.5568287372589111, acc=0.46666666865348816, loss=1.5568287372589111
train: epoch 146, loss 0.6229804158210754, acc=0.7124999761581421, loss=0.6229804158210754
test: epoch 146, loss 1.5356594324111938, acc=0.46388888359069824, loss=1.5356594324111938
train: epoch 147, loss 0.6118720769882202, acc=0.7154444456100464, loss=0.6118720769882202
test: epoch 147, loss 1.5231995582580566, acc=0.4611110985279083, loss=1.5231995582580566
train: epoch 148, loss 0.6311929821968079, acc=0.710277795791626, loss=0.6311929821968079
test: epoch 148, loss 1.4757179021835327, acc=0.46388888359069824, loss=1.4757179021835327
train: epoch 149, loss 0.6152236461639404, acc=0.7122222185134888, loss=0.6152236461639404
test: epoch 149, loss 1.6405880451202393, acc=0.47777777910232544, loss=1.6405880451202393
train: epoch 150, loss 0.6185858249664307, acc=0.711722195148468, loss=0.6185858249664307
test: epoch 150, loss 1.639458417892456, acc=0.46388888359069824, loss=1.639458417892456
