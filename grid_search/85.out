# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=0", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1234961464, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1234961464, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.632206439971924, acc=0.1350555568933487, loss=2.632206439971924
test: epoch 1, loss 4.54400634765625, acc=0.06388889253139496, loss=4.54400634765625
train: epoch 2, loss 2.0349841117858887, acc=0.24016666412353516, loss=2.0349841117858887
test: epoch 2, loss 4.0842156410217285, acc=0.08055555820465088, loss=4.0842156410217285
train: epoch 3, loss 1.8367289304733276, acc=0.2883888781070709, loss=1.8367289304733276
test: epoch 3, loss 4.188929080963135, acc=0.08055555820465088, loss=4.188929080963135
train: epoch 4, loss 1.6989234685897827, acc=0.32777777314186096, loss=1.6989234685897827
test: epoch 4, loss 4.052292346954346, acc=0.09166666865348816, loss=4.052292346954346
train: epoch 5, loss 1.6380283832550049, acc=0.3499999940395355, loss=1.6380283832550049
test: epoch 5, loss 3.45070743560791, acc=0.10277777910232544, loss=3.45070743560791
train: epoch 6, loss 1.5501042604446411, acc=0.3855000138282776, loss=1.5501042604446411
test: epoch 6, loss 3.407866954803467, acc=0.13333334028720856, loss=3.407866954803467
train: epoch 7, loss 1.4968762397766113, acc=0.4008888900279999, loss=1.4968762397766113
test: epoch 7, loss 3.222649574279785, acc=0.13611111044883728, loss=3.222649574279785
train: epoch 8, loss 1.436241626739502, acc=0.4226111173629761, loss=1.436241626739502
test: epoch 8, loss 3.2839858531951904, acc=0.14444445073604584, loss=3.2839858531951904
train: epoch 9, loss 1.39972984790802, acc=0.44466665387153625, loss=1.39972984790802
test: epoch 9, loss 3.362480640411377, acc=0.1527777761220932, loss=3.362480640411377
train: epoch 10, loss 1.3886336088180542, acc=0.4472777843475342, loss=1.3886336088180542
test: epoch 10, loss 3.0935168266296387, acc=0.12222222238779068, loss=3.0935168266296387
train: epoch 11, loss 1.3314611911773682, acc=0.4707777798175812, loss=1.3314611911773682
test: epoch 11, loss 3.261324882507324, acc=0.125, loss=3.261324882507324
train: epoch 12, loss 1.3162299394607544, acc=0.4728333353996277, loss=1.3162299394607544
test: epoch 12, loss 2.641861915588379, acc=0.16944444179534912, loss=2.641861915588379
train: epoch 13, loss 1.2907053232192993, acc=0.49183332920074463, loss=1.2907053232192993
test: epoch 13, loss 2.7396240234375, acc=0.16111111640930176, loss=2.7396240234375
train: epoch 14, loss 1.2911914587020874, acc=0.492333322763443, loss=1.2911914587020874
test: epoch 14, loss 2.7134342193603516, acc=0.14166666567325592, loss=2.7134342193603516
train: epoch 15, loss 1.2456549406051636, acc=0.5061666369438171, loss=1.2456549406051636
test: epoch 15, loss 2.56821608543396, acc=0.13611111044883728, loss=2.56821608543396
train: epoch 16, loss 1.2404122352600098, acc=0.49961110949516296, loss=1.2404122352600098
test: epoch 16, loss 2.4798688888549805, acc=0.20000000298023224, loss=2.4798688888549805
train: epoch 17, loss 1.2123790979385376, acc=0.5140555500984192, loss=1.2123790979385376
test: epoch 17, loss 2.8716700077056885, acc=0.14444445073604584, loss=2.8716700077056885
train: epoch 18, loss 1.2127951383590698, acc=0.5253888964653015, loss=1.2127951383590698
test: epoch 18, loss 3.0224215984344482, acc=0.10000000149011612, loss=3.0224215984344482
train: epoch 19, loss 1.1904445886611938, acc=0.522777795791626, loss=1.1904445886611938
test: epoch 19, loss 2.573572874069214, acc=0.15555556118488312, loss=2.573572874069214
train: epoch 20, loss 1.2153512239456177, acc=0.5212222337722778, loss=1.2153512239456177
test: epoch 20, loss 2.3842008113861084, acc=0.18611110746860504, loss=2.3842008113861084
train: epoch 21, loss 1.1828995943069458, acc=0.5298333168029785, loss=1.1828995943069458
test: epoch 21, loss 2.5015978813171387, acc=0.13611111044883728, loss=2.5015978813171387
train: epoch 22, loss 1.1598068475723267, acc=0.5368333458900452, loss=1.1598068475723267
test: epoch 22, loss 2.2942593097686768, acc=0.2222222238779068, loss=2.2942593097686768
train: epoch 23, loss 1.170140266418457, acc=0.5343888998031616, loss=1.170140266418457
test: epoch 23, loss 2.5091655254364014, acc=0.18888889253139496, loss=2.5091655254364014
train: epoch 24, loss 1.1760143041610718, acc=0.5333889126777649, loss=1.1760143041610718
test: epoch 24, loss 2.264145851135254, acc=0.20555555820465088, loss=2.264145851135254
train: epoch 25, loss 1.1413346529006958, acc=0.546500027179718, loss=1.1413346529006958
test: epoch 25, loss 2.066459894180298, acc=0.23888888955116272, loss=2.066459894180298
train: epoch 26, loss 1.1377161741256714, acc=0.545722246170044, loss=1.1377161741256714
test: epoch 26, loss 2.3656177520751953, acc=0.20277777314186096, loss=2.3656177520751953
train: epoch 27, loss 1.1391117572784424, acc=0.5526111125946045, loss=1.1391117572784424
test: epoch 27, loss 2.2274680137634277, acc=0.25833332538604736, loss=2.2274680137634277
train: epoch 28, loss 1.118607759475708, acc=0.5533333420753479, loss=1.118607759475708
test: epoch 28, loss 2.4139764308929443, acc=0.2361111044883728, loss=2.4139764308929443
train: epoch 29, loss 1.1121255159378052, acc=0.5566111207008362, loss=1.1121255159378052
test: epoch 29, loss 2.277003765106201, acc=0.19166666269302368, loss=2.277003765106201
train: epoch 30, loss 1.1212810277938843, acc=0.5508888959884644, loss=1.1212810277938843
test: epoch 30, loss 2.157195806503296, acc=0.24166665971279144, loss=2.157195806503296
train: epoch 31, loss 1.1172099113464355, acc=0.5557222366333008, loss=1.1172099113464355
test: epoch 31, loss 2.1430606842041016, acc=0.2361111044883728, loss=2.1430606842041016
train: epoch 32, loss 1.103704571723938, acc=0.5561110973358154, loss=1.103704571723938
test: epoch 32, loss 2.5041937828063965, acc=0.19166666269302368, loss=2.5041937828063965
train: epoch 33, loss 1.0999574661254883, acc=0.5657777786254883, loss=1.0999574661254883
test: epoch 33, loss 2.1639649868011475, acc=0.20555555820465088, loss=2.1639649868011475
train: epoch 34, loss 1.0869179964065552, acc=0.5691111087799072, loss=1.0869179964065552
test: epoch 34, loss 2.1791610717773438, acc=0.21388888359069824, loss=2.1791610717773438
train: epoch 35, loss 1.0909606218338013, acc=0.5636110901832581, loss=1.0909606218338013
test: epoch 35, loss 2.1795365810394287, acc=0.23055554926395416, loss=2.1795365810394287
train: epoch 36, loss 1.0879571437835693, acc=0.5694444179534912, loss=1.0879571437835693
test: epoch 36, loss 2.0611093044281006, acc=0.23333333432674408, loss=2.0611093044281006
train: epoch 37, loss 1.0643768310546875, acc=0.574833333492279, loss=1.0643768310546875
test: epoch 37, loss 2.1183338165283203, acc=0.19722221791744232, loss=2.1183338165283203
train: epoch 38, loss 1.0902692079544067, acc=0.5711110830307007, loss=1.0902692079544067
test: epoch 38, loss 2.091597557067871, acc=0.21944443881511688, loss=2.091597557067871
train: epoch 39, loss 1.0610839128494263, acc=0.5734999775886536, loss=1.0610839128494263
test: epoch 39, loss 1.9165596961975098, acc=0.3499999940395355, loss=1.9165596961975098
train: epoch 40, loss 1.060091495513916, acc=0.5746111273765564, loss=1.060091495513916
test: epoch 40, loss 1.975692629814148, acc=0.25555557012557983, loss=1.975692629814148
train: epoch 41, loss 1.066454291343689, acc=0.5724444389343262, loss=1.066454291343689
test: epoch 41, loss 1.810378074645996, acc=0.3027777671813965, loss=1.810378074645996
train: epoch 42, loss 1.043095350265503, acc=0.5773888826370239, loss=1.043095350265503
test: epoch 42, loss 2.071995496749878, acc=0.2666666805744171, loss=2.071995496749878
train: epoch 43, loss 1.0614933967590332, acc=0.577833354473114, loss=1.0614933967590332
test: epoch 43, loss 1.704990029335022, acc=0.3166666626930237, loss=1.704990029335022
train: epoch 44, loss 1.0499299764633179, acc=0.5775555372238159, loss=1.0499299764633179
test: epoch 44, loss 1.985756278038025, acc=0.2750000059604645, loss=1.985756278038025
train: epoch 45, loss 1.0586400032043457, acc=0.5747777819633484, loss=1.0586400032043457
test: epoch 45, loss 1.8813629150390625, acc=0.28333333134651184, loss=1.8813629150390625
train: epoch 46, loss 1.0561610460281372, acc=0.58561110496521, loss=1.0561610460281372
test: epoch 46, loss 1.796515941619873, acc=0.3027777671813965, loss=1.796515941619873
train: epoch 47, loss 1.0488840341567993, acc=0.5798333287239075, loss=1.0488840341567993
test: epoch 47, loss 1.6856257915496826, acc=0.28611111640930176, loss=1.6856257915496826
train: epoch 48, loss 1.0473105907440186, acc=0.581333339214325, loss=1.0473105907440186
test: epoch 48, loss 1.6732381582260132, acc=0.39444443583488464, loss=1.6732381582260132
train: epoch 49, loss 1.0662654638290405, acc=0.5722777843475342, loss=1.0662654638290405
test: epoch 49, loss 1.7601057291030884, acc=0.31388887763023376, loss=1.7601057291030884
train: epoch 50, loss 1.0319697856903076, acc=0.5853888988494873, loss=1.0319697856903076
test: epoch 50, loss 1.620972752571106, acc=0.3499999940395355, loss=1.620972752571106
train: epoch 51, loss 1.0389717817306519, acc=0.5839444398880005, loss=1.0389717817306519
test: epoch 51, loss 1.7427691221237183, acc=0.28333333134651184, loss=1.7427691221237183
train: epoch 52, loss 1.0444481372833252, acc=0.5816666483879089, loss=1.0444481372833252
test: epoch 52, loss 1.8313612937927246, acc=0.3194444477558136, loss=1.8313612937927246
train: epoch 53, loss 1.0152969360351562, acc=0.5912222266197205, loss=1.0152969360351562
test: epoch 53, loss 1.7117645740509033, acc=0.3055555522441864, loss=1.7117645740509033
train: epoch 54, loss 1.0298643112182617, acc=0.5827777981758118, loss=1.0298643112182617
test: epoch 54, loss 2.081092357635498, acc=0.19722221791744232, loss=2.081092357635498
train: epoch 55, loss 1.0240375995635986, acc=0.5907777547836304, loss=1.0240375995635986
test: epoch 55, loss 1.7960879802703857, acc=0.29722222685813904, loss=1.7960879802703857
train: epoch 56, loss 1.045255422592163, acc=0.5877222418785095, loss=1.045255422592163
test: epoch 56, loss 1.7237845659255981, acc=0.3305555582046509, loss=1.7237845659255981
train: epoch 57, loss 1.0131514072418213, acc=0.5943889021873474, loss=1.0131514072418213
test: epoch 57, loss 1.6801649332046509, acc=0.2944444417953491, loss=1.6801649332046509
train: epoch 58, loss 0.9959723353385925, acc=0.593999981880188, loss=0.9959723353385925
test: epoch 58, loss 1.6121304035186768, acc=0.33888888359069824, loss=1.6121304035186768
train: epoch 59, loss 1.0113089084625244, acc=0.5914999842643738, loss=1.0113089084625244
test: epoch 59, loss 1.6811132431030273, acc=0.31388887763023376, loss=1.6811132431030273
train: epoch 60, loss 0.9938597679138184, acc=0.5961111187934875, loss=0.9938597679138184
test: epoch 60, loss 1.6938704252243042, acc=0.35555556416511536, loss=1.6938704252243042
train: epoch 61, loss 0.9762166738510132, acc=0.6008889079093933, loss=0.9762166738510132
test: epoch 61, loss 1.5172981023788452, acc=0.4000000059604645, loss=1.5172981023788452
train: epoch 62, loss 0.9860330820083618, acc=0.5951111316680908, loss=0.9860330820083618
test: epoch 62, loss 1.6719025373458862, acc=0.3305555582046509, loss=1.6719025373458862
train: epoch 63, loss 0.9974440336227417, acc=0.590499997138977, loss=0.9974440336227417
test: epoch 63, loss 1.6083221435546875, acc=0.3194444477558136, loss=1.6083221435546875
train: epoch 64, loss 0.9873230457305908, acc=0.6013333201408386, loss=0.9873230457305908
test: epoch 64, loss 1.8087838888168335, acc=0.24166665971279144, loss=1.8087838888168335
train: epoch 65, loss 0.9787911176681519, acc=0.6056666374206543, loss=0.9787911176681519
test: epoch 65, loss 1.6138399839401245, acc=0.2888889014720917, loss=1.6138399839401245
train: epoch 66, loss 0.9845441579818726, acc=0.6025000214576721, loss=0.9845441579818726
test: epoch 66, loss 1.524194359779358, acc=0.3305555582046509, loss=1.524194359779358
train: epoch 67, loss 0.9757336378097534, acc=0.6071666479110718, loss=0.9757336378097534
test: epoch 67, loss 1.5015662908554077, acc=0.3361110985279083, loss=1.5015662908554077
train: epoch 68, loss 0.9741269946098328, acc=0.5979999899864197, loss=0.9741269946098328
test: epoch 68, loss 1.6447575092315674, acc=0.3361110985279083, loss=1.6447575092315674
train: epoch 69, loss 0.9755691289901733, acc=0.6025000214576721, loss=0.9755691289901733
test: epoch 69, loss 1.6027092933654785, acc=0.3611111044883728, loss=1.6027092933654785
train: epoch 70, loss 0.9946728348731995, acc=0.6004444360733032, loss=0.9946728348731995
test: epoch 70, loss 1.517328143119812, acc=0.3611111044883728, loss=1.517328143119812
train: epoch 71, loss 0.9605295658111572, acc=0.6062222123146057, loss=0.9605295658111572
test: epoch 71, loss 1.5887502431869507, acc=0.35555556416511536, loss=1.5887502431869507
train: epoch 72, loss 0.976352870464325, acc=0.6077222228050232, loss=0.976352870464325
test: epoch 72, loss 1.6720181703567505, acc=0.3027777671813965, loss=1.6720181703567505
train: epoch 73, loss 0.9581553936004639, acc=0.6075555682182312, loss=0.9581553936004639
test: epoch 73, loss 1.5965461730957031, acc=0.33888888359069824, loss=1.5965461730957031
train: epoch 74, loss 0.9342793822288513, acc=0.6132222414016724, loss=0.9342793822288513
test: epoch 74, loss 1.7727895975112915, acc=0.3305555582046509, loss=1.7727895975112915
train: epoch 75, loss 0.9254871606826782, acc=0.6214444637298584, loss=0.9254871606826782
test: epoch 75, loss 1.5180209875106812, acc=0.3444444537162781, loss=1.5180209875106812
train: epoch 76, loss 0.9403203129768372, acc=0.613111138343811, loss=0.9403203129768372
test: epoch 76, loss 1.7299386262893677, acc=0.3333333432674408, loss=1.7299386262893677
train: epoch 77, loss 0.9250079989433289, acc=0.6192222237586975, loss=0.9250079989433289
test: epoch 77, loss 1.5711846351623535, acc=0.3305555582046509, loss=1.5711846351623535
train: epoch 78, loss 0.8986285328865051, acc=0.6348888874053955, loss=0.8986285328865051
test: epoch 78, loss 1.6359264850616455, acc=0.31388887763023376, loss=1.6359264850616455
train: epoch 79, loss 0.9072190523147583, acc=0.6290555596351624, loss=0.9072190523147583
test: epoch 79, loss 1.5568821430206299, acc=0.36944442987442017, loss=1.5568821430206299
train: epoch 80, loss 0.91522216796875, acc=0.6280555725097656, loss=0.91522216796875
test: epoch 80, loss 1.4755648374557495, acc=0.3444444537162781, loss=1.4755648374557495
train: epoch 81, loss 0.8998056650161743, acc=0.6356111168861389, loss=0.8998056650161743
test: epoch 81, loss 1.4821239709854126, acc=0.375, loss=1.4821239709854126
train: epoch 82, loss 0.888583779335022, acc=0.6343888640403748, loss=0.888583779335022
test: epoch 82, loss 1.3435763120651245, acc=0.41111111640930176, loss=1.3435763120651245
train: epoch 83, loss 0.9047360420227051, acc=0.6318333148956299, loss=0.9047360420227051
test: epoch 83, loss 1.5909615755081177, acc=0.32499998807907104, loss=1.5909615755081177
train: epoch 84, loss 0.8880852460861206, acc=0.6348888874053955, loss=0.8880852460861206
test: epoch 84, loss 1.5682958364486694, acc=0.33888888359069824, loss=1.5682958364486694
train: epoch 85, loss 0.9083662033081055, acc=0.6340555548667908, loss=0.9083662033081055
test: epoch 85, loss 1.5217673778533936, acc=0.35277777910232544, loss=1.5217673778533936
train: epoch 86, loss 0.8614606857299805, acc=0.6496111154556274, loss=0.8614606857299805
test: epoch 86, loss 1.522774577140808, acc=0.33888888359069824, loss=1.522774577140808
train: epoch 87, loss 0.8716118335723877, acc=0.6468333601951599, loss=0.8716118335723877
test: epoch 87, loss 1.556727409362793, acc=0.3583333194255829, loss=1.556727409362793
train: epoch 88, loss 0.8607719540596008, acc=0.6489444375038147, loss=0.8607719540596008
test: epoch 88, loss 1.5559196472167969, acc=0.3361110985279083, loss=1.5559196472167969
train: epoch 89, loss 0.8657944202423096, acc=0.6424444317817688, loss=0.8657944202423096
test: epoch 89, loss 1.5951541662216187, acc=0.39722222089767456, loss=1.5951541662216187
train: epoch 90, loss 0.8502431511878967, acc=0.6553888916969299, loss=0.8502431511878967
test: epoch 90, loss 1.4616224765777588, acc=0.38333332538604736, loss=1.4616224765777588
train: epoch 91, loss 0.8538734912872314, acc=0.6517221927642822, loss=0.8538734912872314
test: epoch 91, loss 1.4827919006347656, acc=0.3916666805744171, loss=1.4827919006347656
train: epoch 92, loss 0.8381450176239014, acc=0.6536111235618591, loss=0.8381450176239014
test: epoch 92, loss 1.5822981595993042, acc=0.38055557012557983, loss=1.5822981595993042
train: epoch 93, loss 0.8300417065620422, acc=0.659333348274231, loss=0.8300417065620422
test: epoch 93, loss 1.5870212316513062, acc=0.3611111044883728, loss=1.5870212316513062
train: epoch 94, loss 0.8528879284858704, acc=0.6549444198608398, loss=0.8528879284858704
test: epoch 94, loss 1.5825917720794678, acc=0.38333332538604736, loss=1.5825917720794678
train: epoch 95, loss 0.8284522891044617, acc=0.6644999980926514, loss=0.8284522891044617
test: epoch 95, loss 1.557672142982483, acc=0.3777777850627899, loss=1.557672142982483
train: epoch 96, loss 0.8302438259124756, acc=0.6675000190734863, loss=0.8302438259124756
test: epoch 96, loss 1.4005457162857056, acc=0.4833333194255829, loss=1.4005457162857056
train: epoch 97, loss 0.8123614192008972, acc=0.6737777590751648, loss=0.8123614192008972
test: epoch 97, loss 1.6032071113586426, acc=0.3444444537162781, loss=1.6032071113586426
train: epoch 98, loss 0.818610668182373, acc=0.6684444546699524, loss=0.818610668182373
test: epoch 98, loss 1.4534342288970947, acc=0.3861111104488373, loss=1.4534342288970947
train: epoch 99, loss 0.7935333847999573, acc=0.6773889064788818, loss=0.7935333847999573
test: epoch 99, loss 1.7405022382736206, acc=0.3499999940395355, loss=1.7405022382736206
train: epoch 100, loss 0.8032472729682922, acc=0.6753333210945129, loss=0.8032472729682922
test: epoch 100, loss 1.4085314273834229, acc=0.4055555462837219, loss=1.4085314273834229
train: epoch 101, loss 0.7797120809555054, acc=0.6877222061157227, loss=0.7797120809555054
test: epoch 101, loss 1.5677754878997803, acc=0.3472222089767456, loss=1.5677754878997803
train: epoch 102, loss 0.7838205695152283, acc=0.6882777810096741, loss=0.7838205695152283
test: epoch 102, loss 1.3511120080947876, acc=0.42500001192092896, loss=1.3511120080947876
train: epoch 103, loss 0.7960823774337769, acc=0.6806666851043701, loss=0.7960823774337769
test: epoch 103, loss 1.2972115278244019, acc=0.40833333134651184, loss=1.2972115278244019
train: epoch 104, loss 0.7778765559196472, acc=0.6848333477973938, loss=0.7778765559196472
test: epoch 104, loss 1.4269123077392578, acc=0.4333333373069763, loss=1.4269123077392578
train: epoch 105, loss 0.7722396850585938, acc=0.690500020980835, loss=0.7722396850585938
test: epoch 105, loss 1.5755293369293213, acc=0.4055555462837219, loss=1.5755293369293213
train: epoch 106, loss 0.7826271057128906, acc=0.6857777833938599, loss=0.7826271057128906
test: epoch 106, loss 1.3113933801651, acc=0.4166666567325592, loss=1.3113933801651
train: epoch 107, loss 0.7681666016578674, acc=0.6886110901832581, loss=0.7681666016578674
test: epoch 107, loss 1.5038182735443115, acc=0.3638888895511627, loss=1.5038182735443115
train: epoch 108, loss 0.7843098044395447, acc=0.6822222471237183, loss=0.7843098044395447
test: epoch 108, loss 1.5063222646713257, acc=0.43611112236976624, loss=1.5063222646713257
train: epoch 109, loss 0.8004886507987976, acc=0.6868333220481873, loss=0.8004886507987976
test: epoch 109, loss 1.2963271141052246, acc=0.43611112236976624, loss=1.2963271141052246
train: epoch 110, loss 0.7530562877655029, acc=0.7015555500984192, loss=0.7530562877655029
test: epoch 110, loss 1.3042665719985962, acc=0.42500001192092896, loss=1.3042665719985962
train: epoch 111, loss 0.7521613836288452, acc=0.7007222175598145, loss=0.7521613836288452
test: epoch 111, loss 1.5741597414016724, acc=0.4277777671813965, loss=1.5741597414016724
train: epoch 112, loss 0.7655909061431885, acc=0.6941666603088379, loss=0.7655909061431885
test: epoch 112, loss 1.3444932699203491, acc=0.35277777910232544, loss=1.3444932699203491
train: epoch 113, loss 0.799637496471405, acc=0.6840000152587891, loss=0.799637496471405
test: epoch 113, loss 1.5116673707962036, acc=0.38055557012557983, loss=1.5116673707962036
train: epoch 114, loss 0.7395804524421692, acc=0.7075555324554443, loss=0.7395804524421692
test: epoch 114, loss 1.4521427154541016, acc=0.35555556416511536, loss=1.4521427154541016
train: epoch 115, loss 0.7568780779838562, acc=0.6965555548667908, loss=0.7568780779838562
test: epoch 115, loss 1.3672736883163452, acc=0.42500001192092896, loss=1.3672736883163452
train: epoch 116, loss 0.7449196577072144, acc=0.6997777819633484, loss=0.7449196577072144
test: epoch 116, loss 1.3417643308639526, acc=0.4166666567325592, loss=1.3417643308639526
train: epoch 117, loss 0.7251783013343811, acc=0.7072222232818604, loss=0.7251783013343811
test: epoch 117, loss 1.2934833765029907, acc=0.40833333134651184, loss=1.2934833765029907
train: epoch 118, loss 0.7579620480537415, acc=0.7014999985694885, loss=0.7579620480537415
test: epoch 118, loss 1.4186638593673706, acc=0.39444443583488464, loss=1.4186638593673706
train: epoch 119, loss 0.729228675365448, acc=0.707111120223999, loss=0.729228675365448
test: epoch 119, loss 1.4058902263641357, acc=0.4138889014720917, loss=1.4058902263641357
train: epoch 120, loss 0.7280805110931396, acc=0.7074999809265137, loss=0.7280805110931396
test: epoch 120, loss 1.2487716674804688, acc=0.4749999940395355, loss=1.2487716674804688
train: epoch 121, loss 0.7256581783294678, acc=0.7105000019073486, loss=0.7256581783294678
test: epoch 121, loss 1.538007140159607, acc=0.38333332538604736, loss=1.538007140159607
train: epoch 122, loss 0.7176820635795593, acc=0.7131666541099548, loss=0.7176820635795593
test: epoch 122, loss 1.5046546459197998, acc=0.4000000059604645, loss=1.5046546459197998
train: epoch 123, loss 0.7112751603126526, acc=0.7169444561004639, loss=0.7112751603126526
test: epoch 123, loss 1.3600634336471558, acc=0.3777777850627899, loss=1.3600634336471558
train: epoch 124, loss 0.7207077741622925, acc=0.7124444246292114, loss=0.7207077741622925
test: epoch 124, loss 1.2388622760772705, acc=0.5138888955116272, loss=1.2388622760772705
train: epoch 125, loss 0.7288786768913269, acc=0.714722216129303, loss=0.7288786768913269
test: epoch 125, loss 1.5448734760284424, acc=0.4444444477558136, loss=1.5448734760284424
train: epoch 126, loss 0.7002840638160706, acc=0.7163333296775818, loss=0.7002840638160706
test: epoch 126, loss 1.370753288269043, acc=0.38055557012557983, loss=1.370753288269043
train: epoch 127, loss 0.698646068572998, acc=0.7161666750907898, loss=0.698646068572998
test: epoch 127, loss 1.436171531677246, acc=0.4305555522441864, loss=1.436171531677246
train: epoch 128, loss 0.7069569826126099, acc=0.7166110873222351, loss=0.7069569826126099
test: epoch 128, loss 1.4525294303894043, acc=0.4416666626930237, loss=1.4525294303894043
train: epoch 129, loss 0.6968826055526733, acc=0.7196666598320007, loss=0.6968826055526733
test: epoch 129, loss 1.302654504776001, acc=0.44999998807907104, loss=1.302654504776001
train: epoch 130, loss 0.7094435691833496, acc=0.7207221984863281, loss=0.7094435691833496
test: epoch 130, loss 1.7042820453643799, acc=0.3499999940395355, loss=1.7042820453643799
train: epoch 131, loss 0.69084233045578, acc=0.7230555415153503, loss=0.69084233045578
test: epoch 131, loss 1.4236687421798706, acc=0.4305555522441864, loss=1.4236687421798706
train: epoch 132, loss 0.6883383989334106, acc=0.7269999980926514, loss=0.6883383989334106
test: epoch 132, loss 1.3839874267578125, acc=0.4277777671813965, loss=1.3839874267578125
train: epoch 133, loss 0.6949259638786316, acc=0.7206666469573975, loss=0.6949259638786316
test: epoch 133, loss 1.3328343629837036, acc=0.45277777314186096, loss=1.3328343629837036
train: epoch 134, loss 0.6891947388648987, acc=0.7232221961021423, loss=0.6891947388648987
test: epoch 134, loss 1.4032520055770874, acc=0.4888888895511627, loss=1.4032520055770874
train: epoch 135, loss 0.6822065711021423, acc=0.7271111011505127, loss=0.6822065711021423
test: epoch 135, loss 1.3051894903182983, acc=0.4583333432674408, loss=1.3051894903182983
train: epoch 136, loss 0.6790446043014526, acc=0.7266666889190674, loss=0.6790446043014526
test: epoch 136, loss 1.4461934566497803, acc=0.4444444477558136, loss=1.4461934566497803
train: epoch 137, loss 0.6920139193534851, acc=0.7234444618225098, loss=0.6920139193534851
test: epoch 137, loss 1.3899917602539062, acc=0.4166666567325592, loss=1.3899917602539062
train: epoch 138, loss 0.6837294697761536, acc=0.7229999899864197, loss=0.6837294697761536
test: epoch 138, loss 1.2668464183807373, acc=0.5249999761581421, loss=1.2668464183807373
train: epoch 139, loss 0.6818716526031494, acc=0.7242777943611145, loss=0.6818716526031494
test: epoch 139, loss 1.4971418380737305, acc=0.3916666805744171, loss=1.4971418380737305
train: epoch 140, loss 0.6578991413116455, acc=0.7343888878822327, loss=0.6578991413116455
test: epoch 140, loss 1.2330597639083862, acc=0.49444442987442017, loss=1.2330597639083862
train: epoch 141, loss 0.6549262404441833, acc=0.7402222156524658, loss=0.6549262404441833
test: epoch 141, loss 1.3427759408950806, acc=0.4694444537162781, loss=1.3427759408950806
train: epoch 142, loss 0.6732428669929504, acc=0.7298333048820496, loss=0.6732428669929504
test: epoch 142, loss 1.2757132053375244, acc=0.4888888895511627, loss=1.2757132053375244
train: epoch 143, loss 0.6679170727729797, acc=0.7317222356796265, loss=0.6679170727729797
test: epoch 143, loss 1.3930811882019043, acc=0.4277777671813965, loss=1.3930811882019043
train: epoch 144, loss 0.6755170822143555, acc=0.7321666479110718, loss=0.6755170822143555
test: epoch 144, loss 1.6375641822814941, acc=0.5, loss=1.6375641822814941
train: epoch 145, loss 0.6575605273246765, acc=0.738111138343811, loss=0.6575605273246765
test: epoch 145, loss 1.1089853048324585, acc=0.5083333253860474, loss=1.1089853048324585
train: epoch 146, loss 0.6493279337882996, acc=0.7383333444595337, loss=0.6493279337882996
test: epoch 146, loss 1.3006377220153809, acc=0.43888887763023376, loss=1.3006377220153809
train: epoch 147, loss 0.6615632176399231, acc=0.7378333210945129, loss=0.6615632176399231
test: epoch 147, loss 1.4743715524673462, acc=0.44999998807907104, loss=1.4743715524673462
train: epoch 148, loss 0.6739936470985413, acc=0.730222225189209, loss=0.6739936470985413
test: epoch 148, loss 1.3014302253723145, acc=0.4472222328186035, loss=1.3014302253723145
train: epoch 149, loss 0.6425968408584595, acc=0.7465000152587891, loss=0.6425968408584595
test: epoch 149, loss 1.4255622625350952, acc=0.5083333253860474, loss=1.4255622625350952
train: epoch 150, loss 0.6462124586105347, acc=0.7463889122009277, loss=0.6462124586105347
test: epoch 150, loss 1.2317649126052856, acc=0.5083333253860474, loss=1.2317649126052856
