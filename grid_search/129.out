# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1.5", "--temp_decay=1", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1890046877, receiver_embed_dim=128, save_run=0, temp_decay=1.0, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1890046877, receiver_embed_dim=128, save_run=False, temp_decay=1.0, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.526599645614624, acc=0.04977777600288391, loss=3.526599645614624
test: epoch 1, loss 3.3102328777313232, acc=0.05833333358168602, loss=3.3102328777313232
train: epoch 2, loss 3.4575161933898926, acc=0.05550000071525574, loss=3.4575161933898926
test: epoch 2, loss 3.171792984008789, acc=0.07777778059244156, loss=3.171792984008789
train: epoch 3, loss 3.2229163646698, acc=0.07494444400072098, loss=3.2229163646698
test: epoch 3, loss 4.905071258544922, acc=0.03611111268401146, loss=4.905071258544922
train: epoch 4, loss 2.8430566787719727, acc=0.11977777630090714, loss=2.8430566787719727
test: epoch 4, loss 5.641300201416016, acc=0.03611111268401146, loss=5.641300201416016
train: epoch 5, loss 2.629702568054199, acc=0.14527778327465057, loss=2.629702568054199
test: epoch 5, loss 5.6306657791137695, acc=0.05000000074505806, loss=5.6306657791137695
train: epoch 6, loss 2.5057566165924072, acc=0.162222221493721, loss=2.5057566165924072
test: epoch 6, loss 5.7447829246521, acc=0.04722222313284874, loss=5.7447829246521
train: epoch 7, loss 2.4178764820098877, acc=0.18577778339385986, loss=2.4178764820098877
test: epoch 7, loss 5.386496543884277, acc=0.03888889029622078, loss=5.386496543884277
train: epoch 8, loss 2.3392574787139893, acc=0.20127777755260468, loss=2.3392574787139893
test: epoch 8, loss 5.458459377288818, acc=0.06388889253139496, loss=5.458459377288818
train: epoch 9, loss 2.2924063205718994, acc=0.21033333241939545, loss=2.2924063205718994
test: epoch 9, loss 5.479206562042236, acc=0.06111111119389534, loss=5.479206562042236
train: epoch 10, loss 2.2439346313476562, acc=0.2224999964237213, loss=2.2439346313476562
test: epoch 10, loss 5.3540425300598145, acc=0.06666667014360428, loss=5.3540425300598145
train: epoch 11, loss 2.200254201889038, acc=0.2300555557012558, loss=2.200254201889038
test: epoch 11, loss 5.583563327789307, acc=0.0555555559694767, loss=5.583563327789307
train: epoch 12, loss 2.1623449325561523, acc=0.24194444715976715, loss=2.1623449325561523
test: epoch 12, loss 5.563554286956787, acc=0.0694444477558136, loss=5.563554286956787
train: epoch 13, loss 2.1308107376098633, acc=0.25494444370269775, loss=2.1308107376098633
test: epoch 13, loss 5.581116676330566, acc=0.0833333358168602, loss=5.581116676330566
train: epoch 14, loss 2.081559419631958, acc=0.26455554366111755, loss=2.081559419631958
test: epoch 14, loss 5.53239107131958, acc=0.06388889253139496, loss=5.53239107131958
train: epoch 15, loss 2.068167209625244, acc=0.26738888025283813, loss=2.068167209625244
test: epoch 15, loss 5.609594821929932, acc=0.0833333358168602, loss=5.609594821929932
train: epoch 16, loss 2.0494327545166016, acc=0.27355554699897766, loss=2.0494327545166016
test: epoch 16, loss 5.820676326751709, acc=0.07777778059244156, loss=5.820676326751709
train: epoch 17, loss 2.007347822189331, acc=0.28488889336586, loss=2.007347822189331
test: epoch 17, loss 5.7372307777404785, acc=0.0833333358168602, loss=5.7372307777404785
train: epoch 18, loss 1.9875677824020386, acc=0.29011112451553345, loss=1.9875677824020386
test: epoch 18, loss 5.843877792358398, acc=0.0972222238779068, loss=5.843877792358398
train: epoch 19, loss 1.9653626680374146, acc=0.2994999885559082, loss=1.9653626680374146
test: epoch 19, loss 5.842146873474121, acc=0.0833333358168602, loss=5.842146873474121
train: epoch 20, loss 1.9367151260375977, acc=0.3070000112056732, loss=1.9367151260375977
test: epoch 20, loss 5.663809299468994, acc=0.09166666865348816, loss=5.663809299468994
train: epoch 21, loss 1.9074089527130127, acc=0.3182777762413025, loss=1.9074089527130127
test: epoch 21, loss 5.750508785247803, acc=0.07500000298023224, loss=5.750508785247803
train: epoch 22, loss 1.8929530382156372, acc=0.3256111145019531, loss=1.8929530382156372
test: epoch 22, loss 5.355252742767334, acc=0.0972222238779068, loss=5.355252742767334
train: epoch 23, loss 1.8534239530563354, acc=0.33088889718055725, loss=1.8534239530563354
test: epoch 23, loss 5.213528156280518, acc=0.11388888955116272, loss=5.213528156280518
train: epoch 24, loss 1.8455498218536377, acc=0.32855555415153503, loss=1.8455498218536377
test: epoch 24, loss 5.128011703491211, acc=0.0972222238779068, loss=5.128011703491211
train: epoch 25, loss 1.8222253322601318, acc=0.34433332085609436, loss=1.8222253322601318
test: epoch 25, loss 4.9367899894714355, acc=0.09166666865348816, loss=4.9367899894714355
train: epoch 26, loss 1.7882990837097168, acc=0.35394445061683655, loss=1.7882990837097168
test: epoch 26, loss 4.631083011627197, acc=0.09166666865348816, loss=4.631083011627197
train: epoch 27, loss 1.7990020513534546, acc=0.3573888838291168, loss=1.7990020513534546
test: epoch 27, loss 4.5458855628967285, acc=0.11944444477558136, loss=4.5458855628967285
train: epoch 28, loss 1.747535228729248, acc=0.3689444363117218, loss=1.747535228729248
test: epoch 28, loss 4.338094234466553, acc=0.12222222238779068, loss=4.338094234466553
train: epoch 29, loss 1.7333946228027344, acc=0.38038888573646545, loss=1.7333946228027344
test: epoch 29, loss 4.2152581214904785, acc=0.11666666716337204, loss=4.2152581214904785
train: epoch 30, loss 1.7135260105133057, acc=0.3882777690887451, loss=1.7135260105133057
test: epoch 30, loss 4.0467915534973145, acc=0.14166666567325592, loss=4.0467915534973145
train: epoch 31, loss 1.6873457431793213, acc=0.394388884305954, loss=1.6873457431793213
test: epoch 31, loss 4.123839855194092, acc=0.11944444477558136, loss=4.123839855194092
train: epoch 32, loss 1.6579025983810425, acc=0.4082777798175812, loss=1.6579025983810425
test: epoch 32, loss 4.020673751831055, acc=0.12777778506278992, loss=4.020673751831055
train: epoch 33, loss 1.6321674585342407, acc=0.4134444296360016, loss=1.6321674585342407
test: epoch 33, loss 3.8873162269592285, acc=0.14166666567325592, loss=3.8873162269592285
train: epoch 34, loss 1.6076709032058716, acc=0.42677778005599976, loss=1.6076709032058716
test: epoch 34, loss 3.790274143218994, acc=0.14722222089767456, loss=3.790274143218994
train: epoch 35, loss 1.5874574184417725, acc=0.43227776885032654, loss=1.5874574184417725
test: epoch 35, loss 3.4728646278381348, acc=0.16111111640930176, loss=3.4728646278381348
train: epoch 36, loss 1.5554410219192505, acc=0.44394445419311523, loss=1.5554410219192505
test: epoch 36, loss 3.457749605178833, acc=0.13611111044883728, loss=3.457749605178833
train: epoch 37, loss 1.5320215225219727, acc=0.45616665482521057, loss=1.5320215225219727
test: epoch 37, loss 3.408545732498169, acc=0.16944444179534912, loss=3.408545732498169
train: epoch 38, loss 1.5059689283370972, acc=0.46399998664855957, loss=1.5059689283370972
test: epoch 38, loss 3.2956807613372803, acc=0.18333333730697632, loss=3.2956807613372803
train: epoch 39, loss 1.4745283126831055, acc=0.4782777726650238, loss=1.4745283126831055
test: epoch 39, loss 3.1659438610076904, acc=0.19166666269302368, loss=3.1659438610076904
train: epoch 40, loss 1.4568384885787964, acc=0.49238887429237366, loss=1.4568384885787964
test: epoch 40, loss 3.0937294960021973, acc=0.21388888359069824, loss=3.0937294960021973
train: epoch 41, loss 1.438554286956787, acc=0.4977777898311615, loss=1.438554286956787
test: epoch 41, loss 2.9225258827209473, acc=0.21111111342906952, loss=2.9225258827209473
train: epoch 42, loss 1.4057519435882568, acc=0.51583331823349, loss=1.4057519435882568
test: epoch 42, loss 2.9823155403137207, acc=0.2361111044883728, loss=2.9823155403137207
train: epoch 43, loss 1.3590456247329712, acc=0.5291110873222351, loss=1.3590456247329712
test: epoch 43, loss 2.876331090927124, acc=0.20277777314186096, loss=2.876331090927124
train: epoch 44, loss 1.333720326423645, acc=0.5429999828338623, loss=1.333720326423645
test: epoch 44, loss 2.990323066711426, acc=0.19722221791744232, loss=2.990323066711426
train: epoch 45, loss 1.3085607290267944, acc=0.5578333139419556, loss=1.3085607290267944
test: epoch 45, loss 2.9130442142486572, acc=0.21944443881511688, loss=2.9130442142486572
train: epoch 46, loss 1.2709897756576538, acc=0.5698333382606506, loss=1.2709897756576538
test: epoch 46, loss 2.915666341781616, acc=0.22777777910232544, loss=2.915666341781616
train: epoch 47, loss 1.243524432182312, acc=0.5772222280502319, loss=1.243524432182312
test: epoch 47, loss 2.940861463546753, acc=0.20277777314186096, loss=2.940861463546753
train: epoch 48, loss 1.220819115638733, acc=0.5913888812065125, loss=1.220819115638733
test: epoch 48, loss 2.9321811199188232, acc=0.21111111342906952, loss=2.9321811199188232
train: epoch 49, loss 1.1888518333435059, acc=0.6030555367469788, loss=1.1888518333435059
test: epoch 49, loss 2.820399045944214, acc=0.21944443881511688, loss=2.820399045944214
train: epoch 50, loss 1.1607158184051514, acc=0.6160555481910706, loss=1.1607158184051514
test: epoch 50, loss 2.7983639240264893, acc=0.23333333432674408, loss=2.7983639240264893
train: epoch 51, loss 1.1432780027389526, acc=0.6301110982894897, loss=1.1432780027389526
test: epoch 51, loss 2.6300296783447266, acc=0.23888888955116272, loss=2.6300296783447266
train: epoch 52, loss 1.1143763065338135, acc=0.640666663646698, loss=1.1143763065338135
test: epoch 52, loss 2.7697346210479736, acc=0.2222222238779068, loss=2.7697346210479736
train: epoch 53, loss 1.0762500762939453, acc=0.6528333425521851, loss=1.0762500762939453
test: epoch 53, loss 2.8803162574768066, acc=0.23055554926395416, loss=2.8803162574768066
train: epoch 54, loss 1.066162109375, acc=0.6621111035346985, loss=1.066162109375
test: epoch 54, loss 2.7763783931732178, acc=0.24722221493721008, loss=2.7763783931732178
train: epoch 55, loss 1.0280781984329224, acc=0.6796666383743286, loss=1.0280781984329224
test: epoch 55, loss 2.735729932785034, acc=0.24722221493721008, loss=2.735729932785034
train: epoch 56, loss 1.0263525247573853, acc=0.6850555539131165, loss=1.0263525247573853
test: epoch 56, loss 2.7871110439300537, acc=0.23888888955116272, loss=2.7871110439300537
train: epoch 57, loss 0.9717453122138977, acc=0.7003333568572998, loss=0.9717453122138977
test: epoch 57, loss 2.7054097652435303, acc=0.25, loss=2.7054097652435303
train: epoch 58, loss 0.9373343586921692, acc=0.7139444351196289, loss=0.9373343586921692
test: epoch 58, loss 2.6969797611236572, acc=0.24722221493721008, loss=2.6969797611236572
train: epoch 59, loss 0.9169604182243347, acc=0.7214999794960022, loss=0.9169604182243347
test: epoch 59, loss 2.6358001232147217, acc=0.24722221493721008, loss=2.6358001232147217
train: epoch 60, loss 0.8800792694091797, acc=0.7331110835075378, loss=0.8800792694091797
test: epoch 60, loss 2.5982260704040527, acc=0.24166665971279144, loss=2.5982260704040527
train: epoch 61, loss 0.86996990442276, acc=0.7434444427490234, loss=0.86996990442276
test: epoch 61, loss 2.619821548461914, acc=0.25, loss=2.619821548461914
train: epoch 62, loss 0.8252938389778137, acc=0.754277765750885, loss=0.8252938389778137
test: epoch 62, loss 2.6233952045440674, acc=0.25833332538604736, loss=2.6233952045440674
train: epoch 63, loss 0.7971819639205933, acc=0.7642222046852112, loss=0.7971819639205933
test: epoch 63, loss 2.5775246620178223, acc=0.25555557012557983, loss=2.5775246620178223
train: epoch 64, loss 0.7751631736755371, acc=0.7788333296775818, loss=0.7751631736755371
test: epoch 64, loss 2.5946366786956787, acc=0.25, loss=2.5946366786956787
train: epoch 65, loss 0.7720184326171875, acc=0.7832221984863281, loss=0.7720184326171875
test: epoch 65, loss 2.533923387527466, acc=0.25, loss=2.533923387527466
train: epoch 66, loss 0.7387723922729492, acc=0.792555570602417, loss=0.7387723922729492
test: epoch 66, loss 2.458599805831909, acc=0.2777777910232544, loss=2.458599805831909
train: epoch 67, loss 0.7180303335189819, acc=0.8066666722297668, loss=0.7180303335189819
test: epoch 67, loss 2.48919677734375, acc=0.27222222089767456, loss=2.48919677734375
train: epoch 68, loss 0.6847324371337891, acc=0.8146111369132996, loss=0.6847324371337891
test: epoch 68, loss 2.530797004699707, acc=0.2666666805744171, loss=2.530797004699707
train: epoch 69, loss 0.6623952984809875, acc=0.8221666812896729, loss=0.6623952984809875
test: epoch 69, loss 2.4712107181549072, acc=0.2888889014720917, loss=2.4712107181549072
train: epoch 70, loss 0.6463421583175659, acc=0.8327777981758118, loss=0.6463421583175659
test: epoch 70, loss 2.5284125804901123, acc=0.2638888955116272, loss=2.5284125804901123
train: epoch 71, loss 0.6253623962402344, acc=0.8342221975326538, loss=0.6253623962402344
test: epoch 71, loss 2.5542120933532715, acc=0.25, loss=2.5542120933532715
train: epoch 72, loss 0.592719554901123, acc=0.8442222476005554, loss=0.592719554901123
test: epoch 72, loss 2.423992872238159, acc=0.27222222089767456, loss=2.423992872238159
train: epoch 73, loss 0.5744112133979797, acc=0.850777804851532, loss=0.5744112133979797
test: epoch 73, loss 2.471830368041992, acc=0.2666666805744171, loss=2.471830368041992
train: epoch 74, loss 0.5443915128707886, acc=0.8603333234786987, loss=0.5443915128707886
test: epoch 74, loss 2.490091323852539, acc=0.2611111104488373, loss=2.490091323852539
train: epoch 75, loss 0.555779218673706, acc=0.8616666793823242, loss=0.555779218673706
test: epoch 75, loss 2.519049644470215, acc=0.24722221493721008, loss=2.519049644470215
train: epoch 76, loss 0.5379931926727295, acc=0.8690000176429749, loss=0.5379931926727295
test: epoch 76, loss 2.5307064056396484, acc=0.2611111104488373, loss=2.5307064056396484
train: epoch 77, loss 0.5052866339683533, acc=0.8730000257492065, loss=0.5052866339683533
test: epoch 77, loss 2.5684709548950195, acc=0.2638888955116272, loss=2.5684709548950195
train: epoch 78, loss 0.48064643144607544, acc=0.8868333101272583, loss=0.48064643144607544
test: epoch 78, loss 2.6660966873168945, acc=0.25, loss=2.6660966873168945
train: epoch 79, loss 0.4788192808628082, acc=0.8841666579246521, loss=0.4788192808628082
test: epoch 79, loss 2.520303249359131, acc=0.25833332538604736, loss=2.520303249359131
train: epoch 80, loss 0.4672338366508484, acc=0.8918889164924622, loss=0.4672338366508484
test: epoch 80, loss 2.643272876739502, acc=0.24722221493721008, loss=2.643272876739502
train: epoch 81, loss 0.43077945709228516, acc=0.899222195148468, loss=0.43077945709228516
test: epoch 81, loss 2.5196645259857178, acc=0.2527777850627899, loss=2.5196645259857178
train: epoch 82, loss 0.43832871317863464, acc=0.8971666693687439, loss=0.43832871317863464
test: epoch 82, loss 2.475710868835449, acc=0.27222222089767456, loss=2.475710868835449
train: epoch 83, loss 0.418826162815094, acc=0.9054999947547913, loss=0.418826162815094
test: epoch 83, loss 2.403599262237549, acc=0.2527777850627899, loss=2.403599262237549
train: epoch 84, loss 0.3918173015117645, acc=0.909333348274231, loss=0.3918173015117645
test: epoch 84, loss 2.3695385456085205, acc=0.2666666805744171, loss=2.3695385456085205
train: epoch 85, loss 0.3825244605541229, acc=0.9114999771118164, loss=0.3825244605541229
test: epoch 85, loss 2.292548418045044, acc=0.28611111640930176, loss=2.292548418045044
train: epoch 86, loss 0.3550654351711273, acc=0.918666660785675, loss=0.3550654351711273
test: epoch 86, loss 2.459052085876465, acc=0.2777777910232544, loss=2.459052085876465
train: epoch 87, loss 0.3631640374660492, acc=0.9176666736602783, loss=0.3631640374660492
test: epoch 87, loss 2.377871513366699, acc=0.2916666567325592, loss=2.377871513366699
train: epoch 88, loss 0.3434053659439087, acc=0.9208333492279053, loss=0.3434053659439087
test: epoch 88, loss 2.364379644393921, acc=0.28611111640930176, loss=2.364379644393921
train: epoch 89, loss 0.3540266752243042, acc=0.9222221970558167, loss=0.3540266752243042
test: epoch 89, loss 2.3935792446136475, acc=0.2777777910232544, loss=2.3935792446136475
train: epoch 90, loss 0.3469334542751312, acc=0.9240555763244629, loss=0.3469334542751312
test: epoch 90, loss 2.2974042892456055, acc=0.26944443583488464, loss=2.2974042892456055
train: epoch 91, loss 0.331598162651062, acc=0.9280555844306946, loss=0.331598162651062
test: epoch 91, loss 2.3215529918670654, acc=0.2944444417953491, loss=2.3215529918670654
train: epoch 92, loss 0.3240533769130707, acc=0.9285555481910706, loss=0.3240533769130707
test: epoch 92, loss 2.3291501998901367, acc=0.2888889014720917, loss=2.3291501998901367
train: epoch 93, loss 0.2943328320980072, acc=0.933555543422699, loss=0.2943328320980072
test: epoch 93, loss 2.2781777381896973, acc=0.3083333373069763, loss=2.2781777381896973
train: epoch 94, loss 0.3091210722923279, acc=0.9357222318649292, loss=0.3091210722923279
test: epoch 94, loss 2.3204426765441895, acc=0.29722222685813904, loss=2.3204426765441895
train: epoch 95, loss 0.3164469599723816, acc=0.9323333501815796, loss=0.3164469599723816
test: epoch 95, loss 2.3243520259857178, acc=0.29722222685813904, loss=2.3243520259857178
train: epoch 96, loss 0.295427531003952, acc=0.9368888735771179, loss=0.295427531003952
test: epoch 96, loss 2.242420196533203, acc=0.3027777671813965, loss=2.242420196533203
train: epoch 97, loss 0.2789995074272156, acc=0.9438333511352539, loss=0.2789995074272156
test: epoch 97, loss 2.295767307281494, acc=0.30000001192092896, loss=2.295767307281494
train: epoch 98, loss 0.2861382067203522, acc=0.9412222504615784, loss=0.2861382067203522
test: epoch 98, loss 2.2480056285858154, acc=0.2944444417953491, loss=2.2480056285858154
train: epoch 99, loss 0.24547918140888214, acc=0.9450555443763733, loss=0.24547918140888214
test: epoch 99, loss 2.187511920928955, acc=0.29722222685813904, loss=2.187511920928955
train: epoch 100, loss 0.2512097656726837, acc=0.9480555653572083, loss=0.2512097656726837
test: epoch 100, loss 2.0926566123962402, acc=0.3027777671813965, loss=2.0926566123962402
train: epoch 101, loss 0.2581106126308441, acc=0.9446666836738586, loss=0.2581106126308441
test: epoch 101, loss 2.1569509506225586, acc=0.3222222328186035, loss=2.1569509506225586
train: epoch 102, loss 0.24631507694721222, acc=0.9465000033378601, loss=0.24631507694721222
test: epoch 102, loss 2.0981099605560303, acc=0.3027777671813965, loss=2.0981099605560303
train: epoch 103, loss 0.2297714650630951, acc=0.9518888592720032, loss=0.2297714650630951
test: epoch 103, loss 2.081347942352295, acc=0.31388887763023376, loss=2.081347942352295
train: epoch 104, loss 0.2330840826034546, acc=0.9501110911369324, loss=0.2330840826034546
test: epoch 104, loss 2.0781238079071045, acc=0.3055555522441864, loss=2.0781238079071045
train: epoch 105, loss 0.24125544726848602, acc=0.9524999856948853, loss=0.24125544726848602
test: epoch 105, loss 2.079075813293457, acc=0.31388887763023376, loss=2.079075813293457
train: epoch 106, loss 0.23957602679729462, acc=0.9507777690887451, loss=0.23957602679729462
test: epoch 106, loss 2.146028995513916, acc=0.3166666626930237, loss=2.146028995513916
train: epoch 107, loss 0.23586015403270721, acc=0.9523888826370239, loss=0.23586015403270721
test: epoch 107, loss 2.114090919494629, acc=0.32499998807907104, loss=2.114090919494629
train: epoch 108, loss 0.2154429405927658, acc=0.957611083984375, loss=0.2154429405927658
test: epoch 108, loss 2.0578157901763916, acc=0.3194444477558136, loss=2.0578157901763916
train: epoch 109, loss 0.20800098776817322, acc=0.9568333625793457, loss=0.20800098776817322
test: epoch 109, loss 2.1173007488250732, acc=0.32499998807907104, loss=2.1173007488250732
train: epoch 110, loss 0.22181609272956848, acc=0.9555555582046509, loss=0.22181609272956848
test: epoch 110, loss 2.140183210372925, acc=0.3166666626930237, loss=2.140183210372925
train: epoch 111, loss 0.20906198024749756, acc=0.9559999704360962, loss=0.20906198024749756
test: epoch 111, loss 2.075207233428955, acc=0.3055555522441864, loss=2.075207233428955
train: epoch 112, loss 0.2091795802116394, acc=0.9573333263397217, loss=0.2091795802116394
test: epoch 112, loss 2.0719916820526123, acc=0.3083333373069763, loss=2.0719916820526123
train: epoch 113, loss 0.20313286781311035, acc=0.9567777514457703, loss=0.20313286781311035
test: epoch 113, loss 2.1296298503875732, acc=0.3055555522441864, loss=2.1296298503875732
train: epoch 114, loss 0.1845417320728302, acc=0.9596111178398132, loss=0.1845417320728302
test: epoch 114, loss 2.01588773727417, acc=0.3083333373069763, loss=2.01588773727417
train: epoch 115, loss 0.19329698383808136, acc=0.9609444737434387, loss=0.19329698383808136
test: epoch 115, loss 2.0188920497894287, acc=0.3222222328186035, loss=2.0188920497894287
train: epoch 116, loss 0.19023284316062927, acc=0.9599999785423279, loss=0.19023284316062927
test: epoch 116, loss 2.012632369995117, acc=0.3305555582046509, loss=2.012632369995117
train: epoch 117, loss 0.18874071538448334, acc=0.9618889093399048, loss=0.18874071538448334
test: epoch 117, loss 1.989633560180664, acc=0.3222222328186035, loss=1.989633560180664
train: epoch 118, loss 0.18687155842781067, acc=0.961555540561676, loss=0.18687155842781067
test: epoch 118, loss 1.9856538772583008, acc=0.3222222328186035, loss=1.9856538772583008
train: epoch 119, loss 0.18683098256587982, acc=0.9623333215713501, loss=0.18683098256587982
test: epoch 119, loss 1.976800560951233, acc=0.3333333432674408, loss=1.976800560951233
train: epoch 120, loss 0.17882417142391205, acc=0.9628888964653015, loss=0.17882417142391205
test: epoch 120, loss 2.002185344696045, acc=0.32499998807907104, loss=2.002185344696045
train: epoch 121, loss 0.1770751029253006, acc=0.9638888835906982, loss=0.1770751029253006
test: epoch 121, loss 1.9519765377044678, acc=0.3583333194255829, loss=1.9519765377044678
train: epoch 122, loss 0.1695987582206726, acc=0.9636111259460449, loss=0.1695987582206726
test: epoch 122, loss 1.9437886476516724, acc=0.3194444477558136, loss=1.9437886476516724
train: epoch 123, loss 0.16866058111190796, acc=0.9651666879653931, loss=0.16866058111190796
test: epoch 123, loss 1.9976766109466553, acc=0.3055555522441864, loss=1.9976766109466553
train: epoch 124, loss 0.17516706883907318, acc=0.964888870716095, loss=0.17516706883907318
test: epoch 124, loss 2.0282740592956543, acc=0.3194444477558136, loss=2.0282740592956543
train: epoch 125, loss 0.16511955857276917, acc=0.9666666388511658, loss=0.16511955857276917
test: epoch 125, loss 1.9644019603729248, acc=0.3305555582046509, loss=1.9644019603729248
train: epoch 126, loss 0.16041579842567444, acc=0.965666651725769, loss=0.16041579842567444
test: epoch 126, loss 1.9553630352020264, acc=0.34166666865348816, loss=1.9553630352020264
train: epoch 127, loss 0.15169861912727356, acc=0.9682222008705139, loss=0.15169861912727356
test: epoch 127, loss 1.9760314226150513, acc=0.3333333432674408, loss=1.9760314226150513
train: epoch 128, loss 0.1613806188106537, acc=0.9682222008705139, loss=0.1613806188106537
test: epoch 128, loss 1.9189155101776123, acc=0.3638888895511627, loss=1.9189155101776123
train: epoch 129, loss 0.15083976089954376, acc=0.967555582523346, loss=0.15083976089954376
test: epoch 129, loss 1.9179189205169678, acc=0.33888888359069824, loss=1.9179189205169678
train: epoch 130, loss 0.1519642472267151, acc=0.9685555696487427, loss=0.1519642472267151
test: epoch 130, loss 1.9038913249969482, acc=0.3361110985279083, loss=1.9038913249969482
train: epoch 131, loss 0.14367042481899261, acc=0.9697777628898621, loss=0.14367042481899261
test: epoch 131, loss 2.016104221343994, acc=0.3222222328186035, loss=2.016104221343994
train: epoch 132, loss 0.1591057926416397, acc=0.9701666831970215, loss=0.1591057926416397
test: epoch 132, loss 1.9187039136886597, acc=0.34166666865348816, loss=1.9187039136886597
train: epoch 133, loss 0.14718495309352875, acc=0.9713333249092102, loss=0.14718495309352875
test: epoch 133, loss 1.9148503541946411, acc=0.36666667461395264, loss=1.9148503541946411
train: epoch 134, loss 0.1581386923789978, acc=0.9693333506584167, loss=0.1581386923789978
test: epoch 134, loss 1.9082527160644531, acc=0.3638888895511627, loss=1.9082527160644531
train: epoch 135, loss 0.1425175666809082, acc=0.9710000157356262, loss=0.1425175666809082
test: epoch 135, loss 1.8912713527679443, acc=0.3611111044883728, loss=1.8912713527679443
train: epoch 136, loss 0.1427319496870041, acc=0.9697777628898621, loss=0.1427319496870041
test: epoch 136, loss 1.8844125270843506, acc=0.32499998807907104, loss=1.8844125270843506
train: epoch 137, loss 0.1469261199235916, acc=0.9698333144187927, loss=0.1469261199235916
test: epoch 137, loss 1.950384259223938, acc=0.3333333432674408, loss=1.950384259223938
train: epoch 138, loss 0.1467665135860443, acc=0.972000002861023, loss=0.1467665135860443
test: epoch 138, loss 1.8897725343704224, acc=0.35555556416511536, loss=1.8897725343704224
train: epoch 139, loss 0.14424052834510803, acc=0.9736111164093018, loss=0.14424052834510803
test: epoch 139, loss 1.8311089277267456, acc=0.375, loss=1.8311089277267456
train: epoch 140, loss 0.12913991510868073, acc=0.9739444255828857, loss=0.12913991510868073
test: epoch 140, loss 1.9071255922317505, acc=0.3777777850627899, loss=1.9071255922317505
train: epoch 141, loss 0.1298389583826065, acc=0.9740555286407471, loss=0.1298389583826065
test: epoch 141, loss 1.9363662004470825, acc=0.3583333194255829, loss=1.9363662004470825
train: epoch 142, loss 0.12663108110427856, acc=0.9741666913032532, loss=0.12663108110427856
test: epoch 142, loss 1.8758589029312134, acc=0.3305555582046509, loss=1.8758589029312134
train: epoch 143, loss 0.1183423176407814, acc=0.9762222170829773, loss=0.1183423176407814
test: epoch 143, loss 1.9552321434020996, acc=0.33888888359069824, loss=1.9552321434020996
train: epoch 144, loss 0.12557294964790344, acc=0.9731666445732117, loss=0.12557294964790344
test: epoch 144, loss 2.004962205886841, acc=0.34166666865348816, loss=2.004962205886841
train: epoch 145, loss 0.12755301594734192, acc=0.9766111373901367, loss=0.12755301594734192
test: epoch 145, loss 1.9079692363739014, acc=0.35277777910232544, loss=1.9079692363739014
train: epoch 146, loss 0.11657782644033432, acc=0.9747222065925598, loss=0.11657782644033432
test: epoch 146, loss 1.853738784790039, acc=0.375, loss=1.853738784790039
train: epoch 147, loss 0.11429281532764435, acc=0.9769444465637207, loss=0.11429281532764435
test: epoch 147, loss 1.9297757148742676, acc=0.36666667461395264, loss=1.9297757148742676
train: epoch 148, loss 0.11779388040304184, acc=0.9750555753707886, loss=0.11779388040304184
test: epoch 148, loss 1.8370561599731445, acc=0.36666667461395264, loss=1.8370561599731445
train: epoch 149, loss 0.11424293369054794, acc=0.9760000109672546, loss=0.11424293369054794
test: epoch 149, loss 1.9138641357421875, acc=0.3194444477558136, loss=1.9138641357421875
train: epoch 150, loss 0.13423413038253784, acc=0.975944459438324, loss=0.13423413038253784
test: epoch 150, loss 1.9056427478790283, acc=0.3611111044883728, loss=1.9056427478790283
