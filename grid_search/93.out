# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=2.0", "--temp_decay=1", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=559953266, receiver_embed_dim=64, save_run=0, temp_decay=1.0, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=559953266, receiver_embed_dim=64, save_run=False, temp_decay=1.0, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.5211989879608154, acc=0.05194444581866264, loss=3.5211989879608154
test: epoch 1, loss 3.309481620788574, acc=0.07500000298023224, loss=3.309481620788574
train: epoch 2, loss 3.417963981628418, acc=0.053555555641651154, loss=3.417963981628418
test: epoch 2, loss 3.371934175491333, acc=0.06111111119389534, loss=3.371934175491333
train: epoch 3, loss 2.968993663787842, acc=0.11144444346427917, loss=2.968993663787842
test: epoch 3, loss 7.364917278289795, acc=0.03333333507180214, loss=7.364917278289795
train: epoch 4, loss 2.5383732318878174, acc=0.17499999701976776, loss=2.5383732318878174
test: epoch 4, loss 9.130472183227539, acc=0.03888889029622078, loss=9.130472183227539
train: epoch 5, loss 2.3186962604522705, acc=0.21627777814865112, loss=2.3186962604522705
test: epoch 5, loss 8.832306861877441, acc=0.04444444552063942, loss=8.832306861877441
train: epoch 6, loss 2.183868169784546, acc=0.24622222781181335, loss=2.183868169784546
test: epoch 6, loss 8.852242469787598, acc=0.0416666679084301, loss=8.852242469787598
train: epoch 7, loss 2.0926411151885986, acc=0.2708333432674408, loss=2.0926411151885986
test: epoch 7, loss 8.78539752960205, acc=0.05000000074505806, loss=8.78539752960205
train: epoch 8, loss 2.015599250793457, acc=0.2911111116409302, loss=2.015599250793457
test: epoch 8, loss 8.468538284301758, acc=0.05277777835726738, loss=8.468538284301758
train: epoch 9, loss 1.9579172134399414, acc=0.3076111078262329, loss=1.9579172134399414
test: epoch 9, loss 8.459606170654297, acc=0.06666667014360428, loss=8.459606170654297
train: epoch 10, loss 1.8981032371520996, acc=0.3213889002799988, loss=1.8981032371520996
test: epoch 10, loss 8.232637405395508, acc=0.06388889253139496, loss=8.232637405395508
train: epoch 11, loss 1.8541882038116455, acc=0.3371666669845581, loss=1.8541882038116455
test: epoch 11, loss 7.900523662567139, acc=0.07222222536802292, loss=7.900523662567139
train: epoch 12, loss 1.8137388229370117, acc=0.3484444320201874, loss=1.8137388229370117
test: epoch 12, loss 8.214486122131348, acc=0.0833333358168602, loss=8.214486122131348
train: epoch 13, loss 1.7748829126358032, acc=0.362888902425766, loss=1.7748829126358032
test: epoch 13, loss 8.226543426513672, acc=0.0972222238779068, loss=8.226543426513672
train: epoch 14, loss 1.7439641952514648, acc=0.37477776408195496, loss=1.7439641952514648
test: epoch 14, loss 8.280569076538086, acc=0.09166666865348816, loss=8.280569076538086
train: epoch 15, loss 1.7015976905822754, acc=0.3880000114440918, loss=1.7015976905822754
test: epoch 15, loss 8.61926555633545, acc=0.0833333358168602, loss=8.61926555633545
train: epoch 16, loss 1.679893970489502, acc=0.39988890290260315, loss=1.679893970489502
test: epoch 16, loss 8.651383399963379, acc=0.0972222238779068, loss=8.651383399963379
train: epoch 17, loss 1.6480110883712769, acc=0.410444438457489, loss=1.6480110883712769
test: epoch 17, loss 8.923380851745605, acc=0.1111111119389534, loss=8.923380851745605
train: epoch 18, loss 1.6209275722503662, acc=0.4157777726650238, loss=1.6209275722503662
test: epoch 18, loss 8.896428108215332, acc=0.09444444626569748, loss=8.896428108215332
train: epoch 19, loss 1.591861605644226, acc=0.43094444274902344, loss=1.591861605644226
test: epoch 19, loss 9.005293846130371, acc=0.10000000149011612, loss=9.005293846130371
train: epoch 20, loss 1.5579955577850342, acc=0.43666666746139526, loss=1.5579955577850342
test: epoch 20, loss 8.98786735534668, acc=0.10277777910232544, loss=8.98786735534668
train: epoch 21, loss 1.535557508468628, acc=0.4487777650356293, loss=1.535557508468628
test: epoch 21, loss 8.867365837097168, acc=0.10277777910232544, loss=8.867365837097168
train: epoch 22, loss 1.5074567794799805, acc=0.45988887548446655, loss=1.5074567794799805
test: epoch 22, loss 8.50912094116211, acc=0.0972222238779068, loss=8.50912094116211
train: epoch 23, loss 1.470131754875183, acc=0.47138887643814087, loss=1.470131754875183
test: epoch 23, loss 7.677631378173828, acc=0.10000000149011612, loss=7.677631378173828
train: epoch 24, loss 1.4375455379486084, acc=0.480222225189209, loss=1.4375455379486084
test: epoch 24, loss 7.199851036071777, acc=0.10555555671453476, loss=7.199851036071777
train: epoch 25, loss 1.421598196029663, acc=0.4905555546283722, loss=1.421598196029663
test: epoch 25, loss 7.04100227355957, acc=0.13055555522441864, loss=7.04100227355957
train: epoch 26, loss 1.3940068483352661, acc=0.5016666650772095, loss=1.3940068483352661
test: epoch 26, loss 6.898660182952881, acc=0.1111111119389534, loss=6.898660182952881
train: epoch 27, loss 1.3636101484298706, acc=0.519611120223999, loss=1.3636101484298706
test: epoch 27, loss 6.699907302856445, acc=0.13333334028720856, loss=6.699907302856445
train: epoch 28, loss 1.3389989137649536, acc=0.5275555849075317, loss=1.3389989137649536
test: epoch 28, loss 6.59765625, acc=0.14166666567325592, loss=6.59765625
train: epoch 29, loss 1.3049050569534302, acc=0.5340555310249329, loss=1.3049050569534302
test: epoch 29, loss 6.2706732749938965, acc=0.1388888955116272, loss=6.2706732749938965
train: epoch 30, loss 1.287153959274292, acc=0.5397777557373047, loss=1.287153959274292
test: epoch 30, loss 6.18174934387207, acc=0.14166666567325592, loss=6.18174934387207
train: epoch 31, loss 1.253139615058899, acc=0.5543888807296753, loss=1.253139615058899
test: epoch 31, loss 6.150727272033691, acc=0.13055555522441864, loss=6.150727272033691
train: epoch 32, loss 1.2309997081756592, acc=0.5671111345291138, loss=1.2309997081756592
test: epoch 32, loss 6.27501106262207, acc=0.13611111044883728, loss=6.27501106262207
train: epoch 33, loss 1.2026300430297852, acc=0.5814444422721863, loss=1.2026300430297852
test: epoch 33, loss 6.138676643371582, acc=0.14166666567325592, loss=6.138676643371582
train: epoch 34, loss 1.1747549772262573, acc=0.5902777910232544, loss=1.1747549772262573
test: epoch 34, loss 6.122964382171631, acc=0.15000000596046448, loss=6.122964382171631
train: epoch 35, loss 1.1427735090255737, acc=0.6012222170829773, loss=1.1427735090255737
test: epoch 35, loss 6.223221778869629, acc=0.1527777761220932, loss=6.223221778869629
train: epoch 36, loss 1.1212300062179565, acc=0.6140555739402771, loss=1.1212300062179565
test: epoch 36, loss 5.873740196228027, acc=0.17499999701976776, loss=5.873740196228027
train: epoch 37, loss 1.1074612140655518, acc=0.6236666440963745, loss=1.1074612140655518
test: epoch 37, loss 5.854064464569092, acc=0.19166666269302368, loss=5.854064464569092
train: epoch 38, loss 1.0652973651885986, acc=0.6394444704055786, loss=1.0652973651885986
test: epoch 38, loss 5.660578727722168, acc=0.19166666269302368, loss=5.660578727722168
train: epoch 39, loss 1.040919303894043, acc=0.656333327293396, loss=1.040919303894043
test: epoch 39, loss 5.5697174072265625, acc=0.20277777314186096, loss=5.5697174072265625
train: epoch 40, loss 0.9919289350509644, acc=0.6660555601119995, loss=0.9919289350509644
test: epoch 40, loss 5.581161975860596, acc=0.20555555820465088, loss=5.581161975860596
train: epoch 41, loss 0.9749550223350525, acc=0.6770555377006531, loss=0.9749550223350525
test: epoch 41, loss 5.37530517578125, acc=0.20555555820465088, loss=5.37530517578125
train: epoch 42, loss 0.9378693699836731, acc=0.691277801990509, loss=0.9378693699836731
test: epoch 42, loss 5.59207010269165, acc=0.20000000298023224, loss=5.59207010269165
train: epoch 43, loss 0.9177583456039429, acc=0.7021666765213013, loss=0.9177583456039429
test: epoch 43, loss 5.708511829376221, acc=0.21944443881511688, loss=5.708511829376221
train: epoch 44, loss 0.8909772038459778, acc=0.7224444150924683, loss=0.8909772038459778
test: epoch 44, loss 5.605262279510498, acc=0.22499999403953552, loss=5.605262279510498
train: epoch 45, loss 0.8390443325042725, acc=0.7419999837875366, loss=0.8390443325042725
test: epoch 45, loss 5.924280166625977, acc=0.24166665971279144, loss=5.924280166625977
train: epoch 46, loss 0.816041886806488, acc=0.749666690826416, loss=0.816041886806488
test: epoch 46, loss 5.980098724365234, acc=0.24444444477558136, loss=5.980098724365234
train: epoch 47, loss 0.7795789241790771, acc=0.7662222385406494, loss=0.7795789241790771
test: epoch 47, loss 5.8733062744140625, acc=0.24722221493721008, loss=5.8733062744140625
train: epoch 48, loss 0.7573148608207703, acc=0.7821111083030701, loss=0.7573148608207703
test: epoch 48, loss 5.825320720672607, acc=0.2527777850627899, loss=5.825320720672607
train: epoch 49, loss 0.7241238355636597, acc=0.7953888773918152, loss=0.7241238355636597
test: epoch 49, loss 5.736748695373535, acc=0.24722221493721008, loss=5.736748695373535
train: epoch 50, loss 0.6810307502746582, acc=0.8102777600288391, loss=0.6810307502746582
test: epoch 50, loss 5.615842342376709, acc=0.25555557012557983, loss=5.615842342376709
train: epoch 51, loss 0.6721159219741821, acc=0.8182777762413025, loss=0.6721159219741821
test: epoch 51, loss 5.4920334815979, acc=0.2611111104488373, loss=5.4920334815979
train: epoch 52, loss 0.6230694651603699, acc=0.8318889141082764, loss=0.6230694651603699
test: epoch 52, loss 5.71755838394165, acc=0.2611111104488373, loss=5.71755838394165
train: epoch 53, loss 0.6021325588226318, acc=0.8429999947547913, loss=0.6021325588226318
test: epoch 53, loss 5.584012031555176, acc=0.2611111104488373, loss=5.584012031555176
train: epoch 54, loss 0.5622863173484802, acc=0.8515555262565613, loss=0.5622863173484802
test: epoch 54, loss 5.42283821105957, acc=0.25, loss=5.42283821105957
train: epoch 55, loss 0.5337706804275513, acc=0.8644999861717224, loss=0.5337706804275513
test: epoch 55, loss 5.423250675201416, acc=0.26944443583488464, loss=5.423250675201416
train: epoch 56, loss 0.5091700553894043, acc=0.8740555644035339, loss=0.5091700553894043
test: epoch 56, loss 5.4987592697143555, acc=0.26944443583488464, loss=5.4987592697143555
train: epoch 57, loss 0.490473210811615, acc=0.882611095905304, loss=0.490473210811615
test: epoch 57, loss 5.426393032073975, acc=0.28333333134651184, loss=5.426393032073975
train: epoch 58, loss 0.46093782782554626, acc=0.8886666893959045, loss=0.46093782782554626
test: epoch 58, loss 5.49416446685791, acc=0.2916666567325592, loss=5.49416446685791
train: epoch 59, loss 0.4291515350341797, acc=0.9010000228881836, loss=0.4291515350341797
test: epoch 59, loss 5.402103424072266, acc=0.2777777910232544, loss=5.402103424072266
train: epoch 60, loss 0.4136761426925659, acc=0.9023333191871643, loss=0.4136761426925659
test: epoch 60, loss 5.327351093292236, acc=0.2805555462837219, loss=5.327351093292236
train: epoch 61, loss 0.3884143531322479, acc=0.9108333587646484, loss=0.3884143531322479
test: epoch 61, loss 5.125908851623535, acc=0.2944444417953491, loss=5.125908851623535
train: epoch 62, loss 0.3715457618236542, acc=0.9153888821601868, loss=0.3715457618236542
test: epoch 62, loss 4.997499942779541, acc=0.29722222685813904, loss=4.997499942779541
train: epoch 63, loss 0.3593086004257202, acc=0.9203333258628845, loss=0.3593086004257202
test: epoch 63, loss 5.095961570739746, acc=0.28611111640930176, loss=5.095961570739746
train: epoch 64, loss 0.33332356810569763, acc=0.9236666560173035, loss=0.33332356810569763
test: epoch 64, loss 4.91713285446167, acc=0.28333333134651184, loss=4.91713285446167
train: epoch 65, loss 0.3189730644226074, acc=0.9301111102104187, loss=0.3189730644226074
test: epoch 65, loss 4.869694232940674, acc=0.2777777910232544, loss=4.869694232940674
train: epoch 66, loss 0.3107977509498596, acc=0.933555543422699, loss=0.3107977509498596
test: epoch 66, loss 4.791624546051025, acc=0.28611111640930176, loss=4.791624546051025
train: epoch 67, loss 0.29640766978263855, acc=0.9339444637298584, loss=0.29640766978263855
test: epoch 67, loss 4.733900547027588, acc=0.2805555462837219, loss=4.733900547027588
train: epoch 68, loss 0.28423789143562317, acc=0.9393333196640015, loss=0.28423789143562317
test: epoch 68, loss 4.681593418121338, acc=0.2888889014720917, loss=4.681593418121338
train: epoch 69, loss 0.2701437771320343, acc=0.9421666860580444, loss=0.2701437771320343
test: epoch 69, loss 4.624423503875732, acc=0.28611111640930176, loss=4.624423503875732
train: epoch 70, loss 0.25614506006240845, acc=0.945277750492096, loss=0.25614506006240845
test: epoch 70, loss 4.514594078063965, acc=0.2944444417953491, loss=4.514594078063965
train: epoch 71, loss 0.23571355640888214, acc=0.9481666684150696, loss=0.23571355640888214
test: epoch 71, loss 4.2371320724487305, acc=0.3083333373069763, loss=4.2371320724487305
train: epoch 72, loss 0.24604713916778564, acc=0.9483333230018616, loss=0.24604713916778564
test: epoch 72, loss 4.0602216720581055, acc=0.30000001192092896, loss=4.0602216720581055
train: epoch 73, loss 0.22551307082176208, acc=0.9506666660308838, loss=0.22551307082176208
test: epoch 73, loss 4.111286163330078, acc=0.3083333373069763, loss=4.111286163330078
train: epoch 74, loss 0.20562350749969482, acc=0.9552222490310669, loss=0.20562350749969482
test: epoch 74, loss 4.07469367980957, acc=0.3055555522441864, loss=4.07469367980957
train: epoch 75, loss 0.21956351399421692, acc=0.956333339214325, loss=0.21956351399421692
test: epoch 75, loss 4.067149639129639, acc=0.30000001192092896, loss=4.067149639129639
train: epoch 76, loss 0.20234736800193787, acc=0.9561111330986023, loss=0.20234736800193787
test: epoch 76, loss 3.913322687149048, acc=0.29722222685813904, loss=3.913322687149048
train: epoch 77, loss 0.21090665459632874, acc=0.9557777643203735, loss=0.21090665459632874
test: epoch 77, loss 3.9469361305236816, acc=0.3166666626930237, loss=3.9469361305236816
train: epoch 78, loss 0.1944470852613449, acc=0.9621111154556274, loss=0.1944470852613449
test: epoch 78, loss 3.8782899379730225, acc=0.29722222685813904, loss=3.8782899379730225
train: epoch 79, loss 0.17829012870788574, acc=0.9633888602256775, loss=0.17829012870788574
test: epoch 79, loss 3.8890676498413086, acc=0.2916666567325592, loss=3.8890676498413086
train: epoch 80, loss 0.17706073820590973, acc=0.9647777676582336, loss=0.17706073820590973
test: epoch 80, loss 3.7254579067230225, acc=0.2916666567325592, loss=3.7254579067230225
train: epoch 81, loss 0.17338815331459045, acc=0.965666651725769, loss=0.17338815331459045
test: epoch 81, loss 3.712348222732544, acc=0.30000001192092896, loss=3.712348222732544
train: epoch 82, loss 0.16717574000358582, acc=0.965499997138977, loss=0.16717574000358582
test: epoch 82, loss 3.660543203353882, acc=0.30000001192092896, loss=3.660543203353882
train: epoch 83, loss 0.16667895019054413, acc=0.9656111001968384, loss=0.16667895019054413
test: epoch 83, loss 3.604118824005127, acc=0.31388887763023376, loss=3.604118824005127
train: epoch 84, loss 0.15624555945396423, acc=0.9673888683319092, loss=0.15624555945396423
test: epoch 84, loss 3.626842737197876, acc=0.3166666626930237, loss=3.626842737197876
train: epoch 85, loss 0.15794920921325684, acc=0.9679444432258606, loss=0.15794920921325684
test: epoch 85, loss 3.573871612548828, acc=0.3083333373069763, loss=3.573871612548828
train: epoch 86, loss 0.1493310034275055, acc=0.9712222218513489, loss=0.1493310034275055
test: epoch 86, loss 3.628706216812134, acc=0.2944444417953491, loss=3.628706216812134
train: epoch 87, loss 0.1484401524066925, acc=0.9700555801391602, loss=0.1484401524066925
test: epoch 87, loss 3.5003461837768555, acc=0.3194444477558136, loss=3.5003461837768555
train: epoch 88, loss 0.12143699079751968, acc=0.9744444489479065, loss=0.12143699079751968
test: epoch 88, loss 3.3991923332214355, acc=0.3055555522441864, loss=3.3991923332214355
train: epoch 89, loss 0.13874104619026184, acc=0.9708333611488342, loss=0.13874104619026184
test: epoch 89, loss 3.4711689949035645, acc=0.31111112236976624, loss=3.4711689949035645
train: epoch 90, loss 0.13232648372650146, acc=0.976111114025116, loss=0.13232648372650146
test: epoch 90, loss 3.4228289127349854, acc=0.3083333373069763, loss=3.4228289127349854
train: epoch 91, loss 0.12213842570781708, acc=0.9737222194671631, loss=0.12213842570781708
test: epoch 91, loss 3.3791239261627197, acc=0.2916666567325592, loss=3.3791239261627197
train: epoch 92, loss 0.13481755554676056, acc=0.9747777581214905, loss=0.13481755554676056
test: epoch 92, loss 3.3483893871307373, acc=0.31111112236976624, loss=3.3483893871307373
train: epoch 93, loss 0.1363660842180252, acc=0.9736666679382324, loss=0.1363660842180252
test: epoch 93, loss 3.2411465644836426, acc=0.3055555522441864, loss=3.2411465644836426
train: epoch 94, loss 0.12562574446201324, acc=0.9752222299575806, loss=0.12562574446201324
test: epoch 94, loss 3.339120864868164, acc=0.3083333373069763, loss=3.339120864868164
train: epoch 95, loss 0.11278750747442245, acc=0.9769444465637207, loss=0.11278750747442245
test: epoch 95, loss 3.3373448848724365, acc=0.3194444477558136, loss=3.3373448848724365
train: epoch 96, loss 0.12209874391555786, acc=0.9792222380638123, loss=0.12209874391555786
test: epoch 96, loss 3.345599889755249, acc=0.2944444417953491, loss=3.345599889755249
train: epoch 97, loss 0.11503209918737411, acc=0.9775000214576721, loss=0.11503209918737411
test: epoch 97, loss 3.2633402347564697, acc=0.3194444477558136, loss=3.2633402347564697
train: epoch 98, loss 0.10458672046661377, acc=0.9794999957084656, loss=0.10458672046661377
test: epoch 98, loss 3.323296070098877, acc=0.30000001192092896, loss=3.323296070098877
train: epoch 99, loss 0.10993997007608414, acc=0.9787777662277222, loss=0.10993997007608414
test: epoch 99, loss 3.107834577560425, acc=0.31388887763023376, loss=3.107834577560425
train: epoch 100, loss 0.10592039674520493, acc=0.9786666631698608, loss=0.10592039674520493
test: epoch 100, loss 3.059417724609375, acc=0.2944444417953491, loss=3.059417724609375
train: epoch 101, loss 0.10186725854873657, acc=0.9794444441795349, loss=0.10186725854873657
test: epoch 101, loss 3.2023980617523193, acc=0.3194444477558136, loss=3.2023980617523193
train: epoch 102, loss 0.09482048451900482, acc=0.9807778000831604, loss=0.09482048451900482
test: epoch 102, loss 3.116769552230835, acc=0.30000001192092896, loss=3.116769552230835
train: epoch 103, loss 0.10034417361021042, acc=0.9815000295639038, loss=0.10034417361021042
test: epoch 103, loss 3.1457533836364746, acc=0.30000001192092896, loss=3.1457533836364746
train: epoch 104, loss 0.10070743411779404, acc=0.9807778000831604, loss=0.10070743411779404
test: epoch 104, loss 3.1341421604156494, acc=0.31388887763023376, loss=3.1341421604156494
train: epoch 105, loss 0.1068555936217308, acc=0.9812777638435364, loss=0.1068555936217308
test: epoch 105, loss 2.94858980178833, acc=0.3166666626930237, loss=2.94858980178833
train: epoch 106, loss 0.10618656128644943, acc=0.9824444651603699, loss=0.10618656128644943
test: epoch 106, loss 2.946044683456421, acc=0.32777777314186096, loss=2.946044683456421
train: epoch 107, loss 0.08353641629219055, acc=0.9830555319786072, loss=0.08353641629219055
test: epoch 107, loss 2.871922492980957, acc=0.29722222685813904, loss=2.871922492980957
train: epoch 108, loss 0.08733104914426804, acc=0.9833333492279053, loss=0.08733104914426804
test: epoch 108, loss 2.9354872703552246, acc=0.31388887763023376, loss=2.9354872703552246
train: epoch 109, loss 0.0931127741932869, acc=0.9824444651603699, loss=0.0931127741932869
test: epoch 109, loss 2.9078550338745117, acc=0.3083333373069763, loss=2.9078550338745117
train: epoch 110, loss 0.08268847316503525, acc=0.9838333129882812, loss=0.08268847316503525
test: epoch 110, loss 2.9801082611083984, acc=0.3055555522441864, loss=2.9801082611083984
train: epoch 111, loss 0.09390225261449814, acc=0.9828333258628845, loss=0.09390225261449814
test: epoch 111, loss 2.9826714992523193, acc=0.3166666626930237, loss=2.9826714992523193
train: epoch 112, loss 0.07929377257823944, acc=0.984666645526886, loss=0.07929377257823944
test: epoch 112, loss 2.8916335105895996, acc=0.31111112236976624, loss=2.8916335105895996
train: epoch 113, loss 0.08577830344438553, acc=0.9834444522857666, loss=0.08577830344438553
test: epoch 113, loss 2.9415624141693115, acc=0.28611111640930176, loss=2.9415624141693115
train: epoch 114, loss 0.08085211366415024, acc=0.984333336353302, loss=0.08085211366415024
test: epoch 114, loss 2.90201997756958, acc=0.3027777671813965, loss=2.90201997756958
train: epoch 115, loss 0.07920940965414047, acc=0.9854999780654907, loss=0.07920940965414047
test: epoch 115, loss 2.938511848449707, acc=0.2944444417953491, loss=2.938511848449707
train: epoch 116, loss 0.07850015163421631, acc=0.9849444627761841, loss=0.07850015163421631
test: epoch 116, loss 2.884103775024414, acc=0.31111112236976624, loss=2.884103775024414
train: epoch 117, loss 0.08030474931001663, acc=0.9856666922569275, loss=0.08030474931001663
test: epoch 117, loss 2.745159864425659, acc=0.3222222328186035, loss=2.745159864425659
train: epoch 118, loss 0.07796207815408707, acc=0.984666645526886, loss=0.07796207815408707
test: epoch 118, loss 2.6023576259613037, acc=0.32777777314186096, loss=2.6023576259613037
train: epoch 119, loss 0.08940158039331436, acc=0.9822777509689331, loss=0.08940158039331436
test: epoch 119, loss 2.680541753768921, acc=0.32777777314186096, loss=2.680541753768921
train: epoch 120, loss 0.08685029298067093, acc=0.9850555658340454, loss=0.08685029298067093
test: epoch 120, loss 2.6464178562164307, acc=0.29722222685813904, loss=2.6464178562164307
train: epoch 121, loss 0.07366926968097687, acc=0.9862777590751648, loss=0.07366926968097687
test: epoch 121, loss 2.7817647457122803, acc=0.3055555522441864, loss=2.7817647457122803
train: epoch 122, loss 0.08195868134498596, acc=0.9850000143051147, loss=0.08195868134498596
test: epoch 122, loss 2.72314453125, acc=0.3194444477558136, loss=2.72314453125
train: epoch 123, loss 0.07352368533611298, acc=0.9856111407279968, loss=0.07352368533611298
test: epoch 123, loss 2.680828809738159, acc=0.3194444477558136, loss=2.680828809738159
train: epoch 124, loss 0.06478124856948853, acc=0.9871666431427002, loss=0.06478124856948853
test: epoch 124, loss 2.703303098678589, acc=0.3333333432674408, loss=2.703303098678589
train: epoch 125, loss 0.06227761134505272, acc=0.9871666431427002, loss=0.06227761134505272
test: epoch 125, loss 2.616419553756714, acc=0.3194444477558136, loss=2.616419553756714
train: epoch 126, loss 0.06788939982652664, acc=0.9871110916137695, loss=0.06788939982652664
test: epoch 126, loss 2.7053840160369873, acc=0.32499998807907104, loss=2.7053840160369873
train: epoch 127, loss 0.06806042790412903, acc=0.9859444499015808, loss=0.06806042790412903
test: epoch 127, loss 2.7141101360321045, acc=0.3166666626930237, loss=2.7141101360321045
train: epoch 128, loss 0.06703479588031769, acc=0.9880555272102356, loss=0.06703479588031769
test: epoch 128, loss 2.6433300971984863, acc=0.3194444477558136, loss=2.6433300971984863
train: epoch 129, loss 0.06461447477340698, acc=0.9879444241523743, loss=0.06461447477340698
test: epoch 129, loss 2.5848195552825928, acc=0.3305555582046509, loss=2.5848195552825928
train: epoch 130, loss 0.06605830043554306, acc=0.9882222414016724, loss=0.06605830043554306
test: epoch 130, loss 2.565480947494507, acc=0.3055555522441864, loss=2.565480947494507
train: epoch 131, loss 0.06323052197694778, acc=0.9878333210945129, loss=0.06323052197694778
test: epoch 131, loss 2.6152167320251465, acc=0.3166666626930237, loss=2.6152167320251465
train: epoch 132, loss 0.06402841210365295, acc=0.9877777695655823, loss=0.06402841210365295
test: epoch 132, loss 2.47329044342041, acc=0.3305555582046509, loss=2.47329044342041
train: epoch 133, loss 0.06434589624404907, acc=0.988611102104187, loss=0.06434589624404907
test: epoch 133, loss 2.6030056476593018, acc=0.3305555582046509, loss=2.6030056476593018
train: epoch 134, loss 0.05204404518008232, acc=0.9903333187103271, loss=0.05204404518008232
test: epoch 134, loss 2.631054162979126, acc=0.29722222685813904, loss=2.631054162979126
train: epoch 135, loss 0.05548488348722458, acc=0.9894444346427917, loss=0.05548488348722458
test: epoch 135, loss 2.592580556869507, acc=0.3166666626930237, loss=2.592580556869507
train: epoch 136, loss 0.05507028475403786, acc=0.9891666769981384, loss=0.05507028475403786
test: epoch 136, loss 2.6095023155212402, acc=0.3027777671813965, loss=2.6095023155212402
train: epoch 137, loss 0.05895119532942772, acc=0.9896666407585144, loss=0.05895119532942772
test: epoch 137, loss 2.505758285522461, acc=0.32499998807907104, loss=2.505758285522461
train: epoch 138, loss 0.05168696492910385, acc=0.9900555610656738, loss=0.05168696492910385
test: epoch 138, loss 2.5219461917877197, acc=0.3333333432674408, loss=2.5219461917877197
train: epoch 139, loss 0.04875821992754936, acc=0.99144446849823, loss=0.04875821992754936
test: epoch 139, loss 2.6214711666107178, acc=0.3166666626930237, loss=2.6214711666107178
train: epoch 140, loss 0.05448508262634277, acc=0.9893333315849304, loss=0.05448508262634277
test: epoch 140, loss 2.5861287117004395, acc=0.3305555582046509, loss=2.5861287117004395
train: epoch 141, loss 0.051420196890830994, acc=0.9903888702392578, loss=0.051420196890830994
test: epoch 141, loss 2.6005144119262695, acc=0.3055555522441864, loss=2.6005144119262695
train: epoch 142, loss 0.052175283432006836, acc=0.988444447517395, loss=0.052175283432006836
test: epoch 142, loss 2.650265693664551, acc=0.3055555522441864, loss=2.650265693664551
train: epoch 143, loss 0.05254451930522919, acc=0.9890555739402771, loss=0.05254451930522919
test: epoch 143, loss 2.467054843902588, acc=0.32499998807907104, loss=2.467054843902588
train: epoch 144, loss 0.04864940047264099, acc=0.9900555610656738, loss=0.04864940047264099
test: epoch 144, loss 2.5032317638397217, acc=0.32499998807907104, loss=2.5032317638397217
train: epoch 145, loss 0.051913149654865265, acc=0.9912777543067932, loss=0.051913149654865265
test: epoch 145, loss 2.3628427982330322, acc=0.3166666626930237, loss=2.3628427982330322
train: epoch 146, loss 0.04346596077084541, acc=0.9912777543067932, loss=0.04346596077084541
test: epoch 146, loss 2.4126784801483154, acc=0.3472222089767456, loss=2.4126784801483154
train: epoch 147, loss 0.04630545154213905, acc=0.9913889169692993, loss=0.04630545154213905
test: epoch 147, loss 2.4969232082366943, acc=0.32499998807907104, loss=2.4969232082366943
train: epoch 148, loss 0.04331088438630104, acc=0.9916666746139526, loss=0.04331088438630104
test: epoch 148, loss 2.4540624618530273, acc=0.3083333373069763, loss=2.4540624618530273
train: epoch 149, loss 0.04868448153138161, acc=0.9915555715560913, loss=0.04868448153138161
test: epoch 149, loss 2.4687745571136475, acc=0.32777777314186096, loss=2.4687745571136475
train: epoch 150, loss 0.05024917796254158, acc=0.9913333058357239, loss=0.05024917796254158
test: epoch 150, loss 2.509190320968628, acc=0.3083333373069763, loss=2.509190320968628
