# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=100", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=1", "--one_hot=false", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_summands=2, n_symbols=100, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=153362526, receiver_embed_dim=128, temp_decay=1.0, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.9853885173797607, acc=0.08788888901472092, loss=2.9853885173797607
test: epoch 1, loss 2.7321877479553223, acc=0.1111111119389534, loss=2.7321877479553223
train: epoch 2, loss 1.752120852470398, acc=0.2984444499015808, loss=1.752120852470398
test: epoch 2, loss 1.7213884592056274, acc=0.28333333134651184, loss=1.7213884592056274
train: epoch 3, loss 1.3234543800354004, acc=0.43477776646614075, loss=1.3234543800354004
test: epoch 3, loss 1.6905364990234375, acc=0.32499998807907104, loss=1.6905364990234375
train: epoch 4, loss 1.1475125551223755, acc=0.5055000185966492, loss=1.1475125551223755
test: epoch 4, loss 1.577547550201416, acc=0.3222222328186035, loss=1.577547550201416
train: epoch 5, loss 1.0439937114715576, acc=0.5524444580078125, loss=1.0439937114715576
test: epoch 5, loss 1.5887280702590942, acc=0.375, loss=1.5887280702590942
train: epoch 6, loss 0.9533174633979797, acc=0.5874444246292114, loss=0.9533174633979797
test: epoch 6, loss 1.4004229307174683, acc=0.36666667461395264, loss=1.4004229307174683
train: epoch 7, loss 0.8882392644882202, acc=0.6172778010368347, loss=0.8882392644882202
test: epoch 7, loss 1.5182181596755981, acc=0.3888888955116272, loss=1.5182181596755981
train: epoch 8, loss 0.8124490976333618, acc=0.6483888626098633, loss=0.8124490976333618
test: epoch 8, loss 1.5793627500534058, acc=0.38333332538604736, loss=1.5793627500534058
train: epoch 9, loss 0.7512093186378479, acc=0.6711666584014893, loss=0.7512093186378479
test: epoch 9, loss 1.5376991033554077, acc=0.39722222089767456, loss=1.5376991033554077
train: epoch 10, loss 0.7351047992706299, acc=0.6779444217681885, loss=0.7351047992706299
test: epoch 10, loss 1.6955410242080688, acc=0.4055555462837219, loss=1.6955410242080688
train: epoch 11, loss 0.7136133313179016, acc=0.6821666955947876, loss=0.7136133313179016
test: epoch 11, loss 1.5507473945617676, acc=0.4416666626930237, loss=1.5507473945617676
train: epoch 12, loss 0.688949465751648, acc=0.6905555725097656, loss=0.688949465751648
test: epoch 12, loss 1.5816766023635864, acc=0.43888887763023376, loss=1.5816766023635864
train: epoch 13, loss 0.6658438444137573, acc=0.7026110887527466, loss=0.6658438444137573
test: epoch 13, loss 1.5398763418197632, acc=0.4444444477558136, loss=1.5398763418197632
train: epoch 14, loss 0.6424506902694702, acc=0.7106666564941406, loss=0.6424506902694702
test: epoch 14, loss 1.5466856956481934, acc=0.4444444477558136, loss=1.5466856956481934
train: epoch 15, loss 0.6420838236808777, acc=0.711555540561676, loss=0.6420838236808777
test: epoch 15, loss 1.6683624982833862, acc=0.44999998807907104, loss=1.6683624982833862
train: epoch 16, loss 0.6185134649276733, acc=0.7225000262260437, loss=0.6185134649276733
test: epoch 16, loss 1.6879900693893433, acc=0.4444444477558136, loss=1.6879900693893433
train: epoch 17, loss 0.6284676790237427, acc=0.7179999947547913, loss=0.6284676790237427
test: epoch 17, loss 1.3878521919250488, acc=0.4472222328186035, loss=1.3878521919250488
train: epoch 18, loss 0.6034137606620789, acc=0.7283889055252075, loss=0.6034137606620789
test: epoch 18, loss 1.504062294960022, acc=0.4416666626930237, loss=1.504062294960022
train: epoch 19, loss 0.5953424572944641, acc=0.7292777895927429, loss=0.5953424572944641
test: epoch 19, loss 1.7787128686904907, acc=0.4333333373069763, loss=1.7787128686904907
train: epoch 20, loss 0.6057445406913757, acc=0.7245000004768372, loss=0.6057445406913757
test: epoch 20, loss 1.500013828277588, acc=0.4555555582046509, loss=1.500013828277588
train: epoch 21, loss 0.6072192788124084, acc=0.7238888740539551, loss=0.6072192788124084
test: epoch 21, loss 1.6546763181686401, acc=0.4555555582046509, loss=1.6546763181686401
train: epoch 22, loss 0.5784082412719727, acc=0.7351111173629761, loss=0.5784082412719727
test: epoch 22, loss 1.5256614685058594, acc=0.4694444537162781, loss=1.5256614685058594
train: epoch 23, loss 0.5805185437202454, acc=0.737666666507721, loss=0.5805185437202454
test: epoch 23, loss 1.7937300205230713, acc=0.4555555582046509, loss=1.7937300205230713
train: epoch 24, loss 0.5757213234901428, acc=0.7375555634498596, loss=0.5757213234901428
test: epoch 24, loss 1.5802446603775024, acc=0.4861111044883728, loss=1.5802446603775024
train: epoch 25, loss 0.5670077204704285, acc=0.741944432258606, loss=0.5670077204704285
test: epoch 25, loss 1.499375820159912, acc=0.4749999940395355, loss=1.499375820159912
train: epoch 26, loss 0.541671097278595, acc=0.7508888840675354, loss=0.541671097278595
test: epoch 26, loss 1.4937744140625, acc=0.4749999940395355, loss=1.4937744140625
train: epoch 27, loss 0.5639230012893677, acc=0.7471666932106018, loss=0.5639230012893677
test: epoch 27, loss 1.4301769733428955, acc=0.4861111044883728, loss=1.4301769733428955
train: epoch 28, loss 0.5441680550575256, acc=0.7523888945579529, loss=0.5441680550575256
test: epoch 28, loss 1.628005027770996, acc=0.4861111044883728, loss=1.628005027770996
train: epoch 29, loss 0.5546035766601562, acc=0.7516666650772095, loss=0.5546035766601562
test: epoch 29, loss 1.316878318786621, acc=0.4888888895511627, loss=1.316878318786621
train: epoch 30, loss 0.5434038043022156, acc=0.7516111135482788, loss=0.5434038043022156
test: epoch 30, loss 1.3302823305130005, acc=0.4861111044883728, loss=1.3302823305130005
train: epoch 31, loss 0.5400360226631165, acc=0.7561110854148865, loss=0.5400360226631165
test: epoch 31, loss 1.4564015865325928, acc=0.4888888895511627, loss=1.4564015865325928
train: epoch 32, loss 0.5415888428688049, acc=0.7527222037315369, loss=0.5415888428688049
test: epoch 32, loss 1.5209763050079346, acc=0.4888888895511627, loss=1.5209763050079346
train: epoch 33, loss 0.5244541168212891, acc=0.7549999952316284, loss=0.5244541168212891
test: epoch 33, loss 1.5116616487503052, acc=0.49166667461395264, loss=1.5116616487503052
train: epoch 34, loss 0.5345187783241272, acc=0.7557222247123718, loss=0.5345187783241272
test: epoch 34, loss 1.6164734363555908, acc=0.49166667461395264, loss=1.6164734363555908
train: epoch 35, loss 0.5370984077453613, acc=0.7509999871253967, loss=0.5370984077453613
test: epoch 35, loss 1.4706202745437622, acc=0.46666666865348816, loss=1.4706202745437622
train: epoch 36, loss 0.5388842225074768, acc=0.7527777552604675, loss=0.5388842225074768
test: epoch 36, loss 1.5773555040359497, acc=0.49166667461395264, loss=1.5773555040359497
train: epoch 37, loss 0.5139455795288086, acc=0.7655555605888367, loss=0.5139455795288086
test: epoch 37, loss 1.4985982179641724, acc=0.4861111044883728, loss=1.4985982179641724
train: epoch 38, loss 0.5130972862243652, acc=0.761555552482605, loss=0.5130972862243652
test: epoch 38, loss 1.4672472476959229, acc=0.4722222089767456, loss=1.4672472476959229
train: epoch 39, loss 0.5091569423675537, acc=0.7663333415985107, loss=0.5091569423675537
test: epoch 39, loss 1.520145058631897, acc=0.49166667461395264, loss=1.520145058631897
train: epoch 40, loss 0.5043814778327942, acc=0.7700555324554443, loss=0.5043814778327942
test: epoch 40, loss 1.6615163087844849, acc=0.4888888895511627, loss=1.6615163087844849
train: epoch 41, loss 0.5146065950393677, acc=0.7675555348396301, loss=0.5146065950393677
test: epoch 41, loss 1.4261537790298462, acc=0.49166667461395264, loss=1.4261537790298462
train: epoch 42, loss 0.5013729929924011, acc=0.7714999914169312, loss=0.5013729929924011
test: epoch 42, loss 1.5497949123382568, acc=0.49166667461395264, loss=1.5497949123382568
train: epoch 43, loss 0.5194154977798462, acc=0.7628333568572998, loss=0.5194154977798462
test: epoch 43, loss 1.469834804534912, acc=0.49166667461395264, loss=1.469834804534912
train: epoch 44, loss 0.5218626260757446, acc=0.7611111402511597, loss=0.5218626260757446
test: epoch 44, loss 1.470212459564209, acc=0.4888888895511627, loss=1.470212459564209
train: epoch 45, loss 0.4995179772377014, acc=0.7723888754844666, loss=0.4995179772377014
test: epoch 45, loss 1.7002856731414795, acc=0.49166667461395264, loss=1.7002856731414795
train: epoch 46, loss 0.501576840877533, acc=0.7696666717529297, loss=0.501576840877533
test: epoch 46, loss 1.4811357259750366, acc=0.49166667461395264, loss=1.4811357259750366
train: epoch 47, loss 0.49431318044662476, acc=0.7745555639266968, loss=0.49431318044662476
test: epoch 47, loss 1.5969194173812866, acc=0.49166667461395264, loss=1.5969194173812866
train: epoch 48, loss 0.4991159737110138, acc=0.7746666669845581, loss=0.4991159737110138
test: epoch 48, loss 1.5670336484909058, acc=0.49166667461395264, loss=1.5670336484909058
train: epoch 49, loss 0.4894971251487732, acc=0.774222195148468, loss=0.4894971251487732
test: epoch 49, loss 1.7254855632781982, acc=0.49166667461395264, loss=1.7254855632781982
train: epoch 50, loss 0.5065329670906067, acc=0.7695000171661377, loss=0.5065329670906067
test: epoch 50, loss 1.466947078704834, acc=0.49444442987442017, loss=1.466947078704834
train: epoch 51, loss 0.4821901023387909, acc=0.7786666750907898, loss=0.4821901023387909
test: epoch 51, loss 1.4737627506256104, acc=0.4833333194255829, loss=1.4737627506256104
train: epoch 52, loss 0.489154189825058, acc=0.7749444246292114, loss=0.489154189825058
test: epoch 52, loss 1.6772316694259644, acc=0.4888888895511627, loss=1.6772316694259644
train: epoch 53, loss 0.5036042332649231, acc=0.7673333287239075, loss=0.5036042332649231
test: epoch 53, loss 1.57308828830719, acc=0.4888888895511627, loss=1.57308828830719
train: epoch 54, loss 0.47931960225105286, acc=0.7747222185134888, loss=0.47931960225105286
test: epoch 54, loss 1.6711015701293945, acc=0.48055556416511536, loss=1.6711015701293945
train: epoch 55, loss 0.4980698227882385, acc=0.7711111307144165, loss=0.4980698227882385
test: epoch 55, loss 1.7784475088119507, acc=0.48055556416511536, loss=1.7784475088119507
train: epoch 56, loss 0.46336179971694946, acc=0.7822777628898621, loss=0.46336179971694946
test: epoch 56, loss 1.7752444744110107, acc=0.4833333194255829, loss=1.7752444744110107
train: epoch 57, loss 0.46577441692352295, acc=0.7878888845443726, loss=0.46577441692352295
test: epoch 57, loss 1.6509531736373901, acc=0.49166667461395264, loss=1.6509531736373901
train: epoch 58, loss 0.47635722160339355, acc=0.7837222218513489, loss=0.47635722160339355
test: epoch 58, loss 1.4422000646591187, acc=0.5222222208976746, loss=1.4422000646591187
train: epoch 59, loss 0.4545838236808777, acc=0.7899444699287415, loss=0.4545838236808777
test: epoch 59, loss 1.5087074041366577, acc=0.5249999761581421, loss=1.5087074041366577
train: epoch 60, loss 0.45998600125312805, acc=0.7890555262565613, loss=0.45998600125312805
test: epoch 60, loss 1.4474836587905884, acc=0.519444465637207, loss=1.4474836587905884
train: epoch 61, loss 0.46928849816322327, acc=0.7898889183998108, loss=0.46928849816322327
test: epoch 61, loss 1.5516995191574097, acc=0.5249999761581421, loss=1.5516995191574097
train: epoch 62, loss 0.44224098324775696, acc=0.7885000109672546, loss=0.44224098324775696
test: epoch 62, loss 1.6991530656814575, acc=0.5166666507720947, loss=1.6991530656814575
train: epoch 63, loss 0.4380161166191101, acc=0.7932222485542297, loss=0.4380161166191101
test: epoch 63, loss 1.5342768430709839, acc=0.5222222208976746, loss=1.5342768430709839
train: epoch 64, loss 0.44853654503822327, acc=0.7875555753707886, loss=0.44853654503822327
test: epoch 64, loss 1.4252756834030151, acc=0.5249999761581421, loss=1.4252756834030151
train: epoch 65, loss 0.4386601448059082, acc=0.7959444522857666, loss=0.4386601448059082
test: epoch 65, loss 1.4186877012252808, acc=0.5249999761581421, loss=1.4186877012252808
train: epoch 66, loss 0.439222127199173, acc=0.7954999804496765, loss=0.439222127199173
test: epoch 66, loss 1.540828824043274, acc=0.519444465637207, loss=1.540828824043274
train: epoch 67, loss 0.42577141523361206, acc=0.795722246170044, loss=0.42577141523361206
test: epoch 67, loss 1.460386872291565, acc=0.5249999761581421, loss=1.460386872291565
train: epoch 68, loss 0.4270816743373871, acc=0.7959444522857666, loss=0.4270816743373871
test: epoch 68, loss 1.542055606842041, acc=0.5222222208976746, loss=1.542055606842041
train: epoch 69, loss 0.4501403868198395, acc=0.7903888821601868, loss=0.4501403868198395
test: epoch 69, loss 1.3745850324630737, acc=0.5527777671813965, loss=1.3745850324630737
train: epoch 70, loss 0.4184836447238922, acc=0.8057777881622314, loss=0.4184836447238922
test: epoch 70, loss 1.4865492582321167, acc=0.5555555820465088, loss=1.4865492582321167
train: epoch 71, loss 0.42464911937713623, acc=0.8084999918937683, loss=0.42464911937713623
test: epoch 71, loss 1.420830488204956, acc=0.5555555820465088, loss=1.420830488204956
train: epoch 72, loss 0.4151518642902374, acc=0.8157222270965576, loss=0.4151518642902374
test: epoch 72, loss 1.4741913080215454, acc=0.5472221970558167, loss=1.4741913080215454
train: epoch 73, loss 0.4034191966056824, acc=0.8218888640403748, loss=0.4034191966056824
test: epoch 73, loss 1.5506025552749634, acc=0.5472221970558167, loss=1.5506025552749634
train: epoch 74, loss 0.399281769990921, acc=0.8237777948379517, loss=0.399281769990921
test: epoch 74, loss 1.4801121950149536, acc=0.5527777671813965, loss=1.4801121950149536
train: epoch 75, loss 0.3923477530479431, acc=0.8294444680213928, loss=0.3923477530479431
test: epoch 75, loss 1.3091614246368408, acc=0.5444444417953491, loss=1.3091614246368408
train: epoch 76, loss 0.3817374110221863, acc=0.8348333239555359, loss=0.3817374110221863
test: epoch 76, loss 1.6412289142608643, acc=0.5555555820465088, loss=1.6412289142608643
train: epoch 77, loss 0.3812631368637085, acc=0.835277795791626, loss=0.3812631368637085
test: epoch 77, loss 1.5110745429992676, acc=0.5555555820465088, loss=1.5110745429992676
train: epoch 78, loss 0.3791155219078064, acc=0.8353333473205566, loss=0.3791155219078064
test: epoch 78, loss 1.3415532112121582, acc=0.5555555820465088, loss=1.3415532112121582
train: epoch 79, loss 0.39553797245025635, acc=0.8289444446563721, loss=0.39553797245025635
test: epoch 79, loss 1.5400633811950684, acc=0.5527777671813965, loss=1.5400633811950684
train: epoch 80, loss 0.37966328859329224, acc=0.8382777571678162, loss=0.37966328859329224
test: epoch 80, loss 1.3747261762619019, acc=0.5555555820465088, loss=1.3747261762619019
train: epoch 81, loss 0.36193788051605225, acc=0.8423333168029785, loss=0.36193788051605225
test: epoch 81, loss 1.4294606447219849, acc=0.5555555820465088, loss=1.4294606447219849
train: epoch 82, loss 0.35796818137168884, acc=0.8457777500152588, loss=0.35796818137168884
test: epoch 82, loss 1.4951276779174805, acc=0.5555555820465088, loss=1.4951276779174805
train: epoch 83, loss 0.35238736867904663, acc=0.8444444537162781, loss=0.35238736867904663
test: epoch 83, loss 1.549782395362854, acc=0.5555555820465088, loss=1.549782395362854
train: epoch 84, loss 0.3582759499549866, acc=0.8441666960716248, loss=0.3582759499549866
test: epoch 84, loss 1.6178362369537354, acc=0.5555555820465088, loss=1.6178362369537354
train: epoch 85, loss 0.35210126638412476, acc=0.8423333168029785, loss=0.35210126638412476
test: epoch 85, loss 1.5883145332336426, acc=0.5555555820465088, loss=1.5883145332336426
train: epoch 86, loss 0.35020115971565247, acc=0.8446666598320007, loss=0.35020115971565247
test: epoch 86, loss 1.6379743814468384, acc=0.5555555820465088, loss=1.6379743814468384
train: epoch 87, loss 0.34326887130737305, acc=0.8443333506584167, loss=0.34326887130737305
test: epoch 87, loss 1.456805944442749, acc=0.5555555820465088, loss=1.456805944442749
train: epoch 88, loss 0.35655784606933594, acc=0.8457221984863281, loss=0.35655784606933594
test: epoch 88, loss 1.6046465635299683, acc=0.5555555820465088, loss=1.6046465635299683
train: epoch 89, loss 0.36049869656562805, acc=0.8404444456100464, loss=0.36049869656562805
test: epoch 89, loss 1.649306058883667, acc=0.5555555820465088, loss=1.649306058883667
train: epoch 90, loss 0.3528231680393219, acc=0.8482221961021423, loss=0.3528231680393219
test: epoch 90, loss 1.4173412322998047, acc=0.5555555820465088, loss=1.4173412322998047
train: epoch 91, loss 0.33157968521118164, acc=0.8485555648803711, loss=0.33157968521118164
test: epoch 91, loss 1.4946805238723755, acc=0.5833333134651184, loss=1.4946805238723755
train: epoch 92, loss 0.3101661503314972, acc=0.8587777614593506, loss=0.3101661503314972
test: epoch 92, loss 1.7879610061645508, acc=0.5861111283302307, loss=1.7879610061645508
train: epoch 93, loss 0.30977120995521545, acc=0.863277792930603, loss=0.30977120995521545
test: epoch 93, loss 1.7160515785217285, acc=0.5861111283302307, loss=1.7160515785217285
train: epoch 94, loss 0.2893703281879425, acc=0.8726666569709778, loss=0.2893703281879425
test: epoch 94, loss 1.5755497217178345, acc=0.5833333134651184, loss=1.5755497217178345
train: epoch 95, loss 0.30637386441230774, acc=0.8685555458068848, loss=0.30637386441230774
test: epoch 95, loss 1.5600738525390625, acc=0.5805555582046509, loss=1.5600738525390625
train: epoch 96, loss 0.2876143157482147, acc=0.8703333139419556, loss=0.2876143157482147
test: epoch 96, loss 1.5149848461151123, acc=0.5833333134651184, loss=1.5149848461151123
train: epoch 97, loss 0.28943657875061035, acc=0.879444420337677, loss=0.28943657875061035
test: epoch 97, loss 1.5514119863510132, acc=0.5833333134651184, loss=1.5514119863510132
train: epoch 98, loss 0.28521299362182617, acc=0.8830000162124634, loss=0.28521299362182617
test: epoch 98, loss 1.5799696445465088, acc=0.5833333134651184, loss=1.5799696445465088
train: epoch 99, loss 0.2788996696472168, acc=0.8818333148956299, loss=0.2788996696472168
test: epoch 99, loss 1.5851712226867676, acc=0.5833333134651184, loss=1.5851712226867676
train: epoch 100, loss 0.2642761468887329, acc=0.8888888955116272, loss=0.2642761468887329
test: epoch 100, loss 1.6703135967254639, acc=0.5833333134651184, loss=1.6703135967254639
train: epoch 101, loss 0.27475130558013916, acc=0.8860555291175842, loss=0.27475130558013916
test: epoch 101, loss 1.4377646446228027, acc=0.5833333134651184, loss=1.4377646446228027
train: epoch 102, loss 0.2724768817424774, acc=0.8860555291175842, loss=0.2724768817424774
test: epoch 102, loss 1.4350773096084595, acc=0.5833333134651184, loss=1.4350773096084595
train: epoch 103, loss 0.27613365650177, acc=0.8864444494247437, loss=0.27613365650177
test: epoch 103, loss 1.6171467304229736, acc=0.5861111283302307, loss=1.6171467304229736
train: epoch 104, loss 0.2709737718105316, acc=0.8928889036178589, loss=0.2709737718105316
test: epoch 104, loss 1.5315409898757935, acc=0.5833333134651184, loss=1.5315409898757935
train: epoch 105, loss 0.24819432199001312, acc=0.8989444375038147, loss=0.24819432199001312
test: epoch 105, loss 1.606117606163025, acc=0.5833333134651184, loss=1.606117606163025
train: epoch 106, loss 0.2478019744157791, acc=0.9025555849075317, loss=0.2478019744157791
test: epoch 106, loss 1.606755256652832, acc=0.5833333134651184, loss=1.606755256652832
train: epoch 107, loss 0.2657949924468994, acc=0.894944429397583, loss=0.2657949924468994
test: epoch 107, loss 1.3873920440673828, acc=0.5861111283302307, loss=1.3873920440673828
train: epoch 108, loss 0.2549167275428772, acc=0.8999444246292114, loss=0.2549167275428772
test: epoch 108, loss 1.5880951881408691, acc=0.5972222089767456, loss=1.5880951881408691
train: epoch 109, loss 0.25303417444229126, acc=0.8960000276565552, loss=0.25303417444229126
test: epoch 109, loss 1.6682024002075195, acc=0.605555534362793, loss=1.6682024002075195
train: epoch 110, loss 0.2402091771364212, acc=0.9038333296775818, loss=0.2402091771364212
test: epoch 110, loss 1.567458987236023, acc=0.6027777791023254, loss=1.567458987236023
train: epoch 111, loss 0.23647232353687286, acc=0.9046111106872559, loss=0.23647232353687286
test: epoch 111, loss 1.778229832649231, acc=0.6138888597488403, loss=1.778229832649231
train: epoch 112, loss 0.23687541484832764, acc=0.9068333506584167, loss=0.23687541484832764
test: epoch 112, loss 1.5898061990737915, acc=0.6166666746139526, loss=1.5898061990737915
train: epoch 113, loss 0.23523765802383423, acc=0.9032777547836304, loss=0.23523765802383423
test: epoch 113, loss 1.420734167098999, acc=0.6166666746139526, loss=1.420734167098999
train: epoch 114, loss 0.24649782478809357, acc=0.9017778038978577, loss=0.24649782478809357
test: epoch 114, loss 1.4798569679260254, acc=0.6194444298744202, loss=1.4798569679260254
train: epoch 115, loss 0.2557930648326874, acc=0.902222216129303, loss=0.2557930648326874
test: epoch 115, loss 1.5271228551864624, acc=0.6305555701255798, loss=1.5271228551864624
train: epoch 116, loss 0.2302629053592682, acc=0.9034444689750671, loss=0.2302629053592682
test: epoch 116, loss 1.3309181928634644, acc=0.6111111044883728, loss=1.3309181928634644
train: epoch 117, loss 0.22961531579494476, acc=0.9020000100135803, loss=0.22961531579494476
test: epoch 117, loss 1.4070475101470947, acc=0.625, loss=1.4070475101470947
train: epoch 118, loss 0.2412438988685608, acc=0.9015555381774902, loss=0.2412438988685608
test: epoch 118, loss 1.3749717473983765, acc=0.625, loss=1.3749717473983765
train: epoch 119, loss 0.23472335934638977, acc=0.9025555849075317, loss=0.23472335934638977
test: epoch 119, loss 1.2351115942001343, acc=0.6527777910232544, loss=1.2351115942001343
train: epoch 120, loss 0.25183382630348206, acc=0.8987777829170227, loss=0.25183382630348206
test: epoch 120, loss 1.3373222351074219, acc=0.6499999761581421, loss=1.3373222351074219
train: epoch 121, loss 0.24129976332187653, acc=0.9035555720329285, loss=0.24129976332187653
test: epoch 121, loss 1.225434422492981, acc=0.6499999761581421, loss=1.225434422492981
train: epoch 122, loss 0.23314522206783295, acc=0.9051111340522766, loss=0.23314522206783295
test: epoch 122, loss 1.2198328971862793, acc=0.6555555462837219, loss=1.2198328971862793
train: epoch 123, loss 0.22092357277870178, acc=0.9068333506584167, loss=0.22092357277870178
test: epoch 123, loss 1.4401485919952393, acc=0.6555555462837219, loss=1.4401485919952393
train: epoch 124, loss 0.2290700078010559, acc=0.9089999794960022, loss=0.2290700078010559
test: epoch 124, loss 1.2849513292312622, acc=0.6555555462837219, loss=1.2849513292312622
train: epoch 125, loss 0.21241971850395203, acc=0.9123333096504211, loss=0.21241971850395203
test: epoch 125, loss 1.3354086875915527, acc=0.6555555462837219, loss=1.3354086875915527
train: epoch 126, loss 0.22257889807224274, acc=0.9075555801391602, loss=0.22257889807224274
test: epoch 126, loss 1.3450193405151367, acc=0.6555555462837219, loss=1.3450193405151367
train: epoch 127, loss 0.2343088686466217, acc=0.9077777862548828, loss=0.2343088686466217
test: epoch 127, loss 1.2820192575454712, acc=0.6833333373069763, loss=1.2820192575454712
train: epoch 128, loss 0.21337710320949554, acc=0.9087777733802795, loss=0.21337710320949554
test: epoch 128, loss 1.1873562335968018, acc=0.6861110925674438, loss=1.1873562335968018
train: epoch 129, loss 0.21416053175926208, acc=0.9120000004768372, loss=0.21416053175926208
test: epoch 129, loss 0.9435887932777405, acc=0.7277777791023254, loss=0.9435887932777405
train: epoch 130, loss 0.18932273983955383, acc=0.9172222018241882, loss=0.18932273983955383
test: epoch 130, loss 1.0474371910095215, acc=0.730555534362793, loss=1.0474371910095215
train: epoch 131, loss 0.18647252023220062, acc=0.9194444417953491, loss=0.18647252023220062
test: epoch 131, loss 0.9722391963005066, acc=0.7250000238418579, loss=0.9722391963005066
train: epoch 132, loss 0.18840299546718597, acc=0.9204444289207458, loss=0.18840299546718597
test: epoch 132, loss 0.8968226313591003, acc=0.7611111402511597, loss=0.8968226313591003
train: epoch 133, loss 0.174287348985672, acc=0.9202222228050232, loss=0.174287348985672
test: epoch 133, loss 1.0074752569198608, acc=0.75, loss=1.0074752569198608
train: epoch 134, loss 0.1684582680463791, acc=0.9213333129882812, loss=0.1684582680463791
test: epoch 134, loss 0.96163010597229, acc=0.7611111402511597, loss=0.96163010597229
train: epoch 135, loss 0.17285430431365967, acc=0.9200000166893005, loss=0.17285430431365967
test: epoch 135, loss 1.029502272605896, acc=0.7611111402511597, loss=1.029502272605896
train: epoch 136, loss 0.17114593088626862, acc=0.9217222332954407, loss=0.17114593088626862
test: epoch 136, loss 1.0659418106079102, acc=0.7611111402511597, loss=1.0659418106079102
train: epoch 137, loss 0.18656869232654572, acc=0.9192222356796265, loss=0.18656869232654572
test: epoch 137, loss 0.8639724254608154, acc=0.7611111402511597, loss=0.8639724254608154
train: epoch 138, loss 0.18377657234668732, acc=0.9222221970558167, loss=0.18377657234668732
test: epoch 138, loss 0.9940665364265442, acc=0.7611111402511597, loss=0.9940665364265442
train: epoch 139, loss 0.17300942540168762, acc=0.9225555658340454, loss=0.17300942540168762
test: epoch 139, loss 0.8757472634315491, acc=0.7611111402511597, loss=0.8757472634315491
train: epoch 140, loss 0.16113176941871643, acc=0.9256666898727417, loss=0.16113176941871643
test: epoch 140, loss 0.9596796631813049, acc=0.7611111402511597, loss=0.9596796631813049
train: epoch 141, loss 0.15967650711536407, acc=0.9254444241523743, loss=0.15967650711536407
test: epoch 141, loss 0.9456825256347656, acc=0.7611111402511597, loss=0.9456825256347656
train: epoch 142, loss 0.20947876572608948, acc=0.9202777743339539, loss=0.20947876572608948
test: epoch 142, loss 0.9096848368644714, acc=0.7583333253860474, loss=0.9096848368644714
train: epoch 143, loss 0.17735616862773895, acc=0.9199444651603699, loss=0.17735616862773895
test: epoch 143, loss 0.9088250994682312, acc=0.7583333253860474, loss=0.9088250994682312
train: epoch 144, loss 0.1810971051454544, acc=0.920722246170044, loss=0.1810971051454544
test: epoch 144, loss 0.9953798055648804, acc=0.7527777552604675, loss=0.9953798055648804
train: epoch 145, loss 0.18776176869869232, acc=0.9193888902664185, loss=0.18776176869869232
test: epoch 145, loss 0.9500936269760132, acc=0.7583333253860474, loss=0.9500936269760132
train: epoch 146, loss 0.185465469956398, acc=0.9164999723434448, loss=0.185465469956398
test: epoch 146, loss 0.9181289672851562, acc=0.7583333253860474, loss=0.9181289672851562
train: epoch 147, loss 0.19648779928684235, acc=0.9157222509384155, loss=0.19648779928684235
test: epoch 147, loss 0.9397201538085938, acc=0.7555555701255798, loss=0.9397201538085938
train: epoch 148, loss 0.18456938862800598, acc=0.9191666841506958, loss=0.18456938862800598
test: epoch 148, loss 0.9060720205307007, acc=0.7555555701255798, loss=0.9060720205307007
train: epoch 149, loss 0.21190997958183289, acc=0.913277804851532, loss=0.21190997958183289
test: epoch 149, loss 1.1173858642578125, acc=0.7527777552604675, loss=1.1173858642578125
train: epoch 150, loss 0.19759351015090942, acc=0.9161666631698608, loss=0.19759351015090942
test: epoch 150, loss 0.7782492637634277, acc=0.7555555701255798, loss=0.7782492637634277
