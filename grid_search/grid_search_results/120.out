# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=100", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.99", "--one_hot=true", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_summands=2, n_symbols=100, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1700856964, receiver_embed_dim=128, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.2912468910217285, acc=0.06088889017701149, loss=3.2912468910217285
test: epoch 1, loss 2.5830039978027344, acc=0.125, loss=2.5830039978027344
train: epoch 2, loss 2.0922834873199463, acc=0.19433332979679108, loss=2.0922834873199463
test: epoch 2, loss 2.170471668243408, acc=0.16111111640930176, loss=2.170471668243408
train: epoch 3, loss 1.73831045627594, acc=0.28922221064567566, loss=1.73831045627594
test: epoch 3, loss 2.0998950004577637, acc=0.18333333730697632, loss=2.0998950004577637
train: epoch 4, loss 1.5807751417160034, acc=0.3462222218513489, loss=1.5807751417160034
test: epoch 4, loss 2.0658981800079346, acc=0.2083333283662796, loss=2.0658981800079346
train: epoch 5, loss 1.4649258852005005, acc=0.38877779245376587, loss=1.4649258852005005
test: epoch 5, loss 2.0221197605133057, acc=0.21111111342906952, loss=2.0221197605133057
train: epoch 6, loss 1.4113863706588745, acc=0.4125555455684662, loss=1.4113863706588745
test: epoch 6, loss 2.037632703781128, acc=0.21388888359069824, loss=2.037632703781128
train: epoch 7, loss 1.3517186641693115, acc=0.43994444608688354, loss=1.3517186641693115
test: epoch 7, loss 2.0270416736602783, acc=0.21388888359069824, loss=2.0270416736602783
train: epoch 8, loss 1.3184709548950195, acc=0.4509444534778595, loss=1.3184709548950195
test: epoch 8, loss 2.0695743560791016, acc=0.2222222238779068, loss=2.0695743560791016
train: epoch 9, loss 1.2894574403762817, acc=0.4648333191871643, loss=1.2894574403762817
test: epoch 9, loss 2.0283572673797607, acc=0.22499999403953552, loss=2.0283572673797607
train: epoch 10, loss 1.2576695680618286, acc=0.4724999964237213, loss=1.2576695680618286
test: epoch 10, loss 2.0452935695648193, acc=0.2083333283662796, loss=2.0452935695648193
train: epoch 11, loss 1.2481749057769775, acc=0.4690000116825104, loss=1.2481749057769775
test: epoch 11, loss 2.0991129875183105, acc=0.2222222238779068, loss=2.0991129875183105
train: epoch 12, loss 1.2387361526489258, acc=0.4671666622161865, loss=1.2387361526489258
test: epoch 12, loss 2.2730324268341064, acc=0.21944443881511688, loss=2.2730324268341064
train: epoch 13, loss 1.2233177423477173, acc=0.4769444465637207, loss=1.2233177423477173
test: epoch 13, loss 2.1874873638153076, acc=0.23055554926395416, loss=2.1874873638153076
train: epoch 14, loss 1.2078801393508911, acc=0.4815555512905121, loss=1.2078801393508911
test: epoch 14, loss 2.056919574737549, acc=0.2222222238779068, loss=2.056919574737549
train: epoch 15, loss 1.203092336654663, acc=0.4834444522857666, loss=1.203092336654663
test: epoch 15, loss 2.0601773262023926, acc=0.22499999403953552, loss=2.0601773262023926
train: epoch 16, loss 1.1914968490600586, acc=0.4856666624546051, loss=1.1914968490600586
test: epoch 16, loss 2.0773189067840576, acc=0.21944443881511688, loss=2.0773189067840576
train: epoch 17, loss 1.1867892742156982, acc=0.48633334040641785, loss=1.1867892742156982
test: epoch 17, loss 2.176356315612793, acc=0.22499999403953552, loss=2.176356315612793
train: epoch 18, loss 1.1884121894836426, acc=0.4831666648387909, loss=1.1884121894836426
test: epoch 18, loss 2.0962581634521484, acc=0.23333333432674408, loss=2.0962581634521484
train: epoch 19, loss 1.1826754808425903, acc=0.4833333194255829, loss=1.1826754808425903
test: epoch 19, loss 2.1734015941619873, acc=0.23055554926395416, loss=2.1734015941619873
train: epoch 20, loss 1.1630779504776, acc=0.4929444491863251, loss=1.1630779504776
test: epoch 20, loss 2.278087615966797, acc=0.22499999403953552, loss=2.278087615966797
train: epoch 21, loss 1.1806859970092773, acc=0.4828333258628845, loss=1.1806859970092773
test: epoch 21, loss 2.221358060836792, acc=0.23333333432674408, loss=2.221358060836792
train: epoch 22, loss 1.1594423055648804, acc=0.49183332920074463, loss=1.1594423055648804
test: epoch 22, loss 2.241694450378418, acc=0.2361111044883728, loss=2.241694450378418
train: epoch 23, loss 1.166940450668335, acc=0.48483332991600037, loss=1.166940450668335
test: epoch 23, loss 2.2452800273895264, acc=0.23333333432674408, loss=2.2452800273895264
train: epoch 24, loss 1.1659586429595947, acc=0.4863888919353485, loss=1.1659586429595947
test: epoch 24, loss 2.2965002059936523, acc=0.22777777910232544, loss=2.2965002059936523
train: epoch 25, loss 1.1624633073806763, acc=0.49283334612846375, loss=1.1624633073806763
test: epoch 25, loss 2.3568451404571533, acc=0.2361111044883728, loss=2.3568451404571533
train: epoch 26, loss 1.1698024272918701, acc=0.49272221326828003, loss=1.1698024272918701
test: epoch 26, loss 2.24603271484375, acc=0.21944443881511688, loss=2.24603271484375
train: epoch 27, loss 1.1678930521011353, acc=0.4902777671813965, loss=1.1678930521011353
test: epoch 27, loss 2.296154260635376, acc=0.23888888955116272, loss=2.296154260635376
train: epoch 28, loss 1.1555638313293457, acc=0.4893888831138611, loss=1.1555638313293457
test: epoch 28, loss 2.331861734390259, acc=0.24444444477558136, loss=2.331861734390259
train: epoch 29, loss 1.1646995544433594, acc=0.48322221636772156, loss=1.1646995544433594
test: epoch 29, loss 2.298246383666992, acc=0.2361111044883728, loss=2.298246383666992
train: epoch 30, loss 1.1703455448150635, acc=0.48366665840148926, loss=1.1703455448150635
test: epoch 30, loss 2.152071714401245, acc=0.24166665971279144, loss=2.152071714401245
train: epoch 31, loss 1.1576169729232788, acc=0.4851111173629761, loss=1.1576169729232788
test: epoch 31, loss 2.3981175422668457, acc=0.24166665971279144, loss=2.3981175422668457
train: epoch 32, loss 1.1565035581588745, acc=0.48677778244018555, loss=1.1565035581588745
test: epoch 32, loss 2.2863500118255615, acc=0.24444444477558136, loss=2.2863500118255615
train: epoch 33, loss 1.1490784883499146, acc=0.49149999022483826, loss=1.1490784883499146
test: epoch 33, loss 2.200911283493042, acc=0.24166665971279144, loss=2.200911283493042
train: epoch 34, loss 1.1523394584655762, acc=0.4890555441379547, loss=1.1523394584655762
test: epoch 34, loss 2.4122190475463867, acc=0.24444444477558136, loss=2.4122190475463867
train: epoch 35, loss 1.1306315660476685, acc=0.4945000112056732, loss=1.1306315660476685
test: epoch 35, loss 2.4187123775482178, acc=0.24722221493721008, loss=2.4187123775482178
train: epoch 36, loss 1.148422360420227, acc=0.49000000953674316, loss=1.148422360420227
test: epoch 36, loss 2.2842843532562256, acc=0.24722221493721008, loss=2.2842843532562256
train: epoch 37, loss 1.1372530460357666, acc=0.4976111054420471, loss=1.1372530460357666
test: epoch 37, loss 2.2798233032226562, acc=0.25, loss=2.2798233032226562
train: epoch 38, loss 1.13650381565094, acc=0.4997222125530243, loss=1.13650381565094
test: epoch 38, loss 2.317465305328369, acc=0.2527777850627899, loss=2.317465305328369
train: epoch 39, loss 1.1175117492675781, acc=0.5051110982894897, loss=1.1175117492675781
test: epoch 39, loss 2.7722854614257812, acc=0.23888888955116272, loss=2.7722854614257812
train: epoch 40, loss 1.1268693208694458, acc=0.49594444036483765, loss=1.1268693208694458
test: epoch 40, loss 2.394843578338623, acc=0.25, loss=2.394843578338623
train: epoch 41, loss 1.1293457746505737, acc=0.5021666884422302, loss=1.1293457746505737
test: epoch 41, loss 2.3310980796813965, acc=0.2527777850627899, loss=2.3310980796813965
train: epoch 42, loss 1.1105225086212158, acc=0.5053333044052124, loss=1.1105225086212158
test: epoch 42, loss 2.6021716594696045, acc=0.24722221493721008, loss=2.6021716594696045
train: epoch 43, loss 1.1333897113800049, acc=0.49577778577804565, loss=1.1333897113800049
test: epoch 43, loss 2.2685751914978027, acc=0.25, loss=2.2685751914978027
train: epoch 44, loss 1.1189873218536377, acc=0.5021111369132996, loss=1.1189873218536377
test: epoch 44, loss 2.27010440826416, acc=0.24722221493721008, loss=2.27010440826416
train: epoch 45, loss 1.109998106956482, acc=0.5059999823570251, loss=1.109998106956482
test: epoch 45, loss 2.5293304920196533, acc=0.24722221493721008, loss=2.5293304920196533
train: epoch 46, loss 1.1256452798843384, acc=0.5011110901832581, loss=1.1256452798843384
test: epoch 46, loss 2.3756871223449707, acc=0.25555557012557983, loss=2.3756871223449707
train: epoch 47, loss 1.1140038967132568, acc=0.5032777786254883, loss=1.1140038967132568
test: epoch 47, loss 2.4943747520446777, acc=0.25555557012557983, loss=2.4943747520446777
train: epoch 48, loss 1.104198694229126, acc=0.5069444179534912, loss=1.104198694229126
test: epoch 48, loss 2.2720272541046143, acc=0.2527777850627899, loss=2.2720272541046143
train: epoch 49, loss 1.1162954568862915, acc=0.5028333067893982, loss=1.1162954568862915
test: epoch 49, loss 2.332747220993042, acc=0.2638888955116272, loss=2.332747220993042
train: epoch 50, loss 1.106784462928772, acc=0.5064444541931152, loss=1.106784462928772
test: epoch 50, loss 2.3705577850341797, acc=0.2611111104488373, loss=2.3705577850341797
train: epoch 51, loss 1.0951141119003296, acc=0.5059444308280945, loss=1.0951141119003296
test: epoch 51, loss 2.4171807765960693, acc=0.2638888955116272, loss=2.4171807765960693
train: epoch 52, loss 1.1117287874221802, acc=0.5066666603088379, loss=1.1117287874221802
test: epoch 52, loss 2.487377882003784, acc=0.25555557012557983, loss=2.487377882003784
train: epoch 53, loss 1.0896247625350952, acc=0.5084999799728394, loss=1.0896247625350952
test: epoch 53, loss 2.379868984222412, acc=0.2638888955116272, loss=2.379868984222412
train: epoch 54, loss 1.0918242931365967, acc=0.5024444460868835, loss=1.0918242931365967
test: epoch 54, loss 2.286773204803467, acc=0.25833332538604736, loss=2.286773204803467
train: epoch 55, loss 1.0865586996078491, acc=0.5096111297607422, loss=1.0865586996078491
test: epoch 55, loss 2.4310314655303955, acc=0.25833332538604736, loss=2.4310314655303955
train: epoch 56, loss 1.0930883884429932, acc=0.5098333358764648, loss=1.0930883884429932
test: epoch 56, loss 2.5590176582336426, acc=0.2611111104488373, loss=2.5590176582336426
train: epoch 57, loss 1.086271047592163, acc=0.5138888955116272, loss=1.086271047592163
test: epoch 57, loss 2.5777475833892822, acc=0.2611111104488373, loss=2.5777475833892822
train: epoch 58, loss 1.094860553741455, acc=0.5070000290870667, loss=1.094860553741455
test: epoch 58, loss 2.462002754211426, acc=0.25833332538604736, loss=2.462002754211426
train: epoch 59, loss 1.0853831768035889, acc=0.5110555291175842, loss=1.0853831768035889
test: epoch 59, loss 2.3582005500793457, acc=0.2611111104488373, loss=2.3582005500793457
train: epoch 60, loss 1.0804344415664673, acc=0.5127778053283691, loss=1.0804344415664673
test: epoch 60, loss 2.3108696937561035, acc=0.2611111104488373, loss=2.3108696937561035
train: epoch 61, loss 1.0832345485687256, acc=0.5145555734634399, loss=1.0832345485687256
test: epoch 61, loss 2.3609514236450195, acc=0.25833332538604736, loss=2.3609514236450195
train: epoch 62, loss 1.0623743534088135, acc=0.5224444270133972, loss=1.0623743534088135
test: epoch 62, loss 2.3488593101501465, acc=0.2527777850627899, loss=2.3488593101501465
train: epoch 63, loss 1.0771914720535278, acc=0.515666663646698, loss=1.0771914720535278
test: epoch 63, loss 2.4277729988098145, acc=0.2527777850627899, loss=2.4277729988098145
train: epoch 64, loss 1.0739738941192627, acc=0.5174999833106995, loss=1.0739738941192627
test: epoch 64, loss 2.417396068572998, acc=0.25833332538604736, loss=2.417396068572998
train: epoch 65, loss 1.071548581123352, acc=0.5178333520889282, loss=1.071548581123352
test: epoch 65, loss 2.6408843994140625, acc=0.25833332538604736, loss=2.6408843994140625
train: epoch 66, loss 1.064521074295044, acc=0.5218333601951599, loss=1.064521074295044
test: epoch 66, loss 2.3726084232330322, acc=0.25833332538604736, loss=2.3726084232330322
train: epoch 67, loss 1.0710010528564453, acc=0.5179444551467896, loss=1.0710010528564453
test: epoch 67, loss 2.1882312297821045, acc=0.2611111104488373, loss=2.1882312297821045
train: epoch 68, loss 1.0643924474716187, acc=0.5235555768013, loss=1.0643924474716187
test: epoch 68, loss 2.36859130859375, acc=0.2611111104488373, loss=2.36859130859375
train: epoch 69, loss 1.0842550992965698, acc=0.5165555477142334, loss=1.0842550992965698
test: epoch 69, loss 2.402346134185791, acc=0.2611111104488373, loss=2.402346134185791
train: epoch 70, loss 1.0653740167617798, acc=0.5177778005599976, loss=1.0653740167617798
test: epoch 70, loss 2.2384891510009766, acc=0.25833332538604736, loss=2.2384891510009766
train: epoch 71, loss 1.0684975385665894, acc=0.5211111307144165, loss=1.0684975385665894
test: epoch 71, loss 2.5540611743927, acc=0.25833332538604736, loss=2.5540611743927
train: epoch 72, loss 1.0659602880477905, acc=0.5219444632530212, loss=1.0659602880477905
test: epoch 72, loss 2.509417772293091, acc=0.2611111104488373, loss=2.509417772293091
train: epoch 73, loss 1.066727638244629, acc=0.524222195148468, loss=1.066727638244629
test: epoch 73, loss 2.298057794570923, acc=0.2611111104488373, loss=2.298057794570923
train: epoch 74, loss 1.063415765762329, acc=0.5249444246292114, loss=1.063415765762329
test: epoch 74, loss 2.1991264820098877, acc=0.25555557012557983, loss=2.1991264820098877
train: epoch 75, loss 1.0580723285675049, acc=0.5268333554267883, loss=1.0580723285675049
test: epoch 75, loss 2.521191358566284, acc=0.2611111104488373, loss=2.521191358566284
train: epoch 76, loss 1.0632708072662354, acc=0.5248888731002808, loss=1.0632708072662354
test: epoch 76, loss 2.737476110458374, acc=0.2666666805744171, loss=2.737476110458374
train: epoch 77, loss 1.0524314641952515, acc=0.5285555720329285, loss=1.0524314641952515
test: epoch 77, loss 2.510631799697876, acc=0.2666666805744171, loss=2.510631799697876
train: epoch 78, loss 1.059381127357483, acc=0.5260555744171143, loss=1.059381127357483
test: epoch 78, loss 2.1675302982330322, acc=0.26944443583488464, loss=2.1675302982330322
train: epoch 79, loss 1.0533487796783447, acc=0.5274444222450256, loss=1.0533487796783447
test: epoch 79, loss 2.6716935634613037, acc=0.2750000059604645, loss=2.6716935634613037
train: epoch 80, loss 1.0595592260360718, acc=0.5296666622161865, loss=1.0595592260360718
test: epoch 80, loss 2.266505241394043, acc=0.2750000059604645, loss=2.266505241394043
train: epoch 81, loss 1.031704306602478, acc=0.5360000133514404, loss=1.031704306602478
test: epoch 81, loss 2.416778802871704, acc=0.27222222089767456, loss=2.416778802871704
train: epoch 82, loss 1.0422736406326294, acc=0.5304999947547913, loss=1.0422736406326294
test: epoch 82, loss 2.5426626205444336, acc=0.2666666805744171, loss=2.5426626205444336
train: epoch 83, loss 1.0532182455062866, acc=0.5288888812065125, loss=1.0532182455062866
test: epoch 83, loss 2.35683012008667, acc=0.26944443583488464, loss=2.35683012008667
train: epoch 84, loss 1.0547974109649658, acc=0.5288333296775818, loss=1.0547974109649658
test: epoch 84, loss 2.4132487773895264, acc=0.2750000059604645, loss=2.4132487773895264
train: epoch 85, loss 1.041491150856018, acc=0.5398889183998108, loss=1.041491150856018
test: epoch 85, loss 2.554211139678955, acc=0.26944443583488464, loss=2.554211139678955
train: epoch 86, loss 1.0392972230911255, acc=0.5379444360733032, loss=1.0392972230911255
test: epoch 86, loss 2.5287320613861084, acc=0.27222222089767456, loss=2.5287320613861084
train: epoch 87, loss 1.0407297611236572, acc=0.5313888788223267, loss=1.0407297611236572
test: epoch 87, loss 2.3478143215179443, acc=0.26944443583488464, loss=2.3478143215179443
train: epoch 88, loss 1.0474892854690552, acc=0.5278888940811157, loss=1.0474892854690552
test: epoch 88, loss 2.312368392944336, acc=0.2750000059604645, loss=2.312368392944336
train: epoch 89, loss 1.0421969890594482, acc=0.5305555462837219, loss=1.0421969890594482
test: epoch 89, loss 2.323193311691284, acc=0.26944443583488464, loss=2.323193311691284
train: epoch 90, loss 1.037355661392212, acc=0.5343888998031616, loss=1.037355661392212
test: epoch 90, loss 2.2905635833740234, acc=0.2777777910232544, loss=2.2905635833740234
train: epoch 91, loss 1.0441324710845947, acc=0.5295000076293945, loss=1.0441324710845947
test: epoch 91, loss 2.5641229152679443, acc=0.26944443583488464, loss=2.5641229152679443
train: epoch 92, loss 1.0436850786209106, acc=0.5331666469573975, loss=1.0436850786209106
test: epoch 92, loss 2.470513343811035, acc=0.2750000059604645, loss=2.470513343811035
train: epoch 93, loss 1.0463043451309204, acc=0.5369444489479065, loss=1.0463043451309204
test: epoch 93, loss 2.4626386165618896, acc=0.27222222089767456, loss=2.4626386165618896
train: epoch 94, loss 1.0334430932998657, acc=0.5349444150924683, loss=1.0334430932998657
test: epoch 94, loss 2.15753173828125, acc=0.2750000059604645, loss=2.15753173828125
train: epoch 95, loss 1.032677412033081, acc=0.5358889102935791, loss=1.032677412033081
test: epoch 95, loss 2.2213144302368164, acc=0.2750000059604645, loss=2.2213144302368164
train: epoch 96, loss 1.0310901403427124, acc=0.5410555601119995, loss=1.0310901403427124
test: epoch 96, loss 2.5512402057647705, acc=0.2750000059604645, loss=2.5512402057647705
train: epoch 97, loss 1.052506446838379, acc=0.5311111211776733, loss=1.052506446838379
test: epoch 97, loss 2.266348361968994, acc=0.27222222089767456, loss=2.266348361968994
train: epoch 98, loss 1.041020393371582, acc=0.5316110849380493, loss=1.041020393371582
test: epoch 98, loss 2.371797800064087, acc=0.2750000059604645, loss=2.371797800064087
train: epoch 99, loss 1.0176233053207397, acc=0.5462222099304199, loss=1.0176233053207397
test: epoch 99, loss 2.3227427005767822, acc=0.26944443583488464, loss=2.3227427005767822
train: epoch 100, loss 1.0312681198120117, acc=0.5443333387374878, loss=1.0312681198120117
test: epoch 100, loss 2.2283809185028076, acc=0.27222222089767456, loss=2.2283809185028076
train: epoch 101, loss 1.0274893045425415, acc=0.5377222299575806, loss=1.0274893045425415
test: epoch 101, loss 2.282174587249756, acc=0.28333333134651184, loss=2.282174587249756
train: epoch 102, loss 1.0294090509414673, acc=0.538777768611908, loss=1.0294090509414673
test: epoch 102, loss 2.835264205932617, acc=0.27222222089767456, loss=2.835264205932617
train: epoch 103, loss 1.0272353887557983, acc=0.5394444465637207, loss=1.0272353887557983
test: epoch 103, loss 2.430143117904663, acc=0.2638888955116272, loss=2.430143117904663
train: epoch 104, loss 1.0339751243591309, acc=0.5421666502952576, loss=1.0339751243591309
test: epoch 104, loss 2.510363817214966, acc=0.2750000059604645, loss=2.510363817214966
train: epoch 105, loss 1.0275133848190308, acc=0.542388916015625, loss=1.0275133848190308
test: epoch 105, loss 2.350858211517334, acc=0.26944443583488464, loss=2.350858211517334
train: epoch 106, loss 1.0259664058685303, acc=0.5432222485542297, loss=1.0259664058685303
test: epoch 106, loss 2.2587766647338867, acc=0.27222222089767456, loss=2.2587766647338867
train: epoch 107, loss 1.0271079540252686, acc=0.5362222194671631, loss=1.0271079540252686
test: epoch 107, loss 2.171020984649658, acc=0.2750000059604645, loss=2.171020984649658
train: epoch 108, loss 1.02070951461792, acc=0.5401666760444641, loss=1.02070951461792
test: epoch 108, loss 2.635685682296753, acc=0.2611111104488373, loss=2.635685682296753
train: epoch 109, loss 1.025320053100586, acc=0.5409444570541382, loss=1.025320053100586
test: epoch 109, loss 2.4813759326934814, acc=0.27222222089767456, loss=2.4813759326934814
train: epoch 110, loss 1.0079983472824097, acc=0.5424444675445557, loss=1.0079983472824097
test: epoch 110, loss 2.8401663303375244, acc=0.2750000059604645, loss=2.8401663303375244
train: epoch 111, loss 1.0361943244934082, acc=0.5305555462837219, loss=1.0361943244934082
test: epoch 111, loss 2.3153698444366455, acc=0.27222222089767456, loss=2.3153698444366455
train: epoch 112, loss 1.0252598524093628, acc=0.5452222228050232, loss=1.0252598524093628
test: epoch 112, loss 2.4147274494171143, acc=0.2750000059604645, loss=2.4147274494171143
train: epoch 113, loss 1.0389134883880615, acc=0.5318889021873474, loss=1.0389134883880615
test: epoch 113, loss 2.3310344219207764, acc=0.2750000059604645, loss=2.3310344219207764
train: epoch 114, loss 1.0220825672149658, acc=0.5370000004768372, loss=1.0220825672149658
test: epoch 114, loss 2.3453292846679688, acc=0.2805555462837219, loss=2.3453292846679688
train: epoch 115, loss 1.0116156339645386, acc=0.5443333387374878, loss=1.0116156339645386
test: epoch 115, loss 2.3800342082977295, acc=0.27222222089767456, loss=2.3800342082977295
train: epoch 116, loss 1.016168236732483, acc=0.5461111068725586, loss=1.016168236732483
test: epoch 116, loss 2.392547607421875, acc=0.2750000059604645, loss=2.392547607421875
train: epoch 117, loss 1.0231351852416992, acc=0.5469444394111633, loss=1.0231351852416992
test: epoch 117, loss 2.418172597885132, acc=0.2777777910232544, loss=2.418172597885132
train: epoch 118, loss 1.0239442586898804, acc=0.5442222356796265, loss=1.0239442586898804
test: epoch 118, loss 2.197561025619507, acc=0.2750000059604645, loss=2.197561025619507
train: epoch 119, loss 1.0148851871490479, acc=0.5449444651603699, loss=1.0148851871490479
test: epoch 119, loss 2.304914712905884, acc=0.27222222089767456, loss=2.304914712905884
train: epoch 120, loss 1.0198203325271606, acc=0.5444999933242798, loss=1.0198203325271606
test: epoch 120, loss 2.4485344886779785, acc=0.2666666805744171, loss=2.4485344886779785
train: epoch 121, loss 1.02034330368042, acc=0.5397777557373047, loss=1.02034330368042
test: epoch 121, loss 2.466813325881958, acc=0.27222222089767456, loss=2.466813325881958
train: epoch 122, loss 1.025424838066101, acc=0.538611114025116, loss=1.025424838066101
test: epoch 122, loss 2.426913022994995, acc=0.2750000059604645, loss=2.426913022994995
train: epoch 123, loss 1.0120060443878174, acc=0.5444444417953491, loss=1.0120060443878174
test: epoch 123, loss 2.3300893306732178, acc=0.2666666805744171, loss=2.3300893306732178
train: epoch 124, loss 1.0217677354812622, acc=0.5473333597183228, loss=1.0217677354812622
test: epoch 124, loss 2.334182024002075, acc=0.2777777910232544, loss=2.334182024002075
train: epoch 125, loss 1.019423007965088, acc=0.5400555729866028, loss=1.019423007965088
test: epoch 125, loss 2.2832837104797363, acc=0.2750000059604645, loss=2.2832837104797363
train: epoch 126, loss 1.0179405212402344, acc=0.5463333129882812, loss=1.0179405212402344
test: epoch 126, loss 2.371579170227051, acc=0.27222222089767456, loss=2.371579170227051
train: epoch 127, loss 1.017860770225525, acc=0.5372222065925598, loss=1.017860770225525
test: epoch 127, loss 2.802399158477783, acc=0.2777777910232544, loss=2.802399158477783
train: epoch 128, loss 1.0273146629333496, acc=0.5417222380638123, loss=1.0273146629333496
test: epoch 128, loss 2.3882906436920166, acc=0.2750000059604645, loss=2.3882906436920166
train: epoch 129, loss 1.0148835182189941, acc=0.5425000190734863, loss=1.0148835182189941
test: epoch 129, loss 2.430198907852173, acc=0.26944443583488464, loss=2.430198907852173
train: epoch 130, loss 0.9990293383598328, acc=0.5459444522857666, loss=0.9990293383598328
test: epoch 130, loss 2.397531270980835, acc=0.2750000059604645, loss=2.397531270980835
train: epoch 131, loss 1.0181055068969727, acc=0.5435555577278137, loss=1.0181055068969727
test: epoch 131, loss 2.5692594051361084, acc=0.2805555462837219, loss=2.5692594051361084
train: epoch 132, loss 1.0110316276550293, acc=0.5467222332954407, loss=1.0110316276550293
test: epoch 132, loss 2.4687540531158447, acc=0.2805555462837219, loss=2.4687540531158447
train: epoch 133, loss 1.0143541097640991, acc=0.5490555763244629, loss=1.0143541097640991
test: epoch 133, loss 2.3547797203063965, acc=0.28611111640930176, loss=2.3547797203063965
train: epoch 134, loss 1.003273844718933, acc=0.5486666560173035, loss=1.003273844718933
test: epoch 134, loss 2.640038013458252, acc=0.2805555462837219, loss=2.640038013458252
train: epoch 135, loss 1.0129270553588867, acc=0.5453333258628845, loss=1.0129270553588867
test: epoch 135, loss 2.4873173236846924, acc=0.2805555462837219, loss=2.4873173236846924
train: epoch 136, loss 1.00534987449646, acc=0.5401666760444641, loss=1.00534987449646
test: epoch 136, loss 2.4118402004241943, acc=0.2805555462837219, loss=2.4118402004241943
train: epoch 137, loss 1.0100727081298828, acc=0.5444999933242798, loss=1.0100727081298828
test: epoch 137, loss 2.2373905181884766, acc=0.28611111640930176, loss=2.2373905181884766
train: epoch 138, loss 1.007949948310852, acc=0.5437777638435364, loss=1.007949948310852
test: epoch 138, loss 2.5658648014068604, acc=0.28611111640930176, loss=2.5658648014068604
train: epoch 139, loss 1.0084530115127563, acc=0.5467222332954407, loss=1.0084530115127563
test: epoch 139, loss 2.357015371322632, acc=0.28611111640930176, loss=2.357015371322632
train: epoch 140, loss 1.0157159566879272, acc=0.5429444313049316, loss=1.0157159566879272
test: epoch 140, loss 2.4473676681518555, acc=0.28611111640930176, loss=2.4473676681518555
train: epoch 141, loss 1.0022670030593872, acc=0.5463888645172119, loss=1.0022670030593872
test: epoch 141, loss 2.180434226989746, acc=0.2888889014720917, loss=2.180434226989746
train: epoch 142, loss 1.0068724155426025, acc=0.5448889136314392, loss=1.0068724155426025
test: epoch 142, loss 2.4333999156951904, acc=0.28611111640930176, loss=2.4333999156951904
train: epoch 143, loss 0.995543897151947, acc=0.5482777953147888, loss=0.995543897151947
test: epoch 143, loss 2.5910353660583496, acc=0.28611111640930176, loss=2.5910353660583496
train: epoch 144, loss 1.0018532276153564, acc=0.5516111254692078, loss=1.0018532276153564
test: epoch 144, loss 2.1414947509765625, acc=0.2777777910232544, loss=2.1414947509765625
train: epoch 145, loss 0.9903366565704346, acc=0.5511666536331177, loss=0.9903366565704346
test: epoch 145, loss 2.5682532787323, acc=0.28333333134651184, loss=2.5682532787323
train: epoch 146, loss 0.9976286292076111, acc=0.5517222285270691, loss=0.9976286292076111
test: epoch 146, loss 2.6471855640411377, acc=0.28611111640930176, loss=2.6471855640411377
train: epoch 147, loss 0.9860744476318359, acc=0.5528333187103271, loss=0.9860744476318359
test: epoch 147, loss 2.3408398628234863, acc=0.28333333134651184, loss=2.3408398628234863
train: epoch 148, loss 0.9872405529022217, acc=0.5543888807296753, loss=0.9872405529022217
test: epoch 148, loss 2.418948173522949, acc=0.2777777910232544, loss=2.418948173522949
train: epoch 149, loss 0.9880099296569824, acc=0.5519999861717224, loss=0.9880099296569824
test: epoch 149, loss 2.3892781734466553, acc=0.28333333134651184, loss=2.3892781734466553
train: epoch 150, loss 0.9962921738624573, acc=0.5506666898727417, loss=0.9962921738624573
test: epoch 150, loss 2.2902374267578125, acc=0.28611111640930176, loss=2.2902374267578125
