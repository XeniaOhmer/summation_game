# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=641107972, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=641107972, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.510916233062744, acc=0.13833333551883698, loss=2.510916233062744
test: epoch 1, loss 5.401546478271484, acc=0.05833333358168602, loss=5.401546478271484
train: epoch 2, loss 1.7900314331054688, acc=0.2824999988079071, loss=1.7900314331054688
test: epoch 2, loss 4.272446155548096, acc=0.13611111044883728, loss=4.272446155548096
train: epoch 3, loss 1.4870495796203613, acc=0.3850555419921875, loss=1.4870495796203613
test: epoch 3, loss 3.2590408325195312, acc=0.17777778208255768, loss=3.2590408325195312
train: epoch 4, loss 1.3287163972854614, acc=0.4424999952316284, loss=1.3287163972854614
test: epoch 4, loss 2.3571221828460693, acc=0.2083333283662796, loss=2.3571221828460693
train: epoch 5, loss 1.1758155822753906, acc=0.5071666836738586, loss=1.1758155822753906
test: epoch 5, loss 3.7348132133483887, acc=0.20555555820465088, loss=3.7348132133483887
train: epoch 6, loss 1.080216646194458, acc=0.5567222237586975, loss=1.080216646194458
test: epoch 6, loss 2.1449100971221924, acc=0.3166666626930237, loss=2.1449100971221924
train: epoch 7, loss 1.003697395324707, acc=0.5851110816001892, loss=1.003697395324707
test: epoch 7, loss 1.9503177404403687, acc=0.3194444477558136, loss=1.9503177404403687
train: epoch 8, loss 0.9687093496322632, acc=0.6019444465637207, loss=0.9687093496322632
test: epoch 8, loss 2.2619192600250244, acc=0.23055554926395416, loss=2.2619192600250244
train: epoch 9, loss 0.8918225169181824, acc=0.6362777948379517, loss=0.8918225169181824
test: epoch 9, loss 1.7631293535232544, acc=0.3611111044883728, loss=1.7631293535232544
train: epoch 10, loss 0.8771209716796875, acc=0.6392777562141418, loss=0.8771209716796875
test: epoch 10, loss 1.7799696922302246, acc=0.2916666567325592, loss=1.7799696922302246
train: epoch 11, loss 0.8229740858078003, acc=0.6646111011505127, loss=0.8229740858078003
test: epoch 11, loss 1.5765812397003174, acc=0.36944442987442017, loss=1.5765812397003174
train: epoch 12, loss 0.7732857465744019, acc=0.6821666955947876, loss=0.7732857465744019
test: epoch 12, loss 1.7907516956329346, acc=0.38333332538604736, loss=1.7907516956329346
train: epoch 13, loss 0.7755845189094543, acc=0.6856666803359985, loss=0.7755845189094543
test: epoch 13, loss 1.9817817211151123, acc=0.26944443583488464, loss=1.9817817211151123
train: epoch 14, loss 0.7068340182304382, acc=0.7118889093399048, loss=0.7068340182304382
test: epoch 14, loss 2.153865098953247, acc=0.3055555522441864, loss=2.153865098953247
train: epoch 15, loss 0.7309193015098572, acc=0.7026666402816772, loss=0.7309193015098572
test: epoch 15, loss 1.7453138828277588, acc=0.4333333373069763, loss=1.7453138828277588
train: epoch 16, loss 0.7083619832992554, acc=0.7085555791854858, loss=0.7083619832992554
test: epoch 16, loss 1.4857239723205566, acc=0.4055555462837219, loss=1.4857239723205566
train: epoch 17, loss 0.681338906288147, acc=0.7204999923706055, loss=0.681338906288147
test: epoch 17, loss 1.7160395383834839, acc=0.3888888955116272, loss=1.7160395383834839
train: epoch 18, loss 0.6772693395614624, acc=0.7205555438995361, loss=0.6772693395614624
test: epoch 18, loss 1.6069831848144531, acc=0.375, loss=1.6069831848144531
train: epoch 19, loss 0.6405399441719055, acc=0.7389444708824158, loss=0.6405399441719055
test: epoch 19, loss 1.6532098054885864, acc=0.3361110985279083, loss=1.6532098054885864
train: epoch 20, loss 0.6351715326309204, acc=0.741777777671814, loss=0.6351715326309204
test: epoch 20, loss 1.5253486633300781, acc=0.35277777910232544, loss=1.5253486633300781
train: epoch 21, loss 0.6410384178161621, acc=0.7351111173629761, loss=0.6410384178161621
test: epoch 21, loss 1.8550740480422974, acc=0.3638888895511627, loss=1.8550740480422974
train: epoch 22, loss 0.6134554743766785, acc=0.7534444332122803, loss=0.6134554743766785
test: epoch 22, loss 1.4431955814361572, acc=0.4138889014720917, loss=1.4431955814361572
train: epoch 23, loss 0.6118757128715515, acc=0.7490000128746033, loss=0.6118757128715515
test: epoch 23, loss 2.023297071456909, acc=0.3472222089767456, loss=2.023297071456909
train: epoch 24, loss 0.589459240436554, acc=0.7584999799728394, loss=0.589459240436554
test: epoch 24, loss 1.541164517402649, acc=0.3722222149372101, loss=1.541164517402649
train: epoch 25, loss 0.5887718200683594, acc=0.7578333616256714, loss=0.5887718200683594
test: epoch 25, loss 1.723245620727539, acc=0.3499999940395355, loss=1.723245620727539
train: epoch 26, loss 0.5824220180511475, acc=0.7609999775886536, loss=0.5824220180511475
test: epoch 26, loss 1.5079810619354248, acc=0.4277777671813965, loss=1.5079810619354248
train: epoch 27, loss 0.576452910900116, acc=0.7641111016273499, loss=0.576452910900116
test: epoch 27, loss 1.553240180015564, acc=0.3583333194255829, loss=1.553240180015564
train: epoch 28, loss 0.552810549736023, acc=0.7712222337722778, loss=0.552810549736023
test: epoch 28, loss 1.3300044536590576, acc=0.47777777910232544, loss=1.3300044536590576
train: epoch 29, loss 0.5607302188873291, acc=0.7690555453300476, loss=0.5607302188873291
test: epoch 29, loss 1.3747820854187012, acc=0.4333333373069763, loss=1.3747820854187012
train: epoch 30, loss 0.5333899259567261, acc=0.7825000286102295, loss=0.5333899259567261
test: epoch 30, loss 1.1413705348968506, acc=0.47777777910232544, loss=1.1413705348968506
train: epoch 31, loss 0.5529436469078064, acc=0.7789999842643738, loss=0.5529436469078064
test: epoch 31, loss 1.3106390237808228, acc=0.4472222328186035, loss=1.3106390237808228
train: epoch 32, loss 0.5598459243774414, acc=0.7734444737434387, loss=0.5598459243774414
test: epoch 32, loss 1.132112741470337, acc=0.4611110985279083, loss=1.132112741470337
train: epoch 33, loss 0.5518282651901245, acc=0.7759444713592529, loss=0.5518282651901245
test: epoch 33, loss 1.55892813205719, acc=0.39722222089767456, loss=1.55892813205719
train: epoch 34, loss 0.5361204743385315, acc=0.7821111083030701, loss=0.5361204743385315
test: epoch 34, loss 1.087902307510376, acc=0.5, loss=1.087902307510376
train: epoch 35, loss 0.5228691697120667, acc=0.7837222218513489, loss=0.5228691697120667
test: epoch 35, loss 1.025152564048767, acc=0.5222222208976746, loss=1.025152564048767
train: epoch 36, loss 0.5048496723175049, acc=0.7979444265365601, loss=0.5048496723175049
test: epoch 36, loss 1.0532258749008179, acc=0.5027777552604675, loss=1.0532258749008179
train: epoch 37, loss 0.5397276282310486, acc=0.7782777547836304, loss=0.5397276282310486
test: epoch 37, loss 0.9979429841041565, acc=0.5777778029441833, loss=0.9979429841041565
train: epoch 38, loss 0.5189003944396973, acc=0.7853333353996277, loss=0.5189003944396973
test: epoch 38, loss 1.0641474723815918, acc=0.5638889074325562, loss=1.0641474723815918
train: epoch 39, loss 0.4878213405609131, acc=0.8013333082199097, loss=0.4878213405609131
test: epoch 39, loss 0.9958041906356812, acc=0.4861111044883728, loss=0.9958041906356812
train: epoch 40, loss 0.5105839967727661, acc=0.7911666631698608, loss=0.5105839967727661
test: epoch 40, loss 1.046273946762085, acc=0.5777778029441833, loss=1.046273946762085
train: epoch 41, loss 0.47523513436317444, acc=0.8058888912200928, loss=0.47523513436317444
test: epoch 41, loss 0.9716359972953796, acc=0.6083333492279053, loss=0.9716359972953796
train: epoch 42, loss 0.5005566477775574, acc=0.7959444522857666, loss=0.5005566477775574
test: epoch 42, loss 1.0707423686981201, acc=0.5972222089767456, loss=1.0707423686981201
train: epoch 43, loss 0.5290775299072266, acc=0.784166693687439, loss=0.5290775299072266
test: epoch 43, loss 1.0065735578536987, acc=0.5861111283302307, loss=1.0065735578536987
train: epoch 44, loss 0.4896656572818756, acc=0.8020555377006531, loss=0.4896656572818756
test: epoch 44, loss 1.2497406005859375, acc=0.48055556416511536, loss=1.2497406005859375
train: epoch 45, loss 0.5021109580993652, acc=0.7994999885559082, loss=0.5021109580993652
test: epoch 45, loss 1.0110410451889038, acc=0.5388888716697693, loss=1.0110410451889038
train: epoch 46, loss 0.48547330498695374, acc=0.8064444661140442, loss=0.48547330498695374
test: epoch 46, loss 0.937368631362915, acc=0.5472221970558167, loss=0.937368631362915
train: epoch 47, loss 0.49589017033576965, acc=0.7997778058052063, loss=0.49589017033576965
test: epoch 47, loss 0.9227461218833923, acc=0.6000000238418579, loss=0.9227461218833923
train: epoch 48, loss 0.49882131814956665, acc=0.8031111359596252, loss=0.49882131814956665
test: epoch 48, loss 1.0072038173675537, acc=0.6027777791023254, loss=1.0072038173675537
train: epoch 49, loss 0.47872194647789, acc=0.8073333501815796, loss=0.47872194647789
test: epoch 49, loss 1.1658083200454712, acc=0.5694444179534912, loss=1.1658083200454712
train: epoch 50, loss 0.48698359727859497, acc=0.8071110844612122, loss=0.48698359727859497
test: epoch 50, loss 1.2511060237884521, acc=0.4888888895511627, loss=1.2511060237884521
train: epoch 51, loss 0.49695563316345215, acc=0.8008333444595337, loss=0.49695563316345215
test: epoch 51, loss 0.8191060423851013, acc=0.6333333253860474, loss=0.8191060423851013
train: epoch 52, loss 0.4588024318218231, acc=0.8202221989631653, loss=0.4588024318218231
test: epoch 52, loss 0.8715610504150391, acc=0.6472222208976746, loss=0.8715610504150391
train: epoch 53, loss 0.49943479895591736, acc=0.8020555377006531, loss=0.49943479895591736
test: epoch 53, loss 0.9563168287277222, acc=0.6166666746139526, loss=0.9563168287277222
train: epoch 54, loss 0.4805656671524048, acc=0.8094444274902344, loss=0.4805656671524048
test: epoch 54, loss 0.7888749241828918, acc=0.5944444537162781, loss=0.7888749241828918
train: epoch 55, loss 0.4849368929862976, acc=0.8031666874885559, loss=0.4849368929862976
test: epoch 55, loss 0.7734940052032471, acc=0.644444465637207, loss=0.7734940052032471
train: epoch 56, loss 0.5170784592628479, acc=0.7862222194671631, loss=0.5170784592628479
test: epoch 56, loss 0.8669543266296387, acc=0.6194444298744202, loss=0.8669543266296387
train: epoch 57, loss 0.45600223541259766, acc=0.82105553150177, loss=0.45600223541259766
test: epoch 57, loss 0.8453294038772583, acc=0.6694444417953491, loss=0.8453294038772583
train: epoch 58, loss 0.4861980378627777, acc=0.8109999895095825, loss=0.4861980378627777
test: epoch 58, loss 0.8226116895675659, acc=0.6805555820465088, loss=0.8226116895675659
train: epoch 59, loss 0.48923012614250183, acc=0.8055555820465088, loss=0.48923012614250183
test: epoch 59, loss 0.8615451455116272, acc=0.6527777910232544, loss=0.8615451455116272
train: epoch 60, loss 0.4514654278755188, acc=0.8207777738571167, loss=0.4514654278755188
test: epoch 60, loss 0.81964111328125, acc=0.6666666865348816, loss=0.81964111328125
train: epoch 61, loss 0.4583373963832855, acc=0.8159999847412109, loss=0.4583373963832855
test: epoch 61, loss 1.108838438987732, acc=0.5666666626930237, loss=1.108838438987732
train: epoch 62, loss 0.47717025876045227, acc=0.8115555644035339, loss=0.47717025876045227
test: epoch 62, loss 0.8648425936698914, acc=0.6194444298744202, loss=0.8648425936698914
train: epoch 63, loss 0.4710722863674164, acc=0.8142777681350708, loss=0.4710722863674164
test: epoch 63, loss 0.7640374302864075, acc=0.7194444537162781, loss=0.7640374302864075
train: epoch 64, loss 0.48115110397338867, acc=0.8071666955947876, loss=0.48115110397338867
test: epoch 64, loss 0.8727410435676575, acc=0.644444465637207, loss=0.8727410435676575
train: epoch 65, loss 0.4438430964946747, acc=0.8238333463668823, loss=0.4438430964946747
test: epoch 65, loss 0.7093936204910278, acc=0.6111111044883728, loss=0.7093936204910278
train: epoch 66, loss 0.4552763104438782, acc=0.8188889026641846, loss=0.4552763104438782
test: epoch 66, loss 0.8105258941650391, acc=0.6222222447395325, loss=0.8105258941650391
train: epoch 67, loss 0.5016112923622131, acc=0.7985555529594421, loss=0.5016112923622131
test: epoch 67, loss 0.8816055655479431, acc=0.6361111402511597, loss=0.8816055655479431
train: epoch 68, loss 0.482441246509552, acc=0.8058333396911621, loss=0.482441246509552
test: epoch 68, loss 1.0008158683776855, acc=0.5388888716697693, loss=1.0008158683776855
train: epoch 69, loss 0.45843198895454407, acc=0.8160555362701416, loss=0.45843198895454407
test: epoch 69, loss 0.860416829586029, acc=0.6194444298744202, loss=0.860416829586029
train: epoch 70, loss 0.455173522233963, acc=0.8198333382606506, loss=0.455173522233963
test: epoch 70, loss 1.0081473588943481, acc=0.5888888835906982, loss=1.0081473588943481
train: epoch 71, loss 0.48245689272880554, acc=0.8090000152587891, loss=0.48245689272880554
test: epoch 71, loss 0.863172173500061, acc=0.5888888835906982, loss=0.863172173500061
train: epoch 72, loss 0.47231796383857727, acc=0.8146111369132996, loss=0.47231796383857727
test: epoch 72, loss 0.959723949432373, acc=0.6638888716697693, loss=0.959723949432373
train: epoch 73, loss 0.5157695412635803, acc=0.7960555553436279, loss=0.5157695412635803
test: epoch 73, loss 0.8252276182174683, acc=0.6499999761581421, loss=0.8252276182174683
train: epoch 74, loss 0.5079315304756165, acc=0.795722246170044, loss=0.5079315304756165
test: epoch 74, loss 0.7692868113517761, acc=0.6666666865348816, loss=0.7692868113517761
train: epoch 75, loss 0.5548955798149109, acc=0.7888888716697693, loss=0.5548955798149109
test: epoch 75, loss 1.1365360021591187, acc=0.6222222447395325, loss=1.1365360021591187
train: epoch 76, loss 0.5478370189666748, acc=0.7840555310249329, loss=0.5478370189666748
test: epoch 76, loss 0.6693568229675293, acc=0.6888889074325562, loss=0.6693568229675293
train: epoch 77, loss 0.48909488320350647, acc=0.8065000176429749, loss=0.48909488320350647
test: epoch 77, loss 1.0116411447525024, acc=0.6194444298744202, loss=1.0116411447525024
train: epoch 78, loss 0.4508112668991089, acc=0.8212222456932068, loss=0.4508112668991089
test: epoch 78, loss 0.7472133636474609, acc=0.6388888955116272, loss=0.7472133636474609
train: epoch 79, loss 0.3971182405948639, acc=0.8475555777549744, loss=0.3971182405948639
test: epoch 79, loss 0.6838743686676025, acc=0.675000011920929, loss=0.6838743686676025
train: epoch 80, loss 0.4158821702003479, acc=0.8383333086967468, loss=0.4158821702003479
test: epoch 80, loss 0.7388994097709656, acc=0.6972222328186035, loss=0.7388994097709656
train: epoch 81, loss 0.4199303984642029, acc=0.836388885974884, loss=0.4199303984642029
test: epoch 81, loss 0.592807948589325, acc=0.7166666388511658, loss=0.592807948589325
train: epoch 82, loss 0.42647528648376465, acc=0.8346666693687439, loss=0.42647528648376465
test: epoch 82, loss 0.7013473510742188, acc=0.644444465637207, loss=0.7013473510742188
train: epoch 83, loss 0.40075939893722534, acc=0.8442222476005554, loss=0.40075939893722534
test: epoch 83, loss 0.7398018836975098, acc=0.7111111283302307, loss=0.7398018836975098
train: epoch 84, loss 0.4218015968799591, acc=0.8274999856948853, loss=0.4218015968799591
test: epoch 84, loss 0.7759111523628235, acc=0.6722221970558167, loss=0.7759111523628235
train: epoch 85, loss 0.42168736457824707, acc=0.8314444422721863, loss=0.42168736457824707
test: epoch 85, loss 0.7085848450660706, acc=0.6222222447395325, loss=0.7085848450660706
train: epoch 86, loss 0.42121216654777527, acc=0.8348333239555359, loss=0.42121216654777527
test: epoch 86, loss 0.8095920085906982, acc=0.6722221970558167, loss=0.8095920085906982
train: epoch 87, loss 0.3921476900577545, acc=0.8460000157356262, loss=0.3921476900577545
test: epoch 87, loss 0.8228078484535217, acc=0.6722221970558167, loss=0.8228078484535217
train: epoch 88, loss 0.4171382188796997, acc=0.8358888626098633, loss=0.4171382188796997
test: epoch 88, loss 0.7138562798500061, acc=0.6805555820465088, loss=0.7138562798500061
train: epoch 89, loss 0.4191630184650421, acc=0.8385555744171143, loss=0.4191630184650421
test: epoch 89, loss 0.7154795527458191, acc=0.7277777791023254, loss=0.7154795527458191
train: epoch 90, loss 0.41525062918663025, acc=0.8341666460037231, loss=0.41525062918663025
test: epoch 90, loss 0.8244555592536926, acc=0.7111111283302307, loss=0.8244555592536926
train: epoch 91, loss 0.3959638178348541, acc=0.8441666960716248, loss=0.3959638178348541
test: epoch 91, loss 0.795783519744873, acc=0.7027778029441833, loss=0.795783519744873
train: epoch 92, loss 0.36254069209098816, acc=0.8551666736602783, loss=0.36254069209098816
test: epoch 92, loss 0.624808669090271, acc=0.7416666746139526, loss=0.624808669090271
train: epoch 93, loss 0.49722421169281006, acc=0.8093888759613037, loss=0.49722421169281006
test: epoch 93, loss 0.7308745384216309, acc=0.6861110925674438, loss=0.7308745384216309
train: epoch 94, loss 0.4566686153411865, acc=0.8247777819633484, loss=0.4566686153411865
test: epoch 94, loss 0.6178796291351318, acc=0.7388888597488403, loss=0.6178796291351318
train: epoch 95, loss 0.4201878011226654, acc=0.8340555429458618, loss=0.4201878011226654
test: epoch 95, loss 0.6272290945053101, acc=0.7416666746139526, loss=0.6272290945053101
train: epoch 96, loss 0.4187539517879486, acc=0.8358333110809326, loss=0.4187539517879486
test: epoch 96, loss 0.6990038752555847, acc=0.6777777671813965, loss=0.6990038752555847
train: epoch 97, loss 0.4100607931613922, acc=0.8436111211776733, loss=0.4100607931613922
test: epoch 97, loss 0.7389948964118958, acc=0.7444444298744202, loss=0.7389948964118958
train: epoch 98, loss 0.4441584646701813, acc=0.8289444446563721, loss=0.4441584646701813
test: epoch 98, loss 0.5489988923072815, acc=0.7388888597488403, loss=0.5489988923072815
train: epoch 99, loss 0.41859573125839233, acc=0.8371666669845581, loss=0.41859573125839233
test: epoch 99, loss 0.6203857660293579, acc=0.7083333134651184, loss=0.6203857660293579
train: epoch 100, loss 0.47098273038864136, acc=0.8153889179229736, loss=0.47098273038864136
test: epoch 100, loss 0.8629270792007446, acc=0.6722221970558167, loss=0.8629270792007446
train: epoch 101, loss 0.4014892876148224, acc=0.8458889126777649, loss=0.4014892876148224
test: epoch 101, loss 0.6891162395477295, acc=0.7222222089767456, loss=0.6891162395477295
train: epoch 102, loss 0.4451337158679962, acc=0.8270555734634399, loss=0.4451337158679962
test: epoch 102, loss 0.6055250763893127, acc=0.7111111283302307, loss=0.6055250763893127
train: epoch 103, loss 0.43051111698150635, acc=0.8301110863685608, loss=0.43051111698150635
test: epoch 103, loss 0.5709720253944397, acc=0.75, loss=0.5709720253944397
train: epoch 104, loss 0.38981977105140686, acc=0.8473333120346069, loss=0.38981977105140686
test: epoch 104, loss 0.8023220896720886, acc=0.7083333134651184, loss=0.8023220896720886
train: epoch 105, loss 0.3807121515274048, acc=0.8519999980926514, loss=0.3807121515274048
test: epoch 105, loss 0.7151361703872681, acc=0.7361111044883728, loss=0.7151361703872681
train: epoch 106, loss 0.43799054622650146, acc=0.8284444212913513, loss=0.43799054622650146
test: epoch 106, loss 0.6760851740837097, acc=0.7222222089767456, loss=0.6760851740837097
train: epoch 107, loss 0.41151726245880127, acc=0.8391110897064209, loss=0.41151726245880127
test: epoch 107, loss 0.6937226057052612, acc=0.7361111044883728, loss=0.6937226057052612
train: epoch 108, loss 0.38253846764564514, acc=0.8503888845443726, loss=0.38253846764564514
test: epoch 108, loss 0.6411064863204956, acc=0.7388888597488403, loss=0.6411064863204956
train: epoch 109, loss 0.4743131995201111, acc=0.8030555844306946, loss=0.4743131995201111
test: epoch 109, loss 0.8296078443527222, acc=0.6583333611488342, loss=0.8296078443527222
train: epoch 110, loss 0.6271130442619324, acc=0.7329999804496765, loss=0.6271130442619324
test: epoch 110, loss 0.7983153462409973, acc=0.6388888955116272, loss=0.7983153462409973
train: epoch 111, loss 0.5308369994163513, acc=0.7734444737434387, loss=0.5308369994163513
test: epoch 111, loss 0.8353865146636963, acc=0.6722221970558167, loss=0.8353865146636963
train: epoch 112, loss 0.4844258427619934, acc=0.7864999771118164, loss=0.4844258427619934
test: epoch 112, loss 0.801135778427124, acc=0.6499999761581421, loss=0.801135778427124
train: epoch 113, loss 0.5170777440071106, acc=0.7748888731002808, loss=0.5170777440071106
test: epoch 113, loss 0.8170824646949768, acc=0.6277777552604675, loss=0.8170824646949768
train: epoch 114, loss 0.5368704199790955, acc=0.7628333568572998, loss=0.5368704199790955
test: epoch 114, loss 0.7166357636451721, acc=0.6611111164093018, loss=0.7166357636451721
train: epoch 115, loss 0.49235430359840393, acc=0.7836111187934875, loss=0.49235430359840393
test: epoch 115, loss 0.6826607584953308, acc=0.6638888716697693, loss=0.6826607584953308
train: epoch 116, loss 0.541761577129364, acc=0.7695000171661377, loss=0.541761577129364
test: epoch 116, loss 0.7842795848846436, acc=0.6777777671813965, loss=0.7842795848846436
train: epoch 117, loss 0.5279480218887329, acc=0.7686111330986023, loss=0.5279480218887329
test: epoch 117, loss 0.7744274735450745, acc=0.6888889074325562, loss=0.7744274735450745
train: epoch 118, loss 0.49603739380836487, acc=0.7758888602256775, loss=0.49603739380836487
test: epoch 118, loss 0.8396099805831909, acc=0.6722221970558167, loss=0.8396099805831909
train: epoch 119, loss 0.49950534105300903, acc=0.7747777700424194, loss=0.49950534105300903
test: epoch 119, loss 0.7867462635040283, acc=0.644444465637207, loss=0.7867462635040283
train: epoch 120, loss 0.49879351258277893, acc=0.780055582523346, loss=0.49879351258277893
test: epoch 120, loss 0.8320028781890869, acc=0.675000011920929, loss=0.8320028781890869
train: epoch 121, loss 0.49762651324272156, acc=0.7816666960716248, loss=0.49762651324272156
test: epoch 121, loss 0.7254409790039062, acc=0.6722221970558167, loss=0.7254409790039062
train: epoch 122, loss 0.5106560587882996, acc=0.7823888659477234, loss=0.5106560587882996
test: epoch 122, loss 0.855078399181366, acc=0.6583333611488342, loss=0.855078399181366
train: epoch 123, loss 0.540855348110199, acc=0.7763888835906982, loss=0.540855348110199
test: epoch 123, loss 0.8211278319358826, acc=0.6583333611488342, loss=0.8211278319358826
train: epoch 124, loss 0.5208020210266113, acc=0.7726666927337646, loss=0.5208020210266113
test: epoch 124, loss 0.7130810618400574, acc=0.7055555582046509, loss=0.7130810618400574
train: epoch 125, loss 0.4906170070171356, acc=0.773277759552002, loss=0.4906170070171356
test: epoch 125, loss 0.8409194350242615, acc=0.6666666865348816, loss=0.8409194350242615
train: epoch 126, loss 0.5142536163330078, acc=0.7713333368301392, loss=0.5142536163330078
test: epoch 126, loss 0.9232584834098816, acc=0.6527777910232544, loss=0.9232584834098816
train: epoch 127, loss 0.49125733971595764, acc=0.7799999713897705, loss=0.49125733971595764
test: epoch 127, loss 0.8558626174926758, acc=0.6333333253860474, loss=0.8558626174926758
train: epoch 128, loss 0.5109853148460388, acc=0.7706666588783264, loss=0.5109853148460388
test: epoch 128, loss 0.8129990696907043, acc=0.675000011920929, loss=0.8129990696907043
train: epoch 129, loss 0.5161300897598267, acc=0.7672222256660461, loss=0.5161300897598267
test: epoch 129, loss 0.7040097117424011, acc=0.6777777671813965, loss=0.7040097117424011
train: epoch 130, loss 0.4955906867980957, acc=0.7787222266197205, loss=0.4955906867980957
test: epoch 130, loss 0.7393405437469482, acc=0.675000011920929, loss=0.7393405437469482
train: epoch 131, loss 0.5426167249679565, acc=0.7733888626098633, loss=0.5426167249679565
test: epoch 131, loss 0.7682175636291504, acc=0.6805555820465088, loss=0.7682175636291504
train: epoch 132, loss 0.5072510242462158, acc=0.7873888611793518, loss=0.5072510242462158
test: epoch 132, loss 0.8809119462966919, acc=0.6527777910232544, loss=0.8809119462966919
train: epoch 133, loss 0.5008071660995483, acc=0.7924444675445557, loss=0.5008071660995483
test: epoch 133, loss 0.8208186030387878, acc=0.6805555820465088, loss=0.8208186030387878
train: epoch 134, loss 0.5004708170890808, acc=0.7920555472373962, loss=0.5004708170890808
test: epoch 134, loss 0.7430126667022705, acc=0.6888889074325562, loss=0.7430126667022705
train: epoch 135, loss 0.5093460083007812, acc=0.7903888821601868, loss=0.5093460083007812
test: epoch 135, loss 0.8021647334098816, acc=0.6638888716697693, loss=0.8021647334098816
train: epoch 136, loss 0.5619722008705139, acc=0.7698888778686523, loss=0.5619722008705139
test: epoch 136, loss 0.8268205523490906, acc=0.6833333373069763, loss=0.8268205523490906
train: epoch 137, loss 0.5126252770423889, acc=0.7883889079093933, loss=0.5126252770423889
test: epoch 137, loss 0.7487931251525879, acc=0.6861110925674438, loss=0.7487931251525879
train: epoch 138, loss 0.574371337890625, acc=0.7730555534362793, loss=0.574371337890625
test: epoch 138, loss 0.6717137098312378, acc=0.6805555820465088, loss=0.6717137098312378
train: epoch 139, loss 0.5212711691856384, acc=0.7863888740539551, loss=0.5212711691856384
test: epoch 139, loss 0.7414228916168213, acc=0.6833333373069763, loss=0.7414228916168213
train: epoch 140, loss 0.5372802019119263, acc=0.7867222428321838, loss=0.5372802019119263
test: epoch 140, loss 0.7649024128913879, acc=0.6833333373069763, loss=0.7649024128913879
train: epoch 141, loss 0.5200512409210205, acc=0.785277783870697, loss=0.5200512409210205
test: epoch 141, loss 0.7832641005516052, acc=0.6833333373069763, loss=0.7832641005516052
train: epoch 142, loss 0.5197912454605103, acc=0.7761111259460449, loss=0.5197912454605103
test: epoch 142, loss 0.7326014637947083, acc=0.6555555462837219, loss=0.7326014637947083
train: epoch 143, loss 0.49312061071395874, acc=0.7787777781486511, loss=0.49312061071395874
test: epoch 143, loss 0.7806694507598877, acc=0.6583333611488342, loss=0.7806694507598877
train: epoch 144, loss 0.530471682548523, acc=0.769444465637207, loss=0.530471682548523
test: epoch 144, loss 1.6813015937805176, acc=0.5416666865348816, loss=1.6813015937805176
train: epoch 145, loss 0.5088251829147339, acc=0.7733333110809326, loss=0.5088251829147339
test: epoch 145, loss 0.8043688535690308, acc=0.675000011920929, loss=0.8043688535690308
train: epoch 146, loss 0.5015686750411987, acc=0.7783889174461365, loss=0.5015686750411987
test: epoch 146, loss 0.7909502983093262, acc=0.6777777671813965, loss=0.7909502983093262
train: epoch 147, loss 0.5096412301063538, acc=0.7743333578109741, loss=0.5096412301063538
test: epoch 147, loss 0.8044403195381165, acc=0.6777777671813965, loss=0.8044403195381165
train: epoch 148, loss 0.4937145411968231, acc=0.7839444279670715, loss=0.4937145411968231
test: epoch 148, loss 0.7126408219337463, acc=0.6805555820465088, loss=0.7126408219337463
train: epoch 149, loss 0.4859624207019806, acc=0.7817777991294861, loss=0.4859624207019806
test: epoch 149, loss 0.7939909100532532, acc=0.6611111164093018, loss=0.7939909100532532
train: epoch 150, loss 0.45020657777786255, acc=0.7943888902664185, loss=0.45020657777786255
test: epoch 150, loss 0.7849201560020447, acc=0.6833333373069763, loss=0.7849201560020447
