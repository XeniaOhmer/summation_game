# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.995", "--one_hot=0", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1849050399, receiver_embed_dim=128, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1849050399, receiver_embed_dim=128, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.8810007572174072, acc=0.09377777576446533, loss=2.8810007572174072
test: epoch 1, loss 2.967782974243164, acc=0.08888889104127884, loss=2.967782974243164
train: epoch 2, loss 2.4033327102661133, acc=0.1529444456100464, loss=2.4033327102661133
test: epoch 2, loss 2.7173049449920654, acc=0.11666666716337204, loss=2.7173049449920654
train: epoch 3, loss 2.1981964111328125, acc=0.19294443726539612, loss=2.1981964111328125
test: epoch 3, loss 2.792480230331421, acc=0.11944444477558136, loss=2.792480230331421
train: epoch 4, loss 2.09757137298584, acc=0.20494444668293, loss=2.09757137298584
test: epoch 4, loss 2.8211607933044434, acc=0.11666666716337204, loss=2.8211607933044434
train: epoch 5, loss 2.022129774093628, acc=0.22200000286102295, loss=2.022129774093628
test: epoch 5, loss 2.672807455062866, acc=0.10555555671453476, loss=2.672807455062866
train: epoch 6, loss 1.9684165716171265, acc=0.23838889598846436, loss=1.9684165716171265
test: epoch 6, loss 2.8946940898895264, acc=0.13055555522441864, loss=2.8946940898895264
train: epoch 7, loss 1.899849534034729, acc=0.2611111104488373, loss=1.899849534034729
test: epoch 7, loss 2.808884859085083, acc=0.14722222089767456, loss=2.808884859085083
train: epoch 8, loss 1.8571949005126953, acc=0.2728888988494873, loss=1.8571949005126953
test: epoch 8, loss 2.438793659210205, acc=0.1388888955116272, loss=2.438793659210205
train: epoch 9, loss 1.851439356803894, acc=0.277444452047348, loss=1.851439356803894
test: epoch 9, loss 2.279340982437134, acc=0.1666666716337204, loss=2.279340982437134
train: epoch 10, loss 1.8020561933517456, acc=0.28911110758781433, loss=1.8020561933517456
test: epoch 10, loss 2.4118385314941406, acc=0.16388888657093048, loss=2.4118385314941406
train: epoch 11, loss 1.7676504850387573, acc=0.29766666889190674, loss=1.7676504850387573
test: epoch 11, loss 2.3556430339813232, acc=0.16388888657093048, loss=2.3556430339813232
train: epoch 12, loss 1.734192132949829, acc=0.304833322763443, loss=1.734192132949829
test: epoch 12, loss 2.2774791717529297, acc=0.1527777761220932, loss=2.2774791717529297
train: epoch 13, loss 1.7044947147369385, acc=0.324444442987442, loss=1.7044947147369385
test: epoch 13, loss 2.3478305339813232, acc=0.16111111640930176, loss=2.3478305339813232
train: epoch 14, loss 1.665709376335144, acc=0.3327777683734894, loss=1.665709376335144
test: epoch 14, loss 2.4204604625701904, acc=0.15555556118488312, loss=2.4204604625701904
train: epoch 15, loss 1.6375106573104858, acc=0.33677777647972107, loss=1.6375106573104858
test: epoch 15, loss 2.299769878387451, acc=0.1666666716337204, loss=2.299769878387451
train: epoch 16, loss 1.5868892669677734, acc=0.35811111330986023, loss=1.5868892669677734
test: epoch 16, loss 2.365856647491455, acc=0.1666666716337204, loss=2.365856647491455
train: epoch 17, loss 1.5715222358703613, acc=0.3583333194255829, loss=1.5715222358703613
test: epoch 17, loss 2.1971328258514404, acc=0.18333333730697632, loss=2.1971328258514404
train: epoch 18, loss 1.5452871322631836, acc=0.3687777817249298, loss=1.5452871322631836
test: epoch 18, loss 2.328076124191284, acc=0.13333334028720856, loss=2.328076124191284
train: epoch 19, loss 1.5381333827972412, acc=0.37488889694213867, loss=1.5381333827972412
test: epoch 19, loss 2.2175326347351074, acc=0.21111111342906952, loss=2.2175326347351074
train: epoch 20, loss 1.5145937204360962, acc=0.3825555443763733, loss=1.5145937204360962
test: epoch 20, loss 2.2518603801727295, acc=0.16111111640930176, loss=2.2518603801727295
train: epoch 21, loss 1.4906272888183594, acc=0.39266666769981384, loss=1.4906272888183594
test: epoch 21, loss 2.2210381031036377, acc=0.20277777314186096, loss=2.2210381031036377
train: epoch 22, loss 1.465730905532837, acc=0.40261110663414, loss=1.465730905532837
test: epoch 22, loss 2.1536576747894287, acc=0.1944444477558136, loss=2.1536576747894287
train: epoch 23, loss 1.4383398294448853, acc=0.4064444303512573, loss=1.4383398294448853
test: epoch 23, loss 2.1131463050842285, acc=0.22499999403953552, loss=2.1131463050842285
train: epoch 24, loss 1.4332603216171265, acc=0.4188888967037201, loss=1.4332603216171265
test: epoch 24, loss 2.012913465499878, acc=0.2666666805744171, loss=2.012913465499878
train: epoch 25, loss 1.4062767028808594, acc=0.42516666650772095, loss=1.4062767028808594
test: epoch 25, loss 2.088909864425659, acc=0.22499999403953552, loss=2.088909864425659
train: epoch 26, loss 1.4031550884246826, acc=0.42961111664772034, loss=1.4031550884246826
test: epoch 26, loss 1.977849006652832, acc=0.24166665971279144, loss=1.977849006652832
train: epoch 27, loss 1.3881956338882446, acc=0.43138888478279114, loss=1.3881956338882446
test: epoch 27, loss 2.101520299911499, acc=0.21944443881511688, loss=2.101520299911499
train: epoch 28, loss 1.379760503768921, acc=0.4421111047267914, loss=1.379760503768921
test: epoch 28, loss 2.040471076965332, acc=0.23055554926395416, loss=2.040471076965332
train: epoch 29, loss 1.339380145072937, acc=0.45394444465637207, loss=1.339380145072937
test: epoch 29, loss 1.8373451232910156, acc=0.2527777850627899, loss=1.8373451232910156
train: epoch 30, loss 1.339441180229187, acc=0.4573333263397217, loss=1.339441180229187
test: epoch 30, loss 1.9519855976104736, acc=0.2361111044883728, loss=1.9519855976104736
train: epoch 31, loss 1.3126024007797241, acc=0.4620000123977661, loss=1.3126024007797241
test: epoch 31, loss 1.8618782758712769, acc=0.2805555462837219, loss=1.8618782758712769
train: epoch 32, loss 1.3015198707580566, acc=0.4676111042499542, loss=1.3015198707580566
test: epoch 32, loss 2.0665979385375977, acc=0.25833332538604736, loss=2.0665979385375977
train: epoch 33, loss 1.2962576150894165, acc=0.47705554962158203, loss=1.2962576150894165
test: epoch 33, loss 1.9417272806167603, acc=0.2750000059604645, loss=1.9417272806167603
train: epoch 34, loss 1.2752541303634644, acc=0.47866666316986084, loss=1.2752541303634644
test: epoch 34, loss 2.245913028717041, acc=0.20000000298023224, loss=2.245913028717041
train: epoch 35, loss 1.2505627870559692, acc=0.48750001192092896, loss=1.2505627870559692
test: epoch 35, loss 2.0858170986175537, acc=0.27222222089767456, loss=2.0858170986175537
train: epoch 36, loss 1.247693419456482, acc=0.48572221398353577, loss=1.247693419456482
test: epoch 36, loss 1.8823561668395996, acc=0.2527777850627899, loss=1.8823561668395996
train: epoch 37, loss 1.2408746480941772, acc=0.49488890171051025, loss=1.2408746480941772
test: epoch 37, loss 1.8547524213790894, acc=0.25, loss=1.8547524213790894
train: epoch 38, loss 1.225340485572815, acc=0.5008333325386047, loss=1.225340485572815
test: epoch 38, loss 1.7581992149353027, acc=0.28333333134651184, loss=1.7581992149353027
train: epoch 39, loss 1.2155262231826782, acc=0.5071666836738586, loss=1.2155262231826782
test: epoch 39, loss 1.918986201286316, acc=0.25555557012557983, loss=1.918986201286316
train: epoch 40, loss 1.2179161310195923, acc=0.5037222504615784, loss=1.2179161310195923
test: epoch 40, loss 1.8475596904754639, acc=0.25833332538604736, loss=1.8475596904754639
train: epoch 41, loss 1.2010716199874878, acc=0.512333333492279, loss=1.2010716199874878
test: epoch 41, loss 1.8932139873504639, acc=0.2916666567325592, loss=1.8932139873504639
train: epoch 42, loss 1.1914193630218506, acc=0.5099999904632568, loss=1.1914193630218506
test: epoch 42, loss 1.9009264707565308, acc=0.2666666805744171, loss=1.9009264707565308
train: epoch 43, loss 1.174120306968689, acc=0.5251666903495789, loss=1.174120306968689
test: epoch 43, loss 1.9236549139022827, acc=0.28333333134651184, loss=1.9236549139022827
train: epoch 44, loss 1.1865551471710205, acc=0.5138333439826965, loss=1.1865551471710205
test: epoch 44, loss 1.873417854309082, acc=0.2750000059604645, loss=1.873417854309082
train: epoch 45, loss 1.1694090366363525, acc=0.5219444632530212, loss=1.1694090366363525
test: epoch 45, loss 1.8095943927764893, acc=0.29722222685813904, loss=1.8095943927764893
train: epoch 46, loss 1.1848031282424927, acc=0.5201666951179504, loss=1.1848031282424927
test: epoch 46, loss 1.8586821556091309, acc=0.2916666567325592, loss=1.8586821556091309
train: epoch 47, loss 1.1611765623092651, acc=0.526888906955719, loss=1.1611765623092651
test: epoch 47, loss 1.8360377550125122, acc=0.28333333134651184, loss=1.8360377550125122
train: epoch 48, loss 1.153329610824585, acc=0.5248888731002808, loss=1.153329610824585
test: epoch 48, loss 1.7075005769729614, acc=0.26944443583488464, loss=1.7075005769729614
train: epoch 49, loss 1.1518406867980957, acc=0.5278888940811157, loss=1.1518406867980957
test: epoch 49, loss 1.7794910669326782, acc=0.2916666567325592, loss=1.7794910669326782
train: epoch 50, loss 1.1338202953338623, acc=0.5410555601119995, loss=1.1338202953338623
test: epoch 50, loss 1.8143666982650757, acc=0.29722222685813904, loss=1.8143666982650757
train: epoch 51, loss 1.1366918087005615, acc=0.5368888974189758, loss=1.1366918087005615
test: epoch 51, loss 1.7986938953399658, acc=0.2944444417953491, loss=1.7986938953399658
train: epoch 52, loss 1.1332504749298096, acc=0.5372777581214905, loss=1.1332504749298096
test: epoch 52, loss 1.8074880838394165, acc=0.30000001192092896, loss=1.8074880838394165
train: epoch 53, loss 1.119053840637207, acc=0.5468888878822327, loss=1.119053840637207
test: epoch 53, loss 1.8265042304992676, acc=0.2666666805744171, loss=1.8265042304992676
train: epoch 54, loss 1.1342378854751587, acc=0.543055534362793, loss=1.1342378854751587
test: epoch 54, loss 1.7232493162155151, acc=0.3083333373069763, loss=1.7232493162155151
train: epoch 55, loss 1.1227554082870483, acc=0.5450555682182312, loss=1.1227554082870483
test: epoch 55, loss 1.780038833618164, acc=0.2805555462837219, loss=1.780038833618164
train: epoch 56, loss 1.105667233467102, acc=0.5534999966621399, loss=1.105667233467102
test: epoch 56, loss 1.7319101095199585, acc=0.3361110985279083, loss=1.7319101095199585
train: epoch 57, loss 1.1271930932998657, acc=0.5456110835075378, loss=1.1271930932998657
test: epoch 57, loss 1.8120558261871338, acc=0.30000001192092896, loss=1.8120558261871338
train: epoch 58, loss 1.1033451557159424, acc=0.5485000014305115, loss=1.1033451557159424
test: epoch 58, loss 1.8178150653839111, acc=0.30000001192092896, loss=1.8178150653839111
train: epoch 59, loss 1.0954856872558594, acc=0.5566666722297668, loss=1.0954856872558594
test: epoch 59, loss 1.8739107847213745, acc=0.29722222685813904, loss=1.8739107847213745
train: epoch 60, loss 1.0878984928131104, acc=0.5607222318649292, loss=1.0878984928131104
test: epoch 60, loss 1.890247106552124, acc=0.3166666626930237, loss=1.890247106552124
train: epoch 61, loss 1.0876474380493164, acc=0.5614444613456726, loss=1.0876474380493164
test: epoch 61, loss 1.75721275806427, acc=0.3027777671813965, loss=1.75721275806427
train: epoch 62, loss 1.0920403003692627, acc=0.5565000176429749, loss=1.0920403003692627
test: epoch 62, loss 1.7878144979476929, acc=0.3472222089767456, loss=1.7878144979476929
train: epoch 63, loss 1.0886162519454956, acc=0.5575555562973022, loss=1.0886162519454956
test: epoch 63, loss 1.715617060661316, acc=0.3444444537162781, loss=1.715617060661316
train: epoch 64, loss 1.0691584348678589, acc=0.566944420337677, loss=1.0691584348678589
test: epoch 64, loss 1.5958466529846191, acc=0.35555556416511536, loss=1.5958466529846191
train: epoch 65, loss 1.0781233310699463, acc=0.5642777681350708, loss=1.0781233310699463
test: epoch 65, loss 1.78028404712677, acc=0.29722222685813904, loss=1.78028404712677
train: epoch 66, loss 1.074002981185913, acc=0.5651111006736755, loss=1.074002981185913
test: epoch 66, loss 1.8743044137954712, acc=0.35277777910232544, loss=1.8743044137954712
train: epoch 67, loss 1.0579973459243774, acc=0.5698333382606506, loss=1.0579973459243774
test: epoch 67, loss 1.7818818092346191, acc=0.3472222089767456, loss=1.7818818092346191
train: epoch 68, loss 1.0431867837905884, acc=0.5736111402511597, loss=1.0431867837905884
test: epoch 68, loss 1.6978131532669067, acc=0.35277777910232544, loss=1.6978131532669067
train: epoch 69, loss 1.0341193675994873, acc=0.5793889164924622, loss=1.0341193675994873
test: epoch 69, loss 1.6945419311523438, acc=0.32499998807907104, loss=1.6945419311523438
train: epoch 70, loss 1.020013451576233, acc=0.5845000147819519, loss=1.020013451576233
test: epoch 70, loss 1.8397841453552246, acc=0.33888888359069824, loss=1.8397841453552246
train: epoch 71, loss 1.0371582508087158, acc=0.5791666507720947, loss=1.0371582508087158
test: epoch 71, loss 1.8433736562728882, acc=0.33888888359069824, loss=1.8433736562728882
train: epoch 72, loss 1.0248446464538574, acc=0.5836666822433472, loss=1.0248446464538574
test: epoch 72, loss 1.8536107540130615, acc=0.29722222685813904, loss=1.8536107540130615
train: epoch 73, loss 1.0206354856491089, acc=0.5913333296775818, loss=1.0206354856491089
test: epoch 73, loss 1.786590576171875, acc=0.33888888359069824, loss=1.786590576171875
train: epoch 74, loss 1.006984829902649, acc=0.5948888659477234, loss=1.006984829902649
test: epoch 74, loss 1.8072123527526855, acc=0.2916666567325592, loss=1.8072123527526855
train: epoch 75, loss 1.0107431411743164, acc=0.5942777991294861, loss=1.0107431411743164
test: epoch 75, loss 1.865258812904358, acc=0.3472222089767456, loss=1.865258812904358
train: epoch 76, loss 0.9889761805534363, acc=0.6058333516120911, loss=0.9889761805534363
test: epoch 76, loss 1.7172268629074097, acc=0.33888888359069824, loss=1.7172268629074097
train: epoch 77, loss 0.9800124764442444, acc=0.6087222099304199, loss=0.9800124764442444
test: epoch 77, loss 1.7098318338394165, acc=0.34166666865348816, loss=1.7098318338394165
train: epoch 78, loss 0.9781537652015686, acc=0.6123889088630676, loss=0.9781537652015686
test: epoch 78, loss 1.7426775693893433, acc=0.3444444537162781, loss=1.7426775693893433
train: epoch 79, loss 0.9906127452850342, acc=0.5991666913032532, loss=0.9906127452850342
test: epoch 79, loss 1.7317414283752441, acc=0.2916666567325592, loss=1.7317414283752441
train: epoch 80, loss 0.981677770614624, acc=0.6066111326217651, loss=0.981677770614624
test: epoch 80, loss 1.7227556705474854, acc=0.33888888359069824, loss=1.7227556705474854
train: epoch 81, loss 0.9726110696792603, acc=0.6076666712760925, loss=0.9726110696792603
test: epoch 81, loss 1.8282077312469482, acc=0.33888888359069824, loss=1.8282077312469482
train: epoch 82, loss 0.9742887020111084, acc=0.6113333106040955, loss=0.9742887020111084
test: epoch 82, loss 1.8400262594223022, acc=0.34166666865348816, loss=1.8400262594223022
train: epoch 83, loss 0.9451509714126587, acc=0.6185555458068848, loss=0.9451509714126587
test: epoch 83, loss 1.7841973304748535, acc=0.33888888359069824, loss=1.7841973304748535
train: epoch 84, loss 0.9521434903144836, acc=0.617555558681488, loss=0.9521434903144836
test: epoch 84, loss 1.7904056310653687, acc=0.33888888359069824, loss=1.7904056310653687
train: epoch 85, loss 0.9449824094772339, acc=0.6209444403648376, loss=0.9449824094772339
test: epoch 85, loss 1.715496301651001, acc=0.33888888359069824, loss=1.715496301651001
train: epoch 86, loss 0.9401530623435974, acc=0.620888888835907, loss=0.9401530623435974
test: epoch 86, loss 1.8605382442474365, acc=0.2944444417953491, loss=1.8605382442474365
train: epoch 87, loss 0.9513798356056213, acc=0.6172778010368347, loss=0.9513798356056213
test: epoch 87, loss 1.85755455493927, acc=0.33888888359069824, loss=1.85755455493927
train: epoch 88, loss 0.9327967166900635, acc=0.628000020980835, loss=0.9327967166900635
test: epoch 88, loss 1.8882815837860107, acc=0.32499998807907104, loss=1.8882815837860107
train: epoch 89, loss 0.9391011595726013, acc=0.6216111183166504, loss=0.9391011595726013
test: epoch 89, loss 1.7839680910110474, acc=0.3333333432674408, loss=1.7839680910110474
train: epoch 90, loss 0.925831139087677, acc=0.6318888664245605, loss=0.925831139087677
test: epoch 90, loss 1.7745617628097534, acc=0.34166666865348816, loss=1.7745617628097534
train: epoch 91, loss 0.9141882658004761, acc=0.6400555372238159, loss=0.9141882658004761
test: epoch 91, loss 1.754936933517456, acc=0.3333333432674408, loss=1.754936933517456
train: epoch 92, loss 0.9244286417961121, acc=0.629444420337677, loss=0.9244286417961121
test: epoch 92, loss 1.9306467771530151, acc=0.34166666865348816, loss=1.9306467771530151
train: epoch 93, loss 0.909895122051239, acc=0.6361666917800903, loss=0.909895122051239
test: epoch 93, loss 1.8050340414047241, acc=0.34166666865348816, loss=1.8050340414047241
train: epoch 94, loss 0.9104430079460144, acc=0.6378889083862305, loss=0.9104430079460144
test: epoch 94, loss 1.9386042356491089, acc=0.34166666865348816, loss=1.9386042356491089
train: epoch 95, loss 0.909911036491394, acc=0.6352777481079102, loss=0.909911036491394
test: epoch 95, loss 2.0218758583068848, acc=0.34166666865348816, loss=2.0218758583068848
train: epoch 96, loss 0.902102530002594, acc=0.6360555291175842, loss=0.902102530002594
test: epoch 96, loss 1.85757315158844, acc=0.33888888359069824, loss=1.85757315158844
train: epoch 97, loss 0.9170539975166321, acc=0.6333333253860474, loss=0.9170539975166321
test: epoch 97, loss 1.8286241292953491, acc=0.3333333432674408, loss=1.8286241292953491
train: epoch 98, loss 0.9048391580581665, acc=0.6395555734634399, loss=0.9048391580581665
test: epoch 98, loss 1.6995805501937866, acc=0.34166666865348816, loss=1.6995805501937866
train: epoch 99, loss 0.8859893679618835, acc=0.6438888907432556, loss=0.8859893679618835
test: epoch 99, loss 1.9055050611495972, acc=0.3333333432674408, loss=1.9055050611495972
train: epoch 100, loss 0.8972086310386658, acc=0.637499988079071, loss=0.8972086310386658
test: epoch 100, loss 1.924243688583374, acc=0.3305555582046509, loss=1.924243688583374
train: epoch 101, loss 0.9051677584648132, acc=0.6385555267333984, loss=0.9051677584648132
test: epoch 101, loss 1.9344825744628906, acc=0.3361110985279083, loss=1.9344825744628906
train: epoch 102, loss 0.9010167121887207, acc=0.6421666741371155, loss=0.9010167121887207
test: epoch 102, loss 1.7354788780212402, acc=0.34166666865348816, loss=1.7354788780212402
train: epoch 103, loss 0.8718452453613281, acc=0.6466666460037231, loss=0.8718452453613281
test: epoch 103, loss 1.9255008697509766, acc=0.3361110985279083, loss=1.9255008697509766
train: epoch 104, loss 0.8754963874816895, acc=0.6512777805328369, loss=0.8754963874816895
test: epoch 104, loss 1.8918653726577759, acc=0.32499998807907104, loss=1.8918653726577759
train: epoch 105, loss 0.8837213516235352, acc=0.6442777514457703, loss=0.8837213516235352
test: epoch 105, loss 1.9310462474822998, acc=0.3305555582046509, loss=1.9310462474822998
train: epoch 106, loss 0.8858628273010254, acc=0.6486111283302307, loss=0.8858628273010254
test: epoch 106, loss 1.816458821296692, acc=0.3333333432674408, loss=1.816458821296692
train: epoch 107, loss 0.8690999746322632, acc=0.6532777547836304, loss=0.8690999746322632
test: epoch 107, loss 2.0043258666992188, acc=0.3305555582046509, loss=2.0043258666992188
train: epoch 108, loss 0.8710368871688843, acc=0.6487777829170227, loss=0.8710368871688843
test: epoch 108, loss 1.9923964738845825, acc=0.3361110985279083, loss=1.9923964738845825
train: epoch 109, loss 0.8748411536216736, acc=0.6470555663108826, loss=0.8748411536216736
test: epoch 109, loss 2.099937915802002, acc=0.3305555582046509, loss=2.099937915802002
train: epoch 110, loss 0.871812641620636, acc=0.6445555686950684, loss=0.871812641620636
test: epoch 110, loss 1.9877619743347168, acc=0.3333333432674408, loss=1.9877619743347168
train: epoch 111, loss 0.8657131791114807, acc=0.6537222266197205, loss=0.8657131791114807
test: epoch 111, loss 1.9987413883209229, acc=0.3361110985279083, loss=1.9987413883209229
train: epoch 112, loss 0.8713914155960083, acc=0.6539444327354431, loss=0.8713914155960083
test: epoch 112, loss 1.8378106355667114, acc=0.3305555582046509, loss=1.8378106355667114
train: epoch 113, loss 0.8645756840705872, acc=0.652999997138977, loss=0.8645756840705872
test: epoch 113, loss 1.960172176361084, acc=0.32777777314186096, loss=1.960172176361084
train: epoch 114, loss 0.8855512142181396, acc=0.64083331823349, loss=0.8855512142181396
test: epoch 114, loss 1.9536423683166504, acc=0.32499998807907104, loss=1.9536423683166504
train: epoch 115, loss 0.8539857864379883, acc=0.6551666855812073, loss=0.8539857864379883
test: epoch 115, loss 2.0226125717163086, acc=0.3444444537162781, loss=2.0226125717163086
train: epoch 116, loss 0.8611940741539001, acc=0.6560555696487427, loss=0.8611940741539001
test: epoch 116, loss 1.9855690002441406, acc=0.34166666865348816, loss=1.9855690002441406
train: epoch 117, loss 0.8572236895561218, acc=0.653333306312561, loss=0.8572236895561218
test: epoch 117, loss 1.907876968383789, acc=0.3333333432674408, loss=1.907876968383789
train: epoch 118, loss 0.8471001982688904, acc=0.6568889021873474, loss=0.8471001982688904
test: epoch 118, loss 1.9306118488311768, acc=0.3361110985279083, loss=1.9306118488311768
train: epoch 119, loss 0.8507500290870667, acc=0.6557222008705139, loss=0.8507500290870667
test: epoch 119, loss 1.863039493560791, acc=0.33888888359069824, loss=1.863039493560791
train: epoch 120, loss 0.8443462252616882, acc=0.6612222194671631, loss=0.8443462252616882
test: epoch 120, loss 2.118255376815796, acc=0.3361110985279083, loss=2.118255376815796
train: epoch 121, loss 0.845457911491394, acc=0.6585555672645569, loss=0.845457911491394
test: epoch 121, loss 1.7818667888641357, acc=0.32499998807907104, loss=1.7818667888641357
train: epoch 122, loss 0.8495446443557739, acc=0.6595555543899536, loss=0.8495446443557739
test: epoch 122, loss 2.0846445560455322, acc=0.3361110985279083, loss=2.0846445560455322
train: epoch 123, loss 0.8390330076217651, acc=0.6595555543899536, loss=0.8390330076217651
test: epoch 123, loss 1.9492383003234863, acc=0.3305555582046509, loss=1.9492383003234863
train: epoch 124, loss 0.8448031544685364, acc=0.657444417476654, loss=0.8448031544685364
test: epoch 124, loss 1.975236177444458, acc=0.33888888359069824, loss=1.975236177444458
train: epoch 125, loss 0.8267884254455566, acc=0.6638333201408386, loss=0.8267884254455566
test: epoch 125, loss 2.023383617401123, acc=0.3222222328186035, loss=2.023383617401123
train: epoch 126, loss 0.8180094361305237, acc=0.6665555834770203, loss=0.8180094361305237
test: epoch 126, loss 1.941695213317871, acc=0.3444444537162781, loss=1.941695213317871
train: epoch 127, loss 0.8261092305183411, acc=0.6670555472373962, loss=0.8261092305183411
test: epoch 127, loss 1.9353755712509155, acc=0.32499998807907104, loss=1.9353755712509155
train: epoch 128, loss 0.8281819820404053, acc=0.6676111221313477, loss=0.8281819820404053
test: epoch 128, loss 1.8334949016571045, acc=0.3361110985279083, loss=1.8334949016571045
train: epoch 129, loss 0.8194769620895386, acc=0.6692222356796265, loss=0.8194769620895386
test: epoch 129, loss 2.069888114929199, acc=0.3361110985279083, loss=2.069888114929199
train: epoch 130, loss 0.821757972240448, acc=0.6679999828338623, loss=0.821757972240448
test: epoch 130, loss 1.9412815570831299, acc=0.32777777314186096, loss=1.9412815570831299
train: epoch 131, loss 0.8176573514938354, acc=0.6672777533531189, loss=0.8176573514938354
test: epoch 131, loss 2.0289812088012695, acc=0.3361110985279083, loss=2.0289812088012695
train: epoch 132, loss 0.8224449157714844, acc=0.6706110835075378, loss=0.8224449157714844
test: epoch 132, loss 1.949843406677246, acc=0.3361110985279083, loss=1.949843406677246
train: epoch 133, loss 0.8234186768531799, acc=0.6713333129882812, loss=0.8234186768531799
test: epoch 133, loss 2.044837713241577, acc=0.3444444537162781, loss=2.044837713241577
train: epoch 134, loss 0.8104436993598938, acc=0.6717777848243713, loss=0.8104436993598938
test: epoch 134, loss 2.2428226470947266, acc=0.3305555582046509, loss=2.2428226470947266
train: epoch 135, loss 0.8139435052871704, acc=0.6693888902664185, loss=0.8139435052871704
test: epoch 135, loss 2.075265645980835, acc=0.32777777314186096, loss=2.075265645980835
train: epoch 136, loss 0.8003864884376526, acc=0.679111123085022, loss=0.8003864884376526
test: epoch 136, loss 2.1820571422576904, acc=0.3333333432674408, loss=2.1820571422576904
train: epoch 137, loss 0.8031145334243774, acc=0.6773333549499512, loss=0.8031145334243774
test: epoch 137, loss 2.039912700653076, acc=0.33888888359069824, loss=2.039912700653076
train: epoch 138, loss 0.8035719394683838, acc=0.6816111207008362, loss=0.8035719394683838
test: epoch 138, loss 1.8944368362426758, acc=0.3305555582046509, loss=1.8944368362426758
train: epoch 139, loss 0.8055531978607178, acc=0.6840000152587891, loss=0.8055531978607178
test: epoch 139, loss 1.9070651531219482, acc=0.3305555582046509, loss=1.9070651531219482
train: epoch 140, loss 0.7922224402427673, acc=0.6892777681350708, loss=0.7922224402427673
test: epoch 140, loss 2.014082431793213, acc=0.32777777314186096, loss=2.014082431793213
train: epoch 141, loss 0.8023284673690796, acc=0.6836110949516296, loss=0.8023284673690796
test: epoch 141, loss 2.123528003692627, acc=0.32777777314186096, loss=2.123528003692627
train: epoch 142, loss 0.7899798154830933, acc=0.6881666779518127, loss=0.7899798154830933
test: epoch 142, loss 1.8137891292572021, acc=0.3361110985279083, loss=1.8137891292572021
train: epoch 143, loss 0.7925094962120056, acc=0.691277801990509, loss=0.7925094962120056
test: epoch 143, loss 1.949668049812317, acc=0.32777777314186096, loss=1.949668049812317
train: epoch 144, loss 0.7867691516876221, acc=0.6919999718666077, loss=0.7867691516876221
test: epoch 144, loss 2.113436698913574, acc=0.3333333432674408, loss=2.113436698913574
train: epoch 145, loss 0.7923997044563293, acc=0.6876111030578613, loss=0.7923997044563293
test: epoch 145, loss 2.052405834197998, acc=0.3305555582046509, loss=2.052405834197998
train: epoch 146, loss 0.7909908890724182, acc=0.6904444694519043, loss=0.7909908890724182
test: epoch 146, loss 1.9505767822265625, acc=0.3333333432674408, loss=1.9505767822265625
train: epoch 147, loss 0.7875308394432068, acc=0.6915555596351624, loss=0.7875308394432068
test: epoch 147, loss 2.034412145614624, acc=0.3333333432674408, loss=2.034412145614624
train: epoch 148, loss 0.785656213760376, acc=0.6935555338859558, loss=0.785656213760376
test: epoch 148, loss 1.9511756896972656, acc=0.3333333432674408, loss=1.9511756896972656
train: epoch 149, loss 0.7832586765289307, acc=0.6933333277702332, loss=0.7832586765289307
test: epoch 149, loss 1.8057712316513062, acc=0.32777777314186096, loss=1.8057712316513062
train: epoch 150, loss 0.7706378698348999, acc=0.6995000243186951, loss=0.7706378698348999
test: epoch 150, loss 2.0493850708007812, acc=0.3333333432674408, loss=2.0493850708007812
