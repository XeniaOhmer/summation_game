# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=2.0", "--temp_decay=0.99", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=361743665, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=361743665, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.5412912368774414, acc=0.049611110240221024, loss=3.5412912368774414
test: epoch 1, loss 3.4433438777923584, acc=0.05277777835726738, loss=3.4433438777923584
train: epoch 2, loss 3.448225259780884, acc=0.05305555462837219, loss=3.448225259780884
test: epoch 2, loss 3.0875144004821777, acc=0.05000000074505806, loss=3.0875144004821777
train: epoch 3, loss 3.1752026081085205, acc=0.0842222198843956, loss=3.1752026081085205
test: epoch 3, loss 4.803987979888916, acc=0.04444444552063942, loss=4.803987979888916
train: epoch 4, loss 2.707329750061035, acc=0.1541111171245575, loss=2.707329750061035
test: epoch 4, loss 6.395884990692139, acc=0.0416666679084301, loss=6.395884990692139
train: epoch 5, loss 2.444845676422119, acc=0.189444437623024, loss=2.444845676422119
test: epoch 5, loss 7.135706901550293, acc=0.0416666679084301, loss=7.135706901550293
train: epoch 6, loss 2.302075147628784, acc=0.21511110663414001, loss=2.302075147628784
test: epoch 6, loss 7.2743401527404785, acc=0.05000000074505806, loss=7.2743401527404785
train: epoch 7, loss 2.2052383422851562, acc=0.23561111092567444, loss=2.2052383422851562
test: epoch 7, loss 7.477651596069336, acc=0.06388889253139496, loss=7.477651596069336
train: epoch 8, loss 2.132880210876465, acc=0.25861111283302307, loss=2.132880210876465
test: epoch 8, loss 7.534297943115234, acc=0.06666667014360428, loss=7.534297943115234
train: epoch 9, loss 2.0740442276000977, acc=0.26472222805023193, loss=2.0740442276000977
test: epoch 9, loss 7.64668607711792, acc=0.06666667014360428, loss=7.64668607711792
train: epoch 10, loss 2.0363752841949463, acc=0.2814444303512573, loss=2.0363752841949463
test: epoch 10, loss 7.491971969604492, acc=0.07777778059244156, loss=7.491971969604492
train: epoch 11, loss 1.9887003898620605, acc=0.29161110520362854, loss=1.9887003898620605
test: epoch 11, loss 7.49302339553833, acc=0.07500000298023224, loss=7.49302339553833
train: epoch 12, loss 1.9591985940933228, acc=0.29899999499320984, loss=1.9591985940933228
test: epoch 12, loss 7.235620498657227, acc=0.07222222536802292, loss=7.235620498657227
train: epoch 13, loss 1.9287289381027222, acc=0.3076666593551636, loss=1.9287289381027222
test: epoch 13, loss 6.9877448081970215, acc=0.09444444626569748, loss=6.9877448081970215
train: epoch 14, loss 1.905989646911621, acc=0.3147222101688385, loss=1.905989646911621
test: epoch 14, loss 6.405211925506592, acc=0.1111111119389534, loss=6.405211925506592
train: epoch 15, loss 1.8798143863677979, acc=0.3273888826370239, loss=1.8798143863677979
test: epoch 15, loss 6.108346939086914, acc=0.11666666716337204, loss=6.108346939086914
train: epoch 16, loss 1.8411952257156372, acc=0.33544445037841797, loss=1.8411952257156372
test: epoch 16, loss 5.771152019500732, acc=0.11666666716337204, loss=5.771152019500732
train: epoch 17, loss 1.8258603811264038, acc=0.34672221541404724, loss=1.8258603811264038
test: epoch 17, loss 5.404261589050293, acc=0.1111111119389534, loss=5.404261589050293
train: epoch 18, loss 1.801596760749817, acc=0.3449999988079071, loss=1.801596760749817
test: epoch 18, loss 5.252004623413086, acc=0.11388888955116272, loss=5.252004623413086
train: epoch 19, loss 1.7871265411376953, acc=0.36238887906074524, loss=1.7871265411376953
test: epoch 19, loss 4.970937728881836, acc=0.12777778506278992, loss=4.970937728881836
train: epoch 20, loss 1.7570997476577759, acc=0.36072221398353577, loss=1.7570997476577759
test: epoch 20, loss 4.788347244262695, acc=0.11944444477558136, loss=4.788347244262695
train: epoch 21, loss 1.73356294631958, acc=0.36294445395469666, loss=1.73356294631958
test: epoch 21, loss 4.616270065307617, acc=0.125, loss=4.616270065307617
train: epoch 22, loss 1.7222944498062134, acc=0.37522223591804504, loss=1.7222944498062134
test: epoch 22, loss 4.4785542488098145, acc=0.11388888955116272, loss=4.4785542488098145
train: epoch 23, loss 1.7109041213989258, acc=0.3801666796207428, loss=1.7109041213989258
test: epoch 23, loss 4.467226028442383, acc=0.10555555671453476, loss=4.467226028442383
train: epoch 24, loss 1.686545729637146, acc=0.38316667079925537, loss=1.686545729637146
test: epoch 24, loss 4.2481231689453125, acc=0.1111111119389534, loss=4.2481231689453125
train: epoch 25, loss 1.676203727722168, acc=0.3957222104072571, loss=1.676203727722168
test: epoch 25, loss 4.110851287841797, acc=0.11666666716337204, loss=4.110851287841797
train: epoch 26, loss 1.6834079027175903, acc=0.39366665482521057, loss=1.6834079027175903
test: epoch 26, loss 3.9647216796875, acc=0.13055555522441864, loss=3.9647216796875
train: epoch 27, loss 1.652464747428894, acc=0.39694443345069885, loss=1.652464747428894
test: epoch 27, loss 3.836077928543091, acc=0.1388888955116272, loss=3.836077928543091
train: epoch 28, loss 1.6379358768463135, acc=0.402055561542511, loss=1.6379358768463135
test: epoch 28, loss 3.8355772495269775, acc=0.14166666567325592, loss=3.8355772495269775
train: epoch 29, loss 1.6338034868240356, acc=0.4106111228466034, loss=1.6338034868240356
test: epoch 29, loss 3.7395522594451904, acc=0.1388888955116272, loss=3.7395522594451904
train: epoch 30, loss 1.6231588125228882, acc=0.41233333945274353, loss=1.6231588125228882
test: epoch 30, loss 3.5749635696411133, acc=0.14444445073604584, loss=3.5749635696411133
train: epoch 31, loss 1.6023082733154297, acc=0.4174444377422333, loss=1.6023082733154297
test: epoch 31, loss 3.5413479804992676, acc=0.14166666567325592, loss=3.5413479804992676
train: epoch 32, loss 1.6042437553405762, acc=0.42149999737739563, loss=1.6042437553405762
test: epoch 32, loss 3.3914835453033447, acc=0.13611111044883728, loss=3.3914835453033447
train: epoch 33, loss 1.57108473777771, acc=0.43077778816223145, loss=1.57108473777771
test: epoch 33, loss 3.2160167694091797, acc=0.1527777761220932, loss=3.2160167694091797
train: epoch 34, loss 1.5815565586090088, acc=0.43566668033599854, loss=1.5815565586090088
test: epoch 34, loss 3.0725643634796143, acc=0.17777778208255768, loss=3.0725643634796143
train: epoch 35, loss 1.5951160192489624, acc=0.4308333396911621, loss=1.5951160192489624
test: epoch 35, loss 3.0688164234161377, acc=0.1666666716337204, loss=3.0688164234161377
train: epoch 36, loss 1.5634775161743164, acc=0.43149998784065247, loss=1.5634775161743164
test: epoch 36, loss 2.938016891479492, acc=0.18888889253139496, loss=2.938016891479492
train: epoch 37, loss 1.5420982837677002, acc=0.449055552482605, loss=1.5420982837677002
test: epoch 37, loss 2.8863306045532227, acc=0.1944444477558136, loss=2.8863306045532227
train: epoch 38, loss 1.5300196409225464, acc=0.4537777900695801, loss=1.5300196409225464
test: epoch 38, loss 2.8893189430236816, acc=0.20277777314186096, loss=2.8893189430236816
train: epoch 39, loss 1.5351417064666748, acc=0.45055556297302246, loss=1.5351417064666748
test: epoch 39, loss 2.868633985519409, acc=0.20000000298023224, loss=2.868633985519409
train: epoch 40, loss 1.5211454629898071, acc=0.4654444456100464, loss=1.5211454629898071
test: epoch 40, loss 2.7165687084198, acc=0.21666666865348816, loss=2.7165687084198
train: epoch 41, loss 1.5116157531738281, acc=0.46433332562446594, loss=1.5116157531738281
test: epoch 41, loss 2.6819639205932617, acc=0.21666666865348816, loss=2.6819639205932617
train: epoch 42, loss 1.5064105987548828, acc=0.4692777693271637, loss=1.5064105987548828
test: epoch 42, loss 2.6778223514556885, acc=0.20555555820465088, loss=2.6778223514556885
train: epoch 43, loss 1.532200574874878, acc=0.4680555462837219, loss=1.532200574874878
test: epoch 43, loss 2.674691915512085, acc=0.2083333283662796, loss=2.674691915512085
train: epoch 44, loss 1.5132911205291748, acc=0.47394445538520813, loss=1.5132911205291748
test: epoch 44, loss 2.615140676498413, acc=0.21111111342906952, loss=2.615140676498413
train: epoch 45, loss 1.503867745399475, acc=0.4777222275733948, loss=1.503867745399475
test: epoch 45, loss 2.630378246307373, acc=0.2222222238779068, loss=2.630378246307373
train: epoch 46, loss 1.4840115308761597, acc=0.48616665601730347, loss=1.4840115308761597
test: epoch 46, loss 2.551232099533081, acc=0.21388888359069824, loss=2.551232099533081
train: epoch 47, loss 1.4653973579406738, acc=0.4893888831138611, loss=1.4653973579406738
test: epoch 47, loss 2.5559990406036377, acc=0.2222222238779068, loss=2.5559990406036377
train: epoch 48, loss 1.472793459892273, acc=0.49272221326828003, loss=1.472793459892273
test: epoch 48, loss 2.5539939403533936, acc=0.21666666865348816, loss=2.5539939403533936
train: epoch 49, loss 1.4689208269119263, acc=0.49861112236976624, loss=1.4689208269119263
test: epoch 49, loss 2.4897778034210205, acc=0.21666666865348816, loss=2.4897778034210205
train: epoch 50, loss 1.4552348852157593, acc=0.503000020980835, loss=1.4552348852157593
test: epoch 50, loss 2.4090845584869385, acc=0.22499999403953552, loss=2.4090845584869385
train: epoch 51, loss 1.4410173892974854, acc=0.5098888874053955, loss=1.4410173892974854
test: epoch 51, loss 2.3207056522369385, acc=0.25, loss=2.3207056522369385
train: epoch 52, loss 1.4307702779769897, acc=0.5178333520889282, loss=1.4307702779769897
test: epoch 52, loss 2.30119252204895, acc=0.23333333432674408, loss=2.30119252204895
train: epoch 53, loss 1.4455407857894897, acc=0.5206111073493958, loss=1.4455407857894897
test: epoch 53, loss 2.2490146160125732, acc=0.25555557012557983, loss=2.2490146160125732
train: epoch 54, loss 1.4328315258026123, acc=0.5295555591583252, loss=1.4328315258026123
test: epoch 54, loss 2.252053737640381, acc=0.25, loss=2.252053737640381
train: epoch 55, loss 1.4136865139007568, acc=0.5277777910232544, loss=1.4136865139007568
test: epoch 55, loss 2.166937828063965, acc=0.26944443583488464, loss=2.166937828063965
train: epoch 56, loss 1.4059057235717773, acc=0.5343888998031616, loss=1.4059057235717773
test: epoch 56, loss 2.1208133697509766, acc=0.2527777850627899, loss=2.1208133697509766
train: epoch 57, loss 1.401471734046936, acc=0.5391111373901367, loss=1.401471734046936
test: epoch 57, loss 2.094874858856201, acc=0.2666666805744171, loss=2.094874858856201
train: epoch 58, loss 1.3813971281051636, acc=0.546500027179718, loss=1.3813971281051636
test: epoch 58, loss 2.042808771133423, acc=0.29722222685813904, loss=2.042808771133423
train: epoch 59, loss 1.3846622705459595, acc=0.5499444603919983, loss=1.3846622705459595
test: epoch 59, loss 2.0437350273132324, acc=0.2916666567325592, loss=2.0437350273132324
train: epoch 60, loss 1.3667491674423218, acc=0.5601111054420471, loss=1.3667491674423218
test: epoch 60, loss 2.033054828643799, acc=0.28333333134651184, loss=2.033054828643799
train: epoch 61, loss 1.3465855121612549, acc=0.5662222504615784, loss=1.3465855121612549
test: epoch 61, loss 1.987891674041748, acc=0.3027777671813965, loss=1.987891674041748
train: epoch 62, loss 1.3311949968338013, acc=0.5720555782318115, loss=1.3311949968338013
test: epoch 62, loss 1.9785866737365723, acc=0.2944444417953491, loss=1.9785866737365723
train: epoch 63, loss 1.354587197303772, acc=0.5761666893959045, loss=1.354587197303772
test: epoch 63, loss 1.9678282737731934, acc=0.29722222685813904, loss=1.9678282737731934
train: epoch 64, loss 1.3263535499572754, acc=0.5768888592720032, loss=1.3263535499572754
test: epoch 64, loss 1.9346259832382202, acc=0.2916666567325592, loss=1.9346259832382202
train: epoch 65, loss 1.303348183631897, acc=0.593500018119812, loss=1.303348183631897
test: epoch 65, loss 1.905317783355713, acc=0.31388887763023376, loss=1.905317783355713
train: epoch 66, loss 1.3446450233459473, acc=0.5885555744171143, loss=1.3446450233459473
test: epoch 66, loss 1.8716996908187866, acc=0.3194444477558136, loss=1.8716996908187866
train: epoch 67, loss 1.306175947189331, acc=0.5950000286102295, loss=1.306175947189331
test: epoch 67, loss 1.8657208681106567, acc=0.31111112236976624, loss=1.8657208681106567
train: epoch 68, loss 1.2738457918167114, acc=0.6065000295639038, loss=1.2738457918167114
test: epoch 68, loss 1.84556245803833, acc=0.31111112236976624, loss=1.84556245803833
train: epoch 69, loss 1.2649061679840088, acc=0.6063888669013977, loss=1.2649061679840088
test: epoch 69, loss 1.8270267248153687, acc=0.31388887763023376, loss=1.8270267248153687
train: epoch 70, loss 1.264504075050354, acc=0.6125555634498596, loss=1.264504075050354
test: epoch 70, loss 1.8077996969223022, acc=0.3222222328186035, loss=1.8077996969223022
train: epoch 71, loss 1.247072458267212, acc=0.6168333292007446, loss=1.247072458267212
test: epoch 71, loss 1.7487136125564575, acc=0.3444444537162781, loss=1.7487136125564575
train: epoch 72, loss 1.2194472551345825, acc=0.620722234249115, loss=1.2194472551345825
test: epoch 72, loss 1.7454276084899902, acc=0.33888888359069824, loss=1.7454276084899902
train: epoch 73, loss 1.229188084602356, acc=0.6234999895095825, loss=1.229188084602356
test: epoch 73, loss 1.7115604877471924, acc=0.34166666865348816, loss=1.7115604877471924
train: epoch 74, loss 1.2210243940353394, acc=0.6284999847412109, loss=1.2210243940353394
test: epoch 74, loss 1.713202953338623, acc=0.3472222089767456, loss=1.713202953338623
train: epoch 75, loss 1.2138421535491943, acc=0.6317777633666992, loss=1.2138421535491943
test: epoch 75, loss 1.6877312660217285, acc=0.3499999940395355, loss=1.6877312660217285
train: epoch 76, loss 1.2117055654525757, acc=0.6327221989631653, loss=1.2117055654525757
test: epoch 76, loss 1.6831468343734741, acc=0.3499999940395355, loss=1.6831468343734741
train: epoch 77, loss 1.1884921789169312, acc=0.6459444165229797, loss=1.1884921789169312
test: epoch 77, loss 1.6684435606002808, acc=0.35277777910232544, loss=1.6684435606002808
train: epoch 78, loss 1.1971477270126343, acc=0.6370000243186951, loss=1.1971477270126343
test: epoch 78, loss 1.642447829246521, acc=0.35277777910232544, loss=1.642447829246521
train: epoch 79, loss 1.1820905208587646, acc=0.6477222442626953, loss=1.1820905208587646
test: epoch 79, loss 1.6374397277832031, acc=0.3499999940395355, loss=1.6374397277832031
train: epoch 80, loss 1.1808061599731445, acc=0.6497777700424194, loss=1.1808061599731445
test: epoch 80, loss 1.5960450172424316, acc=0.375, loss=1.5960450172424316
train: epoch 81, loss 1.175398826599121, acc=0.6564444303512573, loss=1.175398826599121
test: epoch 81, loss 1.6172648668289185, acc=0.35277777910232544, loss=1.6172648668289185
train: epoch 82, loss 1.1440755128860474, acc=0.6563888788223267, loss=1.1440755128860474
test: epoch 82, loss 1.600009560585022, acc=0.36666667461395264, loss=1.600009560585022
train: epoch 83, loss 1.1322615146636963, acc=0.6620000004768372, loss=1.1322615146636963
test: epoch 83, loss 1.5647505521774292, acc=0.3638888895511627, loss=1.5647505521774292
train: epoch 84, loss 1.1574335098266602, acc=0.6625000238418579, loss=1.1574335098266602
test: epoch 84, loss 1.5090934038162231, acc=0.375, loss=1.5090934038162231
train: epoch 85, loss 1.1632328033447266, acc=0.6616666913032532, loss=1.1632328033447266
test: epoch 85, loss 1.534132719039917, acc=0.38055557012557983, loss=1.534132719039917
train: epoch 86, loss 1.1154022216796875, acc=0.663777768611908, loss=1.1154022216796875
test: epoch 86, loss 1.5076558589935303, acc=0.3916666805744171, loss=1.5076558589935303
train: epoch 87, loss 1.1208078861236572, acc=0.6637222170829773, loss=1.1208078861236572
test: epoch 87, loss 1.478981375694275, acc=0.4027777910232544, loss=1.478981375694275
train: epoch 88, loss 1.1208343505859375, acc=0.6723889112472534, loss=1.1208343505859375
test: epoch 88, loss 1.440576195716858, acc=0.39722222089767456, loss=1.440576195716858
train: epoch 89, loss 1.1103368997573853, acc=0.6690000295639038, loss=1.1103368997573853
test: epoch 89, loss 1.4463655948638916, acc=0.39722222089767456, loss=1.4463655948638916
train: epoch 90, loss 1.084141731262207, acc=0.6758888959884644, loss=1.084141731262207
test: epoch 90, loss 1.429114818572998, acc=0.4027777910232544, loss=1.429114818572998
train: epoch 91, loss 1.0663955211639404, acc=0.6811666488647461, loss=1.0663955211639404
test: epoch 91, loss 1.437897801399231, acc=0.39722222089767456, loss=1.437897801399231
train: epoch 92, loss 1.09003746509552, acc=0.6799444556236267, loss=1.09003746509552
test: epoch 92, loss 1.4260642528533936, acc=0.40833333134651184, loss=1.4260642528533936
train: epoch 93, loss 1.078599214553833, acc=0.6777222156524658, loss=1.078599214553833
test: epoch 93, loss 1.4081754684448242, acc=0.4000000059604645, loss=1.4081754684448242
train: epoch 94, loss 1.084667444229126, acc=0.6812777519226074, loss=1.084667444229126
test: epoch 94, loss 1.3934272527694702, acc=0.4166666567325592, loss=1.3934272527694702
train: epoch 95, loss 1.103021264076233, acc=0.6763888597488403, loss=1.103021264076233
test: epoch 95, loss 1.3854560852050781, acc=0.4166666567325592, loss=1.3854560852050781
train: epoch 96, loss 1.08513343334198, acc=0.6771110892295837, loss=1.08513343334198
test: epoch 96, loss 1.3797084093093872, acc=0.4138889014720917, loss=1.3797084093093872
train: epoch 97, loss 1.0688101053237915, acc=0.6857777833938599, loss=1.0688101053237915
test: epoch 97, loss 1.3696900606155396, acc=0.4166666567325592, loss=1.3696900606155396
train: epoch 98, loss 1.0709589719772339, acc=0.6878888607025146, loss=1.0709589719772339
test: epoch 98, loss 1.3573514223098755, acc=0.4194444417953491, loss=1.3573514223098755
train: epoch 99, loss 1.069993495941162, acc=0.6866666674613953, loss=1.069993495941162
test: epoch 99, loss 1.3562463521957397, acc=0.4194444417953491, loss=1.3562463521957397
train: epoch 100, loss 1.0487090349197388, acc=0.6927777528762817, loss=1.0487090349197388
test: epoch 100, loss 1.3428384065628052, acc=0.42222222685813904, loss=1.3428384065628052
train: epoch 101, loss 1.015671968460083, acc=0.6978333592414856, loss=1.015671968460083
test: epoch 101, loss 1.3402715921401978, acc=0.4277777671813965, loss=1.3402715921401978
train: epoch 102, loss 1.0281352996826172, acc=0.6984444260597229, loss=1.0281352996826172
test: epoch 102, loss 1.3361709117889404, acc=0.4305555522441864, loss=1.3361709117889404
train: epoch 103, loss 0.9961203336715698, acc=0.710777759552002, loss=0.9961203336715698
test: epoch 103, loss 1.3175904750823975, acc=0.4305555522441864, loss=1.3175904750823975
train: epoch 104, loss 1.0083969831466675, acc=0.7052778005599976, loss=1.0083969831466675
test: epoch 104, loss 1.3182011842727661, acc=0.42500001192092896, loss=1.3182011842727661
train: epoch 105, loss 1.003537893295288, acc=0.707111120223999, loss=1.003537893295288
test: epoch 105, loss 1.3098138570785522, acc=0.42500001192092896, loss=1.3098138570785522
train: epoch 106, loss 0.9679562449455261, acc=0.7157222032546997, loss=0.9679562449455261
test: epoch 106, loss 1.312699794769287, acc=0.4333333373069763, loss=1.312699794769287
train: epoch 107, loss 0.9637983441352844, acc=0.7092221975326538, loss=0.9637983441352844
test: epoch 107, loss 1.3026494979858398, acc=0.4333333373069763, loss=1.3026494979858398
train: epoch 108, loss 0.9705022573471069, acc=0.7223333120346069, loss=0.9705022573471069
test: epoch 108, loss 1.295568823814392, acc=0.4333333373069763, loss=1.295568823814392
train: epoch 109, loss 0.9538604617118835, acc=0.7200000286102295, loss=0.9538604617118835
test: epoch 109, loss 1.2962440252304077, acc=0.4333333373069763, loss=1.2962440252304077
train: epoch 110, loss 0.9643085598945618, acc=0.7268333435058594, loss=0.9643085598945618
test: epoch 110, loss 1.2916921377182007, acc=0.44999998807907104, loss=1.2916921377182007
train: epoch 111, loss 0.9376776814460754, acc=0.727222204208374, loss=0.9376776814460754
test: epoch 111, loss 1.2904713153839111, acc=0.4305555522441864, loss=1.2904713153839111
train: epoch 112, loss 0.9235427975654602, acc=0.7312222123146057, loss=0.9235427975654602
test: epoch 112, loss 1.2942355871200562, acc=0.43888887763023376, loss=1.2942355871200562
train: epoch 113, loss 0.8937519788742065, acc=0.7369999885559082, loss=0.8937519788742065
test: epoch 113, loss 1.274170160293579, acc=0.4444444477558136, loss=1.274170160293579
train: epoch 114, loss 0.8841840028762817, acc=0.7390555739402771, loss=0.8841840028762817
test: epoch 114, loss 1.2549710273742676, acc=0.4583333432674408, loss=1.2549710273742676
train: epoch 115, loss 0.8950501084327698, acc=0.7410555481910706, loss=0.8950501084327698
test: epoch 115, loss 1.2503942251205444, acc=0.4555555582046509, loss=1.2503942251205444
train: epoch 116, loss 0.9007434844970703, acc=0.7408888936042786, loss=0.9007434844970703
test: epoch 116, loss 1.2503119707107544, acc=0.4555555582046509, loss=1.2503119707107544
train: epoch 117, loss 0.8690469264984131, acc=0.7481111288070679, loss=0.8690469264984131
test: epoch 117, loss 1.2383372783660889, acc=0.4611110985279083, loss=1.2383372783660889
train: epoch 118, loss 0.850006639957428, acc=0.7472777962684631, loss=0.850006639957428
test: epoch 118, loss 1.237367868423462, acc=0.4611110985279083, loss=1.237367868423462
train: epoch 119, loss 0.871863603591919, acc=0.7465555667877197, loss=0.871863603591919
test: epoch 119, loss 1.2222663164138794, acc=0.4722222089767456, loss=1.2222663164138794
train: epoch 120, loss 0.8655134439468384, acc=0.7532222270965576, loss=0.8655134439468384
test: epoch 120, loss 1.224900722503662, acc=0.4583333432674408, loss=1.224900722503662
train: epoch 121, loss 0.8292611837387085, acc=0.757777750492096, loss=0.8292611837387085
test: epoch 121, loss 1.2133785486221313, acc=0.46388888359069824, loss=1.2133785486221313
train: epoch 122, loss 0.8210859298706055, acc=0.7568888664245605, loss=0.8210859298706055
test: epoch 122, loss 1.196993350982666, acc=0.4611110985279083, loss=1.196993350982666
train: epoch 123, loss 0.8193602561950684, acc=0.7622777819633484, loss=0.8193602561950684
test: epoch 123, loss 1.2076741456985474, acc=0.4722222089767456, loss=1.2076741456985474
train: epoch 124, loss 0.8274509906768799, acc=0.7593333125114441, loss=0.8274509906768799
test: epoch 124, loss 1.1608552932739258, acc=0.4888888895511627, loss=1.1608552932739258
train: epoch 125, loss 0.8005552887916565, acc=0.7661111354827881, loss=0.8005552887916565
test: epoch 125, loss 1.1760165691375732, acc=0.4722222089767456, loss=1.1760165691375732
train: epoch 126, loss 0.7875121235847473, acc=0.7726110816001892, loss=0.7875121235847473
test: epoch 126, loss 1.1784831285476685, acc=0.4722222089767456, loss=1.1784831285476685
train: epoch 127, loss 0.7913147211074829, acc=0.7710000276565552, loss=0.7913147211074829
test: epoch 127, loss 1.2003754377365112, acc=0.4861111044883728, loss=1.2003754377365112
train: epoch 128, loss 0.8101992607116699, acc=0.7774444222450256, loss=0.8101992607116699
test: epoch 128, loss 1.1652553081512451, acc=0.49166667461395264, loss=1.1652553081512451
train: epoch 129, loss 0.7907825708389282, acc=0.7764999866485596, loss=0.7907825708389282
test: epoch 129, loss 1.174527883529663, acc=0.4861111044883728, loss=1.174527883529663
train: epoch 130, loss 0.7659051418304443, acc=0.777388870716095, loss=0.7659051418304443
test: epoch 130, loss 1.1548490524291992, acc=0.4861111044883728, loss=1.1548490524291992
train: epoch 131, loss 0.7651629447937012, acc=0.7808889150619507, loss=0.7651629447937012
test: epoch 131, loss 1.1592462062835693, acc=0.4888888895511627, loss=1.1592462062835693
train: epoch 132, loss 0.7493938207626343, acc=0.788277804851532, loss=0.7493938207626343
test: epoch 132, loss 1.160528540611267, acc=0.4833333194255829, loss=1.160528540611267
train: epoch 133, loss 0.7588077187538147, acc=0.7828333377838135, loss=0.7588077187538147
test: epoch 133, loss 1.1481034755706787, acc=0.4861111044883728, loss=1.1481034755706787
train: epoch 134, loss 0.7502769231796265, acc=0.7891666889190674, loss=0.7502769231796265
test: epoch 134, loss 1.1436049938201904, acc=0.4972222149372101, loss=1.1436049938201904
train: epoch 135, loss 0.7461142539978027, acc=0.7872777581214905, loss=0.7461142539978027
test: epoch 135, loss 1.1237422227859497, acc=0.4972222149372101, loss=1.1237422227859497
train: epoch 136, loss 0.7147233486175537, acc=0.7965555787086487, loss=0.7147233486175537
test: epoch 136, loss 1.136320948600769, acc=0.4972222149372101, loss=1.136320948600769
train: epoch 137, loss 0.7237325310707092, acc=0.7910000085830688, loss=0.7237325310707092
test: epoch 137, loss 1.1309926509857178, acc=0.49444442987442017, loss=1.1309926509857178
train: epoch 138, loss 0.7230041027069092, acc=0.7938888669013977, loss=0.7230041027069092
test: epoch 138, loss 1.1215862035751343, acc=0.4749999940395355, loss=1.1215862035751343
train: epoch 139, loss 0.7309431433677673, acc=0.799833357334137, loss=0.7309431433677673
test: epoch 139, loss 1.110154628753662, acc=0.5027777552604675, loss=1.110154628753662
train: epoch 140, loss 0.7216217517852783, acc=0.7964444160461426, loss=0.7216217517852783
test: epoch 140, loss 1.102988839149475, acc=0.5, loss=1.102988839149475
train: epoch 141, loss 0.7035056352615356, acc=0.8037222027778625, loss=0.7035056352615356
test: epoch 141, loss 1.104662299156189, acc=0.4972222149372101, loss=1.104662299156189
train: epoch 142, loss 0.6867515444755554, acc=0.8068333268165588, loss=0.6867515444755554
test: epoch 142, loss 1.0863994359970093, acc=0.5027777552604675, loss=1.0863994359970093
train: epoch 143, loss 0.6925308108329773, acc=0.8075000047683716, loss=0.6925308108329773
test: epoch 143, loss 1.1090905666351318, acc=0.5083333253860474, loss=1.1090905666351318
train: epoch 144, loss 0.6823770403862, acc=0.808555543422699, loss=0.6823770403862
test: epoch 144, loss 1.0907188653945923, acc=0.5111111402511597, loss=1.0907188653945923
train: epoch 145, loss 0.6752288937568665, acc=0.8086110949516296, loss=0.6752288937568665
test: epoch 145, loss 1.0919533967971802, acc=0.5138888955116272, loss=1.0919533967971802
train: epoch 146, loss 0.6833985447883606, acc=0.8071666955947876, loss=0.6833985447883606
test: epoch 146, loss 1.083938717842102, acc=0.5083333253860474, loss=1.083938717842102
train: epoch 147, loss 0.6864518523216248, acc=0.8086110949516296, loss=0.6864518523216248
test: epoch 147, loss 1.0849015712738037, acc=0.5083333253860474, loss=1.0849015712738037
train: epoch 148, loss 0.6722445487976074, acc=0.8097222447395325, loss=0.6722445487976074
test: epoch 148, loss 1.0723083019256592, acc=0.5027777552604675, loss=1.0723083019256592
train: epoch 149, loss 0.6542137861251831, acc=0.8134999871253967, loss=0.6542137861251831
test: epoch 149, loss 1.0808149576187134, acc=0.5083333253860474, loss=1.0808149576187134
train: epoch 150, loss 0.6505695581436157, acc=0.8157222270965576, loss=0.6505695581436157
test: epoch 150, loss 1.0866426229476929, acc=0.5083333253860474, loss=1.0866426229476929
