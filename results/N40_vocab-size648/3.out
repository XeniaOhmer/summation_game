# ["--N=40", "--n_epochs=250", "--batch_size=32", "--lr=0.001", "--receiver_embed_dim=64", "--temperature=2.0", "--temp_decay=0.995", "--one_hot=1", "--n_layers=3", "--save_run=1", "--n_symbols=648"]
Namespace(n_summands=2, N=40, test_split=0.1, data_scaling=50, one_hot=True, receiver_embed_dim=64, n_layers=3, n_symbols=648, temperature=2.0, temp_decay=0.995, early_stopping_acc=0.99, n_runs=1, save_run=True, random_seed=510889608, checkpoint_dir=None, preemptable=False, checkpoint_freq=0, validation_freq=1, n_epochs=250, load_from_checkpoint=None, no_cuda=True, batch_size=32, optimizer='adam', lr=0.001, update_freq=1, vocab_size=10, max_len=1, tensorboard=False, tensorboard_dir='runs/', distributed_port=18363, fp16=False, cuda=False, device=device(type='cpu'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'))
train: epoch 1, loss 2.6753087043762207, acc=0.11779246479272842, loss=2.6753087043762207
test: epoch 1, loss 5.245309352874756, acc=0.07402511686086655, loss=5.245309352874756
train: epoch 2, loss 1.827609896659851, acc=0.2330998033285141, loss=1.827609896659851
test: epoch 2, loss 3.9004809856414795, acc=0.08988764137029648, loss=3.9004809856414795
train: epoch 3, loss 1.6336843967437744, acc=0.29306015372276306, loss=1.6336843967437744
test: epoch 3, loss 4.126715660095215, acc=0.14606741070747375, loss=4.126715660095215
train: epoch 4, loss 1.2763121128082275, acc=0.4539722502231598, loss=1.2763121128082275
test: epoch 4, loss 6.8598785400390625, acc=0.10244547575712204, loss=6.8598785400390625
train: epoch 5, loss 0.8406491279602051, acc=0.6535756587982178, loss=0.8406491279602051
test: epoch 5, loss 3.980656862258911, acc=0.18506279587745667, loss=3.980656862258911
train: epoch 6, loss 0.6561589241027832, acc=0.7337210774421692, loss=0.6561589241027832
test: epoch 6, loss 4.0713982582092285, acc=0.15069398283958435, loss=4.0713982582092285
train: epoch 7, loss 0.5785583853721619, acc=0.7685790061950684, loss=0.5785583853721619
test: epoch 7, loss 3.3355772495269775, acc=0.21678784489631653, loss=3.3355772495269775
train: epoch 8, loss 0.5592169761657715, acc=0.7791275382041931, loss=0.5592169761657715
test: epoch 8, loss 3.2093687057495117, acc=0.20158624649047852, loss=3.2093687057495117
train: epoch 9, loss 0.5598437190055847, acc=0.7724917531013489, loss=0.5598437190055847
test: epoch 9, loss 3.061047077178955, acc=0.2590878903865814, loss=3.061047077178955
train: epoch 10, loss 0.40963512659072876, acc=0.8448644876480103, loss=0.40963512659072876
test: epoch 10, loss 3.2061798572540283, acc=0.22339722514152527, loss=3.2061798572540283
train: epoch 11, loss 0.4069105386734009, acc=0.8453932404518127, loss=0.4069105386734009
test: epoch 11, loss 2.7025344371795654, acc=0.26635822653770447, loss=2.7025344371795654
train: epoch 12, loss 0.3699089586734772, acc=0.8631328344345093, loss=0.3699089586734772
test: epoch 12, loss 2.7701969146728516, acc=0.2723066806793213, loss=2.7701969146728516
train: epoch 13, loss 0.32245150208473206, acc=0.884983479976654, loss=0.32245150208473206
test: epoch 13, loss 3.2907562255859375, acc=0.24785195291042328, loss=3.2907562255859375
train: epoch 14, loss 0.35259392857551575, acc=0.8676272034645081, loss=0.35259392857551575
test: epoch 14, loss 2.461134910583496, acc=0.2914738953113556, loss=2.461134910583496
train: epoch 15, loss 0.36411741375923157, acc=0.8606873750686646, loss=0.36411741375923157
test: epoch 15, loss 2.702617645263672, acc=0.25578320026397705, loss=2.702617645263672
train: epoch 16, loss 0.28450730443000793, acc=0.8976338505744934, loss=0.28450730443000793
test: epoch 16, loss 2.914612054824829, acc=0.28420355916023254, loss=2.914612054824829
train: epoch 17, loss 0.24049624800682068, acc=0.9152677059173584, loss=0.24049624800682068
test: epoch 17, loss 2.756498098373413, acc=0.2914738953113556, loss=2.756498098373413
train: epoch 18, loss 0.18767277896404266, acc=0.9369729161262512, loss=0.18767277896404266
test: epoch 18, loss 2.7191882133483887, acc=0.2538003921508789, loss=2.7191882133483887
train: epoch 19, loss 0.17666436731815338, acc=0.9405155181884766, loss=0.17666436731815338
test: epoch 19, loss 2.0654637813568115, acc=0.31262391805648804, loss=2.0654637813568115
train: epoch 20, loss 0.16982635855674744, acc=0.9437012672424316, loss=0.16982635855674744
test: epoch 20, loss 1.971506118774414, acc=0.320555180311203, loss=1.971506118774414
train: epoch 21, loss 0.1528211086988449, acc=0.9490680694580078, loss=0.1528211086988449
test: epoch 21, loss 2.754809856414795, acc=0.28420355916023254, loss=2.754809856414795
train: epoch 22, loss 0.15065155923366547, acc=0.9503899812698364, loss=0.15065155923366547
test: epoch 22, loss 2.257660388946533, acc=0.31857237219810486, loss=2.257660388946533
train: epoch 23, loss 0.14383597671985626, acc=0.9515796303749084, loss=0.14383597671985626
test: epoch 23, loss 2.188292980194092, acc=0.3965631127357483, loss=2.188292980194092
train: epoch 24, loss 0.1372666209936142, acc=0.9550561904907227, loss=0.1372666209936142
test: epoch 24, loss 2.0907046794891357, acc=0.3820224702358246, loss=2.0907046794891357
train: epoch 25, loss 0.128456249833107, acc=0.9578717947006226, loss=0.128456249833107
test: epoch 25, loss 2.293787717819214, acc=0.34236615896224976, loss=2.293787717819214
train: epoch 26, loss 0.12715306878089905, acc=0.9581229090690613, loss=0.12715306878089905
test: epoch 26, loss 2.1386425495147705, acc=0.392597496509552, loss=2.1386425495147705
train: epoch 27, loss 0.12070260941982269, acc=0.9598149657249451, loss=0.12070260941982269
test: epoch 27, loss 2.280090570449829, acc=0.3721083998680115, loss=2.280090570449829
train: epoch 28, loss 0.12111429125070572, acc=0.9605419635772705, loss=0.12111429125070572
test: epoch 28, loss 1.8436834812164307, acc=0.42696627974510193, loss=1.8436834812164307
train: epoch 29, loss 0.11595062911510468, acc=0.9619696140289307, loss=0.11595062911510468
test: epoch 29, loss 2.2266156673431396, acc=0.3886318504810333, loss=2.2266156673431396
train: epoch 30, loss 0.11360160261392593, acc=0.962908148765564, loss=0.11360160261392593
test: epoch 30, loss 2.1525027751922607, acc=0.37673497200012207, loss=2.1525027751922607
train: epoch 31, loss 0.10928365588188171, acc=0.963965654373169, loss=0.10928365588188171
test: epoch 31, loss 2.0510406494140625, acc=0.42762723565101624, loss=2.0510406494140625
train: epoch 32, loss 0.10478488355875015, acc=0.9660542011260986, loss=0.10478488355875015
test: epoch 32, loss 2.303290367126465, acc=0.4507600665092468, loss=2.303290367126465
train: epoch 33, loss 0.10652002692222595, acc=0.9655386805534363, loss=0.10652002692222595
test: epoch 33, loss 2.0183403491973877, acc=0.4461335241794586, loss=2.0183403491973877
train: epoch 34, loss 0.10125593096017838, acc=0.9673892855644226, loss=0.10125593096017838
test: epoch 34, loss 2.119398593902588, acc=0.44745537638664246, loss=2.119398593902588
train: epoch 35, loss 0.09999240189790726, acc=0.967838704586029, loss=0.09999240189790726
test: epoch 35, loss 1.7516683340072632, acc=0.4653007388114929, loss=1.7516683340072632
train: epoch 36, loss 0.09482559561729431, acc=0.970310628414154, loss=0.09482559561729431
test: epoch 36, loss 1.9609745740890503, acc=0.4302709996700287, loss=1.9609745740890503
train: epoch 37, loss 0.09735472500324249, acc=0.9698479771614075, loss=0.09735472500324249
test: epoch 37, loss 1.702722191810608, acc=0.5181757807731628, loss=1.702722191810608
train: epoch 38, loss 0.09172063320875168, acc=0.9709979891777039, loss=0.09172063320875168
test: epoch 38, loss 1.5271118879318237, acc=0.5049570202827454, loss=1.5271118879318237
train: epoch 39, loss 0.08384527266025543, acc=0.9732319712638855, loss=0.08384527266025543
test: epoch 39, loss 1.6646060943603516, acc=0.5122273564338684, loss=1.6646060943603516
train: epoch 40, loss 0.08929678052663803, acc=0.9716193079948425, loss=0.08929678052663803
test: epoch 40, loss 1.5542060136795044, acc=0.5201586484909058, loss=1.5542060136795044
train: epoch 41, loss 0.08535270392894745, acc=0.9727957844734192, loss=0.08535270392894745
test: epoch 41, loss 1.5515285730361938, acc=0.5042961239814758, loss=1.5515285730361938
train: epoch 42, loss 0.08255171030759811, acc=0.9743027091026306, loss=0.08255171030759811
test: epoch 42, loss 1.786791443824768, acc=0.5142101645469666, loss=1.786791443824768
train: epoch 43, loss 0.08093307912349701, acc=0.9750429391860962, loss=0.08093307912349701
test: epoch 43, loss 1.5452919006347656, acc=0.5670852661132812, loss=1.5452919006347656
train: epoch 44, loss 0.07534484565258026, acc=0.9769200086593628, loss=0.07534484565258026
test: epoch 44, loss 1.5951377153396606, acc=0.584269642829895, loss=1.5951377153396606
train: epoch 45, loss 0.08073753118515015, acc=0.9753337502479553, loss=0.08073753118515015
test: epoch 45, loss 1.6831752061843872, acc=0.5988103151321411, loss=1.6831752061843872
train: epoch 46, loss 0.07618243247270584, acc=0.9773033857345581, loss=0.07618243247270584
test: epoch 46, loss 1.7045483589172363, acc=0.5902181267738342, loss=1.7045483589172363
train: epoch 47, loss 0.0688338652253151, acc=0.978202223777771, loss=0.0688338652253151
test: epoch 47, loss 1.3990955352783203, acc=0.6497025489807129, loss=1.3990955352783203
train: epoch 48, loss 0.07180701941251755, acc=0.978070080280304, loss=0.07180701941251755
test: epoch 48, loss 1.6706318855285645, acc=0.5710508823394775, loss=1.6706318855285645
train: epoch 49, loss 0.06927943974733353, acc=0.978704571723938, loss=0.06927943974733353
test: epoch 49, loss 1.3356462717056274, acc=0.6734963655471802, loss=1.3356462717056274
train: epoch 50, loss 0.061713047325611115, acc=0.9818373918533325, loss=0.061713047325611115
test: epoch 50, loss 1.2971159219741821, acc=0.7091870307922363, loss=1.2971159219741821
train: epoch 51, loss 0.06622590124607086, acc=0.9800131916999817, loss=0.06622590124607086
test: epoch 51, loss 1.3229689598083496, acc=0.6853932738304138, loss=1.3229689598083496
train: epoch 52, loss 0.06289676576852798, acc=0.9817581176757812, loss=0.06289676576852798
test: epoch 52, loss 0.8984335660934448, acc=0.7303370833396912, loss=0.8984335660934448
train: epoch 53, loss 0.059079360216856, acc=0.9829345941543579, loss=0.059079360216856
test: epoch 53, loss 0.6971165537834167, acc=0.7898215651512146, loss=0.6971165537834167
train: epoch 54, loss 0.061534181237220764, acc=0.9820753335952759, loss=0.061534181237220764
test: epoch 54, loss 0.823800265789032, acc=0.7832121849060059, loss=0.823800265789032
train: epoch 55, loss 0.058874234557151794, acc=0.9834765195846558, loss=0.058874234557151794
test: epoch 55, loss 0.7702130079269409, acc=0.8169200420379639, loss=0.7702130079269409
train: epoch 56, loss 0.05370064824819565, acc=0.9854990243911743, loss=0.05370064824819565
test: epoch 56, loss 0.469669371843338, acc=0.8750826120376587, loss=0.469669371843338
train: epoch 57, loss 0.05339467152953148, acc=0.9854990243911743, loss=0.05339467152953148
test: epoch 57, loss 0.42915016412734985, acc=0.8704560399055481, loss=0.42915016412734985
train: epoch 58, loss 0.05043574795126915, acc=0.9862392544746399, loss=0.05043574795126915
test: epoch 58, loss 0.4693458378314972, acc=0.8750826120376587, loss=0.4693458378314972
train: epoch 59, loss 0.047631632536649704, acc=0.9868208765983582, loss=0.047631632536649704
test: epoch 59, loss 0.38246992230415344, acc=0.883013904094696, loss=0.38246992230415344
train: epoch 60, loss 0.044147469103336334, acc=0.988222062587738, loss=0.044147469103336334
test: epoch 60, loss 0.25621989369392395, acc=0.9220092296600342, loss=0.25621989369392395
train: epoch 61, loss 0.0392618291079998, acc=0.9894117712974548, loss=0.0392618291079998
test: epoch 61, loss 0.2189837098121643, acc=0.9451420903205872, loss=0.2189837098121643
train: epoch 62, loss 0.038880541920661926, acc=0.9898215532302856, loss=0.038880541920661926
test: epoch 62, loss 0.30709460377693176, acc=0.9345670938491821, loss=0.30709460377693176
train: epoch 63, loss 0.03744254261255264, acc=0.9901255965232849, loss=0.03744254261255264
test: epoch 63, loss 0.17646993696689606, acc=0.9596827626228333, loss=0.17646993696689606
train: epoch 64, loss 0.03730763867497444, acc=0.990416407585144, loss=0.03730763867497444
test: epoch 64, loss 0.23961485922336578, acc=0.9471248984336853, loss=0.23961485922336578
train: epoch 65, loss 0.03541938588023186, acc=0.9910773038864136, loss=0.03541938588023186
test: epoch 65, loss 0.136897012591362, acc=0.9808327555656433, loss=0.136897012591362
train: epoch 66, loss 0.02858368307352066, acc=0.992729663848877, loss=0.02858368307352066
test: epoch 66, loss 0.12045232951641083, acc=0.9742233753204346, loss=0.12045232951641083
train: epoch 67, loss 0.023451196029782295, acc=0.9946067333221436, loss=0.023451196029782295
test: epoch 67, loss 0.030121853575110435, acc=0.9933906197547913, loss=0.030121853575110435
