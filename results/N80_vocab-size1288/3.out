# ["--N=80", "--n_epochs=250", "--batch_size=32", "--lr=0.001", "--receiver_embed_dim=64", "--temperature=2.0", "--temp_decay=0.995", "--one_hot=1", "--n_layers=3", "--save_run=1", "--n_symbols=1288"]
Namespace(N=80, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=250, n_layers=3, n_runs=1, n_summands=2, n_symbols=1288, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=852656457, receiver_embed_dim=64, save_run=True, temp_decay=0.995, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.4279160499572754, acc=0.19593225419521332, loss=2.4279160499572754
test: epoch 1, loss 7.425151824951172, acc=0.056562233716249466, loss=7.425151824951172
train: epoch 2, loss 1.3210843801498413, acc=0.42826756834983826, loss=1.3210843801498413
test: epoch 2, loss 6.36731481552124, acc=0.07739204168319702, loss=6.36731481552124
train: epoch 3, loss 1.0827523469924927, acc=0.5254631638526917, loss=1.0827523469924927
test: epoch 3, loss 6.199154853820801, acc=0.10872142016887665, loss=6.199154853820801
train: epoch 4, loss 0.8914670348167419, acc=0.6185706853866577, loss=0.8914670348167419
test: epoch 4, loss 5.071356773376465, acc=0.14005079865455627, loss=5.071356773376465
train: epoch 5, loss 0.7308396100997925, acc=0.7016968727111816, loss=0.7308396100997925
test: epoch 5, loss 5.238527297973633, acc=0.15139712393283844, loss=5.238527297973633
train: epoch 6, loss 0.5910241007804871, acc=0.7654259204864502, loss=0.5910241007804871
test: epoch 6, loss 4.340728282928467, acc=0.15241320431232452, loss=4.340728282928467
train: epoch 7, loss 0.48876699805259705, acc=0.8119491934776306, loss=0.48876699805259705
test: epoch 7, loss 3.968618392944336, acc=0.17933954298496246, loss=3.968618392944336
train: epoch 8, loss 0.40540724992752075, acc=0.8477967977523804, loss=0.40540724992752075
test: epoch 8, loss 4.037832736968994, acc=0.20440304279327393, loss=4.037832736968994
train: epoch 9, loss 0.3464113175868988, acc=0.8731176853179932, loss=0.3464113175868988
test: epoch 9, loss 4.244779109954834, acc=0.16088061034679413, loss=4.244779109954834
train: epoch 10, loss 0.2817161977291107, acc=0.899928867816925, loss=0.2817161977291107
test: epoch 10, loss 3.7493677139282227, acc=0.21016088128089905, loss=3.7493677139282227
train: epoch 11, loss 0.22375807166099548, acc=0.9241287112236023, loss=0.22375807166099548
test: epoch 11, loss 3.6650991439819336, acc=0.24606266617774963, loss=3.6650991439819336
train: epoch 12, loss 0.17714937031269073, acc=0.9425605535507202, loss=0.17714937031269073
test: epoch 12, loss 3.487778663635254, acc=0.3033022880554199, loss=3.487778663635254
train: epoch 13, loss 0.15193937718868256, acc=0.9512616395950317, loss=0.15193937718868256
test: epoch 13, loss 3.2944843769073486, acc=0.29974597692489624, loss=3.2944843769073486
train: epoch 14, loss 0.13147231936454773, acc=0.9586926102638245, loss=0.13147231936454773
test: epoch 14, loss 3.0953621864318848, acc=0.29720574617385864, loss=3.0953621864318848
train: epoch 15, loss 0.11759599298238754, acc=0.9637662768363953, loss=0.11759599298238754
test: epoch 15, loss 3.133370876312256, acc=0.3061811923980713, loss=3.133370876312256
train: epoch 16, loss 0.10608478635549545, acc=0.9677832126617432, loss=0.10608478635549545
test: epoch 16, loss 2.9797425270080566, acc=0.3620660603046417, loss=2.9797425270080566
train: epoch 17, loss 0.09888749569654465, acc=0.9706012010574341, loss=0.09888749569654465
test: epoch 17, loss 2.7757010459899902, acc=0.3895004093647003, loss=2.7757010459899902
train: epoch 18, loss 0.0919121652841568, acc=0.9724403023719788, loss=0.0919121652841568
test: epoch 18, loss 2.4473328590393066, acc=0.3903471529483795, loss=2.4473328590393066
train: epoch 19, loss 0.0904260203242302, acc=0.973717212677002, loss=0.0904260203242302
test: epoch 19, loss 2.87528920173645, acc=0.4337002635002136, loss=2.87528920173645
train: epoch 20, loss 0.08069629222154617, acc=0.9764843583106995, loss=0.08069629222154617
test: epoch 20, loss 2.359121799468994, acc=0.4550381004810333, loss=2.359121799468994
train: epoch 21, loss 0.07784795761108398, acc=0.9776223301887512, loss=0.07784795761108398
test: epoch 21, loss 2.313464879989624, acc=0.5011007785797119, loss=2.313464879989624
train: epoch 22, loss 0.07334379851818085, acc=0.9792548418045044, loss=0.07334379851818085
test: epoch 22, loss 2.105900764465332, acc=0.5121083855628967, loss=2.105900764465332
train: epoch 23, loss 0.06864049285650253, acc=0.980508029460907, loss=0.06864049285650253
test: epoch 23, loss 2.0639681816101074, acc=0.5910245776176453, loss=2.0639681816101074
train: epoch 24, loss 0.06676023453474045, acc=0.9820118546485901, loss=0.06676023453474045
test: epoch 24, loss 1.6168338060379028, acc=0.5998306274414062, loss=1.6168338060379028
train: epoch 25, loss 0.061402902007102966, acc=0.9834343791007996, loss=0.061402902007102966
test: epoch 25, loss 1.4900035858154297, acc=0.6824724674224854, loss=1.4900035858154297
train: epoch 26, loss 0.059756163507699966, acc=0.9840033650398254, loss=0.059756163507699966
test: epoch 26, loss 1.4296642541885376, acc=0.681795060634613, loss=1.4296642541885376
train: epoch 27, loss 0.052621014416217804, acc=0.9859474897384644, loss=0.052621014416217804
test: epoch 27, loss 1.2495262622833252, acc=0.7246401309967041, loss=1.2495262622833252
train: epoch 28, loss 0.05043669790029526, acc=0.9869906902313232, loss=0.05043669790029526
test: epoch 28, loss 1.010666012763977, acc=0.7942421436309814, loss=1.010666012763977
train: epoch 29, loss 0.04628905653953552, acc=0.988115131855011, loss=0.04628905653953552
test: epoch 29, loss 0.7720392346382141, acc=0.8191363215446472, loss=0.7720392346382141
train: epoch 30, loss 0.045977745205163956, acc=0.9889110922813416, loss=0.045977745205163956
test: epoch 30, loss 0.5958449840545654, acc=0.861303985118866, loss=0.5958449840545654
train: epoch 31, loss 0.04106692224740982, acc=0.990391194820404, loss=0.04106692224740982
test: epoch 31, loss 0.49184808135032654, acc=0.8839966058731079, loss=0.49184808135032654
train: epoch 32, loss 0.03486533463001251, acc=0.9918374419212341, loss=0.03486533463001251
test: epoch 32, loss 0.3760283887386322, acc=0.9170194864273071, loss=0.3760283887386322
train: epoch 33, loss 0.032035596668720245, acc=0.9928603172302246, loss=0.032035596668720245
test: epoch 33, loss 0.34237590432167053, acc=0.9405588507652283, loss=0.34237590432167053
train: epoch 34, loss 0.031364209949970245, acc=0.9935613870620728, loss=0.031364209949970245
test: epoch 34, loss 0.2584490180015564, acc=0.9583404064178467, loss=0.2584490180015564
train: epoch 35, loss 0.027037180960178375, acc=0.9944589138031006, loss=0.027037180960178375
test: epoch 35, loss 0.12232552468776703, acc=0.9764606356620789, loss=0.12232552468776703
train: epoch 36, loss 0.022848384454846382, acc=0.9953361749649048, loss=0.022848384454846382
test: epoch 36, loss 0.07412157952785492, acc=0.9849280118942261, loss=0.07412157952785492
train: epoch 37, loss 0.01876777969300747, acc=0.9959762692451477, loss=0.01876777969300747
test: epoch 37, loss 0.10309189558029175, acc=0.986960232257843, loss=0.10309189558029175
train: epoch 38, loss 0.020463906228542328, acc=0.9965181946754456, loss=0.020463906228542328
test: epoch 38, loss 0.05559784546494484, acc=0.9908552169799805, loss=0.05559784546494484
