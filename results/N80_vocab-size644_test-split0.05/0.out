# ["--N=80", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--receiver_embed_dim=64", "--temperature=2.0", "--temp_decay=0.995", "--one_hot=1", "--n_layers=3", "--save_run=1", "--test_split=0.05", "--n_symbols=644"]
Namespace(N=80, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=644, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=475852354, receiver_embed_dim=64, save_run=True, temp_decay=0.995, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.05, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.565366744995117, acc=0.1729985624551773, loss=2.565366744995117
test: epoch 1, loss 6.707039833068848, acc=0.06626022607088089, loss=6.707039833068848
train: epoch 2, loss 1.3225257396697998, acc=0.4347633421421051, loss=1.3225257396697998
test: epoch 2, loss 4.554427146911621, acc=0.1236964538693428, loss=4.554427146911621
train: epoch 3, loss 1.0520061254501343, acc=0.5490357875823975, loss=1.0520061254501343
test: epoch 3, loss 3.94775128364563, acc=0.14343012869358063, loss=3.94775128364563
train: epoch 4, loss 0.8477842211723328, acc=0.6467158794403076, loss=0.8477842211723328
test: epoch 4, loss 4.941446781158447, acc=0.1272260546684265, loss=4.941446781158447
train: epoch 5, loss 0.6886193752288818, acc=0.7188320159912109, loss=0.6886193752288818
test: epoch 5, loss 3.834132194519043, acc=0.1546606719493866, loss=3.834132194519043
train: epoch 6, loss 0.5297860503196716, acc=0.7913813591003418, loss=0.5297860503196716
test: epoch 6, loss 3.710749387741089, acc=0.1613990068435669, loss=3.710749387741089
train: epoch 7, loss 0.43369176983833313, acc=0.8344457149505615, loss=0.43369176983833313
test: epoch 7, loss 3.904855489730835, acc=0.17968875169754028, loss=3.904855489730835
train: epoch 8, loss 0.37815189361572266, acc=0.8575806021690369, loss=0.37815189361572266
test: epoch 8, loss 3.6536953449249268, acc=0.2093694806098938, loss=3.6536953449249268
train: epoch 9, loss 0.3253674805164337, acc=0.8799871802330017, loss=0.3253674805164337
test: epoch 9, loss 3.411043405532837, acc=0.20920905470848083, loss=3.411043405532837
train: epoch 10, loss 0.2830621004104614, acc=0.8985913395881653, loss=0.2830621004104614
test: epoch 10, loss 3.2648110389709473, acc=0.24370287358760834, loss=3.2648110389709473
train: epoch 11, loss 0.24338430166244507, acc=0.9157195687294006, loss=0.24338430166244507
test: epoch 11, loss 3.22440767288208, acc=0.2518851161003113, loss=3.22440767288208
train: epoch 12, loss 0.20091888308525085, acc=0.9332424402236938, loss=0.20091888308525085
test: epoch 12, loss 3.262697219848633, acc=0.264078289270401, loss=3.262697219848633
train: epoch 13, loss 0.17412324249744415, acc=0.9425573348999023, loss=0.17412324249744415
test: epoch 13, loss 3.1850810050964355, acc=0.3098026514053345, loss=3.1850810050964355
train: epoch 14, loss 0.15649113059043884, acc=0.9500914216041565, loss=0.15649113059043884
test: epoch 14, loss 2.863396406173706, acc=0.32183539867401123, loss=2.863396406173706
train: epoch 15, loss 0.14138254523277283, acc=0.9556714296340942, loss=0.14138254523277283
test: epoch 15, loss 2.7200398445129395, acc=0.34862828254699707, loss=2.7200398445129395
train: epoch 16, loss 0.1313513219356537, acc=0.9591400623321533, loss=0.1313513219356537
test: epoch 16, loss 2.7078142166137695, acc=0.3759024441242218, loss=2.7078142166137695
train: epoch 17, loss 0.12036988139152527, acc=0.963324248790741, loss=0.12036988139152527
test: epoch 17, loss 2.603908061981201, acc=0.4023744463920593, loss=2.603908061981201
train: epoch 18, loss 0.11157529056072235, acc=0.9658976197242737, loss=0.11157529056072235
test: epoch 18, loss 2.632028341293335, acc=0.34718433022499084, loss=2.632028341293335
train: epoch 19, loss 0.10524733364582062, acc=0.9684165120124817, loss=0.10524733364582062
test: epoch 19, loss 2.7673158645629883, acc=0.40542274713516235, loss=2.7673158645629883
train: epoch 20, loss 0.09837008267641068, acc=0.970126748085022, loss=0.09837008267641068
test: epoch 20, loss 2.376025915145874, acc=0.4448901116847992, loss=2.376025915145874
train: epoch 21, loss 0.0939076691865921, acc=0.9723215103149414, loss=0.0939076691865921
test: epoch 21, loss 2.5572996139526367, acc=0.4132841229438782, loss=2.5572996139526367
train: epoch 22, loss 0.08924689888954163, acc=0.9737911224365234, loss=0.08924689888954163
test: epoch 22, loss 2.4129252433776855, acc=0.4610941708087921, loss=2.4129252433776855
train: epoch 23, loss 0.08322754502296448, acc=0.9753762483596802, loss=0.08322754502296448
test: epoch 23, loss 2.4099698066711426, acc=0.47569388151168823, loss=2.4099698066711426
train: epoch 24, loss 0.08447051793336868, acc=0.9754725098609924, loss=0.08447051793336868
test: epoch 24, loss 2.1658072471618652, acc=0.471843421459198, loss=2.1658072471618652
train: epoch 25, loss 0.0798964872956276, acc=0.9767335057258606, loss=0.0798964872956276
test: epoch 25, loss 2.510101556777954, acc=0.41889941692352295, loss=2.510101556777954
train: epoch 26, loss 0.07586681097745895, acc=0.9780908226966858, loss=0.07586681097745895
test: epoch 26, loss 2.12471604347229, acc=0.512273371219635, loss=2.12471604347229
train: epoch 27, loss 0.07508355379104614, acc=0.9792042374610901, loss=0.07508355379104614
test: epoch 27, loss 2.0442087650299072, acc=0.525108277797699, loss=2.0442087650299072
train: epoch 28, loss 0.07282230257987976, acc=0.9792940616607666, loss=0.07282230257987976
test: epoch 28, loss 2.1810667514801025, acc=0.5209369659423828, loss=2.1810667514801025
train: epoch 29, loss 0.0680292397737503, acc=0.9809144735336304, loss=0.0680292397737503
test: epoch 29, loss 2.068253517150879, acc=0.5551099181175232, loss=2.068253517150879
train: epoch 30, loss 0.06686491519212723, acc=0.9814631938934326, loss=0.06686491519212723
test: epoch 30, loss 1.828048825263977, acc=0.5304027199745178, loss=1.828048825263977
train: epoch 31, loss 0.06319611519575119, acc=0.9825798273086548, loss=0.06319611519575119
test: epoch 31, loss 1.9778512716293335, acc=0.6027594804763794, loss=1.9778512716293335
train: epoch 32, loss 0.06051786243915558, acc=0.9830194115638733, loss=0.06051786243915558
test: epoch 32, loss 1.7032023668289185, acc=0.6054869294166565, loss=1.7032023668289185
train: epoch 33, loss 0.06118917465209961, acc=0.9835264086723328, loss=0.06118917465209961
test: epoch 33, loss 1.5650053024291992, acc=0.6329215168952942, loss=1.5650053024291992
train: epoch 34, loss 0.05673566460609436, acc=0.9844858050346375, loss=0.05673566460609436
test: epoch 34, loss 1.3356398344039917, acc=0.6975774168968201, loss=1.3356398344039917
train: epoch 35, loss 0.05472438782453537, acc=0.9853425025939941, loss=0.05472438782453537
test: epoch 35, loss 1.1616933345794678, acc=0.7285416126251221, loss=1.1616933345794678
train: epoch 36, loss 0.05150430649518967, acc=0.9863564968109131, loss=0.05150430649518967
test: epoch 36, loss 1.2605005502700806, acc=0.7160276174545288, loss=1.2605005502700806
train: epoch 37, loss 0.04961525276303291, acc=0.9870014190673828, loss=0.04961525276303291
test: epoch 37, loss 1.0850918292999268, acc=0.7763516902923584, loss=1.0850918292999268
train: epoch 38, loss 0.049375005066394806, acc=0.9875341057777405, loss=0.049375005066394806
test: epoch 38, loss 0.9312296509742737, acc=0.7771538496017456, loss=0.9312296509742737
train: epoch 39, loss 0.046734943985939026, acc=0.9884100556373596, loss=0.046734943985939026
test: epoch 39, loss 1.12846839427948, acc=0.8183860182762146, loss=1.12846839427948
train: epoch 40, loss 0.04342671111226082, acc=0.9889395236968994, loss=0.04342671111226082
test: epoch 40, loss 0.9618415832519531, acc=0.8008984327316284, loss=0.9618415832519531
train: epoch 41, loss 0.04225268214941025, acc=0.9897031784057617, loss=0.04225268214941025
test: epoch 41, loss 0.6357401609420776, acc=0.8605808019638062, loss=0.6357401609420776
train: epoch 42, loss 0.03889727219939232, acc=0.9904155135154724, loss=0.03889727219939232
test: epoch 42, loss 0.4679728150367737, acc=0.8859297037124634, loss=0.4679728150367737
train: epoch 43, loss 0.03696199879050255, acc=0.9914391040802002, loss=0.03696199879050255
test: epoch 43, loss 0.4311639070510864, acc=0.9027755260467529, loss=0.4311639070510864
train: epoch 44, loss 0.03535977751016617, acc=0.9919171929359436, loss=0.03535977751016617
test: epoch 44, loss 0.3638790547847748, acc=0.9050216674804688, loss=0.3638790547847748
train: epoch 45, loss 0.030913759022951126, acc=0.9928509593009949, loss=0.030913759022951126
test: epoch 45, loss 0.24878773093223572, acc=0.9472164511680603, loss=0.24878773093223572
train: epoch 46, loss 0.031056487932801247, acc=0.993370771408081, loss=0.031056487932801247
test: epoch 46, loss 0.21740223467350006, acc=0.9549173712730408, loss=0.21740223467350006
train: epoch 47, loss 0.028936045244336128, acc=0.9942114353179932, loss=0.028936045244336128
test: epoch 47, loss 0.19632746279239655, acc=0.9653457403182983, loss=0.19632746279239655
train: epoch 48, loss 0.028583567589521408, acc=0.9947152137756348, loss=0.028583567589521408
test: epoch 48, loss 0.06517022848129272, acc=0.9878068566322327, loss=0.06517022848129272
train: epoch 49, loss 0.02666621282696724, acc=0.9949751496315002, loss=0.02666621282696724
test: epoch 49, loss 0.061932213604450226, acc=0.9857211709022522, loss=0.061932213604450226
train: epoch 50, loss 0.0238131582736969, acc=0.9959633946418762, loss=0.0238131582736969
test: epoch 50, loss 0.07532062381505966, acc=0.9910155534744263, loss=0.07532062381505966
