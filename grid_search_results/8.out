# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.995", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1595970534, receiver_embed_dim=32, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1595970534, receiver_embed_dim=32, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.833190441131592, acc=0.10305555909872055, loss=2.833190441131592
test: epoch 1, loss 2.6628222465515137, acc=0.11944444477558136, loss=2.6628222465515137
train: epoch 2, loss 2.0754902362823486, acc=0.21400000154972076, loss=2.0754902362823486
test: epoch 2, loss 2.8717575073242188, acc=0.13333334028720856, loss=2.8717575073242188
train: epoch 3, loss 1.8495218753814697, acc=0.26294443011283875, loss=1.8495218753814697
test: epoch 3, loss 2.57586932182312, acc=0.15833333134651184, loss=2.57586932182312
train: epoch 4, loss 1.6527048349380493, acc=0.33772221207618713, loss=1.6527048349380493
test: epoch 4, loss 2.0951452255249023, acc=0.2083333283662796, loss=2.0951452255249023
train: epoch 5, loss 1.5657702684402466, acc=0.363444447517395, loss=1.5657702684402466
test: epoch 5, loss 1.888869285583496, acc=0.25555557012557983, loss=1.888869285583496
train: epoch 6, loss 1.4689593315124512, acc=0.39977777004241943, loss=1.4689593315124512
test: epoch 6, loss 1.8299012184143066, acc=0.23333333432674408, loss=1.8299012184143066
train: epoch 7, loss 1.4179182052612305, acc=0.4221666753292084, loss=1.4179182052612305
test: epoch 7, loss 1.8253434896469116, acc=0.21388888359069824, loss=1.8253434896469116
train: epoch 8, loss 1.351198434829712, acc=0.4465000033378601, loss=1.351198434829712
test: epoch 8, loss 2.0074238777160645, acc=0.23333333432674408, loss=2.0074238777160645
train: epoch 9, loss 1.3374041318893433, acc=0.45605555176734924, loss=1.3374041318893433
test: epoch 9, loss 1.9648733139038086, acc=0.2777777910232544, loss=1.9648733139038086
train: epoch 10, loss 1.2730703353881836, acc=0.48216667771339417, loss=1.2730703353881836
test: epoch 10, loss 1.7825748920440674, acc=0.28611111640930176, loss=1.7825748920440674
train: epoch 11, loss 1.2651903629302979, acc=0.4834444522857666, loss=1.2651903629302979
test: epoch 11, loss 1.5634046792984009, acc=0.3638888895511627, loss=1.5634046792984009
train: epoch 12, loss 1.2200053930282593, acc=0.5036666393280029, loss=1.2200053930282593
test: epoch 12, loss 1.5365817546844482, acc=0.2750000059604645, loss=1.5365817546844482
train: epoch 13, loss 1.1788814067840576, acc=0.5215555429458618, loss=1.1788814067840576
test: epoch 13, loss 1.636956810951233, acc=0.26944443583488464, loss=1.636956810951233
train: epoch 14, loss 1.1366095542907715, acc=0.5414444208145142, loss=1.1366095542907715
test: epoch 14, loss 1.6374664306640625, acc=0.36666667461395264, loss=1.6374664306640625
train: epoch 15, loss 1.118269920349121, acc=0.5352222323417664, loss=1.118269920349121
test: epoch 15, loss 1.608540415763855, acc=0.32499998807907104, loss=1.608540415763855
train: epoch 16, loss 1.1017612218856812, acc=0.5488333106040955, loss=1.1017612218856812
test: epoch 16, loss 1.5213814973831177, acc=0.36944442987442017, loss=1.5213814973831177
train: epoch 17, loss 1.0579761266708374, acc=0.5636110901832581, loss=1.0579761266708374
test: epoch 17, loss 1.859476089477539, acc=0.3222222328186035, loss=1.859476089477539
train: epoch 18, loss 1.0650702714920044, acc=0.5578888654708862, loss=1.0650702714920044
test: epoch 18, loss 1.6307666301727295, acc=0.31388887763023376, loss=1.6307666301727295
train: epoch 19, loss 1.0357471704483032, acc=0.5708333253860474, loss=1.0357471704483032
test: epoch 19, loss 1.459861159324646, acc=0.3861111104488373, loss=1.459861159324646
train: epoch 20, loss 1.0203114748001099, acc=0.5770555734634399, loss=1.0203114748001099
test: epoch 20, loss 1.4498807191848755, acc=0.375, loss=1.4498807191848755
train: epoch 21, loss 1.0018725395202637, acc=0.5835555791854858, loss=1.0018725395202637
test: epoch 21, loss 1.6166428327560425, acc=0.3166666626930237, loss=1.6166428327560425
train: epoch 22, loss 0.9922928810119629, acc=0.5850555300712585, loss=0.9922928810119629
test: epoch 22, loss 1.6569011211395264, acc=0.3222222328186035, loss=1.6569011211395264
train: epoch 23, loss 0.9735404849052429, acc=0.5931666493415833, loss=0.9735404849052429
test: epoch 23, loss 1.70924973487854, acc=0.36944442987442017, loss=1.70924973487854
train: epoch 24, loss 0.9631540775299072, acc=0.5998333096504211, loss=0.9631540775299072
test: epoch 24, loss 1.6965030431747437, acc=0.375, loss=1.6965030431747437
train: epoch 25, loss 0.9399198293685913, acc=0.6100000143051147, loss=0.9399198293685913
test: epoch 25, loss 1.6343433856964111, acc=0.31388887763023376, loss=1.6343433856964111
train: epoch 26, loss 0.950670599937439, acc=0.6066111326217651, loss=0.950670599937439
test: epoch 26, loss 1.743847370147705, acc=0.3722222149372101, loss=1.743847370147705
train: epoch 27, loss 0.9295676350593567, acc=0.6134999990463257, loss=0.9295676350593567
test: epoch 27, loss 1.5292999744415283, acc=0.3638888895511627, loss=1.5292999744415283
train: epoch 28, loss 0.9338937401771545, acc=0.6134999990463257, loss=0.9338937401771545
test: epoch 28, loss 1.6401422023773193, acc=0.4027777910232544, loss=1.6401422023773193
train: epoch 29, loss 0.9153621196746826, acc=0.6204444169998169, loss=0.9153621196746826
test: epoch 29, loss 1.552232265472412, acc=0.39444443583488464, loss=1.552232265472412
train: epoch 30, loss 0.9016399383544922, acc=0.6271666884422302, loss=0.9016399383544922
test: epoch 30, loss 1.4853007793426514, acc=0.38055557012557983, loss=1.4853007793426514
train: epoch 31, loss 0.9014217257499695, acc=0.6303333044052124, loss=0.9014217257499695
test: epoch 31, loss 1.416874647140503, acc=0.40833333134651184, loss=1.416874647140503
train: epoch 32, loss 0.886409342288971, acc=0.6311666369438171, loss=0.886409342288971
test: epoch 32, loss 1.9864461421966553, acc=0.3638888895511627, loss=1.9864461421966553
train: epoch 33, loss 0.8881984949111938, acc=0.6352777481079102, loss=0.8881984949111938
test: epoch 33, loss 1.4557535648345947, acc=0.375, loss=1.4557535648345947
train: epoch 34, loss 0.8710942268371582, acc=0.6432777643203735, loss=0.8710942268371582
test: epoch 34, loss 1.4706122875213623, acc=0.3861111104488373, loss=1.4706122875213623
train: epoch 35, loss 0.8507378697395325, acc=0.6526666879653931, loss=0.8507378697395325
test: epoch 35, loss 1.5327588319778442, acc=0.36944442987442017, loss=1.5327588319778442
train: epoch 36, loss 0.8589566349983215, acc=0.6452222466468811, loss=0.8589566349983215
test: epoch 36, loss 1.4089282751083374, acc=0.3499999940395355, loss=1.4089282751083374
train: epoch 37, loss 0.8249361515045166, acc=0.6583333611488342, loss=0.8249361515045166
test: epoch 37, loss 1.5872509479522705, acc=0.31388887763023376, loss=1.5872509479522705
train: epoch 38, loss 0.8261858820915222, acc=0.660111129283905, loss=0.8261858820915222
test: epoch 38, loss 1.5539144277572632, acc=0.40833333134651184, loss=1.5539144277572632
train: epoch 39, loss 0.8522407412528992, acc=0.644611120223999, loss=0.8522407412528992
test: epoch 39, loss 1.650413155555725, acc=0.3861111104488373, loss=1.650413155555725
train: epoch 40, loss 0.80885910987854, acc=0.6747778058052063, loss=0.80885910987854
test: epoch 40, loss 1.3563477993011475, acc=0.42222222685813904, loss=1.3563477993011475
train: epoch 41, loss 0.8171523809432983, acc=0.6653333306312561, loss=0.8171523809432983
test: epoch 41, loss 1.675166368484497, acc=0.38055557012557983, loss=1.675166368484497
train: epoch 42, loss 0.8143085241317749, acc=0.6696110963821411, loss=0.8143085241317749
test: epoch 42, loss 1.5313819646835327, acc=0.38055557012557983, loss=1.5313819646835327
train: epoch 43, loss 0.7912819981575012, acc=0.6811110973358154, loss=0.7912819981575012
test: epoch 43, loss 1.6032520532608032, acc=0.36666667461395264, loss=1.6032520532608032
train: epoch 44, loss 0.7981075048446655, acc=0.675611138343811, loss=0.7981075048446655
test: epoch 44, loss 1.4643117189407349, acc=0.4694444537162781, loss=1.4643117189407349
train: epoch 45, loss 0.7984297871589661, acc=0.6779999732971191, loss=0.7984297871589661
test: epoch 45, loss 1.4258370399475098, acc=0.4138889014720917, loss=1.4258370399475098
train: epoch 46, loss 0.8027695417404175, acc=0.6700555682182312, loss=0.8027695417404175
test: epoch 46, loss 1.6100775003433228, acc=0.39722222089767456, loss=1.6100775003433228
train: epoch 47, loss 0.7861647605895996, acc=0.6808888912200928, loss=0.7861647605895996
test: epoch 47, loss 1.5923879146575928, acc=0.3777777850627899, loss=1.5923879146575928
train: epoch 48, loss 0.7756861448287964, acc=0.6854444742202759, loss=0.7756861448287964
test: epoch 48, loss 1.332709789276123, acc=0.47777777910232544, loss=1.332709789276123
train: epoch 49, loss 0.798389732837677, acc=0.675166666507721, loss=0.798389732837677
test: epoch 49, loss 1.570862889289856, acc=0.38333332538604736, loss=1.570862889289856
train: epoch 50, loss 0.7757918834686279, acc=0.6852777600288391, loss=0.7757918834686279
test: epoch 50, loss 1.5861382484436035, acc=0.42222222685813904, loss=1.5861382484436035
train: epoch 51, loss 0.7603523135185242, acc=0.6978333592414856, loss=0.7603523135185242
test: epoch 51, loss 1.4884711503982544, acc=0.4305555522441864, loss=1.4884711503982544
train: epoch 52, loss 0.7616570591926575, acc=0.6920555830001831, loss=0.7616570591926575
test: epoch 52, loss 1.4798177480697632, acc=0.3472222089767456, loss=1.4798177480697632
train: epoch 53, loss 0.7723070383071899, acc=0.6889444589614868, loss=0.7723070383071899
test: epoch 53, loss 1.4110758304595947, acc=0.4305555522441864, loss=1.4110758304595947
train: epoch 54, loss 0.767443835735321, acc=0.6915000081062317, loss=0.767443835735321
test: epoch 54, loss 1.6157426834106445, acc=0.4277777671813965, loss=1.6157426834106445
train: epoch 55, loss 0.7629534602165222, acc=0.6915555596351624, loss=0.7629534602165222
test: epoch 55, loss 1.6067111492156982, acc=0.3861111104488373, loss=1.6067111492156982
train: epoch 56, loss 0.7444896697998047, acc=0.703166663646698, loss=0.7444896697998047
test: epoch 56, loss 1.3730700016021729, acc=0.4305555522441864, loss=1.3730700016021729
train: epoch 57, loss 0.7530516386032104, acc=0.699388861656189, loss=0.7530516386032104
test: epoch 57, loss 1.5230118036270142, acc=0.3861111104488373, loss=1.5230118036270142
train: epoch 58, loss 0.7510236501693726, acc=0.6996111273765564, loss=0.7510236501693726
test: epoch 58, loss 1.5785316228866577, acc=0.4277777671813965, loss=1.5785316228866577
train: epoch 59, loss 0.789307713508606, acc=0.6809444427490234, loss=0.789307713508606
test: epoch 59, loss 1.3352243900299072, acc=0.5055555701255798, loss=1.3352243900299072
train: epoch 60, loss 0.7418575882911682, acc=0.7016111016273499, loss=0.7418575882911682
test: epoch 60, loss 1.5807864665985107, acc=0.4055555462837219, loss=1.5807864665985107
train: epoch 61, loss 0.7416390776634216, acc=0.7011666893959045, loss=0.7416390776634216
test: epoch 61, loss 1.6379541158676147, acc=0.4277777671813965, loss=1.6379541158676147
train: epoch 62, loss 0.7229290008544922, acc=0.7108333110809326, loss=0.7229290008544922
test: epoch 62, loss 1.5864763259887695, acc=0.3861111104488373, loss=1.5864763259887695
train: epoch 63, loss 0.7480302453041077, acc=0.6995000243186951, loss=0.7480302453041077
test: epoch 63, loss 1.5644043684005737, acc=0.4583333432674408, loss=1.5644043684005737
train: epoch 64, loss 0.7466327548027039, acc=0.7005555629730225, loss=0.7466327548027039
test: epoch 64, loss 1.4707123041152954, acc=0.38333332538604736, loss=1.4707123041152954
train: epoch 65, loss 0.7220548391342163, acc=0.7086666822433472, loss=0.7220548391342163
test: epoch 65, loss 1.5178534984588623, acc=0.39444443583488464, loss=1.5178534984588623
train: epoch 66, loss 0.7131124138832092, acc=0.7118333578109741, loss=0.7131124138832092
test: epoch 66, loss 1.6079241037368774, acc=0.4277777671813965, loss=1.6079241037368774
train: epoch 67, loss 0.7315080165863037, acc=0.7079444527626038, loss=0.7315080165863037
test: epoch 67, loss 1.4759854078292847, acc=0.4305555522441864, loss=1.4759854078292847
train: epoch 68, loss 0.7323533892631531, acc=0.7098888754844666, loss=0.7323533892631531
test: epoch 68, loss 1.5253597497940063, acc=0.4277777671813965, loss=1.5253597497940063
train: epoch 69, loss 0.7044298052787781, acc=0.7197777628898621, loss=0.7044298052787781
test: epoch 69, loss 1.4877185821533203, acc=0.38333332538604736, loss=1.4877185821533203
train: epoch 70, loss 0.7147466540336609, acc=0.7172222137451172, loss=0.7147466540336609
test: epoch 70, loss 1.5753045082092285, acc=0.4277777671813965, loss=1.5753045082092285
train: epoch 71, loss 0.7042031288146973, acc=0.7196666598320007, loss=0.7042031288146973
test: epoch 71, loss 1.4875624179840088, acc=0.42222222685813904, loss=1.4875624179840088
train: epoch 72, loss 0.7011187076568604, acc=0.7185555696487427, loss=0.7011187076568604
test: epoch 72, loss 1.5515408515930176, acc=0.4305555522441864, loss=1.5515408515930176
train: epoch 73, loss 0.7096684575080872, acc=0.7169444561004639, loss=0.7096684575080872
test: epoch 73, loss 1.5504144430160522, acc=0.4277777671813965, loss=1.5504144430160522
train: epoch 74, loss 0.6932741403579712, acc=0.7241666913032532, loss=0.6932741403579712
test: epoch 74, loss 1.3044182062149048, acc=0.4305555522441864, loss=1.3044182062149048
train: epoch 75, loss 0.688072681427002, acc=0.7251666784286499, loss=0.688072681427002
test: epoch 75, loss 1.432218074798584, acc=0.48055556416511536, loss=1.432218074798584
train: epoch 76, loss 0.7228028178215027, acc=0.7107222080230713, loss=0.7228028178215027
test: epoch 76, loss 1.530513048171997, acc=0.40833333134651184, loss=1.530513048171997
train: epoch 77, loss 0.689289391040802, acc=0.7252222299575806, loss=0.689289391040802
test: epoch 77, loss 1.3588039875030518, acc=0.5305555462837219, loss=1.3588039875030518
train: epoch 78, loss 0.6805047392845154, acc=0.7291111350059509, loss=0.6805047392845154
test: epoch 78, loss 1.4111131429672241, acc=0.4305555522441864, loss=1.4111131429672241
train: epoch 79, loss 0.6800142526626587, acc=0.7241666913032532, loss=0.6800142526626587
test: epoch 79, loss 1.5010215044021606, acc=0.3722222149372101, loss=1.5010215044021606
train: epoch 80, loss 0.6921050548553467, acc=0.718500018119812, loss=0.6921050548553467
test: epoch 80, loss 1.2655366659164429, acc=0.4749999940395355, loss=1.2655366659164429
train: epoch 81, loss 0.6592738628387451, acc=0.7367777824401855, loss=0.6592738628387451
test: epoch 81, loss 1.2897851467132568, acc=0.5138888955116272, loss=1.2897851467132568
train: epoch 82, loss 0.6724332571029663, acc=0.7320555448532104, loss=0.6724332571029663
test: epoch 82, loss 1.4015297889709473, acc=0.46388888359069824, loss=1.4015297889709473
train: epoch 83, loss 0.666489839553833, acc=0.730555534362793, loss=0.666489839553833
test: epoch 83, loss 1.4985198974609375, acc=0.43611112236976624, loss=1.4985198974609375
train: epoch 84, loss 0.694710373878479, acc=0.7201111316680908, loss=0.694710373878479
test: epoch 84, loss 1.4135994911193848, acc=0.4305555522441864, loss=1.4135994911193848
train: epoch 85, loss 0.6483970880508423, acc=0.7357222437858582, loss=0.6483970880508423
test: epoch 85, loss 1.358604073524475, acc=0.4722222089767456, loss=1.358604073524475
train: epoch 86, loss 0.6528437733650208, acc=0.7354999780654907, loss=0.6528437733650208
test: epoch 86, loss 1.4895199537277222, acc=0.41111111640930176, loss=1.4895199537277222
train: epoch 87, loss 0.6744199395179749, acc=0.7243888974189758, loss=0.6744199395179749
test: epoch 87, loss 1.4538737535476685, acc=0.38055557012557983, loss=1.4538737535476685
train: epoch 88, loss 0.6736215353012085, acc=0.7220555543899536, loss=0.6736215353012085
test: epoch 88, loss 1.2125892639160156, acc=0.4722222089767456, loss=1.2125892639160156
train: epoch 89, loss 0.63289874792099, acc=0.7399444580078125, loss=0.63289874792099
test: epoch 89, loss 1.4685640335083008, acc=0.4722222089767456, loss=1.4685640335083008
train: epoch 90, loss 0.6377888917922974, acc=0.7356111407279968, loss=0.6377888917922974
test: epoch 90, loss 1.7150191068649292, acc=0.4305555522441864, loss=1.7150191068649292
train: epoch 91, loss 0.642830491065979, acc=0.7395555377006531, loss=0.642830491065979
test: epoch 91, loss 1.3731298446655273, acc=0.48055556416511536, loss=1.3731298446655273
train: epoch 92, loss 0.6640504002571106, acc=0.7299444675445557, loss=0.6640504002571106
test: epoch 92, loss 1.4049806594848633, acc=0.4333333373069763, loss=1.4049806594848633
train: epoch 93, loss 0.6203494668006897, acc=0.7461666464805603, loss=0.6203494668006897
test: epoch 93, loss 1.4991497993469238, acc=0.39722222089767456, loss=1.4991497993469238
train: epoch 94, loss 0.6355865001678467, acc=0.7397222518920898, loss=0.6355865001678467
test: epoch 94, loss 1.3944607973098755, acc=0.4555555582046509, loss=1.3944607973098755
train: epoch 95, loss 0.6664240956306458, acc=0.7258889079093933, loss=0.6664240956306458
test: epoch 95, loss 1.302679181098938, acc=0.4444444477558136, loss=1.302679181098938
train: epoch 96, loss 0.6624023914337158, acc=0.7258889079093933, loss=0.6624023914337158
test: epoch 96, loss 1.4045823812484741, acc=0.4444444477558136, loss=1.4045823812484741
train: epoch 97, loss 0.632159411907196, acc=0.7392222285270691, loss=0.632159411907196
test: epoch 97, loss 1.390026569366455, acc=0.4722222089767456, loss=1.390026569366455
train: epoch 98, loss 0.6247374415397644, acc=0.7443333268165588, loss=0.6247374415397644
test: epoch 98, loss 1.576058268547058, acc=0.4444444477558136, loss=1.576058268547058
train: epoch 99, loss 0.6253429651260376, acc=0.7431111335754395, loss=0.6253429651260376
test: epoch 99, loss 1.2518669366836548, acc=0.4972222149372101, loss=1.2518669366836548
train: epoch 100, loss 0.6151803135871887, acc=0.7499444484710693, loss=0.6151803135871887
test: epoch 100, loss 1.4716461896896362, acc=0.44999998807907104, loss=1.4716461896896362
train: epoch 101, loss 0.6529837250709534, acc=0.7335000038146973, loss=0.6529837250709534
test: epoch 101, loss 1.3241233825683594, acc=0.4722222089767456, loss=1.3241233825683594
train: epoch 102, loss 0.6425276398658752, acc=0.7406111359596252, loss=0.6425276398658752
test: epoch 102, loss 1.5351251363754272, acc=0.4694444537162781, loss=1.5351251363754272
train: epoch 103, loss 0.6294434666633606, acc=0.7384999990463257, loss=0.6294434666633606
test: epoch 103, loss 1.4093899726867676, acc=0.4833333194255829, loss=1.4093899726867676
train: epoch 104, loss 0.6294307708740234, acc=0.746999979019165, loss=0.6294307708740234
test: epoch 104, loss 1.4201594591140747, acc=0.4749999940395355, loss=1.4201594591140747
train: epoch 105, loss 0.6317543983459473, acc=0.7453888654708862, loss=0.6317543983459473
test: epoch 105, loss 1.2198072671890259, acc=0.48055556416511536, loss=1.2198072671890259
train: epoch 106, loss 0.6015322804450989, acc=0.7572222352027893, loss=0.6015322804450989
test: epoch 106, loss 1.6366194486618042, acc=0.4277777671813965, loss=1.6366194486618042
train: epoch 107, loss 0.6352134943008423, acc=0.7441111207008362, loss=0.6352134943008423
test: epoch 107, loss 1.3604145050048828, acc=0.4722222089767456, loss=1.3604145050048828
train: epoch 108, loss 0.610644519329071, acc=0.7521111369132996, loss=0.610644519329071
test: epoch 108, loss 1.3054745197296143, acc=0.43888887763023376, loss=1.3054745197296143
train: epoch 109, loss 0.6193658709526062, acc=0.749833345413208, loss=0.6193658709526062
test: epoch 109, loss 1.2477463483810425, acc=0.4722222089767456, loss=1.2477463483810425
train: epoch 110, loss 0.6056008338928223, acc=0.7545555830001831, loss=0.6056008338928223
test: epoch 110, loss 1.7042813301086426, acc=0.35555556416511536, loss=1.7042813301086426
train: epoch 111, loss 0.6122812032699585, acc=0.757444441318512, loss=0.6122812032699585
test: epoch 111, loss 1.2188966274261475, acc=0.519444465637207, loss=1.2188966274261475
train: epoch 112, loss 0.6108907461166382, acc=0.7502222061157227, loss=0.6108907461166382
test: epoch 112, loss 1.2482376098632812, acc=0.46666666865348816, loss=1.2482376098632812
train: epoch 113, loss 0.6077460646629333, acc=0.7556666731834412, loss=0.6077460646629333
test: epoch 113, loss 1.320465326309204, acc=0.4749999940395355, loss=1.320465326309204
train: epoch 114, loss 0.6206938028335571, acc=0.7481666803359985, loss=0.6206938028335571
test: epoch 114, loss 1.3756352663040161, acc=0.4305555522441864, loss=1.3756352663040161
train: epoch 115, loss 0.5898397564888, acc=0.7641666531562805, loss=0.5898397564888
test: epoch 115, loss 1.252897024154663, acc=0.4833333194255829, loss=1.252897024154663
train: epoch 116, loss 0.601766049861908, acc=0.7574999928474426, loss=0.601766049861908
test: epoch 116, loss 1.4028998613357544, acc=0.48055556416511536, loss=1.4028998613357544
train: epoch 117, loss 0.5881054401397705, acc=0.761555552482605, loss=0.5881054401397705
test: epoch 117, loss 1.4019076824188232, acc=0.4722222089767456, loss=1.4019076824188232
train: epoch 118, loss 0.5962533354759216, acc=0.7599999904632568, loss=0.5962533354759216
test: epoch 118, loss 1.294849157333374, acc=0.48055556416511536, loss=1.294849157333374
train: epoch 119, loss 0.5873752236366272, acc=0.7603333592414856, loss=0.5873752236366272
test: epoch 119, loss 1.3197239637374878, acc=0.4694444537162781, loss=1.3197239637374878
train: epoch 120, loss 0.5774984359741211, acc=0.7636666893959045, loss=0.5774984359741211
test: epoch 120, loss 1.4454072713851929, acc=0.4694444537162781, loss=1.4454072713851929
train: epoch 121, loss 0.5844551920890808, acc=0.7661111354827881, loss=0.5844551920890808
test: epoch 121, loss 1.2671542167663574, acc=0.46388888359069824, loss=1.2671542167663574
train: epoch 122, loss 0.5942851901054382, acc=0.7581111192703247, loss=0.5942851901054382
test: epoch 122, loss 1.6320525407791138, acc=0.3611111044883728, loss=1.6320525407791138
train: epoch 123, loss 0.596514105796814, acc=0.7590000033378601, loss=0.596514105796814
test: epoch 123, loss 1.3142434358596802, acc=0.47777777910232544, loss=1.3142434358596802
train: epoch 124, loss 0.5988613367080688, acc=0.7553889155387878, loss=0.5988613367080688
test: epoch 124, loss 1.4322713613510132, acc=0.3916666805744171, loss=1.4322713613510132
train: epoch 125, loss 0.5887101292610168, acc=0.7592222094535828, loss=0.5887101292610168
test: epoch 125, loss 1.4166908264160156, acc=0.44999998807907104, loss=1.4166908264160156
train: epoch 126, loss 0.5805761814117432, acc=0.7666666507720947, loss=0.5805761814117432
test: epoch 126, loss 1.3600623607635498, acc=0.4833333194255829, loss=1.3600623607635498
train: epoch 127, loss 0.5801965594291687, acc=0.7633888721466064, loss=0.5801965594291687
test: epoch 127, loss 1.3786771297454834, acc=0.42500001192092896, loss=1.3786771297454834
train: epoch 128, loss 0.5576182007789612, acc=0.7700555324554443, loss=0.5576182007789612
test: epoch 128, loss 1.3898801803588867, acc=0.39722222089767456, loss=1.3898801803588867
train: epoch 129, loss 0.6015462875366211, acc=0.7545555830001831, loss=0.6015462875366211
test: epoch 129, loss 1.3417460918426514, acc=0.4611110985279083, loss=1.3417460918426514
train: epoch 130, loss 0.6170844435691833, acc=0.7507222294807434, loss=0.6170844435691833
test: epoch 130, loss 1.5334155559539795, acc=0.40833333134651184, loss=1.5334155559539795
train: epoch 131, loss 0.576352059841156, acc=0.7695555686950684, loss=0.576352059841156
test: epoch 131, loss 1.5579195022583008, acc=0.4472222328186035, loss=1.5579195022583008
train: epoch 132, loss 0.5702321529388428, acc=0.7680555582046509, loss=0.5702321529388428
test: epoch 132, loss 1.1861701011657715, acc=0.5083333253860474, loss=1.1861701011657715
train: epoch 133, loss 0.584367036819458, acc=0.7668889164924622, loss=0.584367036819458
test: epoch 133, loss 1.5076687335968018, acc=0.39722222089767456, loss=1.5076687335968018
train: epoch 134, loss 0.57483971118927, acc=0.7676110863685608, loss=0.57483971118927
test: epoch 134, loss 1.4217090606689453, acc=0.46666666865348816, loss=1.4217090606689453
train: epoch 135, loss 0.5787502527236938, acc=0.7668333053588867, loss=0.5787502527236938
test: epoch 135, loss 1.3681745529174805, acc=0.4749999940395355, loss=1.3681745529174805
train: epoch 136, loss 0.5953312516212463, acc=0.7597222328186035, loss=0.5953312516212463
test: epoch 136, loss 1.3883092403411865, acc=0.4861111044883728, loss=1.3883092403411865
train: epoch 137, loss 0.5719093680381775, acc=0.7689444422721863, loss=0.5719093680381775
test: epoch 137, loss 1.3680555820465088, acc=0.4166666567325592, loss=1.3680555820465088
train: epoch 138, loss 0.5620570778846741, acc=0.7753888964653015, loss=0.5620570778846741
test: epoch 138, loss 1.6312856674194336, acc=0.47777777910232544, loss=1.6312856674194336
train: epoch 139, loss 0.5952526330947876, acc=0.7615000009536743, loss=0.5952526330947876
test: epoch 139, loss 1.7160933017730713, acc=0.39444443583488464, loss=1.7160933017730713
train: epoch 140, loss 0.5813732743263245, acc=0.7641666531562805, loss=0.5813732743263245
test: epoch 140, loss 1.4083210229873657, acc=0.46666666865348816, loss=1.4083210229873657
train: epoch 141, loss 0.5665725469589233, acc=0.7705000042915344, loss=0.5665725469589233
test: epoch 141, loss 1.3535581827163696, acc=0.45277777314186096, loss=1.3535581827163696
train: epoch 142, loss 0.6029735803604126, acc=0.754111111164093, loss=0.6029735803604126
test: epoch 142, loss 1.344183325767517, acc=0.5083333253860474, loss=1.344183325767517
train: epoch 143, loss 0.5850498676300049, acc=0.7639999985694885, loss=0.5850498676300049
test: epoch 143, loss 1.2094485759735107, acc=0.5249999761581421, loss=1.2094485759735107
train: epoch 144, loss 0.5822638869285583, acc=0.7659444212913513, loss=0.5822638869285583
test: epoch 144, loss 1.4587537050247192, acc=0.4416666626930237, loss=1.4587537050247192
train: epoch 145, loss 0.5818018913269043, acc=0.7674444317817688, loss=0.5818018913269043
test: epoch 145, loss 1.5693820714950562, acc=0.44999998807907104, loss=1.5693820714950562
train: epoch 146, loss 0.5657000541687012, acc=0.7707222104072571, loss=0.5657000541687012
test: epoch 146, loss 1.4647793769836426, acc=0.4861111044883728, loss=1.4647793769836426
train: epoch 147, loss 0.5751113295555115, acc=0.7687222361564636, loss=0.5751113295555115
test: epoch 147, loss 1.3710955381393433, acc=0.5027777552604675, loss=1.3710955381393433
train: epoch 148, loss 0.5720996260643005, acc=0.7684444189071655, loss=0.5720996260643005
test: epoch 148, loss 1.6896976232528687, acc=0.49166667461395264, loss=1.6896976232528687
train: epoch 149, loss 0.5603289008140564, acc=0.774055540561676, loss=0.5603289008140564
test: epoch 149, loss 1.5702153444290161, acc=0.4333333373069763, loss=1.5702153444290161
train: epoch 150, loss 0.5556765794754028, acc=0.7735555768013, loss=0.5556765794754028
test: epoch 150, loss 1.5638102293014526, acc=0.43611112236976624, loss=1.5638102293014526
