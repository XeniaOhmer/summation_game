# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.99", "--one_hot=1", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=50967153, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=50967153, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.4045846462249756, acc=0.052222222089767456, loss=3.4045846462249756
test: epoch 1, loss 3.5731616020202637, acc=0.05833333358168602, loss=3.5731616020202637
train: epoch 2, loss 2.296407461166382, acc=0.19111111760139465, loss=2.296407461166382
test: epoch 2, loss 2.175753116607666, acc=0.17222222685813904, loss=2.175753116607666
train: epoch 3, loss 1.7092385292053223, acc=0.30444443225860596, loss=1.7092385292053223
test: epoch 3, loss 1.9513696432113647, acc=0.2361111044883728, loss=1.9513696432113647
train: epoch 4, loss 1.5046175718307495, acc=0.3687777817249298, loss=1.5046175718307495
test: epoch 4, loss 1.927381992340088, acc=0.23888888955116272, loss=1.927381992340088
train: epoch 5, loss 1.3746925592422485, acc=0.41583332419395447, loss=1.3746925592422485
test: epoch 5, loss 1.9497044086456299, acc=0.24166665971279144, loss=1.9497044086456299
train: epoch 6, loss 1.276510238647461, acc=0.4611666798591614, loss=1.276510238647461
test: epoch 6, loss 1.9205251932144165, acc=0.25, loss=1.9205251932144165
train: epoch 7, loss 1.2174997329711914, acc=0.48366665840148926, loss=1.2174997329711914
test: epoch 7, loss 1.797011375427246, acc=0.25, loss=1.797011375427246
train: epoch 8, loss 1.1518727540969849, acc=0.5056666731834412, loss=1.1518727540969849
test: epoch 8, loss 1.7988805770874023, acc=0.2777777910232544, loss=1.7988805770874023
train: epoch 9, loss 1.1102139949798584, acc=0.5223333239555359, loss=1.1102139949798584
test: epoch 9, loss 1.7029566764831543, acc=0.28333333134651184, loss=1.7029566764831543
train: epoch 10, loss 1.076317548751831, acc=0.5377222299575806, loss=1.076317548751831
test: epoch 10, loss 1.7575392723083496, acc=0.29722222685813904, loss=1.7575392723083496
train: epoch 11, loss 1.0366621017456055, acc=0.5566666722297668, loss=1.0366621017456055
test: epoch 11, loss 1.7366174459457397, acc=0.2916666567325592, loss=1.7366174459457397
train: epoch 12, loss 0.9988834857940674, acc=0.574222207069397, loss=0.9988834857940674
test: epoch 12, loss 1.8601710796356201, acc=0.3055555522441864, loss=1.8601710796356201
train: epoch 13, loss 0.9782845973968506, acc=0.5778889060020447, loss=0.9782845973968506
test: epoch 13, loss 1.8254611492156982, acc=0.3027777671813965, loss=1.8254611492156982
train: epoch 14, loss 0.9759081602096558, acc=0.5766111016273499, loss=0.9759081602096558
test: epoch 14, loss 1.8373452425003052, acc=0.31388887763023376, loss=1.8373452425003052
train: epoch 15, loss 0.9467778205871582, acc=0.5923333168029785, loss=0.9467778205871582
test: epoch 15, loss 1.8164591789245605, acc=0.31388887763023376, loss=1.8164591789245605
train: epoch 16, loss 0.930182933807373, acc=0.5968888998031616, loss=0.930182933807373
test: epoch 16, loss 1.823529601097107, acc=0.31111112236976624, loss=1.823529601097107
train: epoch 17, loss 0.9215326309204102, acc=0.6060000061988831, loss=0.9215326309204102
test: epoch 17, loss 1.8077021837234497, acc=0.3083333373069763, loss=1.8077021837234497
train: epoch 18, loss 0.9015780687332153, acc=0.6076666712760925, loss=0.9015780687332153
test: epoch 18, loss 1.7584588527679443, acc=0.31388887763023376, loss=1.7584588527679443
train: epoch 19, loss 0.8937258124351501, acc=0.6121666431427002, loss=0.8937258124351501
test: epoch 19, loss 1.8535975217819214, acc=0.32777777314186096, loss=1.8535975217819214
train: epoch 20, loss 0.8777133226394653, acc=0.6165000200271606, loss=0.8777133226394653
test: epoch 20, loss 1.7656147480010986, acc=0.3166666626930237, loss=1.7656147480010986
train: epoch 21, loss 0.867586612701416, acc=0.6231111288070679, loss=0.867586612701416
test: epoch 21, loss 1.8008689880371094, acc=0.32499998807907104, loss=1.8008689880371094
train: epoch 22, loss 0.8539942502975464, acc=0.6203888654708862, loss=0.8539942502975464
test: epoch 22, loss 1.873260498046875, acc=0.32777777314186096, loss=1.873260498046875
train: epoch 23, loss 0.8629955649375916, acc=0.617555558681488, loss=0.8629955649375916
test: epoch 23, loss 1.9586619138717651, acc=0.3305555582046509, loss=1.9586619138717651
train: epoch 24, loss 0.8525763154029846, acc=0.6261110901832581, loss=0.8525763154029846
test: epoch 24, loss 2.0422918796539307, acc=0.3194444477558136, loss=2.0422918796539307
train: epoch 25, loss 0.8528408408164978, acc=0.6254444718360901, loss=0.8528408408164978
test: epoch 25, loss 2.1129419803619385, acc=0.31111112236976624, loss=2.1129419803619385
train: epoch 26, loss 0.8428031206130981, acc=0.6301110982894897, loss=0.8428031206130981
test: epoch 26, loss 1.8729543685913086, acc=0.32499998807907104, loss=1.8729543685913086
train: epoch 27, loss 0.8332318663597107, acc=0.6310555338859558, loss=0.8332318663597107
test: epoch 27, loss 1.9579334259033203, acc=0.3055555522441864, loss=1.9579334259033203
train: epoch 28, loss 0.8406182527542114, acc=0.6247777938842773, loss=0.8406182527542114
test: epoch 28, loss 1.9493496417999268, acc=0.31388887763023376, loss=1.9493496417999268
train: epoch 29, loss 0.8439455628395081, acc=0.6276666522026062, loss=0.8439455628395081
test: epoch 29, loss 1.818798542022705, acc=0.3361110985279083, loss=1.818798542022705
train: epoch 30, loss 0.8362928628921509, acc=0.625, loss=0.8362928628921509
test: epoch 30, loss 1.8990585803985596, acc=0.3222222328186035, loss=1.8990585803985596
train: epoch 31, loss 0.838840663433075, acc=0.6259444355964661, loss=0.838840663433075
test: epoch 31, loss 1.8685823678970337, acc=0.3305555582046509, loss=1.8685823678970337
train: epoch 32, loss 0.818625271320343, acc=0.6320555806159973, loss=0.818625271320343
test: epoch 32, loss 1.8502761125564575, acc=0.3194444477558136, loss=1.8502761125564575
train: epoch 33, loss 0.8264837265014648, acc=0.632444441318512, loss=0.8264837265014648
test: epoch 33, loss 1.856030821800232, acc=0.3472222089767456, loss=1.856030821800232
train: epoch 34, loss 0.809135913848877, acc=0.6350555419921875, loss=0.809135913848877
test: epoch 34, loss 1.8411431312561035, acc=0.3472222089767456, loss=1.8411431312561035
train: epoch 35, loss 0.8025811910629272, acc=0.6371666789054871, loss=0.8025811910629272
test: epoch 35, loss 1.8365139961242676, acc=0.3444444537162781, loss=1.8365139961242676
train: epoch 36, loss 0.7984734177589417, acc=0.6418333053588867, loss=0.7984734177589417
test: epoch 36, loss 2.0217816829681396, acc=0.35555556416511536, loss=2.0217816829681396
train: epoch 37, loss 0.7957502007484436, acc=0.6395000219345093, loss=0.7957502007484436
test: epoch 37, loss 1.8153084516525269, acc=0.3638888895511627, loss=1.8153084516525269
train: epoch 38, loss 0.7822898626327515, acc=0.6517778038978577, loss=0.7822898626327515
test: epoch 38, loss 1.9025887250900269, acc=0.3638888895511627, loss=1.9025887250900269
train: epoch 39, loss 0.7970970273017883, acc=0.6421666741371155, loss=0.7970970273017883
test: epoch 39, loss 1.9720269441604614, acc=0.34166666865348816, loss=1.9720269441604614
train: epoch 40, loss 0.7844913005828857, acc=0.6443333625793457, loss=0.7844913005828857
test: epoch 40, loss 1.9580154418945312, acc=0.3444444537162781, loss=1.9580154418945312
train: epoch 41, loss 0.7904046177864075, acc=0.6462777853012085, loss=0.7904046177864075
test: epoch 41, loss 1.7348394393920898, acc=0.3611111044883728, loss=1.7348394393920898
train: epoch 42, loss 0.7778171896934509, acc=0.649222195148468, loss=0.7778171896934509
test: epoch 42, loss 1.9616156816482544, acc=0.34166666865348816, loss=1.9616156816482544
train: epoch 43, loss 0.7765138745307922, acc=0.6516666412353516, loss=0.7765138745307922
test: epoch 43, loss 1.8692020177841187, acc=0.3583333194255829, loss=1.8692020177841187
train: epoch 44, loss 0.7663719654083252, acc=0.6544444561004639, loss=0.7663719654083252
test: epoch 44, loss 1.7479792833328247, acc=0.3611111044883728, loss=1.7479792833328247
train: epoch 45, loss 0.7780944108963013, acc=0.6497777700424194, loss=0.7780944108963013
test: epoch 45, loss 1.8562519550323486, acc=0.3499999940395355, loss=1.8562519550323486
train: epoch 46, loss 0.7652376294136047, acc=0.6517778038978577, loss=0.7652376294136047
test: epoch 46, loss 1.931821346282959, acc=0.3499999940395355, loss=1.931821346282959
train: epoch 47, loss 0.7564375996589661, acc=0.6554999947547913, loss=0.7564375996589661
test: epoch 47, loss 1.9519380331039429, acc=0.35555556416511536, loss=1.9519380331039429
train: epoch 48, loss 0.7584457993507385, acc=0.6581110954284668, loss=0.7584457993507385
test: epoch 48, loss 1.994842767715454, acc=0.3583333194255829, loss=1.994842767715454
train: epoch 49, loss 0.7589867115020752, acc=0.6587222218513489, loss=0.7589867115020752
test: epoch 49, loss 1.9440652132034302, acc=0.3638888895511627, loss=1.9440652132034302
train: epoch 50, loss 0.7566473484039307, acc=0.6597777605056763, loss=0.7566473484039307
test: epoch 50, loss 1.9982964992523193, acc=0.3611111044883728, loss=1.9982964992523193
train: epoch 51, loss 0.7532745003700256, acc=0.6560555696487427, loss=0.7532745003700256
test: epoch 51, loss 1.9680500030517578, acc=0.35555556416511536, loss=1.9680500030517578
train: epoch 52, loss 0.7519994974136353, acc=0.6589999794960022, loss=0.7519994974136353
test: epoch 52, loss 2.0068154335021973, acc=0.36666667461395264, loss=2.0068154335021973
train: epoch 53, loss 0.7560932040214539, acc=0.6589999794960022, loss=0.7560932040214539
test: epoch 53, loss 2.065145492553711, acc=0.3611111044883728, loss=2.065145492553711
train: epoch 54, loss 0.7455116510391235, acc=0.659500002861023, loss=0.7455116510391235
test: epoch 54, loss 1.9851369857788086, acc=0.3638888895511627, loss=1.9851369857788086
train: epoch 55, loss 0.7336808443069458, acc=0.6601666808128357, loss=0.7336808443069458
test: epoch 55, loss 1.9505929946899414, acc=0.36666667461395264, loss=1.9505929946899414
train: epoch 56, loss 0.7388520836830139, acc=0.6628333330154419, loss=0.7388520836830139
test: epoch 56, loss 1.9489641189575195, acc=0.36944442987442017, loss=1.9489641189575195
train: epoch 57, loss 0.7478480935096741, acc=0.6617222428321838, loss=0.7478480935096741
test: epoch 57, loss 1.9351015090942383, acc=0.36666667461395264, loss=1.9351015090942383
train: epoch 58, loss 0.7452445030212402, acc=0.6636666655540466, loss=0.7452445030212402
test: epoch 58, loss 2.0429575443267822, acc=0.36666667461395264, loss=2.0429575443267822
train: epoch 59, loss 0.7497620582580566, acc=0.6623888611793518, loss=0.7497620582580566
test: epoch 59, loss 1.8444557189941406, acc=0.3638888895511627, loss=1.8444557189941406
train: epoch 60, loss 0.7406010031700134, acc=0.6627222299575806, loss=0.7406010031700134
test: epoch 60, loss 1.919663429260254, acc=0.3611111044883728, loss=1.919663429260254
train: epoch 61, loss 0.7435759902000427, acc=0.6627222299575806, loss=0.7435759902000427
test: epoch 61, loss 1.8681831359863281, acc=0.3638888895511627, loss=1.8681831359863281
train: epoch 62, loss 0.7201181650161743, acc=0.6693888902664185, loss=0.7201181650161743
test: epoch 62, loss 1.977596402168274, acc=0.36666667461395264, loss=1.977596402168274
train: epoch 63, loss 0.730599582195282, acc=0.6678333282470703, loss=0.730599582195282
test: epoch 63, loss 2.0058016777038574, acc=0.36666667461395264, loss=2.0058016777038574
train: epoch 64, loss 0.7281690835952759, acc=0.6698889136314392, loss=0.7281690835952759
test: epoch 64, loss 1.9488719701766968, acc=0.36666667461395264, loss=1.9488719701766968
train: epoch 65, loss 0.7194480299949646, acc=0.671999990940094, loss=0.7194480299949646
test: epoch 65, loss 2.041100025177002, acc=0.35277777910232544, loss=2.041100025177002
train: epoch 66, loss 0.7234787940979004, acc=0.6692222356796265, loss=0.7234787940979004
test: epoch 66, loss 2.194122076034546, acc=0.3722222149372101, loss=2.194122076034546
train: epoch 67, loss 0.7234436273574829, acc=0.6698889136314392, loss=0.7234436273574829
test: epoch 67, loss 1.9944108724594116, acc=0.35555556416511536, loss=1.9944108724594116
train: epoch 68, loss 0.7147993445396423, acc=0.6729999780654907, loss=0.7147993445396423
test: epoch 68, loss 2.0410149097442627, acc=0.3638888895511627, loss=2.0410149097442627
train: epoch 69, loss 0.719021737575531, acc=0.6679999828338623, loss=0.719021737575531
test: epoch 69, loss 2.0101125240325928, acc=0.36666667461395264, loss=2.0101125240325928
train: epoch 70, loss 0.7201991081237793, acc=0.671833336353302, loss=0.7201991081237793
test: epoch 70, loss 2.0545802116394043, acc=0.3638888895511627, loss=2.0545802116394043
train: epoch 71, loss 0.7124997973442078, acc=0.6734444499015808, loss=0.7124997973442078
test: epoch 71, loss 2.202207326889038, acc=0.36666667461395264, loss=2.202207326889038
train: epoch 72, loss 0.7161515355110168, acc=0.6753333210945129, loss=0.7161515355110168
test: epoch 72, loss 2.0610239505767822, acc=0.3638888895511627, loss=2.0610239505767822
train: epoch 73, loss 0.7189012765884399, acc=0.6730555295944214, loss=0.7189012765884399
test: epoch 73, loss 1.9212629795074463, acc=0.3638888895511627, loss=1.9212629795074463
train: epoch 74, loss 0.7154665589332581, acc=0.6726666688919067, loss=0.7154665589332581
test: epoch 74, loss 1.9713542461395264, acc=0.36944442987442017, loss=1.9713542461395264
train: epoch 75, loss 0.717253565788269, acc=0.6754999756813049, loss=0.717253565788269
test: epoch 75, loss 2.0432791709899902, acc=0.36666667461395264, loss=2.0432791709899902
train: epoch 76, loss 0.7216123938560486, acc=0.6740555763244629, loss=0.7216123938560486
test: epoch 76, loss 1.9605337381362915, acc=0.35277777910232544, loss=1.9605337381362915
train: epoch 77, loss 0.7187961339950562, acc=0.6712222099304199, loss=0.7187961339950562
test: epoch 77, loss 1.9881032705307007, acc=0.3638888895511627, loss=1.9881032705307007
train: epoch 78, loss 0.7083965539932251, acc=0.6735000014305115, loss=0.7083965539932251
test: epoch 78, loss 2.017451763153076, acc=0.3777777850627899, loss=2.017451763153076
train: epoch 79, loss 0.7129687666893005, acc=0.6727222204208374, loss=0.7129687666893005
test: epoch 79, loss 2.117990255355835, acc=0.36944442987442017, loss=2.117990255355835
train: epoch 80, loss 0.6979694962501526, acc=0.6836110949516296, loss=0.6979694962501526
test: epoch 80, loss 2.0228586196899414, acc=0.36666667461395264, loss=2.0228586196899414
train: epoch 81, loss 0.7018149495124817, acc=0.6736666560173035, loss=0.7018149495124817
test: epoch 81, loss 2.1130154132843018, acc=0.36666667461395264, loss=2.1130154132843018
train: epoch 82, loss 0.7094235420227051, acc=0.6717777848243713, loss=0.7094235420227051
test: epoch 82, loss 2.021517515182495, acc=0.3722222149372101, loss=2.021517515182495
train: epoch 83, loss 0.6933958530426025, acc=0.6787777543067932, loss=0.6933958530426025
test: epoch 83, loss 2.11515736579895, acc=0.36666667461395264, loss=2.11515736579895
train: epoch 84, loss 0.7020931243896484, acc=0.6788889169692993, loss=0.7020931243896484
test: epoch 84, loss 1.890337347984314, acc=0.36666667461395264, loss=1.890337347984314
train: epoch 85, loss 0.7054620385169983, acc=0.6786666512489319, loss=0.7054620385169983
test: epoch 85, loss 1.9406914710998535, acc=0.375, loss=1.9406914710998535
train: epoch 86, loss 0.6982725858688354, acc=0.6782222390174866, loss=0.6982725858688354
test: epoch 86, loss 1.7556859254837036, acc=0.3722222149372101, loss=1.7556859254837036
train: epoch 87, loss 0.7032979130744934, acc=0.6769444346427917, loss=0.7032979130744934
test: epoch 87, loss 2.22222900390625, acc=0.36944442987442017, loss=2.22222900390625
train: epoch 88, loss 0.7088496685028076, acc=0.6772778034210205, loss=0.7088496685028076
test: epoch 88, loss 2.1592154502868652, acc=0.38055557012557983, loss=2.1592154502868652
train: epoch 89, loss 0.7002166509628296, acc=0.6747778058052063, loss=0.7002166509628296
test: epoch 89, loss 2.0685606002807617, acc=0.3722222149372101, loss=2.0685606002807617
train: epoch 90, loss 0.7010868787765503, acc=0.6761666536331177, loss=0.7010868787765503
test: epoch 90, loss 2.390481948852539, acc=0.375, loss=2.390481948852539
train: epoch 91, loss 0.69037264585495, acc=0.6838333606719971, loss=0.69037264585495
test: epoch 91, loss 2.027068614959717, acc=0.3722222149372101, loss=2.027068614959717
train: epoch 92, loss 0.7029567360877991, acc=0.6788333058357239, loss=0.7029567360877991
test: epoch 92, loss 2.2423601150512695, acc=0.3722222149372101, loss=2.2423601150512695
train: epoch 93, loss 0.6902856826782227, acc=0.6827777624130249, loss=0.6902856826782227
test: epoch 93, loss 2.0205934047698975, acc=0.3722222149372101, loss=2.0205934047698975
train: epoch 94, loss 0.6928039193153381, acc=0.6796666383743286, loss=0.6928039193153381
test: epoch 94, loss 2.0067286491394043, acc=0.3777777850627899, loss=2.0067286491394043
train: epoch 95, loss 0.6920155882835388, acc=0.6790000200271606, loss=0.6920155882835388
test: epoch 95, loss 1.8881629705429077, acc=0.3777777850627899, loss=1.8881629705429077
train: epoch 96, loss 0.6849778890609741, acc=0.679722249507904, loss=0.6849778890609741
test: epoch 96, loss 2.0507001876831055, acc=0.38333332538604736, loss=2.0507001876831055
train: epoch 97, loss 0.7056937217712402, acc=0.6814444661140442, loss=0.7056937217712402
test: epoch 97, loss 1.938848614692688, acc=0.36944442987442017, loss=1.938848614692688
train: epoch 98, loss 0.6921845078468323, acc=0.6834444403648376, loss=0.6921845078468323
test: epoch 98, loss 2.02439284324646, acc=0.375, loss=2.02439284324646
train: epoch 99, loss 0.68832927942276, acc=0.6855000257492065, loss=0.68832927942276
test: epoch 99, loss 2.0170202255249023, acc=0.3777777850627899, loss=2.0170202255249023
train: epoch 100, loss 0.6849218010902405, acc=0.6863333582878113, loss=0.6849218010902405
test: epoch 100, loss 2.112133264541626, acc=0.375, loss=2.112133264541626
train: epoch 101, loss 0.6886742115020752, acc=0.6881111264228821, loss=0.6886742115020752
test: epoch 101, loss 2.1595520973205566, acc=0.3777777850627899, loss=2.1595520973205566
train: epoch 102, loss 0.690619170665741, acc=0.6784999966621399, loss=0.690619170665741
test: epoch 102, loss 2.0897269248962402, acc=0.375, loss=2.0897269248962402
train: epoch 103, loss 0.6938431859016418, acc=0.6817777752876282, loss=0.6938431859016418
test: epoch 103, loss 2.022169828414917, acc=0.38333332538604736, loss=2.022169828414917
train: epoch 104, loss 0.6868335008621216, acc=0.683555543422699, loss=0.6868335008621216
test: epoch 104, loss 1.9570597410202026, acc=0.3722222149372101, loss=1.9570597410202026
train: epoch 105, loss 0.6917307376861572, acc=0.6807222366333008, loss=0.6917307376861572
test: epoch 105, loss 1.917725682258606, acc=0.38333332538604736, loss=1.917725682258606
train: epoch 106, loss 0.6817763447761536, acc=0.6850000023841858, loss=0.6817763447761536
test: epoch 106, loss 2.0005435943603516, acc=0.3777777850627899, loss=2.0005435943603516
train: epoch 107, loss 0.6936598420143127, acc=0.6819999814033508, loss=0.6936598420143127
test: epoch 107, loss 2.0395166873931885, acc=0.36944442987442017, loss=2.0395166873931885
train: epoch 108, loss 0.675397515296936, acc=0.6880555748939514, loss=0.675397515296936
test: epoch 108, loss 1.9970391988754272, acc=0.38333332538604736, loss=1.9970391988754272
train: epoch 109, loss 0.6831154823303223, acc=0.6821666955947876, loss=0.6831154823303223
test: epoch 109, loss 2.029279947280884, acc=0.3777777850627899, loss=2.029279947280884
train: epoch 110, loss 0.6901779174804688, acc=0.6842777729034424, loss=0.6901779174804688
test: epoch 110, loss 1.8997716903686523, acc=0.3777777850627899, loss=1.8997716903686523
train: epoch 111, loss 0.6943659782409668, acc=0.6867777705192566, loss=0.6943659782409668
test: epoch 111, loss 2.0592691898345947, acc=0.36666667461395264, loss=2.0592691898345947
train: epoch 112, loss 0.6684932112693787, acc=0.6923333406448364, loss=0.6684932112693787
test: epoch 112, loss 2.1062450408935547, acc=0.3777777850627899, loss=2.1062450408935547
train: epoch 113, loss 0.6880429983139038, acc=0.6828333139419556, loss=0.6880429983139038
test: epoch 113, loss 2.010657787322998, acc=0.3777777850627899, loss=2.010657787322998
train: epoch 114, loss 0.677116870880127, acc=0.6865000128746033, loss=0.677116870880127
test: epoch 114, loss 1.9697597026824951, acc=0.375, loss=1.9697597026824951
train: epoch 115, loss 0.6812252402305603, acc=0.6866111159324646, loss=0.6812252402305603
test: epoch 115, loss 2.0759522914886475, acc=0.375, loss=2.0759522914886475
train: epoch 116, loss 0.6770333051681519, acc=0.6890000104904175, loss=0.6770333051681519
test: epoch 116, loss 2.0030229091644287, acc=0.375, loss=2.0030229091644287
train: epoch 117, loss 0.6857219338417053, acc=0.6825555562973022, loss=0.6857219338417053
test: epoch 117, loss 2.206865072250366, acc=0.375, loss=2.206865072250366
train: epoch 118, loss 0.6724919676780701, acc=0.6902777552604675, loss=0.6724919676780701
test: epoch 118, loss 1.9300564527511597, acc=0.3861111104488373, loss=1.9300564527511597
train: epoch 119, loss 0.6745448708534241, acc=0.6881111264228821, loss=0.6745448708534241
test: epoch 119, loss 2.069498300552368, acc=0.375, loss=2.069498300552368
train: epoch 120, loss 0.6837173104286194, acc=0.6836666464805603, loss=0.6837173104286194
test: epoch 120, loss 2.1650450229644775, acc=0.38333332538604736, loss=2.1650450229644775
train: epoch 121, loss 0.680864155292511, acc=0.6865555644035339, loss=0.680864155292511
test: epoch 121, loss 2.150759220123291, acc=0.3722222149372101, loss=2.150759220123291
train: epoch 122, loss 0.664509654045105, acc=0.6959444284439087, loss=0.664509654045105
test: epoch 122, loss 1.9543330669403076, acc=0.38055557012557983, loss=1.9543330669403076
train: epoch 123, loss 0.6629826426506042, acc=0.6943333148956299, loss=0.6629826426506042
test: epoch 123, loss 2.005692720413208, acc=0.38055557012557983, loss=2.005692720413208
train: epoch 124, loss 0.6599335670471191, acc=0.6981111168861389, loss=0.6599335670471191
test: epoch 124, loss 2.22455096244812, acc=0.38055557012557983, loss=2.22455096244812
train: epoch 125, loss 0.6744524240493774, acc=0.6925555467605591, loss=0.6744524240493774
test: epoch 125, loss 2.1096694469451904, acc=0.38333332538604736, loss=2.1096694469451904
train: epoch 126, loss 0.6777225136756897, acc=0.691444456577301, loss=0.6777225136756897
test: epoch 126, loss 2.227527618408203, acc=0.38333332538604736, loss=2.227527618408203
train: epoch 127, loss 0.6760160326957703, acc=0.690500020980835, loss=0.6760160326957703
test: epoch 127, loss 2.0041253566741943, acc=0.38333332538604736, loss=2.0041253566741943
train: epoch 128, loss 0.6749302744865417, acc=0.6922222375869751, loss=0.6749302744865417
test: epoch 128, loss 2.04591965675354, acc=0.3777777850627899, loss=2.04591965675354
train: epoch 129, loss 0.6711927056312561, acc=0.6908888816833496, loss=0.6711927056312561
test: epoch 129, loss 1.9417989253997803, acc=0.3777777850627899, loss=1.9417989253997803
train: epoch 130, loss 0.6793802976608276, acc=0.6896666884422302, loss=0.6793802976608276
test: epoch 130, loss 1.9700140953063965, acc=0.3777777850627899, loss=1.9700140953063965
train: epoch 131, loss 0.6681172847747803, acc=0.6908888816833496, loss=0.6681172847747803
test: epoch 131, loss 2.041656494140625, acc=0.38333332538604736, loss=2.041656494140625
train: epoch 132, loss 0.6627357006072998, acc=0.6940555572509766, loss=0.6627357006072998
test: epoch 132, loss 1.905086636543274, acc=0.38333332538604736, loss=1.905086636543274
train: epoch 133, loss 0.6759576201438904, acc=0.69477778673172, loss=0.6759576201438904
test: epoch 133, loss 1.888339638710022, acc=0.38333332538604736, loss=1.888339638710022
train: epoch 134, loss 0.6813363432884216, acc=0.6880555748939514, loss=0.6813363432884216
test: epoch 134, loss 2.0555360317230225, acc=0.3861111104488373, loss=2.0555360317230225
train: epoch 135, loss 0.6803139448165894, acc=0.6827777624130249, loss=0.6803139448165894
test: epoch 135, loss 1.9342267513275146, acc=0.3916666805744171, loss=1.9342267513275146
train: epoch 136, loss 0.6723085045814514, acc=0.6927222013473511, loss=0.6723085045814514
test: epoch 136, loss 2.098686695098877, acc=0.38333332538604736, loss=2.098686695098877
train: epoch 137, loss 0.6618786454200745, acc=0.6929444670677185, loss=0.6618786454200745
test: epoch 137, loss 2.1903254985809326, acc=0.38055557012557983, loss=2.1903254985809326
train: epoch 138, loss 0.6654402613639832, acc=0.695722222328186, loss=0.6654402613639832
test: epoch 138, loss 1.9358900785446167, acc=0.38333332538604736, loss=1.9358900785446167
train: epoch 139, loss 0.6647516489028931, acc=0.6948333382606506, loss=0.6647516489028931
test: epoch 139, loss 2.063169002532959, acc=0.38055557012557983, loss=2.063169002532959
train: epoch 140, loss 0.6733734607696533, acc=0.6891111135482788, loss=0.6733734607696533
test: epoch 140, loss 2.0703296661376953, acc=0.3777777850627899, loss=2.0703296661376953
train: epoch 141, loss 0.66689133644104, acc=0.6958333253860474, loss=0.66689133644104
test: epoch 141, loss 2.396618127822876, acc=0.38055557012557983, loss=2.396618127822876
train: epoch 142, loss 0.6648662686347961, acc=0.6955000162124634, loss=0.6648662686347961
test: epoch 142, loss 1.9851189851760864, acc=0.38333332538604736, loss=1.9851189851760864
train: epoch 143, loss 0.6637205481529236, acc=0.6910555362701416, loss=0.6637205481529236
test: epoch 143, loss 1.9495186805725098, acc=0.38055557012557983, loss=1.9495186805725098
train: epoch 144, loss 0.6611565947532654, acc=0.6971666812896729, loss=0.6611565947532654
test: epoch 144, loss 2.1196048259735107, acc=0.38055557012557983, loss=2.1196048259735107
train: epoch 145, loss 0.6702080965042114, acc=0.695555567741394, loss=0.6702080965042114
test: epoch 145, loss 1.9657182693481445, acc=0.38055557012557983, loss=1.9657182693481445
train: epoch 146, loss 0.6457305550575256, acc=0.7005000114440918, loss=0.6457305550575256
test: epoch 146, loss 2.073082447052002, acc=0.3861111104488373, loss=2.073082447052002
train: epoch 147, loss 0.6693790555000305, acc=0.6951666474342346, loss=0.6693790555000305
test: epoch 147, loss 1.9692670106887817, acc=0.38333332538604736, loss=1.9692670106887817
train: epoch 148, loss 0.6518758535385132, acc=0.6995000243186951, loss=0.6518758535385132
test: epoch 148, loss 2.1730010509490967, acc=0.38055557012557983, loss=2.1730010509490967
train: epoch 149, loss 0.6551924347877502, acc=0.6966666579246521, loss=0.6551924347877502
test: epoch 149, loss 2.246176242828369, acc=0.3777777850627899, loss=2.246176242828369
train: epoch 150, loss 0.6554821133613586, acc=0.69605553150177, loss=0.6554821133613586
test: epoch 150, loss 2.1107239723205566, acc=0.38333332538604736, loss=2.1107239723205566
