# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.99", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=2074548279, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=2074548279, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.5826613903045654, acc=0.042888887226581573, loss=3.5826613903045654
test: epoch 1, loss 3.51051926612854, acc=0.06666667014360428, loss=3.51051926612854
train: epoch 2, loss 3.5024778842926025, acc=0.048944443464279175, loss=3.5024778842926025
test: epoch 2, loss 3.3263602256774902, acc=0.0555555559694767, loss=3.3263602256774902
train: epoch 3, loss 3.479274034500122, acc=0.04933333396911621, loss=3.479274034500122
test: epoch 3, loss 3.104271411895752, acc=0.10000000149011612, loss=3.104271411895752
train: epoch 4, loss 3.4275832176208496, acc=0.05227777734398842, loss=3.4275832176208496
test: epoch 4, loss 3.2201287746429443, acc=0.04722222313284874, loss=3.2201287746429443
train: epoch 5, loss 3.2866811752319336, acc=0.0642777755856514, loss=3.2866811752319336
test: epoch 5, loss 3.942930221557617, acc=0.0416666679084301, loss=3.942930221557617
train: epoch 6, loss 3.0908637046813965, acc=0.08227777481079102, loss=3.0908637046813965
test: epoch 6, loss 4.217861652374268, acc=0.03611111268401146, loss=4.217861652374268
train: epoch 7, loss 2.9440414905548096, acc=0.09399999678134918, loss=2.9440414905548096
test: epoch 7, loss 4.017460823059082, acc=0.03055555559694767, loss=4.017460823059082
train: epoch 8, loss 2.8472678661346436, acc=0.1080000028014183, loss=2.8472678661346436
test: epoch 8, loss 3.912581205368042, acc=0.04722222313284874, loss=3.912581205368042
train: epoch 9, loss 2.780923843383789, acc=0.11233333498239517, loss=2.780923843383789
test: epoch 9, loss 3.7706141471862793, acc=0.04722222313284874, loss=3.7706141471862793
train: epoch 10, loss 2.7264912128448486, acc=0.1256111115217209, loss=2.7264912128448486
test: epoch 10, loss 3.785576105117798, acc=0.04444444552063942, loss=3.785576105117798
train: epoch 11, loss 2.683260440826416, acc=0.12666666507720947, loss=2.683260440826416
test: epoch 11, loss 3.7960898876190186, acc=0.04444444552063942, loss=3.7960898876190186
train: epoch 12, loss 2.6573448181152344, acc=0.13316667079925537, loss=2.6573448181152344
test: epoch 12, loss 3.697195291519165, acc=0.04444444552063942, loss=3.697195291519165
train: epoch 13, loss 2.6276283264160156, acc=0.1386111080646515, loss=2.6276283264160156
test: epoch 13, loss 3.546487808227539, acc=0.04722222313284874, loss=3.546487808227539
train: epoch 14, loss 2.607593297958374, acc=0.13805556297302246, loss=2.607593297958374
test: epoch 14, loss 3.549656867980957, acc=0.04444444552063942, loss=3.549656867980957
train: epoch 15, loss 2.585930824279785, acc=0.14427778124809265, loss=2.585930824279785
test: epoch 15, loss 3.569624900817871, acc=0.04444444552063942, loss=3.569624900817871
train: epoch 16, loss 2.5630106925964355, acc=0.1469999998807907, loss=2.5630106925964355
test: epoch 16, loss 3.496161699295044, acc=0.04722222313284874, loss=3.496161699295044
train: epoch 17, loss 2.556760787963867, acc=0.1478888839483261, loss=2.556760787963867
test: epoch 17, loss 3.5010316371917725, acc=0.06111111119389534, loss=3.5010316371917725
train: epoch 18, loss 2.542313814163208, acc=0.1471666693687439, loss=2.542313814163208
test: epoch 18, loss 3.3967676162719727, acc=0.05277777835726738, loss=3.3967676162719727
train: epoch 19, loss 2.5265538692474365, acc=0.15627777576446533, loss=2.5265538692474365
test: epoch 19, loss 3.4747776985168457, acc=0.06388889253139496, loss=3.4747776985168457
train: epoch 20, loss 2.5104928016662598, acc=0.1550000011920929, loss=2.5104928016662598
test: epoch 20, loss 3.4302315711975098, acc=0.05000000074505806, loss=3.4302315711975098
train: epoch 21, loss 2.513371467590332, acc=0.15227778255939484, loss=2.513371467590332
test: epoch 21, loss 3.438992977142334, acc=0.05000000074505806, loss=3.438992977142334
train: epoch 22, loss 2.4929089546203613, acc=0.15672221779823303, loss=2.4929089546203613
test: epoch 22, loss 3.417510509490967, acc=0.05833333358168602, loss=3.417510509490967
train: epoch 23, loss 2.493880033493042, acc=0.15800000727176666, loss=2.493880033493042
test: epoch 23, loss 3.323078155517578, acc=0.0694444477558136, loss=3.323078155517578
train: epoch 24, loss 2.4767143726348877, acc=0.15961110591888428, loss=2.4767143726348877
test: epoch 24, loss 3.291658878326416, acc=0.0694444477558136, loss=3.291658878326416
train: epoch 25, loss 2.475984573364258, acc=0.1597222238779068, loss=2.475984573364258
test: epoch 25, loss 3.300563335418701, acc=0.06111111119389534, loss=3.300563335418701
train: epoch 26, loss 2.4660611152648926, acc=0.16138888895511627, loss=2.4660611152648926
test: epoch 26, loss 3.303015947341919, acc=0.07222222536802292, loss=3.303015947341919
train: epoch 27, loss 2.4686434268951416, acc=0.1641666740179062, loss=2.4686434268951416
test: epoch 27, loss 3.3206605911254883, acc=0.07222222536802292, loss=3.3206605911254883
train: epoch 28, loss 2.4470927715301514, acc=0.16599999368190765, loss=2.4470927715301514
test: epoch 28, loss 3.283440589904785, acc=0.07222222536802292, loss=3.283440589904785
train: epoch 29, loss 2.4339406490325928, acc=0.1646111160516739, loss=2.4339406490325928
test: epoch 29, loss 3.322260856628418, acc=0.07777778059244156, loss=3.322260856628418
train: epoch 30, loss 2.426832437515259, acc=0.16427777707576752, loss=2.426832437515259
test: epoch 30, loss 3.3020899295806885, acc=0.08055555820465088, loss=3.3020899295806885
train: epoch 31, loss 2.4288341999053955, acc=0.16983333230018616, loss=2.4288341999053955
test: epoch 31, loss 3.2454893589019775, acc=0.08055555820465088, loss=3.2454893589019775
train: epoch 32, loss 2.414147138595581, acc=0.17338888347148895, loss=2.414147138595581
test: epoch 32, loss 3.2317099571228027, acc=0.08888889104127884, loss=3.2317099571228027
train: epoch 33, loss 2.395491600036621, acc=0.16994445025920868, loss=2.395491600036621
test: epoch 33, loss 3.244856119155884, acc=0.0833333358168602, loss=3.244856119155884
train: epoch 34, loss 2.3861618041992188, acc=0.17299999296665192, loss=2.3861618041992188
test: epoch 34, loss 3.2001588344573975, acc=0.08888889104127884, loss=3.2001588344573975
train: epoch 35, loss 2.3720104694366455, acc=0.1811666637659073, loss=2.3720104694366455
test: epoch 35, loss 3.1086103916168213, acc=0.10000000149011612, loss=3.1086103916168213
train: epoch 36, loss 2.3704957962036133, acc=0.17683333158493042, loss=2.3704957962036133
test: epoch 36, loss 3.1755223274230957, acc=0.08888889104127884, loss=3.1755223274230957
train: epoch 37, loss 2.3539249897003174, acc=0.18299999833106995, loss=2.3539249897003174
test: epoch 37, loss 3.1587443351745605, acc=0.10000000149011612, loss=3.1587443351745605
train: epoch 38, loss 2.3271305561065674, acc=0.18372222781181335, loss=2.3271305561065674
test: epoch 38, loss 3.1759636402130127, acc=0.0972222238779068, loss=3.1759636402130127
train: epoch 39, loss 2.331085443496704, acc=0.1875, loss=2.331085443496704
test: epoch 39, loss 3.0981497764587402, acc=0.09444444626569748, loss=3.0981497764587402
train: epoch 40, loss 2.3209574222564697, acc=0.18627777695655823, loss=2.3209574222564697
test: epoch 40, loss 3.108835220336914, acc=0.10000000149011612, loss=3.108835220336914
train: epoch 41, loss 2.2962393760681152, acc=0.1901666671037674, loss=2.2962393760681152
test: epoch 41, loss 3.0712223052978516, acc=0.0972222238779068, loss=3.0712223052978516
train: epoch 42, loss 2.3071820735931396, acc=0.18938888609409332, loss=2.3071820735931396
test: epoch 42, loss 3.06329083442688, acc=0.08888889104127884, loss=3.06329083442688
train: epoch 43, loss 2.280609130859375, acc=0.1982777714729309, loss=2.280609130859375
test: epoch 43, loss 3.0410947799682617, acc=0.10555555671453476, loss=3.0410947799682617
train: epoch 44, loss 2.2724480628967285, acc=0.2002222239971161, loss=2.2724480628967285
test: epoch 44, loss 2.973090648651123, acc=0.10833333432674408, loss=2.973090648651123
train: epoch 45, loss 2.2616965770721436, acc=0.20011110603809357, loss=2.2616965770721436
test: epoch 45, loss 2.981320381164551, acc=0.1111111119389534, loss=2.981320381164551
train: epoch 46, loss 2.248784065246582, acc=0.2015555500984192, loss=2.248784065246582
test: epoch 46, loss 2.9597156047821045, acc=0.11388888955116272, loss=2.9597156047821045
train: epoch 47, loss 2.2344119548797607, acc=0.20855554938316345, loss=2.2344119548797607
test: epoch 47, loss 2.7978439331054688, acc=0.125, loss=2.7978439331054688
train: epoch 48, loss 2.214905023574829, acc=0.20988889038562775, loss=2.214905023574829
test: epoch 48, loss 2.7364115715026855, acc=0.13055555522441864, loss=2.7364115715026855
train: epoch 49, loss 2.199932336807251, acc=0.21400000154972076, loss=2.199932336807251
test: epoch 49, loss 2.5942230224609375, acc=0.13333334028720856, loss=2.5942230224609375
train: epoch 50, loss 2.2034499645233154, acc=0.2130555510520935, loss=2.2034499645233154
test: epoch 50, loss 2.5185422897338867, acc=0.13333334028720856, loss=2.5185422897338867
train: epoch 51, loss 2.191953182220459, acc=0.2201666682958603, loss=2.191953182220459
test: epoch 51, loss 2.506791591644287, acc=0.13611111044883728, loss=2.506791591644287
train: epoch 52, loss 2.1731815338134766, acc=0.2217777818441391, loss=2.1731815338134766
test: epoch 52, loss 2.4665660858154297, acc=0.14166666567325592, loss=2.4665660858154297
train: epoch 53, loss 2.1732327938079834, acc=0.2240000069141388, loss=2.1732327938079834
test: epoch 53, loss 2.3628838062286377, acc=0.14166666567325592, loss=2.3628838062286377
train: epoch 54, loss 2.1493687629699707, acc=0.22727777063846588, loss=2.1493687629699707
test: epoch 54, loss 2.351503610610962, acc=0.1527777761220932, loss=2.351503610610962
train: epoch 55, loss 2.143580198287964, acc=0.23127777874469757, loss=2.143580198287964
test: epoch 55, loss 2.350438356399536, acc=0.15000000596046448, loss=2.350438356399536
train: epoch 56, loss 2.1275718212127686, acc=0.2316666692495346, loss=2.1275718212127686
test: epoch 56, loss 2.3359012603759766, acc=0.1527777761220932, loss=2.3359012603759766
train: epoch 57, loss 2.1129651069641113, acc=0.23894444108009338, loss=2.1129651069641113
test: epoch 57, loss 2.2915279865264893, acc=0.1527777761220932, loss=2.2915279865264893
train: epoch 58, loss 2.1033554077148438, acc=0.24155555665493011, loss=2.1033554077148438
test: epoch 58, loss 2.279627799987793, acc=0.15833333134651184, loss=2.279627799987793
train: epoch 59, loss 2.0793886184692383, acc=0.24383333325386047, loss=2.0793886184692383
test: epoch 59, loss 2.2797231674194336, acc=0.16388888657093048, loss=2.2797231674194336
train: epoch 60, loss 2.0661699771881104, acc=0.2516666650772095, loss=2.0661699771881104
test: epoch 60, loss 2.2675817012786865, acc=0.16388888657093048, loss=2.2675817012786865
train: epoch 61, loss 2.0650577545166016, acc=0.2475000023841858, loss=2.0650577545166016
test: epoch 61, loss 2.2250704765319824, acc=0.17777778208255768, loss=2.2250704765319824
train: epoch 62, loss 2.059110403060913, acc=0.2523333430290222, loss=2.059110403060913
test: epoch 62, loss 2.1840460300445557, acc=0.17222222685813904, loss=2.1840460300445557
train: epoch 63, loss 2.0413801670074463, acc=0.25377777218818665, loss=2.0413801670074463
test: epoch 63, loss 2.173232316970825, acc=0.1805555522441864, loss=2.173232316970825
train: epoch 64, loss 2.0241525173187256, acc=0.25822222232818604, loss=2.0241525173187256
test: epoch 64, loss 2.1754496097564697, acc=0.17499999701976776, loss=2.1754496097564697
train: epoch 65, loss 2.013312578201294, acc=0.2624444365501404, loss=2.013312578201294
test: epoch 65, loss 2.166912794113159, acc=0.18888889253139496, loss=2.166912794113159
train: epoch 66, loss 2.004655599594116, acc=0.2658333480358124, loss=2.004655599594116
test: epoch 66, loss 2.143617868423462, acc=0.19166666269302368, loss=2.143617868423462
train: epoch 67, loss 2.0012059211730957, acc=0.26144444942474365, loss=2.0012059211730957
test: epoch 67, loss 2.1227712631225586, acc=0.1944444477558136, loss=2.1227712631225586
train: epoch 68, loss 1.9955413341522217, acc=0.2700555622577667, loss=1.9955413341522217
test: epoch 68, loss 2.1284003257751465, acc=0.18333333730697632, loss=2.1284003257751465
train: epoch 69, loss 1.981813907623291, acc=0.2702777683734894, loss=1.981813907623291
test: epoch 69, loss 2.1041083335876465, acc=0.19166666269302368, loss=2.1041083335876465
train: epoch 70, loss 1.9546128511428833, acc=0.2761111259460449, loss=1.9546128511428833
test: epoch 70, loss 2.1209933757781982, acc=0.19166666269302368, loss=2.1209933757781982
train: epoch 71, loss 1.9360721111297607, acc=0.2785555422306061, loss=1.9360721111297607
test: epoch 71, loss 2.0980777740478516, acc=0.1944444477558136, loss=2.0980777740478516
train: epoch 72, loss 1.941474199295044, acc=0.2832222282886505, loss=1.941474199295044
test: epoch 72, loss 2.0834267139434814, acc=0.19722221791744232, loss=2.0834267139434814
train: epoch 73, loss 1.9326709508895874, acc=0.28216665983200073, loss=1.9326709508895874
test: epoch 73, loss 2.074284315109253, acc=0.20000000298023224, loss=2.074284315109253
train: epoch 74, loss 1.9276686906814575, acc=0.28216665983200073, loss=1.9276686906814575
test: epoch 74, loss 2.0622661113739014, acc=0.19722221791744232, loss=2.0622661113739014
train: epoch 75, loss 1.9160488843917847, acc=0.2894444465637207, loss=1.9160488843917847
test: epoch 75, loss 2.057121515274048, acc=0.20555555820465088, loss=2.057121515274048
train: epoch 76, loss 1.9205255508422852, acc=0.2892777919769287, loss=1.9205255508422852
test: epoch 76, loss 2.054596424102783, acc=0.20277777314186096, loss=2.054596424102783
train: epoch 77, loss 1.900033950805664, acc=0.29455554485321045, loss=1.900033950805664
test: epoch 77, loss 2.0248725414276123, acc=0.21666666865348816, loss=2.0248725414276123
train: epoch 78, loss 1.878665804862976, acc=0.2963888943195343, loss=1.878665804862976
test: epoch 78, loss 2.029182195663452, acc=0.21944443881511688, loss=2.029182195663452
train: epoch 79, loss 1.861812710762024, acc=0.30061110854148865, loss=1.861812710762024
test: epoch 79, loss 2.0121803283691406, acc=0.21388888359069824, loss=2.0121803283691406
train: epoch 80, loss 1.8511202335357666, acc=0.30283331871032715, loss=1.8511202335357666
test: epoch 80, loss 2.0178472995758057, acc=0.21944443881511688, loss=2.0178472995758057
train: epoch 81, loss 1.8527458906173706, acc=0.3021666705608368, loss=1.8527458906173706
test: epoch 81, loss 2.0044260025024414, acc=0.2222222238779068, loss=2.0044260025024414
train: epoch 82, loss 1.8324918746948242, acc=0.3093888759613037, loss=1.8324918746948242
test: epoch 82, loss 2.006669759750366, acc=0.22777777910232544, loss=2.006669759750366
train: epoch 83, loss 1.8359262943267822, acc=0.31433331966400146, loss=1.8359262943267822
test: epoch 83, loss 1.985159993171692, acc=0.2222222238779068, loss=1.985159993171692
train: epoch 84, loss 1.8189228773117065, acc=0.30944445729255676, loss=1.8189228773117065
test: epoch 84, loss 1.985173225402832, acc=0.22499999403953552, loss=1.985173225402832
train: epoch 85, loss 1.809049129486084, acc=0.3153333365917206, loss=1.809049129486084
test: epoch 85, loss 1.9675476551055908, acc=0.21944443881511688, loss=1.9675476551055908
train: epoch 86, loss 1.795182704925537, acc=0.31788888573646545, loss=1.795182704925537
test: epoch 86, loss 1.9510983228683472, acc=0.2222222238779068, loss=1.9510983228683472
train: epoch 87, loss 1.8032009601593018, acc=0.31888890266418457, loss=1.8032009601593018
test: epoch 87, loss 1.9378477334976196, acc=0.2222222238779068, loss=1.9378477334976196
train: epoch 88, loss 1.7750921249389648, acc=0.324611097574234, loss=1.7750921249389648
test: epoch 88, loss 1.9419105052947998, acc=0.21944443881511688, loss=1.9419105052947998
train: epoch 89, loss 1.784180998802185, acc=0.3252222239971161, loss=1.784180998802185
test: epoch 89, loss 1.9200891256332397, acc=0.22499999403953552, loss=1.9200891256332397
train: epoch 90, loss 1.7710485458374023, acc=0.3256666660308838, loss=1.7710485458374023
test: epoch 90, loss 1.931833267211914, acc=0.22499999403953552, loss=1.931833267211914
train: epoch 91, loss 1.7498685121536255, acc=0.331722229719162, loss=1.7498685121536255
test: epoch 91, loss 1.9404388666152954, acc=0.22777777910232544, loss=1.9404388666152954
train: epoch 92, loss 1.7505723237991333, acc=0.3337777853012085, loss=1.7505723237991333
test: epoch 92, loss 1.9404168128967285, acc=0.22499999403953552, loss=1.9404168128967285
train: epoch 93, loss 1.7411795854568481, acc=0.33561110496520996, loss=1.7411795854568481
test: epoch 93, loss 1.9332013130187988, acc=0.22499999403953552, loss=1.9332013130187988
train: epoch 94, loss 1.742916226387024, acc=0.33855554461479187, loss=1.742916226387024
test: epoch 94, loss 1.9287328720092773, acc=0.22777777910232544, loss=1.9287328720092773
train: epoch 95, loss 1.7338464260101318, acc=0.33844444155693054, loss=1.7338464260101318
test: epoch 95, loss 1.9167042970657349, acc=0.2361111044883728, loss=1.9167042970657349
train: epoch 96, loss 1.720352292060852, acc=0.3448888957500458, loss=1.720352292060852
test: epoch 96, loss 1.8857104778289795, acc=0.23333333432674408, loss=1.8857104778289795
train: epoch 97, loss 1.7098695039749146, acc=0.3487222194671631, loss=1.7098695039749146
test: epoch 97, loss 1.8842912912368774, acc=0.2361111044883728, loss=1.8842912912368774
train: epoch 98, loss 1.7064744234085083, acc=0.3447222113609314, loss=1.7064744234085083
test: epoch 98, loss 1.8828203678131104, acc=0.23333333432674408, loss=1.8828203678131104
train: epoch 99, loss 1.6973927021026611, acc=0.3518333435058594, loss=1.6973927021026611
test: epoch 99, loss 1.8475919961929321, acc=0.23888888955116272, loss=1.8475919961929321
train: epoch 100, loss 1.6923965215682983, acc=0.34938889741897583, loss=1.6923965215682983
test: epoch 100, loss 1.8283108472824097, acc=0.24722221493721008, loss=1.8283108472824097
train: epoch 101, loss 1.6658227443695068, acc=0.3575555682182312, loss=1.6658227443695068
test: epoch 101, loss 1.8413333892822266, acc=0.24166665971279144, loss=1.8413333892822266
train: epoch 102, loss 1.6782525777816772, acc=0.3575555682182312, loss=1.6782525777816772
test: epoch 102, loss 1.8319569826126099, acc=0.23888888955116272, loss=1.8319569826126099
train: epoch 103, loss 1.6667709350585938, acc=0.3600555658340454, loss=1.6667709350585938
test: epoch 103, loss 1.8153563737869263, acc=0.24722221493721008, loss=1.8153563737869263
train: epoch 104, loss 1.6603530645370483, acc=0.3623333275318146, loss=1.6603530645370483
test: epoch 104, loss 1.8383152484893799, acc=0.23888888955116272, loss=1.8383152484893799
train: epoch 105, loss 1.6494277715682983, acc=0.371055543422699, loss=1.6494277715682983
test: epoch 105, loss 1.8212673664093018, acc=0.23888888955116272, loss=1.8212673664093018
train: epoch 106, loss 1.6439666748046875, acc=0.3665555417537689, loss=1.6439666748046875
test: epoch 106, loss 1.8158237934112549, acc=0.25, loss=1.8158237934112549
train: epoch 107, loss 1.6343369483947754, acc=0.37138888239860535, loss=1.6343369483947754
test: epoch 107, loss 1.7968977689743042, acc=0.25555557012557983, loss=1.7968977689743042
train: epoch 108, loss 1.6257736682891846, acc=0.37805554270744324, loss=1.6257736682891846
test: epoch 108, loss 1.8016871213912964, acc=0.24444444477558136, loss=1.8016871213912964
train: epoch 109, loss 1.616370677947998, acc=0.37450000643730164, loss=1.616370677947998
test: epoch 109, loss 1.7940067052841187, acc=0.25, loss=1.7940067052841187
train: epoch 110, loss 1.6172678470611572, acc=0.38155555725097656, loss=1.6172678470611572
test: epoch 110, loss 1.806448221206665, acc=0.2527777850627899, loss=1.806448221206665
train: epoch 111, loss 1.60825777053833, acc=0.38261112570762634, loss=1.60825777053833
test: epoch 111, loss 1.8110506534576416, acc=0.25, loss=1.8110506534576416
train: epoch 112, loss 1.597920298576355, acc=0.3798888921737671, loss=1.597920298576355
test: epoch 112, loss 1.7860257625579834, acc=0.2527777850627899, loss=1.7860257625579834
train: epoch 113, loss 1.589336633682251, acc=0.3881666660308838, loss=1.589336633682251
test: epoch 113, loss 1.797647476196289, acc=0.2527777850627899, loss=1.797647476196289
train: epoch 114, loss 1.5872968435287476, acc=0.38966667652130127, loss=1.5872968435287476
test: epoch 114, loss 1.8061822652816772, acc=0.25555557012557983, loss=1.8061822652816772
train: epoch 115, loss 1.5743598937988281, acc=0.391222208738327, loss=1.5743598937988281
test: epoch 115, loss 1.807166576385498, acc=0.25, loss=1.807166576385498
train: epoch 116, loss 1.5719143152236938, acc=0.39794445037841797, loss=1.5719143152236938
test: epoch 116, loss 1.7958890199661255, acc=0.25, loss=1.7958890199661255
train: epoch 117, loss 1.5552488565444946, acc=0.39883333444595337, loss=1.5552488565444946
test: epoch 117, loss 1.7900575399398804, acc=0.25833332538604736, loss=1.7900575399398804
train: epoch 118, loss 1.5382002592086792, acc=0.4025000035762787, loss=1.5382002592086792
test: epoch 118, loss 1.7906761169433594, acc=0.25555557012557983, loss=1.7906761169433594
train: epoch 119, loss 1.548230528831482, acc=0.4012777805328369, loss=1.548230528831482
test: epoch 119, loss 1.793196201324463, acc=0.25, loss=1.793196201324463
train: epoch 120, loss 1.5410499572753906, acc=0.40577778220176697, loss=1.5410499572753906
test: epoch 120, loss 1.771918773651123, acc=0.25833332538604736, loss=1.771918773651123
train: epoch 121, loss 1.541093111038208, acc=0.4029444456100464, loss=1.541093111038208
test: epoch 121, loss 1.7870063781738281, acc=0.2638888955116272, loss=1.7870063781738281
train: epoch 122, loss 1.5323312282562256, acc=0.4124999940395355, loss=1.5323312282562256
test: epoch 122, loss 1.7652878761291504, acc=0.2638888955116272, loss=1.7652878761291504
train: epoch 123, loss 1.5376330614089966, acc=0.41377776861190796, loss=1.5376330614089966
test: epoch 123, loss 1.76279878616333, acc=0.2638888955116272, loss=1.76279878616333
train: epoch 124, loss 1.5034083127975464, acc=0.4159444570541382, loss=1.5034083127975464
test: epoch 124, loss 1.7680506706237793, acc=0.2666666805744171, loss=1.7680506706237793
train: epoch 125, loss 1.5240334272384644, acc=0.42266666889190674, loss=1.5240334272384644
test: epoch 125, loss 1.770811676979065, acc=0.2666666805744171, loss=1.770811676979065
train: epoch 126, loss 1.5232347249984741, acc=0.41850000619888306, loss=1.5232347249984741
test: epoch 126, loss 1.7667840719223022, acc=0.2666666805744171, loss=1.7667840719223022
train: epoch 127, loss 1.4995909929275513, acc=0.42338889837265015, loss=1.4995909929275513
test: epoch 127, loss 1.7523095607757568, acc=0.27222222089767456, loss=1.7523095607757568
train: epoch 128, loss 1.5047403573989868, acc=0.42811110615730286, loss=1.5047403573989868
test: epoch 128, loss 1.7500485181808472, acc=0.26944443583488464, loss=1.7500485181808472
train: epoch 129, loss 1.4838263988494873, acc=0.4252777695655823, loss=1.4838263988494873
test: epoch 129, loss 1.7510271072387695, acc=0.27222222089767456, loss=1.7510271072387695
train: epoch 130, loss 1.4823799133300781, acc=0.4276111125946045, loss=1.4823799133300781
test: epoch 130, loss 1.7522010803222656, acc=0.27222222089767456, loss=1.7522010803222656
train: epoch 131, loss 1.476347804069519, acc=0.4340555667877197, loss=1.476347804069519
test: epoch 131, loss 1.7508496046066284, acc=0.26944443583488464, loss=1.7508496046066284
train: epoch 132, loss 1.4790199995040894, acc=0.4301111102104187, loss=1.4790199995040894
test: epoch 132, loss 1.7522125244140625, acc=0.27222222089767456, loss=1.7522125244140625
train: epoch 133, loss 1.4655858278274536, acc=0.4361666738986969, loss=1.4655858278274536
test: epoch 133, loss 1.7566487789154053, acc=0.27222222089767456, loss=1.7566487789154053
train: epoch 134, loss 1.4624335765838623, acc=0.4425555467605591, loss=1.4624335765838623
test: epoch 134, loss 1.7456451654434204, acc=0.2750000059604645, loss=1.7456451654434204
train: epoch 135, loss 1.4505265951156616, acc=0.43700000643730164, loss=1.4505265951156616
test: epoch 135, loss 1.722434163093567, acc=0.2805555462837219, loss=1.722434163093567
train: epoch 136, loss 1.447285532951355, acc=0.44377776980400085, loss=1.447285532951355
test: epoch 136, loss 1.7234491109848022, acc=0.2805555462837219, loss=1.7234491109848022
train: epoch 137, loss 1.4449379444122314, acc=0.4484444558620453, loss=1.4449379444122314
test: epoch 137, loss 1.7176893949508667, acc=0.2750000059604645, loss=1.7176893949508667
train: epoch 138, loss 1.4483286142349243, acc=0.4429444372653961, loss=1.4483286142349243
test: epoch 138, loss 1.7247464656829834, acc=0.2750000059604645, loss=1.7247464656829834
train: epoch 139, loss 1.4202179908752441, acc=0.4487777650356293, loss=1.4202179908752441
test: epoch 139, loss 1.7513148784637451, acc=0.2777777910232544, loss=1.7513148784637451
train: epoch 140, loss 1.437873125076294, acc=0.45216667652130127, loss=1.437873125076294
test: epoch 140, loss 1.7403550148010254, acc=0.2805555462837219, loss=1.7403550148010254
train: epoch 141, loss 1.4306869506835938, acc=0.45322221517562866, loss=1.4306869506835938
test: epoch 141, loss 1.7275043725967407, acc=0.2805555462837219, loss=1.7275043725967407
train: epoch 142, loss 1.4199342727661133, acc=0.45399999618530273, loss=1.4199342727661133
test: epoch 142, loss 1.730807900428772, acc=0.2777777910232544, loss=1.730807900428772
train: epoch 143, loss 1.4122599363327026, acc=0.46088889241218567, loss=1.4122599363327026
test: epoch 143, loss 1.7293821573257446, acc=0.2777777910232544, loss=1.7293821573257446
train: epoch 144, loss 1.4033883810043335, acc=0.45783331990242004, loss=1.4033883810043335
test: epoch 144, loss 1.7227193117141724, acc=0.28333333134651184, loss=1.7227193117141724
train: epoch 145, loss 1.4095453023910522, acc=0.4593888819217682, loss=1.4095453023910522
test: epoch 145, loss 1.7284725904464722, acc=0.28333333134651184, loss=1.7284725904464722
train: epoch 146, loss 1.4192609786987305, acc=0.4555000066757202, loss=1.4192609786987305
test: epoch 146, loss 1.749241828918457, acc=0.2805555462837219, loss=1.749241828918457
train: epoch 147, loss 1.3902733325958252, acc=0.46299999952316284, loss=1.3902733325958252
test: epoch 147, loss 1.7238997220993042, acc=0.2805555462837219, loss=1.7238997220993042
train: epoch 148, loss 1.402972936630249, acc=0.4658888876438141, loss=1.402972936630249
test: epoch 148, loss 1.7373772859573364, acc=0.2805555462837219, loss=1.7373772859573364
train: epoch 149, loss 1.3818724155426025, acc=0.47238889336586, loss=1.3818724155426025
test: epoch 149, loss 1.7467657327651978, acc=0.2750000059604645, loss=1.7467657327651978
train: epoch 150, loss 1.3782827854156494, acc=0.4714444577693939, loss=1.3782827854156494
test: epoch 150, loss 1.7445011138916016, acc=0.2777777910232544, loss=1.7445011138916016
