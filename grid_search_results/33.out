# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=816162952, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=816162952, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.553178548812866, acc=0.0486111119389534, loss=3.553178548812866
test: epoch 1, loss 3.537578821182251, acc=0.05000000074505806, loss=3.537578821182251
train: epoch 2, loss 3.470000982284546, acc=0.052166666835546494, loss=3.470000982284546
test: epoch 2, loss 3.2317183017730713, acc=0.07222222536802292, loss=3.2317183017730713
train: epoch 3, loss 3.3637592792510986, acc=0.05672222375869751, loss=3.3637592792510986
test: epoch 3, loss 3.5386898517608643, acc=0.05000000074505806, loss=3.5386898517608643
train: epoch 4, loss 3.041186809539795, acc=0.0985555574297905, loss=3.041186809539795
test: epoch 4, loss 4.354719638824463, acc=0.04444444552063942, loss=4.354719638824463
train: epoch 5, loss 2.759690523147583, acc=0.13255555927753448, loss=2.759690523147583
test: epoch 5, loss 4.509318828582764, acc=0.04722222313284874, loss=4.509318828582764
train: epoch 6, loss 2.6052680015563965, acc=0.1557222157716751, loss=2.6052680015563965
test: epoch 6, loss 4.624524116516113, acc=0.06111111119389534, loss=4.624524116516113
train: epoch 7, loss 2.5136492252349854, acc=0.1682777851819992, loss=2.5136492252349854
test: epoch 7, loss 4.600551605224609, acc=0.06388889253139496, loss=4.600551605224609
train: epoch 8, loss 2.4542975425720215, acc=0.17866666615009308, loss=2.4542975425720215
test: epoch 8, loss 4.5398430824279785, acc=0.0555555559694767, loss=4.5398430824279785
train: epoch 9, loss 2.4056293964385986, acc=0.18988889455795288, loss=2.4056293964385986
test: epoch 9, loss 4.524097442626953, acc=0.05833333358168602, loss=4.524097442626953
train: epoch 10, loss 2.3605551719665527, acc=0.19949999451637268, loss=2.3605551719665527
test: epoch 10, loss 4.475978851318359, acc=0.07222222536802292, loss=4.475978851318359
train: epoch 11, loss 2.3291308879852295, acc=0.20527777075767517, loss=2.3291308879852295
test: epoch 11, loss 4.4376397132873535, acc=0.0694444477558136, loss=4.4376397132873535
train: epoch 12, loss 2.2977187633514404, acc=0.21211111545562744, loss=2.2977187633514404
test: epoch 12, loss 4.464759826660156, acc=0.0694444477558136, loss=4.464759826660156
train: epoch 13, loss 2.267233371734619, acc=0.2166111171245575, loss=2.267233371734619
test: epoch 13, loss 4.344897270202637, acc=0.06388889253139496, loss=4.344897270202637
train: epoch 14, loss 2.2538247108459473, acc=0.22866666316986084, loss=2.2538247108459473
test: epoch 14, loss 4.272187232971191, acc=0.07222222536802292, loss=4.272187232971191
train: epoch 15, loss 2.240259885787964, acc=0.2204444408416748, loss=2.240259885787964
test: epoch 15, loss 4.273958206176758, acc=0.08055555820465088, loss=4.273958206176758
train: epoch 16, loss 2.209427833557129, acc=0.23338888585567474, loss=2.209427833557129
test: epoch 16, loss 4.18861198425293, acc=0.07500000298023224, loss=4.18861198425293
train: epoch 17, loss 2.1977696418762207, acc=0.2372777760028839, loss=2.1977696418762207
test: epoch 17, loss 4.1035614013671875, acc=0.08888889104127884, loss=4.1035614013671875
train: epoch 18, loss 2.189810037612915, acc=0.2409999966621399, loss=2.189810037612915
test: epoch 18, loss 4.04803991317749, acc=0.08055555820465088, loss=4.04803991317749
train: epoch 19, loss 2.1748809814453125, acc=0.24888889491558075, loss=2.1748809814453125
test: epoch 19, loss 3.84930419921875, acc=0.09166666865348816, loss=3.84930419921875
train: epoch 20, loss 2.169250965118408, acc=0.2504444420337677, loss=2.169250965118408
test: epoch 20, loss 3.815061569213867, acc=0.09166666865348816, loss=3.815061569213867
train: epoch 21, loss 2.1556336879730225, acc=0.2504444420337677, loss=2.1556336879730225
test: epoch 21, loss 3.7570483684539795, acc=0.0972222238779068, loss=3.7570483684539795
train: epoch 22, loss 2.1459176540374756, acc=0.2569444477558136, loss=2.1459176540374756
test: epoch 22, loss 3.761117696762085, acc=0.10000000149011612, loss=3.761117696762085
train: epoch 23, loss 2.131568431854248, acc=0.25850000977516174, loss=2.131568431854248
test: epoch 23, loss 3.6233022212982178, acc=0.10277777910232544, loss=3.6233022212982178
train: epoch 24, loss 2.1125504970550537, acc=0.26705554127693176, loss=2.1125504970550537
test: epoch 24, loss 3.6738743782043457, acc=0.10555555671453476, loss=3.6738743782043457
train: epoch 25, loss 2.1138622760772705, acc=0.2657777667045593, loss=2.1138622760772705
test: epoch 25, loss 3.67806077003479, acc=0.10833333432674408, loss=3.67806077003479
train: epoch 26, loss 2.090622663497925, acc=0.26438888907432556, loss=2.090622663497925
test: epoch 26, loss 3.605802059173584, acc=0.10277777910232544, loss=3.605802059173584
train: epoch 27, loss 2.0878419876098633, acc=0.26827776432037354, loss=2.0878419876098633
test: epoch 27, loss 3.4821183681488037, acc=0.10555555671453476, loss=3.4821183681488037
train: epoch 28, loss 2.070171594619751, acc=0.2759999930858612, loss=2.070171594619751
test: epoch 28, loss 3.4146218299865723, acc=0.10555555671453476, loss=3.4146218299865723
train: epoch 29, loss 2.0654425621032715, acc=0.2791111171245575, loss=2.0654425621032715
test: epoch 29, loss 3.3527419567108154, acc=0.1111111119389534, loss=3.3527419567108154
train: epoch 30, loss 2.051081657409668, acc=0.2832777798175812, loss=2.051081657409668
test: epoch 30, loss 3.2687482833862305, acc=0.11388888955116272, loss=3.2687482833862305
train: epoch 31, loss 2.0363552570343018, acc=0.28999999165534973, loss=2.0363552570343018
test: epoch 31, loss 3.211226224899292, acc=0.13333334028720856, loss=3.211226224899292
train: epoch 32, loss 2.0385849475860596, acc=0.2832777798175812, loss=2.0385849475860596
test: epoch 32, loss 3.1266913414001465, acc=0.13333334028720856, loss=3.1266913414001465
train: epoch 33, loss 2.0292844772338867, acc=0.28805556893348694, loss=2.0292844772338867
test: epoch 33, loss 3.0905213356018066, acc=0.13611111044883728, loss=3.0905213356018066
train: epoch 34, loss 2.0102341175079346, acc=0.29899999499320984, loss=2.0102341175079346
test: epoch 34, loss 2.991604804992676, acc=0.14444445073604584, loss=2.991604804992676
train: epoch 35, loss 2.0114667415618896, acc=0.29688888788223267, loss=2.0114667415618896
test: epoch 35, loss 3.005868673324585, acc=0.13055555522441864, loss=3.005868673324585
train: epoch 36, loss 1.993143081665039, acc=0.308944433927536, loss=1.993143081665039
test: epoch 36, loss 2.9500956535339355, acc=0.1388888955116272, loss=2.9500956535339355
train: epoch 37, loss 1.9750081300735474, acc=0.3078888952732086, loss=1.9750081300735474
test: epoch 37, loss 2.8196804523468018, acc=0.14166666567325592, loss=2.8196804523468018
train: epoch 38, loss 1.9754979610443115, acc=0.31283333897590637, loss=1.9754979610443115
test: epoch 38, loss 2.6711220741271973, acc=0.15555556118488312, loss=2.6711220741271973
train: epoch 39, loss 1.9478094577789307, acc=0.3165000081062317, loss=1.9478094577789307
test: epoch 39, loss 2.6532750129699707, acc=0.16388888657093048, loss=2.6532750129699707
train: epoch 40, loss 1.9531834125518799, acc=0.31788888573646545, loss=1.9531834125518799
test: epoch 40, loss 2.580939531326294, acc=0.17222222685813904, loss=2.580939531326294
train: epoch 41, loss 1.9312857389450073, acc=0.3231666684150696, loss=1.9312857389450073
test: epoch 41, loss 2.4713478088378906, acc=0.18888889253139496, loss=2.4713478088378906
train: epoch 42, loss 1.9364359378814697, acc=0.3266666531562805, loss=1.9364359378814697
test: epoch 42, loss 2.442824602127075, acc=0.18611110746860504, loss=2.442824602127075
train: epoch 43, loss 1.9089648723602295, acc=0.3331666588783264, loss=1.9089648723602295
test: epoch 43, loss 2.4252302646636963, acc=0.19722221791744232, loss=2.4252302646636963
train: epoch 44, loss 1.8945152759552002, acc=0.3309444487094879, loss=1.8945152759552002
test: epoch 44, loss 2.3918251991271973, acc=0.19722221791744232, loss=2.3918251991271973
train: epoch 45, loss 1.8975379467010498, acc=0.3383333384990692, loss=1.8975379467010498
test: epoch 45, loss 2.3937880992889404, acc=0.21111111342906952, loss=2.3937880992889404
train: epoch 46, loss 1.8849760293960571, acc=0.339722216129303, loss=1.8849760293960571
test: epoch 46, loss 2.357320785522461, acc=0.21666666865348816, loss=2.357320785522461
train: epoch 47, loss 1.8768703937530518, acc=0.3408888876438141, loss=1.8768703937530518
test: epoch 47, loss 2.272703170776367, acc=0.21388888359069824, loss=2.272703170776367
train: epoch 48, loss 1.8625839948654175, acc=0.35022222995758057, loss=1.8625839948654175
test: epoch 48, loss 2.236398935317993, acc=0.22777777910232544, loss=2.236398935317993
train: epoch 49, loss 1.8557426929473877, acc=0.3485555648803711, loss=1.8557426929473877
test: epoch 49, loss 2.2272658348083496, acc=0.22777777910232544, loss=2.2272658348083496
train: epoch 50, loss 1.8437474966049194, acc=0.35677778720855713, loss=1.8437474966049194
test: epoch 50, loss 2.193178415298462, acc=0.23888888955116272, loss=2.193178415298462
train: epoch 51, loss 1.840337872505188, acc=0.3541666567325592, loss=1.840337872505188
test: epoch 51, loss 2.1494598388671875, acc=0.24166665971279144, loss=2.1494598388671875
train: epoch 52, loss 1.79942786693573, acc=0.3638888895511627, loss=1.79942786693573
test: epoch 52, loss 2.12052845954895, acc=0.23055554926395416, loss=2.12052845954895
train: epoch 53, loss 1.7916887998580933, acc=0.3683333396911621, loss=1.7916887998580933
test: epoch 53, loss 2.1544692516326904, acc=0.23888888955116272, loss=2.1544692516326904
train: epoch 54, loss 1.8095622062683105, acc=0.37033334374427795, loss=1.8095622062683105
test: epoch 54, loss 2.0999014377593994, acc=0.23888888955116272, loss=2.0999014377593994
train: epoch 55, loss 1.7758221626281738, acc=0.37477776408195496, loss=1.7758221626281738
test: epoch 55, loss 2.1088309288024902, acc=0.24722221493721008, loss=2.1088309288024902
train: epoch 56, loss 1.7618749141693115, acc=0.3812222182750702, loss=1.7618749141693115
test: epoch 56, loss 2.0825562477111816, acc=0.25, loss=2.0825562477111816
train: epoch 57, loss 1.749881386756897, acc=0.38411110639572144, loss=1.749881386756897
test: epoch 57, loss 2.062311887741089, acc=0.24722221493721008, loss=2.062311887741089
train: epoch 58, loss 1.7680013179779053, acc=0.38288888335227966, loss=1.7680013179779053
test: epoch 58, loss 2.025083065032959, acc=0.24444444477558136, loss=2.025083065032959
train: epoch 59, loss 1.742861270904541, acc=0.39277777075767517, loss=1.742861270904541
test: epoch 59, loss 2.022275924682617, acc=0.2611111104488373, loss=2.022275924682617
train: epoch 60, loss 1.739074468612671, acc=0.39105555415153503, loss=1.739074468612671
test: epoch 60, loss 1.9549190998077393, acc=0.25555557012557983, loss=1.9549190998077393
train: epoch 61, loss 1.713806390762329, acc=0.390666663646698, loss=1.713806390762329
test: epoch 61, loss 1.9288886785507202, acc=0.2527777850627899, loss=1.9288886785507202
train: epoch 62, loss 1.6909794807434082, acc=0.40077778697013855, loss=1.6909794807434082
test: epoch 62, loss 1.9083980321884155, acc=0.25833332538604736, loss=1.9083980321884155
train: epoch 63, loss 1.7060034275054932, acc=0.39827778935432434, loss=1.7060034275054932
test: epoch 63, loss 1.8925398588180542, acc=0.2527777850627899, loss=1.8925398588180542
train: epoch 64, loss 1.6747431755065918, acc=0.4066111147403717, loss=1.6747431755065918
test: epoch 64, loss 1.863677978515625, acc=0.27222222089767456, loss=1.863677978515625
train: epoch 65, loss 1.6730871200561523, acc=0.4078333377838135, loss=1.6730871200561523
test: epoch 65, loss 1.8588670492172241, acc=0.2666666805744171, loss=1.8588670492172241
train: epoch 66, loss 1.6638294458389282, acc=0.40672221779823303, loss=1.6638294458389282
test: epoch 66, loss 1.834652066230774, acc=0.2750000059604645, loss=1.834652066230774
train: epoch 67, loss 1.65127694606781, acc=0.4088333249092102, loss=1.65127694606781
test: epoch 67, loss 1.845773696899414, acc=0.27222222089767456, loss=1.845773696899414
train: epoch 68, loss 1.6531250476837158, acc=0.41405555605888367, loss=1.6531250476837158
test: epoch 68, loss 1.8227936029434204, acc=0.2777777910232544, loss=1.8227936029434204
train: epoch 69, loss 1.6431454420089722, acc=0.4234444499015808, loss=1.6431454420089722
test: epoch 69, loss 1.822308897972107, acc=0.26944443583488464, loss=1.822308897972107
train: epoch 70, loss 1.6342613697052002, acc=0.42100000381469727, loss=1.6342613697052002
test: epoch 70, loss 1.8135555982589722, acc=0.28333333134651184, loss=1.8135555982589722
train: epoch 71, loss 1.6271860599517822, acc=0.4265555441379547, loss=1.6271860599517822
test: epoch 71, loss 1.7860934734344482, acc=0.2750000059604645, loss=1.7860934734344482
train: epoch 72, loss 1.5939304828643799, acc=0.42488887906074524, loss=1.5939304828643799
test: epoch 72, loss 1.7684133052825928, acc=0.2944444417953491, loss=1.7684133052825928
train: epoch 73, loss 1.5928173065185547, acc=0.43461111187934875, loss=1.5928173065185547
test: epoch 73, loss 1.7815728187561035, acc=0.28611111640930176, loss=1.7815728187561035
train: epoch 74, loss 1.5821704864501953, acc=0.4399999976158142, loss=1.5821704864501953
test: epoch 74, loss 1.7815313339233398, acc=0.28611111640930176, loss=1.7815313339233398
train: epoch 75, loss 1.5535178184509277, acc=0.4477222263813019, loss=1.5535178184509277
test: epoch 75, loss 1.7644459009170532, acc=0.28333333134651184, loss=1.7644459009170532
train: epoch 76, loss 1.5211573839187622, acc=0.4494999945163727, loss=1.5211573839187622
test: epoch 76, loss 1.7522457838058472, acc=0.2888889014720917, loss=1.7522457838058472
train: epoch 77, loss 1.5435378551483154, acc=0.45188888907432556, loss=1.5435378551483154
test: epoch 77, loss 1.7443546056747437, acc=0.29722222685813904, loss=1.7443546056747437
train: epoch 78, loss 1.5234625339508057, acc=0.4537777900695801, loss=1.5234625339508057
test: epoch 78, loss 1.7232792377471924, acc=0.30000001192092896, loss=1.7232792377471924
train: epoch 79, loss 1.5021955966949463, acc=0.4667222201824188, loss=1.5021955966949463
test: epoch 79, loss 1.7094467878341675, acc=0.3027777671813965, loss=1.7094467878341675
train: epoch 80, loss 1.4874109029769897, acc=0.46650001406669617, loss=1.4874109029769897
test: epoch 80, loss 1.7178761959075928, acc=0.30000001192092896, loss=1.7178761959075928
train: epoch 81, loss 1.504703402519226, acc=0.4595000147819519, loss=1.504703402519226
test: epoch 81, loss 1.7098844051361084, acc=0.3027777671813965, loss=1.7098844051361084
train: epoch 82, loss 1.4840790033340454, acc=0.47672221064567566, loss=1.4840790033340454
test: epoch 82, loss 1.687585473060608, acc=0.30000001192092896, loss=1.687585473060608
train: epoch 83, loss 1.4496643543243408, acc=0.47794443368911743, loss=1.4496643543243408
test: epoch 83, loss 1.68862783908844, acc=0.3027777671813965, loss=1.68862783908844
train: epoch 84, loss 1.4499753713607788, acc=0.48311111330986023, loss=1.4499753713607788
test: epoch 84, loss 1.6715431213378906, acc=0.31388887763023376, loss=1.6715431213378906
train: epoch 85, loss 1.4425716400146484, acc=0.48394444584846497, loss=1.4425716400146484
test: epoch 85, loss 1.6651208400726318, acc=0.3194444477558136, loss=1.6651208400726318
train: epoch 86, loss 1.4272916316986084, acc=0.49361109733581543, loss=1.4272916316986084
test: epoch 86, loss 1.6438249349594116, acc=0.3222222328186035, loss=1.6438249349594116
train: epoch 87, loss 1.4092835187911987, acc=0.49194443225860596, loss=1.4092835187911987
test: epoch 87, loss 1.62565279006958, acc=0.32777777314186096, loss=1.62565279006958
train: epoch 88, loss 1.3978313207626343, acc=0.4953888952732086, loss=1.3978313207626343
test: epoch 88, loss 1.6423414945602417, acc=0.32499998807907104, loss=1.6423414945602417
train: epoch 89, loss 1.3891109228134155, acc=0.5011110901832581, loss=1.3891109228134155
test: epoch 89, loss 1.648110270500183, acc=0.32777777314186096, loss=1.648110270500183
train: epoch 90, loss 1.385824203491211, acc=0.500333309173584, loss=1.385824203491211
test: epoch 90, loss 1.6173865795135498, acc=0.3305555582046509, loss=1.6173865795135498
train: epoch 91, loss 1.3737329244613647, acc=0.5070000290870667, loss=1.3737329244613647
test: epoch 91, loss 1.6281287670135498, acc=0.32499998807907104, loss=1.6281287670135498
train: epoch 92, loss 1.3678596019744873, acc=0.511722207069397, loss=1.3678596019744873
test: epoch 92, loss 1.6100969314575195, acc=0.32499998807907104, loss=1.6100969314575195
train: epoch 93, loss 1.3601871728897095, acc=0.5163333415985107, loss=1.3601871728897095
test: epoch 93, loss 1.6019965410232544, acc=0.3333333432674408, loss=1.6019965410232544
train: epoch 94, loss 1.3421427011489868, acc=0.5197222232818604, loss=1.3421427011489868
test: epoch 94, loss 1.5813267230987549, acc=0.3333333432674408, loss=1.5813267230987549
train: epoch 95, loss 1.3385428190231323, acc=0.5257222056388855, loss=1.3385428190231323
test: epoch 95, loss 1.5829088687896729, acc=0.34166666865348816, loss=1.5829088687896729
train: epoch 96, loss 1.307557225227356, acc=0.5312222242355347, loss=1.307557225227356
test: epoch 96, loss 1.585640788078308, acc=0.3361110985279083, loss=1.585640788078308
train: epoch 97, loss 1.3147296905517578, acc=0.528166651725769, loss=1.3147296905517578
test: epoch 97, loss 1.572076439857483, acc=0.33888888359069824, loss=1.572076439857483
train: epoch 98, loss 1.3009413480758667, acc=0.5348333120346069, loss=1.3009413480758667
test: epoch 98, loss 1.5551283359527588, acc=0.3361110985279083, loss=1.5551283359527588
train: epoch 99, loss 1.2918250560760498, acc=0.5357221961021423, loss=1.2918250560760498
test: epoch 99, loss 1.5591665506362915, acc=0.33888888359069824, loss=1.5591665506362915
train: epoch 100, loss 1.2878378629684448, acc=0.5490000247955322, loss=1.2878378629684448
test: epoch 100, loss 1.5400787591934204, acc=0.3361110985279083, loss=1.5400787591934204
train: epoch 101, loss 1.287423849105835, acc=0.5435000061988831, loss=1.287423849105835
test: epoch 101, loss 1.514412522315979, acc=0.3444444537162781, loss=1.514412522315979
train: epoch 102, loss 1.2680550813674927, acc=0.5507222414016724, loss=1.2680550813674927
test: epoch 102, loss 1.5108580589294434, acc=0.3472222089767456, loss=1.5108580589294434
train: epoch 103, loss 1.2503128051757812, acc=0.5606111288070679, loss=1.2503128051757812
test: epoch 103, loss 1.4976803064346313, acc=0.3472222089767456, loss=1.4976803064346313
train: epoch 104, loss 1.2478059530258179, acc=0.5540000200271606, loss=1.2478059530258179
test: epoch 104, loss 1.4867485761642456, acc=0.35277777910232544, loss=1.4867485761642456
train: epoch 105, loss 1.226052165031433, acc=0.5650555491447449, loss=1.226052165031433
test: epoch 105, loss 1.4867312908172607, acc=0.3499999940395355, loss=1.4867312908172607
train: epoch 106, loss 1.2412703037261963, acc=0.5595555305480957, loss=1.2412703037261963
test: epoch 106, loss 1.4984347820281982, acc=0.35555556416511536, loss=1.4984347820281982
train: epoch 107, loss 1.222559928894043, acc=0.566611111164093, loss=1.222559928894043
test: epoch 107, loss 1.4760150909423828, acc=0.36666667461395264, loss=1.4760150909423828
train: epoch 108, loss 1.221379041671753, acc=0.5728333592414856, loss=1.221379041671753
test: epoch 108, loss 1.4574613571166992, acc=0.3638888895511627, loss=1.4574613571166992
train: epoch 109, loss 1.2122060060501099, acc=0.5706111192703247, loss=1.2122060060501099
test: epoch 109, loss 1.4560974836349487, acc=0.36666667461395264, loss=1.4560974836349487
train: epoch 110, loss 1.2097371816635132, acc=0.5709999799728394, loss=1.2097371816635132
test: epoch 110, loss 1.444031000137329, acc=0.3583333194255829, loss=1.444031000137329
train: epoch 111, loss 1.1948038339614868, acc=0.5753889083862305, loss=1.1948038339614868
test: epoch 111, loss 1.4453189373016357, acc=0.3638888895511627, loss=1.4453189373016357
train: epoch 112, loss 1.1964480876922607, acc=0.5800555348396301, loss=1.1964480876922607
test: epoch 112, loss 1.4592173099517822, acc=0.3583333194255829, loss=1.4592173099517822
train: epoch 113, loss 1.1785920858383179, acc=0.5801666378974915, loss=1.1785920858383179
test: epoch 113, loss 1.4365421533584595, acc=0.3638888895511627, loss=1.4365421533584595
train: epoch 114, loss 1.161798357963562, acc=0.5942222476005554, loss=1.161798357963562
test: epoch 114, loss 1.4611304998397827, acc=0.3583333194255829, loss=1.4611304998397827
train: epoch 115, loss 1.1790812015533447, acc=0.5929444432258606, loss=1.1790812015533447
test: epoch 115, loss 1.437767744064331, acc=0.3611111044883728, loss=1.437767744064331
train: epoch 116, loss 1.134246587753296, acc=0.6010000109672546, loss=1.134246587753296
test: epoch 116, loss 1.4440443515777588, acc=0.3638888895511627, loss=1.4440443515777588
train: epoch 117, loss 1.1316500902175903, acc=0.5987777709960938, loss=1.1316500902175903
test: epoch 117, loss 1.4339599609375, acc=0.36944442987442017, loss=1.4339599609375
train: epoch 118, loss 1.1597092151641846, acc=0.5986111164093018, loss=1.1597092151641846
test: epoch 118, loss 1.429998517036438, acc=0.3722222149372101, loss=1.429998517036438
train: epoch 119, loss 1.1216180324554443, acc=0.6083333492279053, loss=1.1216180324554443
test: epoch 119, loss 1.4349284172058105, acc=0.375, loss=1.4349284172058105
train: epoch 120, loss 1.123928189277649, acc=0.605555534362793, loss=1.123928189277649
test: epoch 120, loss 1.4279390573501587, acc=0.3722222149372101, loss=1.4279390573501587
train: epoch 121, loss 1.1331987380981445, acc=0.605055570602417, loss=1.1331987380981445
test: epoch 121, loss 1.3911666870117188, acc=0.38055557012557983, loss=1.3911666870117188
train: epoch 122, loss 1.0944942235946655, acc=0.6152222156524658, loss=1.0944942235946655
test: epoch 122, loss 1.3997231721878052, acc=0.38055557012557983, loss=1.3997231721878052
train: epoch 123, loss 1.1132161617279053, acc=0.6143888831138611, loss=1.1132161617279053
test: epoch 123, loss 1.3994642496109009, acc=0.38333332538604736, loss=1.3994642496109009
train: epoch 124, loss 1.0736284255981445, acc=0.6198889017105103, loss=1.0736284255981445
test: epoch 124, loss 1.4031842947006226, acc=0.38333332538604736, loss=1.4031842947006226
train: epoch 125, loss 1.0974090099334717, acc=0.617111086845398, loss=1.0974090099334717
test: epoch 125, loss 1.409168004989624, acc=0.38333332538604736, loss=1.409168004989624
train: epoch 126, loss 1.067269206047058, acc=0.6215000152587891, loss=1.067269206047058
test: epoch 126, loss 1.3938578367233276, acc=0.3861111104488373, loss=1.3938578367233276
train: epoch 127, loss 1.0710527896881104, acc=0.628333330154419, loss=1.0710527896881104
test: epoch 127, loss 1.3888492584228516, acc=0.3888888955116272, loss=1.3888492584228516
train: epoch 128, loss 1.075350284576416, acc=0.6285555362701416, loss=1.075350284576416
test: epoch 128, loss 1.3628113269805908, acc=0.3888888955116272, loss=1.3628113269805908
train: epoch 129, loss 1.054774522781372, acc=0.6290000081062317, loss=1.054774522781372
test: epoch 129, loss 1.3601508140563965, acc=0.3916666805744171, loss=1.3601508140563965
train: epoch 130, loss 1.0428149700164795, acc=0.6318888664245605, loss=1.0428149700164795
test: epoch 130, loss 1.353229284286499, acc=0.3888888955116272, loss=1.353229284286499
train: epoch 131, loss 1.0494455099105835, acc=0.6290555596351624, loss=1.0494455099105835
test: epoch 131, loss 1.3536231517791748, acc=0.3916666805744171, loss=1.3536231517791748
train: epoch 132, loss 1.0484216213226318, acc=0.6336110830307007, loss=1.0484216213226318
test: epoch 132, loss 1.3446979522705078, acc=0.39722222089767456, loss=1.3446979522705078
train: epoch 133, loss 1.0251086950302124, acc=0.6380000114440918, loss=1.0251086950302124
test: epoch 133, loss 1.3256856203079224, acc=0.4055555462837219, loss=1.3256856203079224
train: epoch 134, loss 1.0364388227462769, acc=0.6397222280502319, loss=1.0364388227462769
test: epoch 134, loss 1.3101674318313599, acc=0.40833333134651184, loss=1.3101674318313599
train: epoch 135, loss 1.009404182434082, acc=0.6491110920906067, loss=1.009404182434082
test: epoch 135, loss 1.291145920753479, acc=0.42222222685813904, loss=1.291145920753479
train: epoch 136, loss 1.0146929025650024, acc=0.6448333263397217, loss=1.0146929025650024
test: epoch 136, loss 1.2960731983184814, acc=0.42500001192092896, loss=1.2960731983184814
train: epoch 137, loss 1.0074173212051392, acc=0.652055561542511, loss=1.0074173212051392
test: epoch 137, loss 1.30113685131073, acc=0.4166666567325592, loss=1.30113685131073
train: epoch 138, loss 1.0057103633880615, acc=0.656333327293396, loss=1.0057103633880615
test: epoch 138, loss 1.2883778810501099, acc=0.42222222685813904, loss=1.2883778810501099
train: epoch 139, loss 0.9900339245796204, acc=0.659500002861023, loss=0.9900339245796204
test: epoch 139, loss 1.2741637229919434, acc=0.4277777671813965, loss=1.2741637229919434
train: epoch 140, loss 0.9833866357803345, acc=0.6600555777549744, loss=0.9833866357803345
test: epoch 140, loss 1.274489402770996, acc=0.4305555522441864, loss=1.274489402770996
train: epoch 141, loss 0.9726340174674988, acc=0.6627777814865112, loss=0.9726340174674988
test: epoch 141, loss 1.271984577178955, acc=0.4277777671813965, loss=1.271984577178955
train: epoch 142, loss 0.9728573560714722, acc=0.6672777533531189, loss=0.9728573560714722
test: epoch 142, loss 1.2664196491241455, acc=0.42500001192092896, loss=1.2664196491241455
train: epoch 143, loss 0.9648751020431519, acc=0.6658889055252075, loss=0.9648751020431519
test: epoch 143, loss 1.2740896940231323, acc=0.4277777671813965, loss=1.2740896940231323
train: epoch 144, loss 0.9577009081840515, acc=0.6668888926506042, loss=0.9577009081840515
test: epoch 144, loss 1.2716774940490723, acc=0.4277777671813965, loss=1.2716774940490723
train: epoch 145, loss 0.946039617061615, acc=0.6747221946716309, loss=0.946039617061615
test: epoch 145, loss 1.2799665927886963, acc=0.42222222685813904, loss=1.2799665927886963
train: epoch 146, loss 0.9567073583602905, acc=0.6735000014305115, loss=0.9567073583602905
test: epoch 146, loss 1.2679455280303955, acc=0.4305555522441864, loss=1.2679455280303955
train: epoch 147, loss 0.9406747221946716, acc=0.6767777800559998, loss=0.9406747221946716
test: epoch 147, loss 1.270763635635376, acc=0.4305555522441864, loss=1.270763635635376
train: epoch 148, loss 0.9263578057289124, acc=0.680055558681488, loss=0.9263578057289124
test: epoch 148, loss 1.2749265432357788, acc=0.4305555522441864, loss=1.2749265432357788
train: epoch 149, loss 0.9214590191841125, acc=0.683055579662323, loss=0.9214590191841125
test: epoch 149, loss 1.2719569206237793, acc=0.4305555522441864, loss=1.2719569206237793
train: epoch 150, loss 0.9330413341522217, acc=0.6848333477973938, loss=0.9330413341522217
test: epoch 150, loss 1.2674448490142822, acc=0.4333333373069763, loss=1.2674448490142822
