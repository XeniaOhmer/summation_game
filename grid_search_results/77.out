# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1.5", "--temp_decay=1", "--one_hot=1", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1672142280, receiver_embed_dim=64, save_run=0, temp_decay=1.0, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1672142280, receiver_embed_dim=64, save_run=False, temp_decay=1.0, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.871151924133301, acc=0.09849999845027924, loss=2.871151924133301
test: epoch 1, loss 3.4244706630706787, acc=0.1111111119389534, loss=3.4244706630706787
train: epoch 2, loss 1.737045407295227, acc=0.32733333110809326, loss=1.737045407295227
test: epoch 2, loss 3.2995641231536865, acc=0.14444445073604584, loss=3.2995641231536865
train: epoch 3, loss 1.2954940795898438, acc=0.4745555520057678, loss=1.2954940795898438
test: epoch 3, loss 2.5859274864196777, acc=0.20277777314186096, loss=2.5859274864196777
train: epoch 4, loss 1.0289604663848877, acc=0.5914444327354431, loss=1.0289604663848877
test: epoch 4, loss 3.0669353008270264, acc=0.18611110746860504, loss=3.0669353008270264
train: epoch 5, loss 0.8615038394927979, acc=0.6631666421890259, loss=0.8615038394927979
test: epoch 5, loss 2.1934573650360107, acc=0.2944444417953491, loss=2.1934573650360107
train: epoch 6, loss 0.7458009719848633, acc=0.7020000219345093, loss=0.7458009719848633
test: epoch 6, loss 2.9741201400756836, acc=0.23333333432674408, loss=2.9741201400756836
train: epoch 7, loss 0.6402500867843628, acc=0.7500555515289307, loss=0.6402500867843628
test: epoch 7, loss 2.20261812210083, acc=0.3305555582046509, loss=2.20261812210083
train: epoch 8, loss 0.5628661513328552, acc=0.7770000100135803, loss=0.5628661513328552
test: epoch 8, loss 2.3697493076324463, acc=0.29722222685813904, loss=2.3697493076324463
train: epoch 9, loss 0.5020158886909485, acc=0.8038889169692993, loss=0.5020158886909485
test: epoch 9, loss 2.0474531650543213, acc=0.3333333432674408, loss=2.0474531650543213
train: epoch 10, loss 0.45622333884239197, acc=0.825166642665863, loss=0.45622333884239197
test: epoch 10, loss 1.9656623601913452, acc=0.32499998807907104, loss=1.9656623601913452
train: epoch 11, loss 0.42555099725723267, acc=0.8348333239555359, loss=0.42555099725723267
test: epoch 11, loss 2.2248950004577637, acc=0.3222222328186035, loss=2.2248950004577637
train: epoch 12, loss 0.3938116729259491, acc=0.8503888845443726, loss=0.3938116729259491
test: epoch 12, loss 1.727994441986084, acc=0.3722222149372101, loss=1.727994441986084
train: epoch 13, loss 0.3713989853858948, acc=0.8639444708824158, loss=0.3713989853858948
test: epoch 13, loss 2.197641611099243, acc=0.3583333194255829, loss=2.197641611099243
train: epoch 14, loss 0.3455444276332855, acc=0.8755000233650208, loss=0.3455444276332855
test: epoch 14, loss 1.7455536127090454, acc=0.3861111104488373, loss=1.7455536127090454
train: epoch 15, loss 0.3209706246852875, acc=0.8878889083862305, loss=0.3209706246852875
test: epoch 15, loss 1.8733540773391724, acc=0.42500001192092896, loss=1.8733540773391724
train: epoch 16, loss 0.3065272569656372, acc=0.8937777876853943, loss=0.3065272569656372
test: epoch 16, loss 1.8461803197860718, acc=0.4833333194255829, loss=1.8461803197860718
train: epoch 17, loss 0.27774283289909363, acc=0.902999997138977, loss=0.27774283289909363
test: epoch 17, loss 1.7767307758331299, acc=0.4861111044883728, loss=1.7767307758331299
train: epoch 18, loss 0.2555869519710541, acc=0.9102222323417664, loss=0.2555869519710541
test: epoch 18, loss 1.8638557195663452, acc=0.4583333432674408, loss=1.8638557195663452
train: epoch 19, loss 0.25103890895843506, acc=0.9156110882759094, loss=0.25103890895843506
test: epoch 19, loss 1.859707236289978, acc=0.39722222089767456, loss=1.859707236289978
train: epoch 20, loss 0.23607610166072845, acc=0.9209444522857666, loss=0.23607610166072845
test: epoch 20, loss 1.730712652206421, acc=0.42500001192092896, loss=1.730712652206421
train: epoch 21, loss 0.21845680475234985, acc=0.9247778058052063, loss=0.21845680475234985
test: epoch 21, loss 2.059711217880249, acc=0.45277777314186096, loss=2.059711217880249
train: epoch 22, loss 0.20314088463783264, acc=0.9311110973358154, loss=0.20314088463783264
test: epoch 22, loss 1.6060669422149658, acc=0.5416666865348816, loss=1.6060669422149658
train: epoch 23, loss 0.20710448920726776, acc=0.930055558681488, loss=0.20710448920726776
test: epoch 23, loss 2.2110917568206787, acc=0.4583333432674408, loss=2.2110917568206787
train: epoch 24, loss 0.20518265664577484, acc=0.9350555539131165, loss=0.20518265664577484
test: epoch 24, loss 1.4845612049102783, acc=0.46388888359069824, loss=1.4845612049102783
train: epoch 25, loss 0.19276249408721924, acc=0.9333333373069763, loss=0.19276249408721924
test: epoch 25, loss 1.8818172216415405, acc=0.519444465637207, loss=1.8818172216415405
train: epoch 26, loss 0.17184919118881226, acc=0.9448888897895813, loss=0.17184919118881226
test: epoch 26, loss 1.7791963815689087, acc=0.5, loss=1.7791963815689087
train: epoch 27, loss 0.17133358120918274, acc=0.9442777633666992, loss=0.17133358120918274
test: epoch 27, loss 1.6957533359527588, acc=0.5138888955116272, loss=1.6957533359527588
train: epoch 28, loss 0.15683841705322266, acc=0.9474999904632568, loss=0.15683841705322266
test: epoch 28, loss 1.5635942220687866, acc=0.5527777671813965, loss=1.5635942220687866
train: epoch 29, loss 0.15216869115829468, acc=0.9491111040115356, loss=0.15216869115829468
test: epoch 29, loss 2.187812089920044, acc=0.39722222089767456, loss=2.187812089920044
train: epoch 30, loss 0.14957040548324585, acc=0.9514444470405579, loss=0.14957040548324585
test: epoch 30, loss 2.0165748596191406, acc=0.45277777314186096, loss=2.0165748596191406
train: epoch 31, loss 0.14098016917705536, acc=0.9535555839538574, loss=0.14098016917705536
test: epoch 31, loss 1.6470463275909424, acc=0.5805555582046509, loss=1.6470463275909424
train: epoch 32, loss 0.13748840987682343, acc=0.9535555839538574, loss=0.13748840987682343
test: epoch 32, loss 1.6615967750549316, acc=0.5694444179534912, loss=1.6615967750549316
train: epoch 33, loss 0.13410231471061707, acc=0.9575555324554443, loss=0.13410231471061707
test: epoch 33, loss 2.0710129737854004, acc=0.5027777552604675, loss=2.0710129737854004
train: epoch 34, loss 0.12825526297092438, acc=0.957611083984375, loss=0.12825526297092438
test: epoch 34, loss 1.7595993280410767, acc=0.4833333194255829, loss=1.7595993280410767
train: epoch 35, loss 0.11823512613773346, acc=0.9618333578109741, loss=0.11823512613773346
test: epoch 35, loss 1.9018727540969849, acc=0.5138888955116272, loss=1.9018727540969849
train: epoch 36, loss 0.13296063244342804, acc=0.9566110968589783, loss=0.13296063244342804
test: epoch 36, loss 1.2662675380706787, acc=0.4833333194255829, loss=1.2662675380706787
train: epoch 37, loss 0.12016554176807404, acc=0.960277795791626, loss=0.12016554176807404
test: epoch 37, loss 1.4084267616271973, acc=0.5861111283302307, loss=1.4084267616271973
train: epoch 38, loss 0.10422605276107788, acc=0.9665555357933044, loss=0.10422605276107788
test: epoch 38, loss 1.6715346574783325, acc=0.5222222208976746, loss=1.6715346574783325
train: epoch 39, loss 0.11067801713943481, acc=0.9632777571678162, loss=0.11067801713943481
test: epoch 39, loss 1.7316633462905884, acc=0.5638889074325562, loss=1.7316633462905884
train: epoch 40, loss 0.10585324466228485, acc=0.9657777547836304, loss=0.10585324466228485
test: epoch 40, loss 2.0365312099456787, acc=0.5138888955116272, loss=2.0365312099456787
train: epoch 41, loss 0.1025991290807724, acc=0.9677222371101379, loss=0.1025991290807724
test: epoch 41, loss 1.4661957025527954, acc=0.6222222447395325, loss=1.4661957025527954
train: epoch 42, loss 0.11122860759496689, acc=0.9637777805328369, loss=0.11122860759496689
test: epoch 42, loss 1.4304269552230835, acc=0.5916666388511658, loss=1.4304269552230835
train: epoch 43, loss 0.09174001216888428, acc=0.9692777991294861, loss=0.09174001216888428
test: epoch 43, loss 1.5309635400772095, acc=0.5888888835906982, loss=1.5309635400772095
train: epoch 44, loss 0.08908593654632568, acc=0.9713333249092102, loss=0.08908593654632568
test: epoch 44, loss 1.6243350505828857, acc=0.6416666507720947, loss=1.6243350505828857
train: epoch 45, loss 0.10017210990190506, acc=0.9680555462837219, loss=0.10017210990190506
test: epoch 45, loss 1.503060221672058, acc=0.5694444179534912, loss=1.503060221672058
train: epoch 46, loss 0.08662539720535278, acc=0.9720555543899536, loss=0.08662539720535278
test: epoch 46, loss 1.3746304512023926, acc=0.6638888716697693, loss=1.3746304512023926
train: epoch 47, loss 0.09352986514568329, acc=0.9710555672645569, loss=0.09352986514568329
test: epoch 47, loss 1.4643440246582031, acc=0.6361111402511597, loss=1.4643440246582031
train: epoch 48, loss 0.08231131732463837, acc=0.9731666445732117, loss=0.08231131732463837
test: epoch 48, loss 1.5975139141082764, acc=0.6361111402511597, loss=1.5975139141082764
train: epoch 49, loss 0.07847009599208832, acc=0.9758889079093933, loss=0.07847009599208832
test: epoch 49, loss 1.1831388473510742, acc=0.6694444417953491, loss=1.1831388473510742
train: epoch 50, loss 0.08674413710832596, acc=0.9738333225250244, loss=0.08674413710832596
test: epoch 50, loss 0.9827954769134521, acc=0.7111111283302307, loss=0.9827954769134521
train: epoch 51, loss 0.08372533321380615, acc=0.9726666808128357, loss=0.08372533321380615
test: epoch 51, loss 1.637080430984497, acc=0.6583333611488342, loss=1.637080430984497
train: epoch 52, loss 0.0801997110247612, acc=0.9748333096504211, loss=0.0801997110247612
test: epoch 52, loss 0.9831066727638245, acc=0.7638888955116272, loss=0.9831066727638245
train: epoch 53, loss 0.07914192974567413, acc=0.9757221937179565, loss=0.07914192974567413
test: epoch 53, loss 1.0253866910934448, acc=0.7027778029441833, loss=1.0253866910934448
train: epoch 54, loss 0.07745988667011261, acc=0.975777804851532, loss=0.07745988667011261
test: epoch 54, loss 1.401464819908142, acc=0.6611111164093018, loss=1.401464819908142
train: epoch 55, loss 0.06947822868824005, acc=0.9789999723434448, loss=0.06947822868824005
test: epoch 55, loss 1.0431361198425293, acc=0.6972222328186035, loss=1.0431361198425293
train: epoch 56, loss 0.07226448506116867, acc=0.9766111373901367, loss=0.07226448506116867
test: epoch 56, loss 1.2206882238388062, acc=0.6888889074325562, loss=1.2206882238388062
train: epoch 57, loss 0.07271506637334824, acc=0.9778888821601868, loss=0.07271506637334824
test: epoch 57, loss 1.0205907821655273, acc=0.7222222089767456, loss=1.0205907821655273
train: epoch 58, loss 0.06601068377494812, acc=0.9796110987663269, loss=0.06601068377494812
test: epoch 58, loss 1.0566619634628296, acc=0.730555534362793, loss=1.0566619634628296
train: epoch 59, loss 0.06836897134780884, acc=0.9784444570541382, loss=0.06836897134780884
test: epoch 59, loss 0.9923665523529053, acc=0.8222222328186035, loss=0.9923665523529053
train: epoch 60, loss 0.059530697762966156, acc=0.980222225189209, loss=0.059530697762966156
test: epoch 60, loss 0.7425470948219299, acc=0.7444444298744202, loss=0.7425470948219299
train: epoch 61, loss 0.06524370610713959, acc=0.9784444570541382, loss=0.06524370610713959
test: epoch 61, loss 1.0434904098510742, acc=0.7583333253860474, loss=1.0434904098510742
train: epoch 62, loss 0.05948971211910248, acc=0.9801666736602783, loss=0.05948971211910248
test: epoch 62, loss 0.7580417394638062, acc=0.8388888835906982, loss=0.7580417394638062
train: epoch 63, loss 0.05552447959780693, acc=0.9829444289207458, loss=0.05552447959780693
test: epoch 63, loss 0.696457028388977, acc=0.8888888955116272, loss=0.696457028388977
train: epoch 64, loss 0.05955373868346214, acc=0.9810555577278137, loss=0.05955373868346214
test: epoch 64, loss 0.921086847782135, acc=0.8416666388511658, loss=0.921086847782135
train: epoch 65, loss 0.05633379891514778, acc=0.9833333492279053, loss=0.05633379891514778
test: epoch 65, loss 0.7739966511726379, acc=0.800000011920929, loss=0.7739966511726379
train: epoch 66, loss 0.0478452630341053, acc=0.9844444394111633, loss=0.0478452630341053
test: epoch 66, loss 1.0809402465820312, acc=0.8055555820465088, loss=1.0809402465820312
train: epoch 67, loss 0.06305292248725891, acc=0.9812222123146057, loss=0.06305292248725891
test: epoch 67, loss 1.0513463020324707, acc=0.7861111164093018, loss=1.0513463020324707
train: epoch 68, loss 0.04434128478169441, acc=0.9863888621330261, loss=0.04434128478169441
test: epoch 68, loss 0.7049192190170288, acc=0.8527777791023254, loss=0.7049192190170288
train: epoch 69, loss 0.04940043017268181, acc=0.9851111173629761, loss=0.04940043017268181
test: epoch 69, loss 0.778286337852478, acc=0.8388888835906982, loss=0.778286337852478
train: epoch 70, loss 0.04707556962966919, acc=0.9849444627761841, loss=0.04707556962966919
test: epoch 70, loss 0.6672556400299072, acc=0.8861111402511597, loss=0.6672556400299072
train: epoch 71, loss 0.04586721956729889, acc=0.9865555763244629, loss=0.04586721956729889
test: epoch 71, loss 0.4095931351184845, acc=0.8972222208976746, loss=0.4095931351184845
train: epoch 72, loss 0.046686574816703796, acc=0.9859444499015808, loss=0.046686574816703796
test: epoch 72, loss 0.3917218744754791, acc=0.894444465637207, loss=0.3917218744754791
train: epoch 73, loss 0.04620840772986412, acc=0.9861666560173035, loss=0.04620840772986412
test: epoch 73, loss 0.5410533547401428, acc=0.8722222447395325, loss=0.5410533547401428
train: epoch 74, loss 0.04237944632768631, acc=0.987500011920929, loss=0.04237944632768631
test: epoch 74, loss 0.41578081250190735, acc=0.9166666865348816, loss=0.41578081250190735
train: epoch 75, loss 0.03761165961623192, acc=0.9887222051620483, loss=0.03761165961623192
test: epoch 75, loss 0.29055675864219666, acc=0.9361110925674438, loss=0.29055675864219666
train: epoch 76, loss 0.03454945981502533, acc=0.9897222518920898, loss=0.03454945981502533
test: epoch 76, loss 0.5153666138648987, acc=0.8722222447395325, loss=0.5153666138648987
train: epoch 77, loss 0.04053858295083046, acc=0.9884999990463257, loss=0.04053858295083046
test: epoch 77, loss 0.3938579261302948, acc=0.9388889074325562, loss=0.3938579261302948
train: epoch 78, loss 0.03607284277677536, acc=0.988777756690979, loss=0.03607284277677536
test: epoch 78, loss 0.3620442748069763, acc=0.9361110925674438, loss=0.3620442748069763
train: epoch 79, loss 0.03722867742180824, acc=0.9887222051620483, loss=0.03722867742180824
test: epoch 79, loss 0.2928992509841919, acc=0.9222221970558167, loss=0.2928992509841919
train: epoch 80, loss 0.03371027484536171, acc=0.9896110892295837, loss=0.03371027484536171
test: epoch 80, loss 0.1423315554857254, acc=0.9750000238418579, loss=0.1423315554857254
train: epoch 81, loss 0.034628160297870636, acc=0.9892777800559998, loss=0.034628160297870636
test: epoch 81, loss 0.24982298910617828, acc=0.949999988079071, loss=0.24982298910617828
train: epoch 82, loss 0.03175542131066322, acc=0.9902222156524658, loss=0.03175542131066322
test: epoch 82, loss 0.32144883275032043, acc=0.9388889074325562, loss=0.32144883275032043
train: epoch 83, loss 0.025514286011457443, acc=0.9918333292007446, loss=0.025514286011457443
test: epoch 83, loss 0.19794106483459473, acc=0.9527778029441833, loss=0.19794106483459473
train: epoch 84, loss 0.034195151180028915, acc=0.9893333315849304, loss=0.034195151180028915
test: epoch 84, loss 0.26870957016944885, acc=0.9361110925674438, loss=0.26870957016944885
train: epoch 85, loss 0.03399461507797241, acc=0.9897222518920898, loss=0.03399461507797241
test: epoch 85, loss 0.04795257747173309, acc=0.9861111044883728, loss=0.04795257747173309
train: epoch 86, loss 0.019915558397769928, acc=0.9936666488647461, loss=0.019915558397769928
test: epoch 86, loss 0.06881298124790192, acc=0.9888888597488403, loss=0.06881298124790192
train: epoch 87, loss 0.03578866645693779, acc=0.9901666641235352, loss=0.03578866645693779
test: epoch 87, loss 0.03263372555375099, acc=0.9916666746139526, loss=0.03263372555375099
