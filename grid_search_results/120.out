# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.99", "--one_hot=0", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=561925956, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=561925956, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.1497175693511963, acc=0.07316666841506958, loss=3.1497175693511963
test: epoch 1, loss 3.732863187789917, acc=0.05277777835726738, loss=3.732863187789917
train: epoch 2, loss 2.8567051887512207, acc=0.109333336353302, loss=2.8567051887512207
test: epoch 2, loss 3.9847893714904785, acc=0.05277777835726738, loss=3.9847893714904785
train: epoch 3, loss 2.7615981101989746, acc=0.11599999666213989, loss=2.7615981101989746
test: epoch 3, loss 4.067755699157715, acc=0.05000000074505806, loss=4.067755699157715
train: epoch 4, loss 2.7134666442871094, acc=0.12738889455795288, loss=2.7134666442871094
test: epoch 4, loss 4.106678485870361, acc=0.05277777835726738, loss=4.106678485870361
train: epoch 5, loss 2.6767592430114746, acc=0.12999999523162842, loss=2.6767592430114746
test: epoch 5, loss 4.2062788009643555, acc=0.05277777835726738, loss=4.2062788009643555
train: epoch 6, loss 2.6643762588500977, acc=0.13261111080646515, loss=2.6643762588500977
test: epoch 6, loss 4.1458001136779785, acc=0.05000000074505806, loss=4.1458001136779785
train: epoch 7, loss 2.64205265045166, acc=0.13527777791023254, loss=2.64205265045166
test: epoch 7, loss 4.119241237640381, acc=0.05833333358168602, loss=4.119241237640381
train: epoch 8, loss 2.6268904209136963, acc=0.14116667211055756, loss=2.6268904209136963
test: epoch 8, loss 4.17482852935791, acc=0.05277777835726738, loss=4.17482852935791
train: epoch 9, loss 2.6036570072174072, acc=0.1393333375453949, loss=2.6036570072174072
test: epoch 9, loss 4.119635105133057, acc=0.05833333358168602, loss=4.119635105133057
train: epoch 10, loss 2.5984020233154297, acc=0.1449444442987442, loss=2.5984020233154297
test: epoch 10, loss 3.9735565185546875, acc=0.05833333358168602, loss=3.9735565185546875
train: epoch 11, loss 2.5970547199249268, acc=0.14327777922153473, loss=2.5970547199249268
test: epoch 11, loss 4.166632652282715, acc=0.0555555559694767, loss=4.166632652282715
train: epoch 12, loss 2.581192970275879, acc=0.13894444704055786, loss=2.581192970275879
test: epoch 12, loss 3.9722719192504883, acc=0.0555555559694767, loss=3.9722719192504883
train: epoch 13, loss 2.5878472328186035, acc=0.14344444870948792, loss=2.5878472328186035
test: epoch 13, loss 3.979658842086792, acc=0.05277777835726738, loss=3.979658842086792
train: epoch 14, loss 2.571755886077881, acc=0.14822222292423248, loss=2.571755886077881
test: epoch 14, loss 3.9711833000183105, acc=0.0555555559694767, loss=3.9711833000183105
train: epoch 15, loss 2.57297420501709, acc=0.14394444227218628, loss=2.57297420501709
test: epoch 15, loss 3.8389101028442383, acc=0.05833333358168602, loss=3.8389101028442383
train: epoch 16, loss 2.5684516429901123, acc=0.1420000046491623, loss=2.5684516429901123
test: epoch 16, loss 3.8297383785247803, acc=0.05277777835726738, loss=3.8297383785247803
train: epoch 17, loss 2.5631608963012695, acc=0.1456666737794876, loss=2.5631608963012695
test: epoch 17, loss 3.8107972145080566, acc=0.05000000074505806, loss=3.8107972145080566
train: epoch 18, loss 2.5549111366271973, acc=0.1418333351612091, loss=2.5549111366271973
test: epoch 18, loss 3.7612345218658447, acc=0.05833333358168602, loss=3.7612345218658447
train: epoch 19, loss 2.5537636280059814, acc=0.14944444596767426, loss=2.5537636280059814
test: epoch 19, loss 3.8235819339752197, acc=0.0555555559694767, loss=3.8235819339752197
train: epoch 20, loss 2.552180528640747, acc=0.14844444394111633, loss=2.552180528640747
test: epoch 20, loss 3.7298245429992676, acc=0.06111111119389534, loss=3.7298245429992676
train: epoch 21, loss 2.561427116394043, acc=0.14499999582767487, loss=2.561427116394043
test: epoch 21, loss 3.7804782390594482, acc=0.05277777835726738, loss=3.7804782390594482
train: epoch 22, loss 2.558518648147583, acc=0.14605554938316345, loss=2.558518648147583
test: epoch 22, loss 3.707564115524292, acc=0.0694444477558136, loss=3.707564115524292
train: epoch 23, loss 2.547454595565796, acc=0.14427778124809265, loss=2.547454595565796
test: epoch 23, loss 3.714205741882324, acc=0.05833333358168602, loss=3.714205741882324
train: epoch 24, loss 2.5534613132476807, acc=0.14355555176734924, loss=2.5534613132476807
test: epoch 24, loss 3.676152229309082, acc=0.05000000074505806, loss=3.676152229309082
train: epoch 25, loss 2.55755615234375, acc=0.14533333480358124, loss=2.55755615234375
test: epoch 25, loss 3.6375157833099365, acc=0.05833333358168602, loss=3.6375157833099365
train: epoch 26, loss 2.5458223819732666, acc=0.14488889276981354, loss=2.5458223819732666
test: epoch 26, loss 3.625905752182007, acc=0.0555555559694767, loss=3.625905752182007
train: epoch 27, loss 2.5553081035614014, acc=0.1474444419145584, loss=2.5553081035614014
test: epoch 27, loss 3.582545757293701, acc=0.0555555559694767, loss=3.582545757293701
train: epoch 28, loss 2.5501105785369873, acc=0.14411111176013947, loss=2.5501105785369873
test: epoch 28, loss 3.600862741470337, acc=0.05833333358168602, loss=3.600862741470337
train: epoch 29, loss 2.5531702041625977, acc=0.14166666567325592, loss=2.5531702041625977
test: epoch 29, loss 3.600931167602539, acc=0.05833333358168602, loss=3.600931167602539
train: epoch 30, loss 2.552342653274536, acc=0.14749999344348907, loss=2.552342653274536
test: epoch 30, loss 3.6673598289489746, acc=0.07500000298023224, loss=3.6673598289489746
train: epoch 31, loss 2.533869743347168, acc=0.15105555951595306, loss=2.533869743347168
test: epoch 31, loss 3.5595459938049316, acc=0.07500000298023224, loss=3.5595459938049316
train: epoch 32, loss 2.526244640350342, acc=0.14866666495800018, loss=2.526244640350342
test: epoch 32, loss 3.5745205879211426, acc=0.0694444477558136, loss=3.5745205879211426
train: epoch 33, loss 2.5221025943756104, acc=0.1493888944387436, loss=2.5221025943756104
test: epoch 33, loss 3.591602325439453, acc=0.07777778059244156, loss=3.591602325439453
train: epoch 34, loss 2.527566432952881, acc=0.1498333364725113, loss=2.527566432952881
test: epoch 34, loss 3.477938175201416, acc=0.07777778059244156, loss=3.477938175201416
train: epoch 35, loss 2.5155656337738037, acc=0.14483332633972168, loss=2.5155656337738037
test: epoch 35, loss 3.4849352836608887, acc=0.06388889253139496, loss=3.4849352836608887
train: epoch 36, loss 2.5144875049591064, acc=0.14961111545562744, loss=2.5144875049591064
test: epoch 36, loss 3.5532538890838623, acc=0.06111111119389534, loss=3.5532538890838623
train: epoch 37, loss 2.5206594467163086, acc=0.14861111342906952, loss=2.5206594467163086
test: epoch 37, loss 3.527179002761841, acc=0.0555555559694767, loss=3.527179002761841
train: epoch 38, loss 2.50573992729187, acc=0.15522222220897675, loss=2.50573992729187
test: epoch 38, loss 3.545088768005371, acc=0.05833333358168602, loss=3.545088768005371
train: epoch 39, loss 2.5111196041107178, acc=0.1489444375038147, loss=2.5111196041107178
test: epoch 39, loss 3.4009759426116943, acc=0.07500000298023224, loss=3.4009759426116943
train: epoch 40, loss 2.520693063735962, acc=0.1529444456100464, loss=2.520693063735962
test: epoch 40, loss 3.4970805644989014, acc=0.06666667014360428, loss=3.4970805644989014
train: epoch 41, loss 2.497889757156372, acc=0.15133333206176758, loss=2.497889757156372
test: epoch 41, loss 3.398559093475342, acc=0.07500000298023224, loss=3.398559093475342
train: epoch 42, loss 2.4960100650787354, acc=0.15355555713176727, loss=2.4960100650787354
test: epoch 42, loss 3.3688995838165283, acc=0.07500000298023224, loss=3.3688995838165283
train: epoch 43, loss 2.4999711513519287, acc=0.1518888920545578, loss=2.4999711513519287
test: epoch 43, loss 3.504629373550415, acc=0.05833333358168602, loss=3.504629373550415
train: epoch 44, loss 2.4896085262298584, acc=0.1525000035762787, loss=2.4896085262298584
test: epoch 44, loss 3.4329094886779785, acc=0.06111111119389534, loss=3.4329094886779785
train: epoch 45, loss 2.480259656906128, acc=0.154388889670372, loss=2.480259656906128
test: epoch 45, loss 3.5437676906585693, acc=0.06111111119389534, loss=3.5437676906585693
train: epoch 46, loss 2.481627941131592, acc=0.15555556118488312, loss=2.481627941131592
test: epoch 46, loss 3.4796807765960693, acc=0.0833333358168602, loss=3.4796807765960693
train: epoch 47, loss 2.486684799194336, acc=0.15638889372348785, loss=2.486684799194336
test: epoch 47, loss 3.523373603820801, acc=0.06388889253139496, loss=3.523373603820801
train: epoch 48, loss 2.4856717586517334, acc=0.15199999511241913, loss=2.4856717586517334
test: epoch 48, loss 3.4009463787078857, acc=0.05833333358168602, loss=3.4009463787078857
train: epoch 49, loss 2.456028461456299, acc=0.1591111123561859, loss=2.456028461456299
test: epoch 49, loss 3.4561514854431152, acc=0.0694444477558136, loss=3.4561514854431152
train: epoch 50, loss 2.459909677505493, acc=0.1558888852596283, loss=2.459909677505493
test: epoch 50, loss 3.421255350112915, acc=0.0555555559694767, loss=3.421255350112915
train: epoch 51, loss 2.4624412059783936, acc=0.15872222185134888, loss=2.4624412059783936
test: epoch 51, loss 3.3619203567504883, acc=0.0833333358168602, loss=3.3619203567504883
train: epoch 52, loss 2.467801570892334, acc=0.1591111123561859, loss=2.467801570892334
test: epoch 52, loss 3.491044759750366, acc=0.06388889253139496, loss=3.491044759750366
train: epoch 53, loss 2.4583394527435303, acc=0.15611110627651215, loss=2.4583394527435303
test: epoch 53, loss 3.41111421585083, acc=0.07500000298023224, loss=3.41111421585083
train: epoch 54, loss 2.4571447372436523, acc=0.15555556118488312, loss=2.4571447372436523
test: epoch 54, loss 3.3945083618164062, acc=0.08055555820465088, loss=3.3945083618164062
train: epoch 55, loss 2.463994026184082, acc=0.1583888828754425, loss=2.463994026184082
test: epoch 55, loss 3.3777289390563965, acc=0.08611111342906952, loss=3.3777289390563965
train: epoch 56, loss 2.455148696899414, acc=0.15872222185134888, loss=2.455148696899414
test: epoch 56, loss 3.3215112686157227, acc=0.07500000298023224, loss=3.3215112686157227
train: epoch 57, loss 2.4531219005584717, acc=0.16599999368190765, loss=2.4531219005584717
test: epoch 57, loss 3.387965202331543, acc=0.08055555820465088, loss=3.387965202331543
train: epoch 58, loss 2.4542860984802246, acc=0.16005556285381317, loss=2.4542860984802246
test: epoch 58, loss 3.314954996109009, acc=0.07500000298023224, loss=3.314954996109009
train: epoch 59, loss 2.4524707794189453, acc=0.16233333945274353, loss=2.4524707794189453
test: epoch 59, loss 3.3646767139434814, acc=0.06666667014360428, loss=3.3646767139434814
train: epoch 60, loss 2.444282293319702, acc=0.16338889300823212, loss=2.444282293319702
test: epoch 60, loss 3.320819616317749, acc=0.07777778059244156, loss=3.320819616317749
train: epoch 61, loss 2.442763566970825, acc=0.16161110997200012, loss=2.442763566970825
test: epoch 61, loss 3.370903968811035, acc=0.0694444477558136, loss=3.370903968811035
train: epoch 62, loss 2.4222683906555176, acc=0.164000004529953, loss=2.4222683906555176
test: epoch 62, loss 3.4004945755004883, acc=0.06388889253139496, loss=3.4004945755004883
train: epoch 63, loss 2.4383068084716797, acc=0.1646111160516739, loss=2.4383068084716797
test: epoch 63, loss 3.3068878650665283, acc=0.08055555820465088, loss=3.3068878650665283
train: epoch 64, loss 2.4216997623443604, acc=0.164000004529953, loss=2.4216997623443604
test: epoch 64, loss 3.345188856124878, acc=0.08055555820465088, loss=3.345188856124878
train: epoch 65, loss 2.4303500652313232, acc=0.1617777794599533, loss=2.4303500652313232
test: epoch 65, loss 3.298085927963257, acc=0.06388889253139496, loss=3.298085927963257
train: epoch 66, loss 2.427215099334717, acc=0.16261111199855804, loss=2.427215099334717
test: epoch 66, loss 3.3186087608337402, acc=0.08055555820465088, loss=3.3186087608337402
train: epoch 67, loss 2.4151015281677246, acc=0.1673888862133026, loss=2.4151015281677246
test: epoch 67, loss 3.3138339519500732, acc=0.09166666865348816, loss=3.3138339519500732
train: epoch 68, loss 2.412022590637207, acc=0.16838888823986053, loss=2.412022590637207
test: epoch 68, loss 3.1521830558776855, acc=0.09166666865348816, loss=3.1521830558776855
train: epoch 69, loss 2.4193384647369385, acc=0.16377778351306915, loss=2.4193384647369385
test: epoch 69, loss 3.2610857486724854, acc=0.05833333358168602, loss=3.2610857486724854
train: epoch 70, loss 2.4088425636291504, acc=0.1655000001192093, loss=2.4088425636291504
test: epoch 70, loss 3.3428261280059814, acc=0.07222222536802292, loss=3.3428261280059814
train: epoch 71, loss 2.399613380432129, acc=0.16566666960716248, loss=2.399613380432129
test: epoch 71, loss 3.386497974395752, acc=0.0555555559694767, loss=3.386497974395752
train: epoch 72, loss 2.426273822784424, acc=0.16699999570846558, loss=2.426273822784424
test: epoch 72, loss 3.205380916595459, acc=0.08055555820465088, loss=3.205380916595459
train: epoch 73, loss 2.396700859069824, acc=0.1684444397687912, loss=2.396700859069824
test: epoch 73, loss 3.1898434162139893, acc=0.08055555820465088, loss=3.1898434162139893
train: epoch 74, loss 2.396622896194458, acc=0.16777777671813965, loss=2.396622896194458
test: epoch 74, loss 3.329580783843994, acc=0.0972222238779068, loss=3.329580783843994
train: epoch 75, loss 2.394224166870117, acc=0.1657777726650238, loss=2.394224166870117
test: epoch 75, loss 3.299395799636841, acc=0.0694444477558136, loss=3.299395799636841
train: epoch 76, loss 2.401296854019165, acc=0.16661110520362854, loss=2.401296854019165
test: epoch 76, loss 3.2913599014282227, acc=0.0833333358168602, loss=3.2913599014282227
train: epoch 77, loss 2.3952605724334717, acc=0.165944442152977, loss=2.3952605724334717
test: epoch 77, loss 3.382112741470337, acc=0.05833333358168602, loss=3.382112741470337
train: epoch 78, loss 2.398026466369629, acc=0.16911111772060394, loss=2.398026466369629
test: epoch 78, loss 3.1702775955200195, acc=0.0833333358168602, loss=3.1702775955200195
train: epoch 79, loss 2.383728504180908, acc=0.16866666078567505, loss=2.383728504180908
test: epoch 79, loss 3.2873446941375732, acc=0.0972222238779068, loss=3.2873446941375732
train: epoch 80, loss 2.365471839904785, acc=0.16677777469158173, loss=2.365471839904785
test: epoch 80, loss 3.2771780490875244, acc=0.07222222536802292, loss=3.2771780490875244
train: epoch 81, loss 2.385924816131592, acc=0.1709444373846054, loss=2.385924816131592
test: epoch 81, loss 3.1935744285583496, acc=0.08611111342906952, loss=3.1935744285583496
train: epoch 82, loss 2.3777804374694824, acc=0.16922222077846527, loss=2.3777804374694824
test: epoch 82, loss 3.1586544513702393, acc=0.07500000298023224, loss=3.1586544513702393
train: epoch 83, loss 2.3717823028564453, acc=0.16988888382911682, loss=2.3717823028564453
test: epoch 83, loss 3.2086853981018066, acc=0.07222222536802292, loss=3.2086853981018066
train: epoch 84, loss 2.3713672161102295, acc=0.16716666519641876, loss=2.3713672161102295
test: epoch 84, loss 3.2401809692382812, acc=0.05833333358168602, loss=3.2401809692382812
train: epoch 85, loss 2.3917391300201416, acc=0.16988888382911682, loss=2.3917391300201416
test: epoch 85, loss 3.216047525405884, acc=0.07777778059244156, loss=3.216047525405884
train: epoch 86, loss 2.3808374404907227, acc=0.16877777874469757, loss=2.3808374404907227
test: epoch 86, loss 3.200160503387451, acc=0.07222222536802292, loss=3.200160503387451
train: epoch 87, loss 2.3887648582458496, acc=0.17255555093288422, loss=2.3887648582458496
test: epoch 87, loss 3.2043280601501465, acc=0.0833333358168602, loss=3.2043280601501465
train: epoch 88, loss 2.3668277263641357, acc=0.1736111044883728, loss=2.3668277263641357
test: epoch 88, loss 3.1890265941619873, acc=0.08611111342906952, loss=3.1890265941619873
train: epoch 89, loss 2.3546173572540283, acc=0.17472222447395325, loss=2.3546173572540283
test: epoch 89, loss 3.21919322013855, acc=0.08055555820465088, loss=3.21919322013855
train: epoch 90, loss 2.3583271503448486, acc=0.17127777636051178, loss=2.3583271503448486
test: epoch 90, loss 3.2123050689697266, acc=0.07777778059244156, loss=3.2123050689697266
train: epoch 91, loss 2.367121458053589, acc=0.17299999296665192, loss=2.367121458053589
test: epoch 91, loss 3.195901870727539, acc=0.07222222536802292, loss=3.195901870727539
train: epoch 92, loss 2.3648269176483154, acc=0.17283333837985992, loss=2.3648269176483154
test: epoch 92, loss 3.1887588500976562, acc=0.07500000298023224, loss=3.1887588500976562
train: epoch 93, loss 2.3509271144866943, acc=0.17577777802944183, loss=2.3509271144866943
test: epoch 93, loss 3.1663150787353516, acc=0.07500000298023224, loss=3.1663150787353516
train: epoch 94, loss 2.379995346069336, acc=0.17100000381469727, loss=2.379995346069336
test: epoch 94, loss 3.201047897338867, acc=0.07222222536802292, loss=3.201047897338867
train: epoch 95, loss 2.3650472164154053, acc=0.171833336353302, loss=2.3650472164154053
test: epoch 95, loss 3.22424054145813, acc=0.0694444477558136, loss=3.22424054145813
train: epoch 96, loss 2.34647798538208, acc=0.17127777636051178, loss=2.34647798538208
test: epoch 96, loss 3.1343183517456055, acc=0.07500000298023224, loss=3.1343183517456055
train: epoch 97, loss 2.3450870513916016, acc=0.17533333599567413, loss=2.3450870513916016
test: epoch 97, loss 3.2300989627838135, acc=0.08055555820465088, loss=3.2300989627838135
train: epoch 98, loss 2.354848861694336, acc=0.17527778446674347, loss=2.354848861694336
test: epoch 98, loss 3.2204225063323975, acc=0.05833333358168602, loss=3.2204225063323975
train: epoch 99, loss 2.3453426361083984, acc=0.1724444478750229, loss=2.3453426361083984
test: epoch 99, loss 3.0548393726348877, acc=0.0833333358168602, loss=3.0548393726348877
train: epoch 100, loss 2.3417961597442627, acc=0.17705555260181427, loss=2.3417961597442627
test: epoch 100, loss 3.0621225833892822, acc=0.08611111342906952, loss=3.0621225833892822
train: epoch 101, loss 2.3381412029266357, acc=0.17411111295223236, loss=2.3381412029266357
test: epoch 101, loss 3.231829881668091, acc=0.07500000298023224, loss=3.231829881668091
train: epoch 102, loss 2.3417952060699463, acc=0.18177777528762817, loss=2.3417952060699463
test: epoch 102, loss 3.183459997177124, acc=0.0555555559694767, loss=3.183459997177124
train: epoch 103, loss 2.3364791870117188, acc=0.18211111426353455, loss=2.3364791870117188
test: epoch 103, loss 3.170966625213623, acc=0.07500000298023224, loss=3.170966625213623
train: epoch 104, loss 2.3292460441589355, acc=0.17633333802223206, loss=2.3292460441589355
test: epoch 104, loss 3.2165989875793457, acc=0.0694444477558136, loss=3.2165989875793457
train: epoch 105, loss 2.34175181388855, acc=0.1801111102104187, loss=2.34175181388855
test: epoch 105, loss 3.078505277633667, acc=0.06388889253139496, loss=3.078505277633667
train: epoch 106, loss 2.3237388134002686, acc=0.17838889360427856, loss=2.3237388134002686
test: epoch 106, loss 3.172313928604126, acc=0.07777778059244156, loss=3.172313928604126
train: epoch 107, loss 2.341392993927002, acc=0.1787777841091156, loss=2.341392993927002
test: epoch 107, loss 3.0654385089874268, acc=0.0694444477558136, loss=3.0654385089874268
train: epoch 108, loss 2.3477108478546143, acc=0.17511111497879028, loss=2.3477108478546143
test: epoch 108, loss 3.200733184814453, acc=0.08055555820465088, loss=3.200733184814453
train: epoch 109, loss 2.338142156600952, acc=0.181611105799675, loss=2.338142156600952
test: epoch 109, loss 3.120405912399292, acc=0.0833333358168602, loss=3.120405912399292
train: epoch 110, loss 2.3311336040496826, acc=0.18088889122009277, loss=2.3311336040496826
test: epoch 110, loss 3.1712117195129395, acc=0.07777778059244156, loss=3.1712117195129395
train: epoch 111, loss 2.3281710147857666, acc=0.17883333563804626, loss=2.3281710147857666
test: epoch 111, loss 3.159688949584961, acc=0.07500000298023224, loss=3.159688949584961
train: epoch 112, loss 2.3125338554382324, acc=0.179666668176651, loss=2.3125338554382324
test: epoch 112, loss 3.1736133098602295, acc=0.07777778059244156, loss=3.1736133098602295
train: epoch 113, loss 2.311985492706299, acc=0.1826111078262329, loss=2.311985492706299
test: epoch 113, loss 3.1006557941436768, acc=0.0694444477558136, loss=3.1006557941436768
train: epoch 114, loss 2.3082783222198486, acc=0.1807222217321396, loss=2.3082783222198486
test: epoch 114, loss 3.2934036254882812, acc=0.06111111119389534, loss=3.2934036254882812
train: epoch 115, loss 2.312608242034912, acc=0.1839444488286972, loss=2.312608242034912
test: epoch 115, loss 3.3558149337768555, acc=0.07500000298023224, loss=3.3558149337768555
train: epoch 116, loss 2.3158223628997803, acc=0.17855554819107056, loss=2.3158223628997803
test: epoch 116, loss 3.220364809036255, acc=0.0694444477558136, loss=3.220364809036255
train: epoch 117, loss 2.3015265464782715, acc=0.18050000071525574, loss=2.3015265464782715
test: epoch 117, loss 3.1984128952026367, acc=0.07222222536802292, loss=3.1984128952026367
train: epoch 118, loss 2.3098223209381104, acc=0.18466666340827942, loss=2.3098223209381104
test: epoch 118, loss 3.246138334274292, acc=0.0694444477558136, loss=3.246138334274292
train: epoch 119, loss 2.30586576461792, acc=0.18383333086967468, loss=2.30586576461792
test: epoch 119, loss 3.1583619117736816, acc=0.08055555820465088, loss=3.1583619117736816
train: epoch 120, loss 2.309333086013794, acc=0.18494445085525513, loss=2.309333086013794
test: epoch 120, loss 3.2839362621307373, acc=0.07500000298023224, loss=3.2839362621307373
train: epoch 121, loss 2.309270143508911, acc=0.18449999392032623, loss=2.309270143508911
test: epoch 121, loss 3.2234184741973877, acc=0.07222222536802292, loss=3.2234184741973877
train: epoch 122, loss 2.3014843463897705, acc=0.18672221899032593, loss=2.3014843463897705
test: epoch 122, loss 3.184267997741699, acc=0.07222222536802292, loss=3.184267997741699
train: epoch 123, loss 2.3050730228424072, acc=0.18466666340827942, loss=2.3050730228424072
test: epoch 123, loss 3.1504130363464355, acc=0.07500000298023224, loss=3.1504130363464355
train: epoch 124, loss 2.2912235260009766, acc=0.18400000035762787, loss=2.2912235260009766
test: epoch 124, loss 3.054718494415283, acc=0.07500000298023224, loss=3.054718494415283
train: epoch 125, loss 2.301360607147217, acc=0.1841111183166504, loss=2.301360607147217
test: epoch 125, loss 3.1294949054718018, acc=0.07777778059244156, loss=3.1294949054718018
train: epoch 126, loss 2.3047075271606445, acc=0.18444444239139557, loss=2.3047075271606445
test: epoch 126, loss 3.1294984817504883, acc=0.07500000298023224, loss=3.1294984817504883
train: epoch 127, loss 2.294299602508545, acc=0.1827777773141861, loss=2.294299602508545
test: epoch 127, loss 3.2037699222564697, acc=0.07222222536802292, loss=3.2037699222564697
train: epoch 128, loss 2.290783643722534, acc=0.1913333386182785, loss=2.290783643722534
test: epoch 128, loss 3.2457845211029053, acc=0.0833333358168602, loss=3.2457845211029053
train: epoch 129, loss 2.2794289588928223, acc=0.18716666102409363, loss=2.2794289588928223
test: epoch 129, loss 3.2193567752838135, acc=0.05277777835726738, loss=3.2193567752838135
train: epoch 130, loss 2.2909109592437744, acc=0.18666666746139526, loss=2.2909109592437744
test: epoch 130, loss 3.1520142555236816, acc=0.07222222536802292, loss=3.1520142555236816
train: epoch 131, loss 2.2831666469573975, acc=0.18711110949516296, loss=2.2831666469573975
test: epoch 131, loss 3.2458765506744385, acc=0.07500000298023224, loss=3.2458765506744385
train: epoch 132, loss 2.2912344932556152, acc=0.18727777898311615, loss=2.2912344932556152
test: epoch 132, loss 3.1930341720581055, acc=0.0694444477558136, loss=3.1930341720581055
train: epoch 133, loss 2.2721962928771973, acc=0.1908888816833496, loss=2.2721962928771973
test: epoch 133, loss 3.2788493633270264, acc=0.07222222536802292, loss=3.2788493633270264
train: epoch 134, loss 2.279879093170166, acc=0.19055555760860443, loss=2.279879093170166
test: epoch 134, loss 3.2407970428466797, acc=0.0694444477558136, loss=3.2407970428466797
train: epoch 135, loss 2.2867016792297363, acc=0.19233334064483643, loss=2.2867016792297363
test: epoch 135, loss 3.1669445037841797, acc=0.08888889104127884, loss=3.1669445037841797
train: epoch 136, loss 2.2717995643615723, acc=0.19172222912311554, loss=2.2717995643615723
test: epoch 136, loss 3.2593350410461426, acc=0.05277777835726738, loss=3.2593350410461426
train: epoch 137, loss 2.269000768661499, acc=0.18833333253860474, loss=2.269000768661499
test: epoch 137, loss 3.2196764945983887, acc=0.07222222536802292, loss=3.2196764945983887
train: epoch 138, loss 2.272313356399536, acc=0.19255556166172028, loss=2.272313356399536
test: epoch 138, loss 3.218996524810791, acc=0.08055555820465088, loss=3.218996524810791
train: epoch 139, loss 2.2649753093719482, acc=0.19172222912311554, loss=2.2649753093719482
test: epoch 139, loss 3.1582493782043457, acc=0.08055555820465088, loss=3.1582493782043457
train: epoch 140, loss 2.2715866565704346, acc=0.19716666638851166, loss=2.2715866565704346
test: epoch 140, loss 3.1712348461151123, acc=0.07222222536802292, loss=3.1712348461151123
train: epoch 141, loss 2.274704694747925, acc=0.19200000166893005, loss=2.274704694747925
test: epoch 141, loss 3.2401585578918457, acc=0.08055555820465088, loss=3.2401585578918457
train: epoch 142, loss 2.262871742248535, acc=0.18788889050483704, loss=2.262871742248535
test: epoch 142, loss 3.1864912509918213, acc=0.08055555820465088, loss=3.1864912509918213
train: epoch 143, loss 2.2669692039489746, acc=0.1882222294807434, loss=2.2669692039489746
test: epoch 143, loss 3.192760705947876, acc=0.07777778059244156, loss=3.192760705947876
train: epoch 144, loss 2.2684285640716553, acc=0.18983332812786102, loss=2.2684285640716553
test: epoch 144, loss 3.364537239074707, acc=0.05833333358168602, loss=3.364537239074707
train: epoch 145, loss 2.27077317237854, acc=0.19083333015441895, loss=2.27077317237854
test: epoch 145, loss 3.1197524070739746, acc=0.08055555820465088, loss=3.1197524070739746
train: epoch 146, loss 2.2624471187591553, acc=0.19411110877990723, loss=2.2624471187591553
test: epoch 146, loss 3.221095561981201, acc=0.06388889253139496, loss=3.221095561981201
train: epoch 147, loss 2.260251522064209, acc=0.19527778029441833, loss=2.260251522064209
test: epoch 147, loss 3.15997052192688, acc=0.07500000298023224, loss=3.15997052192688
train: epoch 148, loss 2.2597265243530273, acc=0.1984444409608841, loss=2.2597265243530273
test: epoch 148, loss 3.3199315071105957, acc=0.07500000298023224, loss=3.3199315071105957
train: epoch 149, loss 2.271152973175049, acc=0.193555548787117, loss=2.271152973175049
test: epoch 149, loss 3.1992135047912598, acc=0.07777778059244156, loss=3.1992135047912598
train: epoch 150, loss 2.275787115097046, acc=0.19027778506278992, loss=2.275787115097046
test: epoch 150, loss 3.142267942428589, acc=0.09166666865348816, loss=3.142267942428589
