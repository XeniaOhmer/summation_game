# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.99", "--one_hot=0", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1644760146, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1644760146, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.3587608337402344, acc=0.056333333253860474, loss=3.3587608337402344
test: epoch 1, loss 3.743541955947876, acc=0.04444444552063942, loss=3.743541955947876
train: epoch 2, loss 3.015911817550659, acc=0.09216666966676712, loss=3.015911817550659
test: epoch 2, loss 3.765192985534668, acc=0.04722222313284874, loss=3.765192985534668
train: epoch 3, loss 2.86458683013916, acc=0.10544444620609283, loss=2.86458683013916
test: epoch 3, loss 3.816770076751709, acc=0.05000000074505806, loss=3.816770076751709
train: epoch 4, loss 2.793670654296875, acc=0.1180555522441864, loss=2.793670654296875
test: epoch 4, loss 3.9439163208007812, acc=0.05277777835726738, loss=3.9439163208007812
train: epoch 5, loss 2.7494332790374756, acc=0.12666666507720947, loss=2.7494332790374756
test: epoch 5, loss 3.9244894981384277, acc=0.05277777835726738, loss=3.9244894981384277
train: epoch 6, loss 2.7243542671203613, acc=0.1266111135482788, loss=2.7243542671203613
test: epoch 6, loss 3.9522109031677246, acc=0.05000000074505806, loss=3.9522109031677246
train: epoch 7, loss 2.685938835144043, acc=0.13172222673892975, loss=2.685938835144043
test: epoch 7, loss 3.9693915843963623, acc=0.05000000074505806, loss=3.9693915843963623
train: epoch 8, loss 2.6710402965545654, acc=0.13583333790302277, loss=2.6710402965545654
test: epoch 8, loss 4.0176496505737305, acc=0.05000000074505806, loss=4.0176496505737305
train: epoch 9, loss 2.6577279567718506, acc=0.13272222876548767, loss=2.6577279567718506
test: epoch 9, loss 3.9258389472961426, acc=0.05000000074505806, loss=3.9258389472961426
train: epoch 10, loss 2.641659736633301, acc=0.14083333313465118, loss=2.641659736633301
test: epoch 10, loss 3.9279558658599854, acc=0.05277777835726738, loss=3.9279558658599854
train: epoch 11, loss 2.6356847286224365, acc=0.1402222216129303, loss=2.6356847286224365
test: epoch 11, loss 3.88084077835083, acc=0.05000000074505806, loss=3.88084077835083
train: epoch 12, loss 2.6342740058898926, acc=0.13761110603809357, loss=2.6342740058898926
test: epoch 12, loss 3.893899917602539, acc=0.05000000074505806, loss=3.893899917602539
train: epoch 13, loss 2.6158132553100586, acc=0.14072221517562866, loss=2.6158132553100586
test: epoch 13, loss 3.7923319339752197, acc=0.05000000074505806, loss=3.7923319339752197
train: epoch 14, loss 2.6059725284576416, acc=0.14149999618530273, loss=2.6059725284576416
test: epoch 14, loss 3.8067357540130615, acc=0.05277777835726738, loss=3.8067357540130615
train: epoch 15, loss 2.607386589050293, acc=0.14166666567325592, loss=2.607386589050293
test: epoch 15, loss 3.8211939334869385, acc=0.05000000074505806, loss=3.8211939334869385
train: epoch 16, loss 2.60290789604187, acc=0.14572222530841827, loss=2.60290789604187
test: epoch 16, loss 3.6802403926849365, acc=0.05277777835726738, loss=3.6802403926849365
train: epoch 17, loss 2.59342098236084, acc=0.14783333241939545, loss=2.59342098236084
test: epoch 17, loss 3.6717231273651123, acc=0.0694444477558136, loss=3.6717231273651123
train: epoch 18, loss 2.5798935890197754, acc=0.14683333039283752, loss=2.5798935890197754
test: epoch 18, loss 3.7146389484405518, acc=0.05833333358168602, loss=3.7146389484405518
train: epoch 19, loss 2.5706911087036133, acc=0.14777778089046478, loss=2.5706911087036133
test: epoch 19, loss 3.7036378383636475, acc=0.06388889253139496, loss=3.7036378383636475
train: epoch 20, loss 2.583721160888672, acc=0.14355555176734924, loss=2.583721160888672
test: epoch 20, loss 3.6445462703704834, acc=0.05833333358168602, loss=3.6445462703704834
train: epoch 21, loss 2.5732927322387695, acc=0.14577777683734894, loss=2.5732927322387695
test: epoch 21, loss 3.6228432655334473, acc=0.05000000074505806, loss=3.6228432655334473
train: epoch 22, loss 2.5770950317382812, acc=0.14300000667572021, loss=2.5770950317382812
test: epoch 22, loss 3.6120922565460205, acc=0.06111111119389534, loss=3.6120922565460205
train: epoch 23, loss 2.577420234680176, acc=0.14755555987358093, loss=2.577420234680176
test: epoch 23, loss 3.5994765758514404, acc=0.0694444477558136, loss=3.5994765758514404
train: epoch 24, loss 2.5757570266723633, acc=0.1493888944387436, loss=2.5757570266723633
test: epoch 24, loss 3.564258337020874, acc=0.06666667014360428, loss=3.564258337020874
train: epoch 25, loss 2.571355104446411, acc=0.14955554902553558, loss=2.571355104446411
test: epoch 25, loss 3.566065788269043, acc=0.06111111119389534, loss=3.566065788269043
train: epoch 26, loss 2.5624115467071533, acc=0.14661110937595367, loss=2.5624115467071533
test: epoch 26, loss 3.596806287765503, acc=0.05833333358168602, loss=3.596806287765503
train: epoch 27, loss 2.565556049346924, acc=0.14933332800865173, loss=2.565556049346924
test: epoch 27, loss 3.5820066928863525, acc=0.05833333358168602, loss=3.5820066928863525
train: epoch 28, loss 2.5665647983551025, acc=0.14577777683734894, loss=2.5665647983551025
test: epoch 28, loss 3.5918185710906982, acc=0.06111111119389534, loss=3.5918185710906982
train: epoch 29, loss 2.5641427040100098, acc=0.1480555534362793, loss=2.5641427040100098
test: epoch 29, loss 3.4361796379089355, acc=0.07222222536802292, loss=3.4361796379089355
train: epoch 30, loss 2.5627033710479736, acc=0.14722222089767456, loss=2.5627033710479736
test: epoch 30, loss 3.5675551891326904, acc=0.05833333358168602, loss=3.5675551891326904
train: epoch 31, loss 2.5551488399505615, acc=0.15066666901111603, loss=2.5551488399505615
test: epoch 31, loss 3.4001705646514893, acc=0.05833333358168602, loss=3.4001705646514893
train: epoch 32, loss 2.5506880283355713, acc=0.15316666662693024, loss=2.5506880283355713
test: epoch 32, loss 3.4501004219055176, acc=0.0694444477558136, loss=3.4501004219055176
train: epoch 33, loss 2.5526275634765625, acc=0.1462777704000473, loss=2.5526275634765625
test: epoch 33, loss 3.389925479888916, acc=0.06111111119389534, loss=3.389925479888916
train: epoch 34, loss 2.5449914932250977, acc=0.1501111090183258, loss=2.5449914932250977
test: epoch 34, loss 3.5065770149230957, acc=0.05833333358168602, loss=3.5065770149230957
train: epoch 35, loss 2.5348000526428223, acc=0.1498333364725113, loss=2.5348000526428223
test: epoch 35, loss 3.511286735534668, acc=0.07500000298023224, loss=3.511286735534668
train: epoch 36, loss 2.5238399505615234, acc=0.15127778053283691, loss=2.5238399505615234
test: epoch 36, loss 3.44887638092041, acc=0.07222222536802292, loss=3.44887638092041
train: epoch 37, loss 2.5326597690582275, acc=0.15033333003520966, loss=2.5326597690582275
test: epoch 37, loss 3.3617570400238037, acc=0.07777778059244156, loss=3.3617570400238037
train: epoch 38, loss 2.519151210784912, acc=0.1507222205400467, loss=2.519151210784912
test: epoch 38, loss 3.490999698638916, acc=0.06111111119389534, loss=3.490999698638916
train: epoch 39, loss 2.525770664215088, acc=0.15333333611488342, loss=2.525770664215088
test: epoch 39, loss 3.351184129714966, acc=0.06388889253139496, loss=3.351184129714966
train: epoch 40, loss 2.53826642036438, acc=0.14916667342185974, loss=2.53826642036438
test: epoch 40, loss 3.3299953937530518, acc=0.07500000298023224, loss=3.3299953937530518
train: epoch 41, loss 2.5168039798736572, acc=0.15355555713176727, loss=2.5168039798736572
test: epoch 41, loss 3.346524477005005, acc=0.06388889253139496, loss=3.346524477005005
train: epoch 42, loss 2.501911163330078, acc=0.15516667068004608, loss=2.501911163330078
test: epoch 42, loss 3.3355231285095215, acc=0.0833333358168602, loss=3.3355231285095215
train: epoch 43, loss 2.4969494342803955, acc=0.15716665983200073, loss=2.4969494342803955
test: epoch 43, loss 3.4650485515594482, acc=0.06388889253139496, loss=3.4650485515594482
train: epoch 44, loss 2.5090997219085693, acc=0.1566111147403717, loss=2.5090997219085693
test: epoch 44, loss 3.323620319366455, acc=0.06666667014360428, loss=3.323620319366455
train: epoch 45, loss 2.49582839012146, acc=0.1631111055612564, loss=2.49582839012146
test: epoch 45, loss 3.3362772464752197, acc=0.06666667014360428, loss=3.3362772464752197
train: epoch 46, loss 2.4875035285949707, acc=0.1592777818441391, loss=2.4875035285949707
test: epoch 46, loss 3.252575159072876, acc=0.0694444477558136, loss=3.252575159072876
train: epoch 47, loss 2.495769500732422, acc=0.15566666424274445, loss=2.495769500732422
test: epoch 47, loss 3.328886032104492, acc=0.05833333358168602, loss=3.328886032104492
train: epoch 48, loss 2.4826581478118896, acc=0.1589999943971634, loss=2.4826581478118896
test: epoch 48, loss 3.2943239212036133, acc=0.0555555559694767, loss=3.2943239212036133
train: epoch 49, loss 2.484558343887329, acc=0.16116666793823242, loss=2.484558343887329
test: epoch 49, loss 3.279041290283203, acc=0.07500000298023224, loss=3.279041290283203
train: epoch 50, loss 2.4861958026885986, acc=0.16172222793102264, loss=2.4861958026885986
test: epoch 50, loss 3.257645845413208, acc=0.0694444477558136, loss=3.257645845413208
train: epoch 51, loss 2.472592830657959, acc=0.16122221946716309, loss=2.472592830657959
test: epoch 51, loss 3.4010989665985107, acc=0.08055555820465088, loss=3.4010989665985107
train: epoch 52, loss 2.474820852279663, acc=0.15516667068004608, loss=2.474820852279663
test: epoch 52, loss 3.3282628059387207, acc=0.08055555820465088, loss=3.3282628059387207
train: epoch 53, loss 2.4589812755584717, acc=0.1628333330154419, loss=2.4589812755584717
test: epoch 53, loss 3.3337619304656982, acc=0.08055555820465088, loss=3.3337619304656982
train: epoch 54, loss 2.468731641769409, acc=0.16255556046962738, loss=2.468731641769409
test: epoch 54, loss 3.203125476837158, acc=0.0833333358168602, loss=3.203125476837158
train: epoch 55, loss 2.457430362701416, acc=0.16144444048404694, loss=2.457430362701416
test: epoch 55, loss 3.1843292713165283, acc=0.07777778059244156, loss=3.1843292713165283
train: epoch 56, loss 2.4591636657714844, acc=0.16733333468437195, loss=2.4591636657714844
test: epoch 56, loss 3.1834447383880615, acc=0.06666667014360428, loss=3.1834447383880615
train: epoch 57, loss 2.4430997371673584, acc=0.16855555772781372, loss=2.4430997371673584
test: epoch 57, loss 3.220024347305298, acc=0.0833333358168602, loss=3.220024347305298
train: epoch 58, loss 2.449988842010498, acc=0.16366666555404663, loss=2.449988842010498
test: epoch 58, loss 3.257915735244751, acc=0.0833333358168602, loss=3.257915735244751
train: epoch 59, loss 2.449059009552002, acc=0.1631111055612564, loss=2.449059009552002
test: epoch 59, loss 3.2081081867218018, acc=0.07777778059244156, loss=3.2081081867218018
train: epoch 60, loss 2.44073486328125, acc=0.16733333468437195, loss=2.44073486328125
test: epoch 60, loss 3.182321548461914, acc=0.08055555820465088, loss=3.182321548461914
train: epoch 61, loss 2.4410157203674316, acc=0.16433332860469818, loss=2.4410157203674316
test: epoch 61, loss 3.1355791091918945, acc=0.08055555820465088, loss=3.1355791091918945
train: epoch 62, loss 2.4280788898468018, acc=0.16811111569404602, loss=2.4280788898468018
test: epoch 62, loss 3.1266801357269287, acc=0.07777778059244156, loss=3.1266801357269287
train: epoch 63, loss 2.4333138465881348, acc=0.17077778279781342, loss=2.4333138465881348
test: epoch 63, loss 3.1831283569335938, acc=0.09166666865348816, loss=3.1831283569335938
train: epoch 64, loss 2.4373724460601807, acc=0.16366666555404663, loss=2.4373724460601807
test: epoch 64, loss 3.208430290222168, acc=0.07777778059244156, loss=3.208430290222168
train: epoch 65, loss 2.436053991317749, acc=0.1637222170829773, loss=2.436053991317749
test: epoch 65, loss 3.2409770488739014, acc=0.07500000298023224, loss=3.2409770488739014
train: epoch 66, loss 2.424112319946289, acc=0.16688889265060425, loss=2.424112319946289
test: epoch 66, loss 3.1683132648468018, acc=0.07500000298023224, loss=3.1683132648468018
train: epoch 67, loss 2.4286623001098633, acc=0.16338889300823212, loss=2.4286623001098633
test: epoch 67, loss 3.0850441455841064, acc=0.0833333358168602, loss=3.0850441455841064
train: epoch 68, loss 2.416846513748169, acc=0.17038889229297638, loss=2.416846513748169
test: epoch 68, loss 3.17525315284729, acc=0.08611111342906952, loss=3.17525315284729
train: epoch 69, loss 2.4083495140075684, acc=0.17005555331707, loss=2.4083495140075684
test: epoch 69, loss 3.1497232913970947, acc=0.06388889253139496, loss=3.1497232913970947
train: epoch 70, loss 2.4209678173065186, acc=0.16955555975437164, loss=2.4209678173065186
test: epoch 70, loss 3.179429769515991, acc=0.0833333358168602, loss=3.179429769515991
train: epoch 71, loss 2.4125380516052246, acc=0.1731666624546051, loss=2.4125380516052246
test: epoch 71, loss 3.150055408477783, acc=0.05833333358168602, loss=3.150055408477783
train: epoch 72, loss 2.4069244861602783, acc=0.17122222483158112, loss=2.4069244861602783
test: epoch 72, loss 3.161971092224121, acc=0.0833333358168602, loss=3.161971092224121
train: epoch 73, loss 2.4090726375579834, acc=0.17427778244018555, loss=2.4090726375579834
test: epoch 73, loss 3.1156604290008545, acc=0.08611111342906952, loss=3.1156604290008545
train: epoch 74, loss 2.3996124267578125, acc=0.16905555129051208, loss=2.3996124267578125
test: epoch 74, loss 3.239844560623169, acc=0.07777778059244156, loss=3.239844560623169
train: epoch 75, loss 2.399188756942749, acc=0.17088888585567474, loss=2.399188756942749
test: epoch 75, loss 3.2060599327087402, acc=0.0833333358168602, loss=3.2060599327087402
train: epoch 76, loss 2.404428005218506, acc=0.17283333837985992, loss=2.404428005218506
test: epoch 76, loss 3.164320230484009, acc=0.08055555820465088, loss=3.164320230484009
train: epoch 77, loss 2.40641713142395, acc=0.1736111044883728, loss=2.40641713142395
test: epoch 77, loss 3.1924386024475098, acc=0.07777778059244156, loss=3.1924386024475098
train: epoch 78, loss 2.391848564147949, acc=0.17044444382190704, loss=2.391848564147949
test: epoch 78, loss 3.203559160232544, acc=0.08055555820465088, loss=3.203559160232544
train: epoch 79, loss 2.379188060760498, acc=0.17266666889190674, loss=2.379188060760498
test: epoch 79, loss 3.1636672019958496, acc=0.0833333358168602, loss=3.1636672019958496
train: epoch 80, loss 2.3797359466552734, acc=0.17755556106567383, loss=2.3797359466552734
test: epoch 80, loss 3.1616740226745605, acc=0.08055555820465088, loss=3.1616740226745605
train: epoch 81, loss 2.3812081813812256, acc=0.1727222204208374, loss=2.3812081813812256
test: epoch 81, loss 3.1487467288970947, acc=0.08055555820465088, loss=3.1487467288970947
train: epoch 82, loss 2.391453742980957, acc=0.17372222244739532, loss=2.391453742980957
test: epoch 82, loss 3.146500825881958, acc=0.07500000298023224, loss=3.146500825881958
train: epoch 83, loss 2.3718481063842773, acc=0.17883333563804626, loss=2.3718481063842773
test: epoch 83, loss 3.2267870903015137, acc=0.07777778059244156, loss=3.2267870903015137
train: epoch 84, loss 2.380261182785034, acc=0.1722777783870697, loss=2.380261182785034
test: epoch 84, loss 3.161196231842041, acc=0.09166666865348816, loss=3.161196231842041
train: epoch 85, loss 2.3712778091430664, acc=0.17177778482437134, loss=2.3712778091430664
test: epoch 85, loss 3.1330347061157227, acc=0.08888889104127884, loss=3.1330347061157227
train: epoch 86, loss 2.3628289699554443, acc=0.17511111497879028, loss=2.3628289699554443
test: epoch 86, loss 3.1460700035095215, acc=0.09444444626569748, loss=3.1460700035095215
train: epoch 87, loss 2.3718857765197754, acc=0.17688888311386108, loss=2.3718857765197754
test: epoch 87, loss 3.1515145301818848, acc=0.09166666865348816, loss=3.1515145301818848
train: epoch 88, loss 2.354947805404663, acc=0.1765555590391159, loss=2.354947805404663
test: epoch 88, loss 3.066398859024048, acc=0.09166666865348816, loss=3.066398859024048
train: epoch 89, loss 2.3624486923217773, acc=0.17338888347148895, loss=2.3624486923217773
test: epoch 89, loss 3.1370437145233154, acc=0.09166666865348816, loss=3.1370437145233154
train: epoch 90, loss 2.362234115600586, acc=0.17961111664772034, loss=2.362234115600586
test: epoch 90, loss 3.0668082237243652, acc=0.09166666865348816, loss=3.0668082237243652
train: epoch 91, loss 2.3638839721679688, acc=0.17927777767181396, loss=2.3638839721679688
test: epoch 91, loss 3.0400519371032715, acc=0.08888889104127884, loss=3.0400519371032715
train: epoch 92, loss 2.366100788116455, acc=0.17161111533641815, loss=2.366100788116455
test: epoch 92, loss 3.006402015686035, acc=0.08888889104127884, loss=3.006402015686035
train: epoch 93, loss 2.3420376777648926, acc=0.1798333376646042, loss=2.3420376777648926
test: epoch 93, loss 3.0723321437835693, acc=0.0833333358168602, loss=3.0723321437835693
train: epoch 94, loss 2.3487114906311035, acc=0.18250000476837158, loss=2.3487114906311035
test: epoch 94, loss 3.112271547317505, acc=0.08611111342906952, loss=3.112271547317505
train: epoch 95, loss 2.354097604751587, acc=0.17916665971279144, loss=2.354097604751587
test: epoch 95, loss 3.0526821613311768, acc=0.0833333358168602, loss=3.0526821613311768
train: epoch 96, loss 2.3496997356414795, acc=0.18244443833827972, loss=2.3496997356414795
test: epoch 96, loss 3.1296544075012207, acc=0.09166666865348816, loss=3.1296544075012207
train: epoch 97, loss 2.353689670562744, acc=0.1827777773141861, loss=2.353689670562744
test: epoch 97, loss 3.1037096977233887, acc=0.08611111342906952, loss=3.1037096977233887
train: epoch 98, loss 2.3398287296295166, acc=0.1789444386959076, loss=2.3398287296295166
test: epoch 98, loss 3.090139865875244, acc=0.08055555820465088, loss=3.090139865875244
train: epoch 99, loss 2.3365652561187744, acc=0.1774444431066513, loss=2.3365652561187744
test: epoch 99, loss 3.01910138130188, acc=0.0555555559694767, loss=3.01910138130188
train: epoch 100, loss 2.350911855697632, acc=0.1792222261428833, loss=2.350911855697632
test: epoch 100, loss 3.0887961387634277, acc=0.06666667014360428, loss=3.0887961387634277
train: epoch 101, loss 2.3398056030273438, acc=0.18211111426353455, loss=2.3398056030273438
test: epoch 101, loss 3.0222291946411133, acc=0.08611111342906952, loss=3.0222291946411133
train: epoch 102, loss 2.3335118293762207, acc=0.18255555629730225, loss=2.3335118293762207
test: epoch 102, loss 3.0759596824645996, acc=0.08055555820465088, loss=3.0759596824645996
train: epoch 103, loss 2.320603370666504, acc=0.18333333730697632, loss=2.320603370666504
test: epoch 103, loss 3.0721898078918457, acc=0.07222222536802292, loss=3.0721898078918457
train: epoch 104, loss 2.324756622314453, acc=0.1881111115217209, loss=2.324756622314453
test: epoch 104, loss 3.1169047355651855, acc=0.07777778059244156, loss=3.1169047355651855
train: epoch 105, loss 2.3396897315979004, acc=0.17961111664772034, loss=2.3396897315979004
test: epoch 105, loss 3.0162365436553955, acc=0.09444444626569748, loss=3.0162365436553955
train: epoch 106, loss 2.3211777210235596, acc=0.1835000067949295, loss=2.3211777210235596
test: epoch 106, loss 3.0564935207366943, acc=0.08888889104127884, loss=3.0564935207366943
train: epoch 107, loss 2.3222570419311523, acc=0.18333333730697632, loss=2.3222570419311523
test: epoch 107, loss 3.0987772941589355, acc=0.07222222536802292, loss=3.0987772941589355
train: epoch 108, loss 2.327820062637329, acc=0.18061110377311707, loss=2.327820062637329
test: epoch 108, loss 3.1008546352386475, acc=0.08611111342906952, loss=3.1008546352386475
train: epoch 109, loss 2.3319666385650635, acc=0.18266665935516357, loss=2.3319666385650635
test: epoch 109, loss 3.0086328983306885, acc=0.09166666865348816, loss=3.0086328983306885
train: epoch 110, loss 2.3241405487060547, acc=0.18299999833106995, loss=2.3241405487060547
test: epoch 110, loss 3.0698740482330322, acc=0.0694444477558136, loss=3.0698740482330322
train: epoch 111, loss 2.3135902881622314, acc=0.18816666305065155, loss=2.3135902881622314
test: epoch 111, loss 3.0104825496673584, acc=0.08611111342906952, loss=3.0104825496673584
train: epoch 112, loss 2.311199426651001, acc=0.18194444477558136, loss=2.311199426651001
test: epoch 112, loss 3.038205146789551, acc=0.0833333358168602, loss=3.038205146789551
train: epoch 113, loss 2.3079068660736084, acc=0.18516667187213898, loss=2.3079068660736084
test: epoch 113, loss 3.0788838863372803, acc=0.08611111342906952, loss=3.0788838863372803
train: epoch 114, loss 2.307588815689087, acc=0.18583333492279053, loss=2.307588815689087
test: epoch 114, loss 3.0926637649536133, acc=0.09166666865348816, loss=3.0926637649536133
train: epoch 115, loss 2.2931642532348633, acc=0.18877777457237244, loss=2.2931642532348633
test: epoch 115, loss 3.096327066421509, acc=0.07500000298023224, loss=3.096327066421509
train: epoch 116, loss 2.3105101585388184, acc=0.1875, loss=2.3105101585388184
test: epoch 116, loss 3.0559499263763428, acc=0.0972222238779068, loss=3.0559499263763428
train: epoch 117, loss 2.2904152870178223, acc=0.19322222471237183, loss=2.2904152870178223
test: epoch 117, loss 3.088773012161255, acc=0.09444444626569748, loss=3.088773012161255
train: epoch 118, loss 2.298485279083252, acc=0.18827778100967407, loss=2.298485279083252
test: epoch 118, loss 2.9633216857910156, acc=0.09444444626569748, loss=2.9633216857910156
train: epoch 119, loss 2.295323371887207, acc=0.193388894200325, loss=2.295323371887207
test: epoch 119, loss 2.9966790676116943, acc=0.08888889104127884, loss=2.9966790676116943
train: epoch 120, loss 2.2901554107666016, acc=0.1940000057220459, loss=2.2901554107666016
test: epoch 120, loss 3.0038676261901855, acc=0.08611111342906952, loss=3.0038676261901855
train: epoch 121, loss 2.2855429649353027, acc=0.19099999964237213, loss=2.2855429649353027
test: epoch 121, loss 3.053931474685669, acc=0.10277777910232544, loss=3.053931474685669
train: epoch 122, loss 2.2901992797851562, acc=0.193555548787117, loss=2.2901992797851562
test: epoch 122, loss 3.072903871536255, acc=0.09444444626569748, loss=3.072903871536255
train: epoch 123, loss 2.295382022857666, acc=0.18983332812786102, loss=2.295382022857666
test: epoch 123, loss 3.0920662879943848, acc=0.10277777910232544, loss=3.0920662879943848
train: epoch 124, loss 2.296818256378174, acc=0.1906111091375351, loss=2.296818256378174
test: epoch 124, loss 3.093834161758423, acc=0.0972222238779068, loss=3.093834161758423
train: epoch 125, loss 2.2866721153259277, acc=0.1895555555820465, loss=2.2866721153259277
test: epoch 125, loss 2.9149272441864014, acc=0.09444444626569748, loss=2.9149272441864014
train: epoch 126, loss 2.2736763954162598, acc=0.19633333384990692, loss=2.2736763954162598
test: epoch 126, loss 3.1112661361694336, acc=0.10555555671453476, loss=3.1112661361694336
train: epoch 127, loss 2.2935516834259033, acc=0.1973888874053955, loss=2.2935516834259033
test: epoch 127, loss 2.9652557373046875, acc=0.08611111342906952, loss=2.9652557373046875
train: epoch 128, loss 2.2680327892303467, acc=0.19388888776302338, loss=2.2680327892303467
test: epoch 128, loss 3.0156590938568115, acc=0.10000000149011612, loss=3.0156590938568115
train: epoch 129, loss 2.2694010734558105, acc=0.1975555568933487, loss=2.2694010734558105
test: epoch 129, loss 2.899747371673584, acc=0.0972222238779068, loss=2.899747371673584
train: epoch 130, loss 2.2686262130737305, acc=0.20100000500679016, loss=2.2686262130737305
test: epoch 130, loss 2.86729097366333, acc=0.10000000149011612, loss=2.86729097366333
train: epoch 131, loss 2.2756001949310303, acc=0.19633333384990692, loss=2.2756001949310303
test: epoch 131, loss 2.9349379539489746, acc=0.10277777910232544, loss=2.9349379539489746
train: epoch 132, loss 2.2649037837982178, acc=0.1960555613040924, loss=2.2649037837982178
test: epoch 132, loss 2.9615800380706787, acc=0.10000000149011612, loss=2.9615800380706787
train: epoch 133, loss 2.255110502243042, acc=0.1979999989271164, loss=2.255110502243042
test: epoch 133, loss 3.079216957092285, acc=0.09444444626569748, loss=3.079216957092285
train: epoch 134, loss 2.2568719387054443, acc=0.19477777183055878, loss=2.2568719387054443
test: epoch 134, loss 2.94181752204895, acc=0.10000000149011612, loss=2.94181752204895
train: epoch 135, loss 2.252814292907715, acc=0.2006666660308838, loss=2.252814292907715
test: epoch 135, loss 3.0041284561157227, acc=0.10555555671453476, loss=3.0041284561157227
train: epoch 136, loss 2.257570266723633, acc=0.19688889384269714, loss=2.257570266723633
test: epoch 136, loss 2.982933282852173, acc=0.10833333432674408, loss=2.982933282852173
train: epoch 137, loss 2.2645890712738037, acc=0.19822221994400024, loss=2.2645890712738037
test: epoch 137, loss 2.936720609664917, acc=0.08611111342906952, loss=2.936720609664917
train: epoch 138, loss 2.2515060901641846, acc=0.20083333551883698, loss=2.2515060901641846
test: epoch 138, loss 2.954829692840576, acc=0.10277777910232544, loss=2.954829692840576
train: epoch 139, loss 2.235137939453125, acc=0.20622222125530243, loss=2.235137939453125
test: epoch 139, loss 2.9429543018341064, acc=0.09444444626569748, loss=2.9429543018341064
train: epoch 140, loss 2.2467782497406006, acc=0.2052222192287445, loss=2.2467782497406006
test: epoch 140, loss 2.9776272773742676, acc=0.0972222238779068, loss=2.9776272773742676
train: epoch 141, loss 2.2344446182250977, acc=0.20622222125530243, loss=2.2344446182250977
test: epoch 141, loss 3.0154032707214355, acc=0.09444444626569748, loss=3.0154032707214355
train: epoch 142, loss 2.2493550777435303, acc=0.20399999618530273, loss=2.2493550777435303
test: epoch 142, loss 2.9882194995880127, acc=0.10277777910232544, loss=2.9882194995880127
train: epoch 143, loss 2.244821071624756, acc=0.20561110973358154, loss=2.244821071624756
test: epoch 143, loss 2.9209320545196533, acc=0.09444444626569748, loss=2.9209320545196533
train: epoch 144, loss 2.221813440322876, acc=0.20827777683734894, loss=2.221813440322876
test: epoch 144, loss 2.906339645385742, acc=0.10000000149011612, loss=2.906339645385742
train: epoch 145, loss 2.2277841567993164, acc=0.20683333277702332, loss=2.2277841567993164
test: epoch 145, loss 2.9623115062713623, acc=0.10000000149011612, loss=2.9623115062713623
train: epoch 146, loss 2.227727174758911, acc=0.20633333921432495, loss=2.227727174758911
test: epoch 146, loss 2.9177334308624268, acc=0.09444444626569748, loss=2.9177334308624268
train: epoch 147, loss 2.2276182174682617, acc=0.20266667008399963, loss=2.2276182174682617
test: epoch 147, loss 2.943936824798584, acc=0.07500000298023224, loss=2.943936824798584
train: epoch 148, loss 2.235823631286621, acc=0.20577777922153473, loss=2.235823631286621
test: epoch 148, loss 2.9402360916137695, acc=0.0833333358168602, loss=2.9402360916137695
train: epoch 149, loss 2.229123830795288, acc=0.2134999930858612, loss=2.229123830795288
test: epoch 149, loss 2.902631998062134, acc=0.0972222238779068, loss=2.902631998062134
train: epoch 150, loss 2.2371809482574463, acc=0.20927777886390686, loss=2.2371809482574463
test: epoch 150, loss 2.972829580307007, acc=0.0833333358168602, loss=2.972829580307007
