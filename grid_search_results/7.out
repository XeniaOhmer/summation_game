# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.995", "--one_hot=0", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1959917832, receiver_embed_dim=32, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1959917832, receiver_embed_dim=32, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.9871745109558105, acc=0.09066666662693024, loss=2.9871745109558105
test: epoch 1, loss 3.9222512245178223, acc=0.05000000074505806, loss=3.9222512245178223
train: epoch 2, loss 2.4556782245635986, acc=0.15611110627651215, loss=2.4556782245635986
test: epoch 2, loss 3.5512430667877197, acc=0.0694444477558136, loss=3.5512430667877197
train: epoch 3, loss 2.2611148357391357, acc=0.19861111044883728, loss=2.2611148357391357
test: epoch 3, loss 2.984097480773926, acc=0.09166666865348816, loss=2.984097480773926
train: epoch 4, loss 2.109415292739868, acc=0.23088888823986053, loss=2.109415292739868
test: epoch 4, loss 2.8479628562927246, acc=0.12222222238779068, loss=2.8479628562927246
train: epoch 5, loss 2.0054337978363037, acc=0.2569444477558136, loss=2.0054337978363037
test: epoch 5, loss 2.8749308586120605, acc=0.10833333432674408, loss=2.8749308586120605
train: epoch 6, loss 1.9505661725997925, acc=0.26827776432037354, loss=1.9505661725997925
test: epoch 6, loss 2.8566360473632812, acc=0.12222222238779068, loss=2.8566360473632812
train: epoch 7, loss 1.900527000427246, acc=0.28255555033683777, loss=1.900527000427246
test: epoch 7, loss 2.794201135635376, acc=0.125, loss=2.794201135635376
train: epoch 8, loss 1.8530032634735107, acc=0.29572221636772156, loss=1.8530032634735107
test: epoch 8, loss 2.7452967166900635, acc=0.12222222238779068, loss=2.7452967166900635
train: epoch 9, loss 1.816686987876892, acc=0.30250000953674316, loss=1.816686987876892
test: epoch 9, loss 2.5542397499084473, acc=0.125, loss=2.5542397499084473
train: epoch 10, loss 1.7758592367172241, acc=0.31744444370269775, loss=1.7758592367172241
test: epoch 10, loss 2.501957893371582, acc=0.1527777761220932, loss=2.501957893371582
train: epoch 11, loss 1.7646864652633667, acc=0.3173888921737671, loss=1.7646864652633667
test: epoch 11, loss 2.5084216594696045, acc=0.16388888657093048, loss=2.5084216594696045
train: epoch 12, loss 1.7219293117523193, acc=0.3277222216129303, loss=1.7219293117523193
test: epoch 12, loss 2.3092639446258545, acc=0.18611110746860504, loss=2.3092639446258545
train: epoch 13, loss 1.685417652130127, acc=0.3401666581630707, loss=1.685417652130127
test: epoch 13, loss 2.462322473526001, acc=0.17222222685813904, loss=2.462322473526001
train: epoch 14, loss 1.666460633277893, acc=0.3470555543899536, loss=1.666460633277893
test: epoch 14, loss 2.5084445476531982, acc=0.17222222685813904, loss=2.5084445476531982
train: epoch 15, loss 1.6432636976242065, acc=0.3538888990879059, loss=1.6432636976242065
test: epoch 15, loss 2.44519305229187, acc=0.17499999701976776, loss=2.44519305229187
train: epoch 16, loss 1.6257697343826294, acc=0.3600555658340454, loss=1.6257697343826294
test: epoch 16, loss 2.2322962284088135, acc=0.16388888657093048, loss=2.2322962284088135
train: epoch 17, loss 1.6016690731048584, acc=0.3704444468021393, loss=1.6016690731048584
test: epoch 17, loss 2.2701921463012695, acc=0.20555555820465088, loss=2.2701921463012695
train: epoch 18, loss 1.559760332107544, acc=0.3838333189487457, loss=1.559760332107544
test: epoch 18, loss 2.220590591430664, acc=0.1666666716337204, loss=2.220590591430664
train: epoch 19, loss 1.5651391744613647, acc=0.3851666748523712, loss=1.5651391744613647
test: epoch 19, loss 2.1023013591766357, acc=0.22777777910232544, loss=2.1023013591766357
train: epoch 20, loss 1.5382392406463623, acc=0.39722222089767456, loss=1.5382392406463623
test: epoch 20, loss 2.2915916442871094, acc=0.17499999701976776, loss=2.2915916442871094
train: epoch 21, loss 1.505592703819275, acc=0.40149998664855957, loss=1.505592703819275
test: epoch 21, loss 2.2131805419921875, acc=0.1805555522441864, loss=2.2131805419921875
train: epoch 22, loss 1.5013842582702637, acc=0.4047222137451172, loss=1.5013842582702637
test: epoch 22, loss 2.2484099864959717, acc=0.2361111044883728, loss=2.2484099864959717
train: epoch 23, loss 1.479271411895752, acc=0.4108888804912567, loss=1.479271411895752
test: epoch 23, loss 2.0780086517333984, acc=0.17777778208255768, loss=2.0780086517333984
train: epoch 24, loss 1.4731526374816895, acc=0.41405555605888367, loss=1.4731526374816895
test: epoch 24, loss 2.059786319732666, acc=0.22499999403953552, loss=2.059786319732666
train: epoch 25, loss 1.470757007598877, acc=0.41227778792381287, loss=1.470757007598877
test: epoch 25, loss 2.054591178894043, acc=0.21111111342906952, loss=2.054591178894043
train: epoch 26, loss 1.4563546180725098, acc=0.4230000078678131, loss=1.4563546180725098
test: epoch 26, loss 2.069748878479004, acc=0.20000000298023224, loss=2.069748878479004
train: epoch 27, loss 1.4561128616333008, acc=0.42149999737739563, loss=1.4561128616333008
test: epoch 27, loss 2.125864028930664, acc=0.17222222685813904, loss=2.125864028930664
train: epoch 28, loss 1.4302418231964111, acc=0.42722222208976746, loss=1.4302418231964111
test: epoch 28, loss 2.0294406414031982, acc=0.22777777910232544, loss=2.0294406414031982
train: epoch 29, loss 1.433637022972107, acc=0.43138888478279114, loss=1.433637022972107
test: epoch 29, loss 1.963683843612671, acc=0.20555555820465088, loss=1.963683843612671
train: epoch 30, loss 1.4037138223648071, acc=0.4331111013889313, loss=1.4037138223648071
test: epoch 30, loss 1.990683674812317, acc=0.22499999403953552, loss=1.990683674812317
train: epoch 31, loss 1.3956339359283447, acc=0.44244444370269775, loss=1.3956339359283447
test: epoch 31, loss 2.086521863937378, acc=0.2083333283662796, loss=2.086521863937378
train: epoch 32, loss 1.3968455791473389, acc=0.4363333284854889, loss=1.3968455791473389
test: epoch 32, loss 1.9518078565597534, acc=0.23055554926395416, loss=1.9518078565597534
train: epoch 33, loss 1.4007147550582886, acc=0.4345555603504181, loss=1.4007147550582886
test: epoch 33, loss 1.9513896703720093, acc=0.18333333730697632, loss=1.9513896703720093
train: epoch 34, loss 1.385175108909607, acc=0.4375, loss=1.385175108909607
test: epoch 34, loss 1.8933926820755005, acc=0.25555557012557983, loss=1.8933926820755005
train: epoch 35, loss 1.3800054788589478, acc=0.4429999887943268, loss=1.3800054788589478
test: epoch 35, loss 1.9374637603759766, acc=0.2666666805744171, loss=1.9374637603759766
train: epoch 36, loss 1.3648667335510254, acc=0.4420555531978607, loss=1.3648667335510254
test: epoch 36, loss 1.896226167678833, acc=0.22499999403953552, loss=1.896226167678833
train: epoch 37, loss 1.371770977973938, acc=0.44761112332344055, loss=1.371770977973938
test: epoch 37, loss 1.8640159368515015, acc=0.2666666805744171, loss=1.8640159368515015
train: epoch 38, loss 1.3572049140930176, acc=0.4477222263813019, loss=1.3572049140930176
test: epoch 38, loss 1.8826663494110107, acc=0.2666666805744171, loss=1.8826663494110107
train: epoch 39, loss 1.3545488119125366, acc=0.4513888955116272, loss=1.3545488119125366
test: epoch 39, loss 1.8920010328292847, acc=0.25833332538604736, loss=1.8920010328292847
train: epoch 40, loss 1.3305773735046387, acc=0.4575555622577667, loss=1.3305773735046387
test: epoch 40, loss 1.8072031736373901, acc=0.24722221493721008, loss=1.8072031736373901
train: epoch 41, loss 1.3485543727874756, acc=0.4578889012336731, loss=1.3485543727874756
test: epoch 41, loss 1.839699625968933, acc=0.25555557012557983, loss=1.839699625968933
train: epoch 42, loss 1.3422657251358032, acc=0.4556666612625122, loss=1.3422657251358032
test: epoch 42, loss 1.9026718139648438, acc=0.23055554926395416, loss=1.9026718139648438
train: epoch 43, loss 1.3431129455566406, acc=0.46094444394111633, loss=1.3431129455566406
test: epoch 43, loss 1.7177371978759766, acc=0.3194444477558136, loss=1.7177371978759766
train: epoch 44, loss 1.3224722146987915, acc=0.4653888940811157, loss=1.3224722146987915
test: epoch 44, loss 1.91097891330719, acc=0.21944443881511688, loss=1.91097891330719
train: epoch 45, loss 1.3161838054656982, acc=0.4690000116825104, loss=1.3161838054656982
test: epoch 45, loss 1.762642741203308, acc=0.3055555522441864, loss=1.762642741203308
train: epoch 46, loss 1.3213363885879517, acc=0.45883333683013916, loss=1.3213363885879517
test: epoch 46, loss 1.7939693927764893, acc=0.2777777910232544, loss=1.7939693927764893
train: epoch 47, loss 1.32454514503479, acc=0.4652777910232544, loss=1.32454514503479
test: epoch 47, loss 1.826878547668457, acc=0.30000001192092896, loss=1.826878547668457
train: epoch 48, loss 1.2967164516448975, acc=0.4713333249092102, loss=1.2967164516448975
test: epoch 48, loss 1.7938088178634644, acc=0.3166666626930237, loss=1.7938088178634644
train: epoch 49, loss 1.2917600870132446, acc=0.4754999876022339, loss=1.2917600870132446
test: epoch 49, loss 1.8256007432937622, acc=0.26944443583488464, loss=1.8256007432937622
train: epoch 50, loss 1.283940076828003, acc=0.47966668009757996, loss=1.283940076828003
test: epoch 50, loss 1.6626346111297607, acc=0.28333333134651184, loss=1.6626346111297607
train: epoch 51, loss 1.2806851863861084, acc=0.4794999957084656, loss=1.2806851863861084
test: epoch 51, loss 1.7496758699417114, acc=0.25833332538604736, loss=1.7496758699417114
train: epoch 52, loss 1.2885088920593262, acc=0.47866666316986084, loss=1.2885088920593262
test: epoch 52, loss 1.7601935863494873, acc=0.2916666567325592, loss=1.7601935863494873
train: epoch 53, loss 1.2934954166412354, acc=0.4790000021457672, loss=1.2934954166412354
test: epoch 53, loss 1.7766798734664917, acc=0.2888889014720917, loss=1.7766798734664917
train: epoch 54, loss 1.2638903856277466, acc=0.48338890075683594, loss=1.2638903856277466
test: epoch 54, loss 1.6423877477645874, acc=0.3333333432674408, loss=1.6423877477645874
train: epoch 55, loss 1.267228364944458, acc=0.4938333332538605, loss=1.267228364944458
test: epoch 55, loss 1.7430789470672607, acc=0.23333333432674408, loss=1.7430789470672607
train: epoch 56, loss 1.2597599029541016, acc=0.4927777647972107, loss=1.2597599029541016
test: epoch 56, loss 1.6116700172424316, acc=0.32499998807907104, loss=1.6116700172424316
train: epoch 57, loss 1.2495416402816772, acc=0.49138888716697693, loss=1.2495416402816772
test: epoch 57, loss 1.5888739824295044, acc=0.33888888359069824, loss=1.5888739824295044
train: epoch 58, loss 1.2456860542297363, acc=0.4943888783454895, loss=1.2456860542297363
test: epoch 58, loss 1.6785075664520264, acc=0.3166666626930237, loss=1.6785075664520264
train: epoch 59, loss 1.2256351709365845, acc=0.5007222294807434, loss=1.2256351709365845
test: epoch 59, loss 1.6289921998977661, acc=0.3083333373069763, loss=1.6289921998977661
train: epoch 60, loss 1.2216182947158813, acc=0.5024999976158142, loss=1.2216182947158813
test: epoch 60, loss 1.574212670326233, acc=0.34166666865348816, loss=1.574212670326233
train: epoch 61, loss 1.2340515851974487, acc=0.5036110877990723, loss=1.2340515851974487
test: epoch 61, loss 1.6159708499908447, acc=0.2916666567325592, loss=1.6159708499908447
train: epoch 62, loss 1.206244707107544, acc=0.5110555291175842, loss=1.206244707107544
test: epoch 62, loss 1.704620599746704, acc=0.2944444417953491, loss=1.704620599746704
train: epoch 63, loss 1.193210482597351, acc=0.5145555734634399, loss=1.193210482597351
test: epoch 63, loss 1.6934211254119873, acc=0.33888888359069824, loss=1.6934211254119873
train: epoch 64, loss 1.1738481521606445, acc=0.5236666798591614, loss=1.1738481521606445
test: epoch 64, loss 1.6166561841964722, acc=0.35555556416511536, loss=1.6166561841964722
train: epoch 65, loss 1.1867625713348389, acc=0.5142777562141418, loss=1.1867625713348389
test: epoch 65, loss 1.5748664140701294, acc=0.3444444537162781, loss=1.5748664140701294
train: epoch 66, loss 1.1867008209228516, acc=0.5218333601951599, loss=1.1867008209228516
test: epoch 66, loss 1.5459762811660767, acc=0.3222222328186035, loss=1.5459762811660767
train: epoch 67, loss 1.1816405057907104, acc=0.5244444608688354, loss=1.1816405057907104
test: epoch 67, loss 1.6747057437896729, acc=0.35277777910232544, loss=1.6747057437896729
train: epoch 68, loss 1.168607473373413, acc=0.5274444222450256, loss=1.168607473373413
test: epoch 68, loss 1.5254817008972168, acc=0.36944442987442017, loss=1.5254817008972168
train: epoch 69, loss 1.1511316299438477, acc=0.5312222242355347, loss=1.1511316299438477
test: epoch 69, loss 1.5655308961868286, acc=0.3611111044883728, loss=1.5655308961868286
train: epoch 70, loss 1.1659047603607178, acc=0.5339444279670715, loss=1.1659047603607178
test: epoch 70, loss 1.6036889553070068, acc=0.3194444477558136, loss=1.6036889553070068
train: epoch 71, loss 1.1288220882415771, acc=0.542888879776001, loss=1.1288220882415771
test: epoch 71, loss 1.583762526512146, acc=0.3638888895511627, loss=1.583762526512146
train: epoch 72, loss 1.129167914390564, acc=0.5437222123146057, loss=1.129167914390564
test: epoch 72, loss 1.5505398511886597, acc=0.33888888359069824, loss=1.5505398511886597
train: epoch 73, loss 1.1437932252883911, acc=0.5404999852180481, loss=1.1437932252883911
test: epoch 73, loss 1.50436270236969, acc=0.38055557012557983, loss=1.50436270236969
train: epoch 74, loss 1.1145790815353394, acc=0.5484444499015808, loss=1.1145790815353394
test: epoch 74, loss 1.6065888404846191, acc=0.32499998807907104, loss=1.6065888404846191
train: epoch 75, loss 1.116963505744934, acc=0.5487222075462341, loss=1.116963505744934
test: epoch 75, loss 1.464700698852539, acc=0.39722222089767456, loss=1.464700698852539
train: epoch 76, loss 1.1306326389312744, acc=0.5481666922569275, loss=1.1306326389312744
test: epoch 76, loss 1.5603758096694946, acc=0.2944444417953491, loss=1.5603758096694946
train: epoch 77, loss 1.0895256996154785, acc=0.5603888630867004, loss=1.0895256996154785
test: epoch 77, loss 1.6499892473220825, acc=0.3222222328186035, loss=1.6499892473220825
train: epoch 78, loss 1.1144481897354126, acc=0.5528888702392578, loss=1.1144481897354126
test: epoch 78, loss 1.4282734394073486, acc=0.3777777850627899, loss=1.4282734394073486
train: epoch 79, loss 1.0795080661773682, acc=0.5613333582878113, loss=1.0795080661773682
test: epoch 79, loss 1.5602390766143799, acc=0.3472222089767456, loss=1.5602390766143799
train: epoch 80, loss 1.1027967929840088, acc=0.5601666569709778, loss=1.1027967929840088
test: epoch 80, loss 1.460545539855957, acc=0.3361110985279083, loss=1.460545539855957
train: epoch 81, loss 1.0851205587387085, acc=0.5643888711929321, loss=1.0851205587387085
test: epoch 81, loss 1.5738937854766846, acc=0.3083333373069763, loss=1.5738937854766846
train: epoch 82, loss 1.0845907926559448, acc=0.5673333406448364, loss=1.0845907926559448
test: epoch 82, loss 1.5943410396575928, acc=0.3194444477558136, loss=1.5943410396575928
train: epoch 83, loss 1.0702378749847412, acc=0.5661666393280029, loss=1.0702378749847412
test: epoch 83, loss 1.4399750232696533, acc=0.3861111104488373, loss=1.4399750232696533
train: epoch 84, loss 1.0498441457748413, acc=0.5762222409248352, loss=1.0498441457748413
test: epoch 84, loss 1.4685360193252563, acc=0.3777777850627899, loss=1.4685360193252563
train: epoch 85, loss 1.065645456314087, acc=0.5756666660308838, loss=1.065645456314087
test: epoch 85, loss 1.564836025238037, acc=0.3083333373069763, loss=1.564836025238037
train: epoch 86, loss 1.0630258321762085, acc=0.5751110911369324, loss=1.0630258321762085
test: epoch 86, loss 1.5609424114227295, acc=0.3166666626930237, loss=1.5609424114227295
train: epoch 87, loss 1.0463286638259888, acc=0.5788888931274414, loss=1.0463286638259888
test: epoch 87, loss 1.5287648439407349, acc=0.31388887763023376, loss=1.5287648439407349
train: epoch 88, loss 1.038767695426941, acc=0.5820555686950684, loss=1.038767695426941
test: epoch 88, loss 1.4744246006011963, acc=0.3583333194255829, loss=1.4744246006011963
train: epoch 89, loss 1.0395487546920776, acc=0.5874999761581421, loss=1.0395487546920776
test: epoch 89, loss 1.5336713790893555, acc=0.34166666865348816, loss=1.5336713790893555
train: epoch 90, loss 1.0333137512207031, acc=0.5797222256660461, loss=1.0333137512207031
test: epoch 90, loss 1.5454671382904053, acc=0.34166666865348816, loss=1.5454671382904053
train: epoch 91, loss 1.0106109380722046, acc=0.593666672706604, loss=1.0106109380722046
test: epoch 91, loss 1.421645164489746, acc=0.34166666865348816, loss=1.421645164489746
train: epoch 92, loss 1.0266097784042358, acc=0.5835555791854858, loss=1.0266097784042358
test: epoch 92, loss 1.527588963508606, acc=0.35277777910232544, loss=1.527588963508606
train: epoch 93, loss 1.0323535203933716, acc=0.5876666903495789, loss=1.0323535203933716
test: epoch 93, loss 1.515758991241455, acc=0.34166666865348816, loss=1.515758991241455
train: epoch 94, loss 1.0138534307479858, acc=0.5899444222450256, loss=1.0138534307479858
test: epoch 94, loss 1.5160845518112183, acc=0.3499999940395355, loss=1.5160845518112183
train: epoch 95, loss 1.0119162797927856, acc=0.5943333506584167, loss=1.0119162797927856
test: epoch 95, loss 1.5659247636795044, acc=0.4000000059604645, loss=1.5659247636795044
train: epoch 96, loss 1.0087441205978394, acc=0.5979999899864197, loss=1.0087441205978394
test: epoch 96, loss 1.5098639726638794, acc=0.3444444537162781, loss=1.5098639726638794
train: epoch 97, loss 1.0095793008804321, acc=0.5927222371101379, loss=1.0095793008804321
test: epoch 97, loss 1.5234627723693848, acc=0.3361110985279083, loss=1.5234627723693848
train: epoch 98, loss 1.007859230041504, acc=0.5956110954284668, loss=1.007859230041504
test: epoch 98, loss 1.4617199897766113, acc=0.39444443583488464, loss=1.4617199897766113
train: epoch 99, loss 1.0060796737670898, acc=0.5918889045715332, loss=1.0060796737670898
test: epoch 99, loss 1.6703310012817383, acc=0.30000001192092896, loss=1.6703310012817383
train: epoch 100, loss 0.9923052787780762, acc=0.6011666655540466, loss=0.9923052787780762
test: epoch 100, loss 1.4199388027191162, acc=0.34166666865348816, loss=1.4199388027191162
train: epoch 101, loss 0.9928150177001953, acc=0.6047222018241882, loss=0.9928150177001953
test: epoch 101, loss 1.6114273071289062, acc=0.33888888359069824, loss=1.6114273071289062
train: epoch 102, loss 0.9691952466964722, acc=0.6138333082199097, loss=0.9691952466964722
test: epoch 102, loss 1.511757731437683, acc=0.3777777850627899, loss=1.511757731437683
train: epoch 103, loss 0.9801830053329468, acc=0.6100555658340454, loss=0.9801830053329468
test: epoch 103, loss 1.5217421054840088, acc=0.3583333194255829, loss=1.5217421054840088
train: epoch 104, loss 0.9681555032730103, acc=0.6117777824401855, loss=0.9681555032730103
test: epoch 104, loss 1.623668909072876, acc=0.3194444477558136, loss=1.623668909072876
train: epoch 105, loss 0.9760209918022156, acc=0.612333357334137, loss=0.9760209918022156
test: epoch 105, loss 1.4876189231872559, acc=0.375, loss=1.4876189231872559
train: epoch 106, loss 0.9601480960845947, acc=0.61772221326828, loss=0.9601480960845947
test: epoch 106, loss 1.5935434103012085, acc=0.3444444537162781, loss=1.5935434103012085
train: epoch 107, loss 0.9615855813026428, acc=0.6196110844612122, loss=0.9615855813026428
test: epoch 107, loss 1.5482382774353027, acc=0.3166666626930237, loss=1.5482382774353027
train: epoch 108, loss 0.963245153427124, acc=0.6101666688919067, loss=0.963245153427124
test: epoch 108, loss 1.4257296323776245, acc=0.4000000059604645, loss=1.4257296323776245
train: epoch 109, loss 0.9540784955024719, acc=0.6215000152587891, loss=0.9540784955024719
test: epoch 109, loss 1.6373881101608276, acc=0.3361110985279083, loss=1.6373881101608276
train: epoch 110, loss 0.9727344512939453, acc=0.6129999756813049, loss=0.9727344512939453
test: epoch 110, loss 1.606029987335205, acc=0.32777777314186096, loss=1.606029987335205
train: epoch 111, loss 0.9379175305366516, acc=0.6237221956253052, loss=0.9379175305366516
test: epoch 111, loss 1.4942032098770142, acc=0.30000001192092896, loss=1.4942032098770142
train: epoch 112, loss 0.9330651760101318, acc=0.6258333325386047, loss=0.9330651760101318
test: epoch 112, loss 1.7410879135131836, acc=0.3499999940395355, loss=1.7410879135131836
train: epoch 113, loss 0.9645349979400635, acc=0.6166666746139526, loss=0.9645349979400635
test: epoch 113, loss 1.4370225667953491, acc=0.3888888955116272, loss=1.4370225667953491
train: epoch 114, loss 0.9287263751029968, acc=0.6276666522026062, loss=0.9287263751029968
test: epoch 114, loss 1.5527255535125732, acc=0.41111111640930176, loss=1.5527255535125732
train: epoch 115, loss 0.9350644946098328, acc=0.633055567741394, loss=0.9350644946098328
test: epoch 115, loss 1.553977131843567, acc=0.3444444537162781, loss=1.553977131843567
train: epoch 116, loss 0.9425816535949707, acc=0.6240555644035339, loss=0.9425816535949707
test: epoch 116, loss 1.4925761222839355, acc=0.3777777850627899, loss=1.4925761222839355
train: epoch 117, loss 0.9290696978569031, acc=0.633055567741394, loss=0.9290696978569031
test: epoch 117, loss 1.61546790599823, acc=0.3166666626930237, loss=1.61546790599823
train: epoch 118, loss 0.9294058680534363, acc=0.6317222118377686, loss=0.9294058680534363
test: epoch 118, loss 1.5897736549377441, acc=0.3305555582046509, loss=1.5897736549377441
train: epoch 119, loss 0.9203457832336426, acc=0.6309999823570251, loss=0.9203457832336426
test: epoch 119, loss 1.4979089498519897, acc=0.3916666805744171, loss=1.4979089498519897
train: epoch 120, loss 0.9214425683021545, acc=0.6314444541931152, loss=0.9214425683021545
test: epoch 120, loss 1.49967622756958, acc=0.375, loss=1.49967622756958
train: epoch 121, loss 0.9094895720481873, acc=0.6402222514152527, loss=0.9094895720481873
test: epoch 121, loss 1.5852640867233276, acc=0.3472222089767456, loss=1.5852640867233276
train: epoch 122, loss 0.9057432413101196, acc=0.6377221941947937, loss=0.9057432413101196
test: epoch 122, loss 1.4379210472106934, acc=0.3444444537162781, loss=1.4379210472106934
train: epoch 123, loss 0.9056126475334167, acc=0.6451666951179504, loss=0.9056126475334167
test: epoch 123, loss 1.491590976715088, acc=0.38055557012557983, loss=1.491590976715088
train: epoch 124, loss 0.9203119277954102, acc=0.64083331823349, loss=0.9203119277954102
test: epoch 124, loss 1.4594460725784302, acc=0.3444444537162781, loss=1.4594460725784302
train: epoch 125, loss 0.8980215191841125, acc=0.6438888907432556, loss=0.8980215191841125
test: epoch 125, loss 1.668501853942871, acc=0.34166666865348816, loss=1.668501853942871
train: epoch 126, loss 0.8948572874069214, acc=0.6431666612625122, loss=0.8948572874069214
test: epoch 126, loss 1.6381179094314575, acc=0.3166666626930237, loss=1.6381179094314575
train: epoch 127, loss 0.9167853593826294, acc=0.6375555396080017, loss=0.9167853593826294
test: epoch 127, loss 1.570189118385315, acc=0.3472222089767456, loss=1.570189118385315
train: epoch 128, loss 0.9035247564315796, acc=0.643666684627533, loss=0.9035247564315796
test: epoch 128, loss 1.4407196044921875, acc=0.36944442987442017, loss=1.4407196044921875
train: epoch 129, loss 0.8911945819854736, acc=0.6451666951179504, loss=0.8911945819854736
test: epoch 129, loss 1.7330187559127808, acc=0.3194444477558136, loss=1.7330187559127808
train: epoch 130, loss 0.8966567516326904, acc=0.6475555300712585, loss=0.8966567516326904
test: epoch 130, loss 1.492581844329834, acc=0.4333333373069763, loss=1.492581844329834
train: epoch 131, loss 0.8851975798606873, acc=0.6434444189071655, loss=0.8851975798606873
test: epoch 131, loss 1.4287458658218384, acc=0.3583333194255829, loss=1.4287458658218384
train: epoch 132, loss 0.8907039165496826, acc=0.6467221975326538, loss=0.8907039165496826
test: epoch 132, loss 1.5504212379455566, acc=0.3722222149372101, loss=1.5504212379455566
train: epoch 133, loss 0.8784224987030029, acc=0.6506111025810242, loss=0.8784224987030029
test: epoch 133, loss 1.6687639951705933, acc=0.32777777314186096, loss=1.6687639951705933
train: epoch 134, loss 0.8702363967895508, acc=0.652999997138977, loss=0.8702363967895508
test: epoch 134, loss 1.535119652748108, acc=0.39444443583488464, loss=1.535119652748108
train: epoch 135, loss 0.8954920172691345, acc=0.647777795791626, loss=0.8954920172691345
test: epoch 135, loss 1.4870585203170776, acc=0.36666667461395264, loss=1.4870585203170776
train: epoch 136, loss 0.8908953666687012, acc=0.6539999842643738, loss=0.8908953666687012
test: epoch 136, loss 1.5978825092315674, acc=0.3083333373069763, loss=1.5978825092315674
train: epoch 137, loss 0.8633745312690735, acc=0.6600555777549744, loss=0.8633745312690735
test: epoch 137, loss 1.682366967201233, acc=0.38333332538604736, loss=1.682366967201233
train: epoch 138, loss 0.8759632110595703, acc=0.653166651725769, loss=0.8759632110595703
test: epoch 138, loss 1.3421510457992554, acc=0.38055557012557983, loss=1.3421510457992554
train: epoch 139, loss 0.8792849183082581, acc=0.6530555486679077, loss=0.8792849183082581
test: epoch 139, loss 1.548840045928955, acc=0.3916666805744171, loss=1.548840045928955
train: epoch 140, loss 0.8767155408859253, acc=0.652055561542511, loss=0.8767155408859253
test: epoch 140, loss 1.502509593963623, acc=0.3499999940395355, loss=1.502509593963623
train: epoch 141, loss 0.8724976778030396, acc=0.6557777523994446, loss=0.8724976778030396
test: epoch 141, loss 1.6796702146530151, acc=0.3222222328186035, loss=1.6796702146530151
train: epoch 142, loss 0.8648395538330078, acc=0.6606666445732117, loss=0.8648395538330078
test: epoch 142, loss 1.453400731086731, acc=0.3777777850627899, loss=1.453400731086731
train: epoch 143, loss 0.8700979948043823, acc=0.6556110978126526, loss=0.8700979948043823
test: epoch 143, loss 1.693784236907959, acc=0.3305555582046509, loss=1.693784236907959
train: epoch 144, loss 0.8831883668899536, acc=0.6563888788223267, loss=0.8831883668899536
test: epoch 144, loss 1.484256386756897, acc=0.33888888359069824, loss=1.484256386756897
train: epoch 145, loss 0.8550766706466675, acc=0.6588333249092102, loss=0.8550766706466675
test: epoch 145, loss 1.4626743793487549, acc=0.34166666865348816, loss=1.4626743793487549
train: epoch 146, loss 0.857604444026947, acc=0.657444417476654, loss=0.857604444026947
test: epoch 146, loss 1.4060155153274536, acc=0.3472222089767456, loss=1.4060155153274536
train: epoch 147, loss 0.872800886631012, acc=0.6598333120346069, loss=0.872800886631012
test: epoch 147, loss 1.543332576751709, acc=0.34166666865348816, loss=1.543332576751709
train: epoch 148, loss 0.8361960053443909, acc=0.6688888669013977, loss=0.8361960053443909
test: epoch 148, loss 1.4067473411560059, acc=0.3777777850627899, loss=1.4067473411560059
train: epoch 149, loss 0.8543695211410522, acc=0.6592222452163696, loss=0.8543695211410522
test: epoch 149, loss 1.5210766792297363, acc=0.3472222089767456, loss=1.5210766792297363
train: epoch 150, loss 0.8432380557060242, acc=0.6713333129882812, loss=0.8432380557060242
test: epoch 150, loss 1.4295661449432373, acc=0.3916666805744171, loss=1.4295661449432373
