# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=0", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=683145052, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=683145052, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.7191004753112793, acc=0.13344444334506989, loss=2.7191004753112793
test: epoch 1, loss 4.4504523277282715, acc=0.07222222536802292, loss=4.4504523277282715
train: epoch 2, loss 2.0637803077697754, acc=0.2418888956308365, loss=2.0637803077697754
test: epoch 2, loss 4.3843841552734375, acc=0.08055555820465088, loss=4.3843841552734375
train: epoch 3, loss 1.858415126800537, acc=0.2948888838291168, loss=1.858415126800537
test: epoch 3, loss 4.685764789581299, acc=0.06666667014360428, loss=4.685764789581299
train: epoch 4, loss 1.7445311546325684, acc=0.3311111032962799, loss=1.7445311546325684
test: epoch 4, loss 4.498701572418213, acc=0.0833333358168602, loss=4.498701572418213
train: epoch 5, loss 1.6398645639419556, acc=0.3639444410800934, loss=1.6398645639419556
test: epoch 5, loss 4.1822919845581055, acc=0.10833333432674408, loss=4.1822919845581055
train: epoch 6, loss 1.5893248319625854, acc=0.37833333015441895, loss=1.5893248319625854
test: epoch 6, loss 4.732909679412842, acc=0.10833333432674408, loss=4.732909679412842
train: epoch 7, loss 1.5427252054214478, acc=0.40372222661972046, loss=1.5427252054214478
test: epoch 7, loss 3.760199785232544, acc=0.125, loss=3.760199785232544
train: epoch 8, loss 1.5005052089691162, acc=0.41394445300102234, loss=1.5005052089691162
test: epoch 8, loss 3.7471423149108887, acc=0.14722222089767456, loss=3.7471423149108887
train: epoch 9, loss 1.462239384651184, acc=0.42916667461395264, loss=1.462239384651184
test: epoch 9, loss 3.935275077819824, acc=0.12222222238779068, loss=3.935275077819824
train: epoch 10, loss 1.4331928491592407, acc=0.44216665625572205, loss=1.4331928491592407
test: epoch 10, loss 3.572810173034668, acc=0.14722222089767456, loss=3.572810173034668
train: epoch 11, loss 1.3902864456176758, acc=0.45355555415153503, loss=1.3902864456176758
test: epoch 11, loss 3.619442939758301, acc=0.16388888657093048, loss=3.619442939758301
train: epoch 12, loss 1.3868205547332764, acc=0.4645000100135803, loss=1.3868205547332764
test: epoch 12, loss 3.206923484802246, acc=0.15000000596046448, loss=3.206923484802246
train: epoch 13, loss 1.3542777299880981, acc=0.47261109948158264, loss=1.3542777299880981
test: epoch 13, loss 3.62579345703125, acc=0.15833333134651184, loss=3.62579345703125
train: epoch 14, loss 1.3368467092514038, acc=0.4787222146987915, loss=1.3368467092514038
test: epoch 14, loss 3.7954771518707275, acc=0.13055555522441864, loss=3.7954771518707275
train: epoch 15, loss 1.3286395072937012, acc=0.4856666624546051, loss=1.3286395072937012
test: epoch 15, loss 3.0183780193328857, acc=0.1527777761220932, loss=3.0183780193328857
train: epoch 16, loss 1.3065497875213623, acc=0.4890555441379547, loss=1.3065497875213623
test: epoch 16, loss 3.0228383541107178, acc=0.1388888955116272, loss=3.0228383541107178
train: epoch 17, loss 1.2841846942901611, acc=0.5, loss=1.2841846942901611
test: epoch 17, loss 3.4541947841644287, acc=0.1388888955116272, loss=3.4541947841644287
train: epoch 18, loss 1.2834248542785645, acc=0.5070555806159973, loss=1.2834248542785645
test: epoch 18, loss 2.9968361854553223, acc=0.16388888657093048, loss=2.9968361854553223
train: epoch 19, loss 1.2754504680633545, acc=0.5116666555404663, loss=1.2754504680633545
test: epoch 19, loss 3.0072057247161865, acc=0.14166666567325592, loss=3.0072057247161865
train: epoch 20, loss 1.2776435613632202, acc=0.5109444260597229, loss=1.2776435613632202
test: epoch 20, loss 2.864072799682617, acc=0.18333333730697632, loss=2.864072799682617
train: epoch 21, loss 1.2500706911087036, acc=0.5147777795791626, loss=1.2500706911087036
test: epoch 21, loss 2.724644422531128, acc=0.20000000298023224, loss=2.724644422531128
train: epoch 22, loss 1.2558434009552002, acc=0.5202222466468811, loss=1.2558434009552002
test: epoch 22, loss 2.7755966186523438, acc=0.23055554926395416, loss=2.7755966186523438
train: epoch 23, loss 1.2326475381851196, acc=0.5293333530426025, loss=1.2326475381851196
test: epoch 23, loss 3.114403009414673, acc=0.17222222685813904, loss=3.114403009414673
train: epoch 24, loss 1.2444382905960083, acc=0.5246666669845581, loss=1.2444382905960083
test: epoch 24, loss 2.520827054977417, acc=0.20277777314186096, loss=2.520827054977417
train: epoch 25, loss 1.223271369934082, acc=0.5220555663108826, loss=1.223271369934082
test: epoch 25, loss 2.6991496086120605, acc=0.21666666865348816, loss=2.6991496086120605
train: epoch 26, loss 1.222579836845398, acc=0.5291666388511658, loss=1.222579836845398
test: epoch 26, loss 2.5828299522399902, acc=0.20000000298023224, loss=2.5828299522399902
train: epoch 27, loss 1.2328861951828003, acc=0.5216666460037231, loss=1.2328861951828003
test: epoch 27, loss 2.3610260486602783, acc=0.17777778208255768, loss=2.3610260486602783
train: epoch 28, loss 1.2360689640045166, acc=0.5239999890327454, loss=1.2360689640045166
test: epoch 28, loss 2.4336001873016357, acc=0.18611110746860504, loss=2.4336001873016357
train: epoch 29, loss 1.2159223556518555, acc=0.5236111283302307, loss=1.2159223556518555
test: epoch 29, loss 2.4868221282958984, acc=0.16388888657093048, loss=2.4868221282958984
train: epoch 30, loss 1.222774624824524, acc=0.5316110849380493, loss=1.222774624824524
test: epoch 30, loss 2.714139461517334, acc=0.20277777314186096, loss=2.714139461517334
train: epoch 31, loss 1.2213155031204224, acc=0.5286111235618591, loss=1.2213155031204224
test: epoch 31, loss 2.585383892059326, acc=0.20555555820465088, loss=2.585383892059326
train: epoch 32, loss 1.2238131761550903, acc=0.5280555486679077, loss=1.2238131761550903
test: epoch 32, loss 2.082361936569214, acc=0.25, loss=2.082361936569214
train: epoch 33, loss 1.2228443622589111, acc=0.5302222371101379, loss=1.2228443622589111
test: epoch 33, loss 2.373464822769165, acc=0.2222222238779068, loss=2.373464822769165
train: epoch 34, loss 1.2109180688858032, acc=0.5267778038978577, loss=1.2109180688858032
test: epoch 34, loss 2.208789110183716, acc=0.21666666865348816, loss=2.208789110183716
train: epoch 35, loss 1.2173203229904175, acc=0.5267778038978577, loss=1.2173203229904175
test: epoch 35, loss 2.0565805435180664, acc=0.23055554926395416, loss=2.0565805435180664
train: epoch 36, loss 1.2197529077529907, acc=0.527388870716095, loss=1.2197529077529907
test: epoch 36, loss 2.2358996868133545, acc=0.20277777314186096, loss=2.2358996868133545
train: epoch 37, loss 1.2084509134292603, acc=0.5284444689750671, loss=1.2084509134292603
test: epoch 37, loss 2.2051267623901367, acc=0.2222222238779068, loss=2.2051267623901367
train: epoch 38, loss 1.2043073177337646, acc=0.5303333401679993, loss=1.2043073177337646
test: epoch 38, loss 2.107828378677368, acc=0.21666666865348816, loss=2.107828378677368
train: epoch 39, loss 1.1884727478027344, acc=0.53938889503479, loss=1.1884727478027344
test: epoch 39, loss 2.094372034072876, acc=0.23888888955116272, loss=2.094372034072876
train: epoch 40, loss 1.2053874731063843, acc=0.5256666541099548, loss=1.2053874731063843
test: epoch 40, loss 2.1831398010253906, acc=0.21666666865348816, loss=2.1831398010253906
train: epoch 41, loss 1.1985284090042114, acc=0.5303333401679993, loss=1.1985284090042114
test: epoch 41, loss 2.0117154121398926, acc=0.24166665971279144, loss=2.0117154121398926
train: epoch 42, loss 1.1877846717834473, acc=0.5329444408416748, loss=1.1877846717834473
test: epoch 42, loss 1.9095546007156372, acc=0.2805555462837219, loss=1.9095546007156372
train: epoch 43, loss 1.194709062576294, acc=0.5299444198608398, loss=1.194709062576294
test: epoch 43, loss 2.03574538230896, acc=0.2222222238779068, loss=2.03574538230896
train: epoch 44, loss 1.190291166305542, acc=0.5327222347259521, loss=1.190291166305542
test: epoch 44, loss 2.094536066055298, acc=0.2222222238779068, loss=2.094536066055298
train: epoch 45, loss 1.1929680109024048, acc=0.5332221984863281, loss=1.1929680109024048
test: epoch 45, loss 2.2829787731170654, acc=0.20000000298023224, loss=2.2829787731170654
train: epoch 46, loss 1.1805840730667114, acc=0.5371111035346985, loss=1.1805840730667114
test: epoch 46, loss 1.811130166053772, acc=0.30000001192092896, loss=1.811130166053772
train: epoch 47, loss 1.2151886224746704, acc=0.5318333506584167, loss=1.2151886224746704
test: epoch 47, loss 1.8481967449188232, acc=0.2611111104488373, loss=1.8481967449188232
train: epoch 48, loss 1.1932355165481567, acc=0.5292778015136719, loss=1.1932355165481567
test: epoch 48, loss 1.8778557777404785, acc=0.23888888955116272, loss=1.8778557777404785
train: epoch 49, loss 1.1977925300598145, acc=0.5299444198608398, loss=1.1977925300598145
test: epoch 49, loss 1.8517524003982544, acc=0.2083333283662796, loss=1.8517524003982544
train: epoch 50, loss 1.1802582740783691, acc=0.538611114025116, loss=1.1802582740783691
test: epoch 50, loss 1.9318381547927856, acc=0.25555557012557983, loss=1.9318381547927856
train: epoch 51, loss 1.1954097747802734, acc=0.5352222323417664, loss=1.1954097747802734
test: epoch 51, loss 1.8802837133407593, acc=0.28333333134651184, loss=1.8802837133407593
train: epoch 52, loss 1.184613823890686, acc=0.5388888716697693, loss=1.184613823890686
test: epoch 52, loss 1.9366682767868042, acc=0.32777777314186096, loss=1.9366682767868042
train: epoch 53, loss 1.1793292760849, acc=0.5335000157356262, loss=1.1793292760849
test: epoch 53, loss 1.7752821445465088, acc=0.39444443583488464, loss=1.7752821445465088
train: epoch 54, loss 1.1689791679382324, acc=0.5397777557373047, loss=1.1689791679382324
test: epoch 54, loss 1.740728497505188, acc=0.29722222685813904, loss=1.740728497505188
train: epoch 55, loss 1.1845297813415527, acc=0.5318889021873474, loss=1.1845297813415527
test: epoch 55, loss 1.752768874168396, acc=0.2944444417953491, loss=1.752768874168396
train: epoch 56, loss 1.1733399629592896, acc=0.5327777862548828, loss=1.1733399629592896
test: epoch 56, loss 1.7699278593063354, acc=0.3333333432674408, loss=1.7699278593063354
train: epoch 57, loss 1.1602100133895874, acc=0.5433333516120911, loss=1.1602100133895874
test: epoch 57, loss 1.7162889242172241, acc=0.2666666805744171, loss=1.7162889242172241
train: epoch 58, loss 1.1671233177185059, acc=0.5362777709960938, loss=1.1671233177185059
test: epoch 58, loss 1.7325668334960938, acc=0.2944444417953491, loss=1.7325668334960938
train: epoch 59, loss 1.1727198362350464, acc=0.5332221984863281, loss=1.1727198362350464
test: epoch 59, loss 1.8289204835891724, acc=0.2777777910232544, loss=1.8289204835891724
train: epoch 60, loss 1.190129041671753, acc=0.5336111187934875, loss=1.190129041671753
test: epoch 60, loss 1.6596571207046509, acc=0.31111112236976624, loss=1.6596571207046509
train: epoch 61, loss 1.167670488357544, acc=0.5362777709960938, loss=1.167670488357544
test: epoch 61, loss 1.667502522468567, acc=0.28333333134651184, loss=1.667502522468567
train: epoch 62, loss 1.2082315683364868, acc=0.5316666960716248, loss=1.2082315683364868
test: epoch 62, loss 1.7060452699661255, acc=0.2666666805744171, loss=1.7060452699661255
train: epoch 63, loss 1.1648764610290527, acc=0.538444459438324, loss=1.1648764610290527
test: epoch 63, loss 1.6879868507385254, acc=0.2805555462837219, loss=1.6879868507385254
train: epoch 64, loss 1.1600099802017212, acc=0.5380555391311646, loss=1.1600099802017212
test: epoch 64, loss 1.703599214553833, acc=0.2944444417953491, loss=1.703599214553833
train: epoch 65, loss 1.1595698595046997, acc=0.5360000133514404, loss=1.1595698595046997
test: epoch 65, loss 1.641038417816162, acc=0.2916666567325592, loss=1.641038417816162
train: epoch 66, loss 1.1698943376541138, acc=0.535111129283905, loss=1.1698943376541138
test: epoch 66, loss 1.6719411611557007, acc=0.3333333432674408, loss=1.6719411611557007
train: epoch 67, loss 1.1626031398773193, acc=0.5325555801391602, loss=1.1626031398773193
test: epoch 67, loss 1.653287410736084, acc=0.375, loss=1.653287410736084
train: epoch 68, loss 1.1566299200057983, acc=0.5364999771118164, loss=1.1566299200057983
test: epoch 68, loss 1.6233707666397095, acc=0.3722222149372101, loss=1.6233707666397095
train: epoch 69, loss 1.1611745357513428, acc=0.5360555648803711, loss=1.1611745357513428
test: epoch 69, loss 1.5972304344177246, acc=0.3194444477558136, loss=1.5972304344177246
train: epoch 70, loss 1.1737004518508911, acc=0.5360555648803711, loss=1.1737004518508911
test: epoch 70, loss 1.7872202396392822, acc=0.25833332538604736, loss=1.7872202396392822
train: epoch 71, loss 1.155821681022644, acc=0.5379999876022339, loss=1.155821681022644
test: epoch 71, loss 1.6252187490463257, acc=0.3472222089767456, loss=1.6252187490463257
train: epoch 72, loss 1.1482954025268555, acc=0.5346111059188843, loss=1.1482954025268555
test: epoch 72, loss 1.5089478492736816, acc=0.39722222089767456, loss=1.5089478492736816
train: epoch 73, loss 1.1437629461288452, acc=0.5468888878822327, loss=1.1437629461288452
test: epoch 73, loss 1.615653157234192, acc=0.3055555522441864, loss=1.615653157234192
train: epoch 74, loss 1.1373240947723389, acc=0.5448333621025085, loss=1.1373240947723389
test: epoch 74, loss 1.7250697612762451, acc=0.2638888955116272, loss=1.7250697612762451
train: epoch 75, loss 1.1251599788665771, acc=0.5509999990463257, loss=1.1251599788665771
test: epoch 75, loss 1.6214559078216553, acc=0.32499998807907104, loss=1.6214559078216553
train: epoch 76, loss 1.1469268798828125, acc=0.5467222332954407, loss=1.1469268798828125
test: epoch 76, loss 1.5146516561508179, acc=0.31388887763023376, loss=1.5146516561508179
train: epoch 77, loss 1.1032166481018066, acc=0.554111123085022, loss=1.1032166481018066
test: epoch 77, loss 1.5990480184555054, acc=0.3361110985279083, loss=1.5990480184555054
train: epoch 78, loss 1.1301850080490112, acc=0.5453888773918152, loss=1.1301850080490112
test: epoch 78, loss 1.5770543813705444, acc=0.3888888955116272, loss=1.5770543813705444
train: epoch 79, loss 1.0908536911010742, acc=0.5631111264228821, loss=1.0908536911010742
test: epoch 79, loss 1.6301120519638062, acc=0.36666667461395264, loss=1.6301120519638062
train: epoch 80, loss 1.102907419204712, acc=0.559499979019165, loss=1.102907419204712
test: epoch 80, loss 1.6296107769012451, acc=0.3472222089767456, loss=1.6296107769012451
train: epoch 81, loss 1.1142898797988892, acc=0.5619444251060486, loss=1.1142898797988892
test: epoch 81, loss 1.4708993434906006, acc=0.3861111104488373, loss=1.4708993434906006
train: epoch 82, loss 1.0908020734786987, acc=0.5657777786254883, loss=1.0908020734786987
test: epoch 82, loss 1.5439541339874268, acc=0.3444444537162781, loss=1.5439541339874268
train: epoch 83, loss 1.0729000568389893, acc=0.570111095905304, loss=1.0729000568389893
test: epoch 83, loss 1.5446503162384033, acc=0.3333333432674408, loss=1.5446503162384033
train: epoch 84, loss 1.074345588684082, acc=0.5670555830001831, loss=1.074345588684082
test: epoch 84, loss 1.4906830787658691, acc=0.29722222685813904, loss=1.4906830787658691
train: epoch 85, loss 1.055571436882019, acc=0.5742777585983276, loss=1.055571436882019
test: epoch 85, loss 1.4373934268951416, acc=0.4000000059604645, loss=1.4373934268951416
train: epoch 86, loss 1.0385043621063232, acc=0.5817777514457703, loss=1.0385043621063232
test: epoch 86, loss 1.4538004398345947, acc=0.38333332538604736, loss=1.4538004398345947
train: epoch 87, loss 1.0317381620407104, acc=0.5835555791854858, loss=1.0317381620407104
test: epoch 87, loss 1.4331339597702026, acc=0.35277777910232544, loss=1.4331339597702026
train: epoch 88, loss 1.0487732887268066, acc=0.5815555453300476, loss=1.0487732887268066
test: epoch 88, loss 1.4149776697158813, acc=0.36944442987442017, loss=1.4149776697158813
train: epoch 89, loss 1.040330171585083, acc=0.5860555768013, loss=1.040330171585083
test: epoch 89, loss 1.5216392278671265, acc=0.3444444537162781, loss=1.5216392278671265
train: epoch 90, loss 1.0216541290283203, acc=0.5939444303512573, loss=1.0216541290283203
test: epoch 90, loss 1.491552472114563, acc=0.3055555522441864, loss=1.491552472114563
train: epoch 91, loss 1.0364822149276733, acc=0.5833333134651184, loss=1.0364822149276733
test: epoch 91, loss 1.4769896268844604, acc=0.38333332538604736, loss=1.4769896268844604
train: epoch 92, loss 1.0089659690856934, acc=0.5943889021873474, loss=1.0089659690856934
test: epoch 92, loss 1.480332851409912, acc=0.35555556416511536, loss=1.480332851409912
train: epoch 93, loss 1.0201269388198853, acc=0.5926111340522766, loss=1.0201269388198853
test: epoch 93, loss 1.4001004695892334, acc=0.4027777910232544, loss=1.4001004695892334
train: epoch 94, loss 1.0168101787567139, acc=0.5977222323417664, loss=1.0168101787567139
test: epoch 94, loss 1.5694081783294678, acc=0.3444444537162781, loss=1.5694081783294678
train: epoch 95, loss 0.9928412437438965, acc=0.5978333353996277, loss=0.9928412437438965
test: epoch 95, loss 1.5373908281326294, acc=0.35277777910232544, loss=1.5373908281326294
train: epoch 96, loss 1.0199956893920898, acc=0.5955555438995361, loss=1.0199956893920898
test: epoch 96, loss 1.5262119770050049, acc=0.31388887763023376, loss=1.5262119770050049
train: epoch 97, loss 0.996437668800354, acc=0.5998333096504211, loss=0.996437668800354
test: epoch 97, loss 1.479029655456543, acc=0.3777777850627899, loss=1.479029655456543
train: epoch 98, loss 0.9918338656425476, acc=0.5966110825538635, loss=0.9918338656425476
test: epoch 98, loss 1.413189172744751, acc=0.3222222328186035, loss=1.413189172744751
train: epoch 99, loss 0.9785648584365845, acc=0.6006110906600952, loss=0.9785648584365845
test: epoch 99, loss 1.4275691509246826, acc=0.3638888895511627, loss=1.4275691509246826
train: epoch 100, loss 0.9619210362434387, acc=0.6119444370269775, loss=0.9619210362434387
test: epoch 100, loss 1.5287542343139648, acc=0.3611111044883728, loss=1.5287542343139648
train: epoch 101, loss 1.0191471576690674, acc=0.5952222347259521, loss=1.0191471576690674
test: epoch 101, loss 1.459112286567688, acc=0.32777777314186096, loss=1.459112286567688
train: epoch 102, loss 0.9725144505500793, acc=0.616944432258606, loss=0.9725144505500793
test: epoch 102, loss 1.378292202949524, acc=0.3611111044883728, loss=1.378292202949524
train: epoch 103, loss 0.9605732560157776, acc=0.6089444160461426, loss=0.9605732560157776
test: epoch 103, loss 1.3622972965240479, acc=0.38055557012557983, loss=1.3622972965240479
train: epoch 104, loss 0.9802653789520264, acc=0.609499990940094, loss=0.9802653789520264
test: epoch 104, loss 1.4192862510681152, acc=0.375, loss=1.4192862510681152
train: epoch 105, loss 0.9676019549369812, acc=0.6117777824401855, loss=0.9676019549369812
test: epoch 105, loss 1.3860936164855957, acc=0.43888887763023376, loss=1.3860936164855957
train: epoch 106, loss 0.9454154968261719, acc=0.6191666722297668, loss=0.9454154968261719
test: epoch 106, loss 1.5912196636199951, acc=0.32499998807907104, loss=1.5912196636199951
train: epoch 107, loss 0.9366059303283691, acc=0.6204444169998169, loss=0.9366059303283691
test: epoch 107, loss 1.5375192165374756, acc=0.3472222089767456, loss=1.5375192165374756
train: epoch 108, loss 0.9515625834465027, acc=0.6244999766349792, loss=0.9515625834465027
test: epoch 108, loss 1.6445732116699219, acc=0.3611111044883728, loss=1.6445732116699219
train: epoch 109, loss 0.9339399337768555, acc=0.6223888993263245, loss=0.9339399337768555
test: epoch 109, loss 1.4159977436065674, acc=0.38333332538604736, loss=1.4159977436065674
train: epoch 110, loss 0.934852659702301, acc=0.6250555515289307, loss=0.934852659702301
test: epoch 110, loss 1.5461229085922241, acc=0.3777777850627899, loss=1.5461229085922241
train: epoch 111, loss 0.9585588574409485, acc=0.6228333115577698, loss=0.9585588574409485
test: epoch 111, loss 1.491724967956543, acc=0.4000000059604645, loss=1.491724967956543
train: epoch 112, loss 0.9138906002044678, acc=0.6328333616256714, loss=0.9138906002044678
test: epoch 112, loss 1.7349567413330078, acc=0.3166666626930237, loss=1.7349567413330078
train: epoch 113, loss 0.9331794381141663, acc=0.6238333582878113, loss=0.9331794381141663
test: epoch 113, loss 1.6087433099746704, acc=0.3916666805744171, loss=1.6087433099746704
train: epoch 114, loss 0.9223110675811768, acc=0.6270555257797241, loss=0.9223110675811768
test: epoch 114, loss 1.5224744081497192, acc=0.32777777314186096, loss=1.5224744081497192
train: epoch 115, loss 0.9126110076904297, acc=0.6355555653572083, loss=0.9126110076904297
test: epoch 115, loss 1.5483858585357666, acc=0.35277777910232544, loss=1.5483858585357666
train: epoch 116, loss 0.917158842086792, acc=0.6346666812896729, loss=0.917158842086792
test: epoch 116, loss 1.5034383535385132, acc=0.39444443583488464, loss=1.5034383535385132
train: epoch 117, loss 0.9117974042892456, acc=0.6304444670677185, loss=0.9117974042892456
test: epoch 117, loss 1.5068162679672241, acc=0.32499998807907104, loss=1.5068162679672241
train: epoch 118, loss 0.8805144429206848, acc=0.6466666460037231, loss=0.8805144429206848
test: epoch 118, loss 1.5236715078353882, acc=0.3777777850627899, loss=1.5236715078353882
train: epoch 119, loss 0.9003899097442627, acc=0.637666642665863, loss=0.9003899097442627
test: epoch 119, loss 1.5146211385726929, acc=0.3583333194255829, loss=1.5146211385726929
train: epoch 120, loss 0.8976852297782898, acc=0.636388897895813, loss=0.8976852297782898
test: epoch 120, loss 1.4968020915985107, acc=0.4027777910232544, loss=1.4968020915985107
train: epoch 121, loss 0.8872713446617126, acc=0.6430000066757202, loss=0.8872713446617126
test: epoch 121, loss 1.492186188697815, acc=0.35277777910232544, loss=1.492186188697815
train: epoch 122, loss 0.8736738562583923, acc=0.6449999809265137, loss=0.8736738562583923
test: epoch 122, loss 1.520444393157959, acc=0.35277777910232544, loss=1.520444393157959
train: epoch 123, loss 0.864324152469635, acc=0.6498333215713501, loss=0.864324152469635
test: epoch 123, loss 1.6423367261886597, acc=0.3611111044883728, loss=1.6423367261886597
train: epoch 124, loss 0.8694392442703247, acc=0.6460000276565552, loss=0.8694392442703247
test: epoch 124, loss 1.3902742862701416, acc=0.3722222149372101, loss=1.3902742862701416
train: epoch 125, loss 0.882960319519043, acc=0.6489999890327454, loss=0.882960319519043
test: epoch 125, loss 1.587084174156189, acc=0.4000000059604645, loss=1.587084174156189
train: epoch 126, loss 0.8573263883590698, acc=0.6519444584846497, loss=0.8573263883590698
test: epoch 126, loss 1.5773632526397705, acc=0.38333332538604736, loss=1.5773632526397705
train: epoch 127, loss 0.866641104221344, acc=0.649222195148468, loss=0.866641104221344
test: epoch 127, loss 1.5706112384796143, acc=0.40833333134651184, loss=1.5706112384796143
train: epoch 128, loss 0.8613011240959167, acc=0.655055582523346, loss=0.8613011240959167
test: epoch 128, loss 1.509186863899231, acc=0.39722222089767456, loss=1.509186863899231
train: epoch 129, loss 0.8654927611351013, acc=0.6517778038978577, loss=0.8654927611351013
test: epoch 129, loss 1.6203607320785522, acc=0.3444444537162781, loss=1.6203607320785522
train: epoch 130, loss 0.8640490770339966, acc=0.6527777910232544, loss=0.8640490770339966
test: epoch 130, loss 1.4119027853012085, acc=0.375, loss=1.4119027853012085
train: epoch 131, loss 0.8497067093849182, acc=0.6559444665908813, loss=0.8497067093849182
test: epoch 131, loss 1.4322407245635986, acc=0.36666667461395264, loss=1.4322407245635986
train: epoch 132, loss 0.8477262854576111, acc=0.6633889079093933, loss=0.8477262854576111
test: epoch 132, loss 1.57798433303833, acc=0.4055555462837219, loss=1.57798433303833
train: epoch 133, loss 0.8574851155281067, acc=0.6616666913032532, loss=0.8574851155281067
test: epoch 133, loss 1.5964751243591309, acc=0.3638888895511627, loss=1.5964751243591309
train: epoch 134, loss 0.8544957041740417, acc=0.6566666960716248, loss=0.8544957041740417
test: epoch 134, loss 1.4494167566299438, acc=0.38055557012557983, loss=1.4494167566299438
train: epoch 135, loss 0.837046205997467, acc=0.6648333072662354, loss=0.837046205997467
test: epoch 135, loss 1.4043676853179932, acc=0.4277777671813965, loss=1.4043676853179932
train: epoch 136, loss 0.8363648056983948, acc=0.6618888974189758, loss=0.8363648056983948
test: epoch 136, loss 1.5361591577529907, acc=0.39722222089767456, loss=1.5361591577529907
train: epoch 137, loss 0.8384966850280762, acc=0.6631666421890259, loss=0.8384966850280762
test: epoch 137, loss 1.4210785627365112, acc=0.4138889014720917, loss=1.4210785627365112
train: epoch 138, loss 0.840558648109436, acc=0.6657778024673462, loss=0.840558648109436
test: epoch 138, loss 1.638045310974121, acc=0.3722222149372101, loss=1.638045310974121
train: epoch 139, loss 0.8340203762054443, acc=0.6703333258628845, loss=0.8340203762054443
test: epoch 139, loss 1.4975987672805786, acc=0.38055557012557983, loss=1.4975987672805786
train: epoch 140, loss 0.809401273727417, acc=0.6726666688919067, loss=0.809401273727417
test: epoch 140, loss 1.462534785270691, acc=0.38055557012557983, loss=1.462534785270691
train: epoch 141, loss 0.8206217885017395, acc=0.6726111173629761, loss=0.8206217885017395
test: epoch 141, loss 1.458133339881897, acc=0.4166666567325592, loss=1.458133339881897
train: epoch 142, loss 0.834691047668457, acc=0.6753333210945129, loss=0.834691047668457
test: epoch 142, loss 1.4653451442718506, acc=0.40833333134651184, loss=1.4653451442718506
train: epoch 143, loss 0.8120116591453552, acc=0.6769444346427917, loss=0.8120116591453552
test: epoch 143, loss 1.340795874595642, acc=0.4027777910232544, loss=1.340795874595642
train: epoch 144, loss 0.8382384777069092, acc=0.6663888692855835, loss=0.8382384777069092
test: epoch 144, loss 1.6648361682891846, acc=0.3472222089767456, loss=1.6648361682891846
train: epoch 145, loss 0.8063162565231323, acc=0.6801111102104187, loss=0.8063162565231323
test: epoch 145, loss 1.4337362051010132, acc=0.4277777671813965, loss=1.4337362051010132
train: epoch 146, loss 0.8055909276008606, acc=0.680055558681488, loss=0.8055909276008606
test: epoch 146, loss 1.5378472805023193, acc=0.3916666805744171, loss=1.5378472805023193
train: epoch 147, loss 0.8008386492729187, acc=0.6779999732971191, loss=0.8008386492729187
test: epoch 147, loss 1.614413857460022, acc=0.40833333134651184, loss=1.614413857460022
train: epoch 148, loss 0.787541389465332, acc=0.6819999814033508, loss=0.787541389465332
test: epoch 148, loss 1.501259684562683, acc=0.44999998807907104, loss=1.501259684562683
train: epoch 149, loss 0.7832503914833069, acc=0.6842777729034424, loss=0.7832503914833069
test: epoch 149, loss 1.4248281717300415, acc=0.38055557012557983, loss=1.4248281717300415
train: epoch 150, loss 0.781054675579071, acc=0.6865555644035339, loss=0.781054675579071
test: epoch 150, loss 1.4202775955200195, acc=0.42222222685813904, loss=1.4202775955200195
