# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=100", "--receiver_embed_dim=128", "--temperature=2.0", "--temp_decay=0.995", "--one_hot=false", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_summands=2, n_symbols=100, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1032684898, receiver_embed_dim=128, temp_decay=0.995, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.7268433570861816, acc=0.11649999767541885, loss=2.7268433570861816
test: epoch 1, loss 4.455564022064209, acc=0.10555555671453476, loss=4.455564022064209
train: epoch 2, loss 1.3805153369903564, acc=0.43533334136009216, loss=1.3805153369903564
test: epoch 2, loss 3.620482921600342, acc=0.20555555820465088, loss=3.620482921600342
train: epoch 3, loss 0.7718700766563416, acc=0.6865000128746033, loss=0.7718700766563416
test: epoch 3, loss 4.2991814613342285, acc=0.17499999701976776, loss=4.2991814613342285
train: epoch 4, loss 0.5595481395721436, acc=0.7734444737434387, loss=0.5595481395721436
test: epoch 4, loss 2.891350746154785, acc=0.3055555522441864, loss=2.891350746154785
train: epoch 5, loss 0.4632642865180969, acc=0.8180000185966492, loss=0.4632642865180969
test: epoch 5, loss 3.1414222717285156, acc=0.2888889014720917, loss=3.1414222717285156
train: epoch 6, loss 0.37549981474876404, acc=0.8606666922569275, loss=0.37549981474876404
test: epoch 6, loss 2.5955421924591064, acc=0.32777777314186096, loss=2.5955421924591064
train: epoch 7, loss 0.3216477632522583, acc=0.8842777609825134, loss=0.3216477632522583
test: epoch 7, loss 2.4939348697662354, acc=0.3472222089767456, loss=2.4939348697662354
train: epoch 8, loss 0.3000388443470001, acc=0.895111083984375, loss=0.3000388443470001
test: epoch 8, loss 2.816099166870117, acc=0.3444444537162781, loss=2.816099166870117
train: epoch 9, loss 0.2445627748966217, acc=0.9156666398048401, loss=0.2445627748966217
test: epoch 9, loss 2.726876974105835, acc=0.3611111044883728, loss=2.726876974105835
train: epoch 10, loss 0.23214079439640045, acc=0.9202222228050232, loss=0.23214079439640045
test: epoch 10, loss 2.228872299194336, acc=0.42222222685813904, loss=2.228872299194336
train: epoch 11, loss 0.211544007062912, acc=0.9287777543067932, loss=0.211544007062912
test: epoch 11, loss 2.7639448642730713, acc=0.32777777314186096, loss=2.7639448642730713
train: epoch 12, loss 0.19160129129886627, acc=0.9352777600288391, loss=0.19160129129886627
test: epoch 12, loss 2.733496904373169, acc=0.42222222685813904, loss=2.733496904373169
train: epoch 13, loss 0.1934056431055069, acc=0.934333324432373, loss=0.1934056431055069
test: epoch 13, loss 2.143092155456543, acc=0.4277777671813965, loss=2.143092155456543
train: epoch 14, loss 0.17392978072166443, acc=0.9399444460868835, loss=0.17392978072166443
test: epoch 14, loss 2.9508330821990967, acc=0.3499999940395355, loss=2.9508330821990967
train: epoch 15, loss 0.1730709820985794, acc=0.9393888711929321, loss=0.1730709820985794
test: epoch 15, loss 2.2324423789978027, acc=0.38333332538604736, loss=2.2324423789978027
train: epoch 16, loss 0.15121440589427948, acc=0.9503889083862305, loss=0.15121440589427948
test: epoch 16, loss 2.760481834411621, acc=0.3583333194255829, loss=2.760481834411621
train: epoch 17, loss 0.1557784229516983, acc=0.948888897895813, loss=0.1557784229516983
test: epoch 17, loss 2.630098819732666, acc=0.4277777671813965, loss=2.630098819732666
train: epoch 18, loss 0.14299651980400085, acc=0.9526110887527466, loss=0.14299651980400085
test: epoch 18, loss 2.1376113891601562, acc=0.43888887763023376, loss=2.1376113891601562
train: epoch 19, loss 0.14174339175224304, acc=0.9532777667045593, loss=0.14174339175224304
test: epoch 19, loss 1.889407753944397, acc=0.4694444537162781, loss=1.889407753944397
train: epoch 20, loss 0.12791717052459717, acc=0.9573333263397217, loss=0.12791717052459717
test: epoch 20, loss 2.6516780853271484, acc=0.4611110985279083, loss=2.6516780853271484
train: epoch 21, loss 0.1280161738395691, acc=0.9591110944747925, loss=0.1280161738395691
test: epoch 21, loss 2.430187940597534, acc=0.375, loss=2.430187940597534
train: epoch 22, loss 0.1209486573934555, acc=0.9599999785423279, loss=0.1209486573934555
test: epoch 22, loss 2.234100818634033, acc=0.4027777910232544, loss=2.234100818634033
train: epoch 23, loss 0.12176783382892609, acc=0.961222231388092, loss=0.12176783382892609
test: epoch 23, loss 2.1112282276153564, acc=0.40833333134651184, loss=2.1112282276153564
train: epoch 24, loss 0.12126505374908447, acc=0.9601666927337646, loss=0.12126505374908447
test: epoch 24, loss 2.513500452041626, acc=0.43611112236976624, loss=2.513500452041626
train: epoch 25, loss 0.12810291349887848, acc=0.9572222232818604, loss=0.12810291349887848
test: epoch 25, loss 3.01291823387146, acc=0.43611112236976624, loss=3.01291823387146
train: epoch 26, loss 0.10074808448553085, acc=0.9679999947547913, loss=0.10074808448553085
test: epoch 26, loss 2.081695556640625, acc=0.43611112236976624, loss=2.081695556640625
train: epoch 27, loss 0.10368641465902328, acc=0.9660555720329285, loss=0.10368641465902328
test: epoch 27, loss 2.2700915336608887, acc=0.47777777910232544, loss=2.2700915336608887
train: epoch 28, loss 0.09731818735599518, acc=0.9687222242355347, loss=0.09731818735599518
test: epoch 28, loss 1.9764928817749023, acc=0.4749999940395355, loss=1.9764928817749023
train: epoch 29, loss 0.07248873263597488, acc=0.9763888716697693, loss=0.07248873263597488
test: epoch 29, loss 2.065326690673828, acc=0.5305555462837219, loss=2.065326690673828
train: epoch 30, loss 0.0992046371102333, acc=0.9679999947547913, loss=0.0992046371102333
test: epoch 30, loss 2.35404634475708, acc=0.5416666865348816, loss=2.35404634475708
train: epoch 31, loss 0.08241644501686096, acc=0.9731666445732117, loss=0.08241644501686096
test: epoch 31, loss 2.3116044998168945, acc=0.5416666865348816, loss=2.3116044998168945
train: epoch 32, loss 0.08342436701059341, acc=0.9753333330154419, loss=0.08342436701059341
test: epoch 32, loss 1.7133742570877075, acc=0.6000000238418579, loss=1.7133742570877075
train: epoch 33, loss 0.07399493455886841, acc=0.975777804851532, loss=0.07399493455886841
test: epoch 33, loss 2.1512112617492676, acc=0.5166666507720947, loss=2.1512112617492676
train: epoch 34, loss 0.07182370871305466, acc=0.9775000214576721, loss=0.07182370871305466
test: epoch 34, loss 1.7589962482452393, acc=0.5972222089767456, loss=1.7589962482452393
train: epoch 35, loss 0.08214868605136871, acc=0.9742777943611145, loss=0.08214868605136871
test: epoch 35, loss 1.833992600440979, acc=0.5805555582046509, loss=1.833992600440979
train: epoch 36, loss 0.07865861058235168, acc=0.9762222170829773, loss=0.07865861058235168
test: epoch 36, loss 1.8592495918273926, acc=0.6361111402511597, loss=1.8592495918273926
train: epoch 37, loss 0.08092092722654343, acc=0.9760000109672546, loss=0.08092092722654343
test: epoch 37, loss 1.5859929323196411, acc=0.625, loss=1.5859929323196411
train: epoch 38, loss 0.07145035266876221, acc=0.9775000214576721, loss=0.07145035266876221
test: epoch 38, loss 1.5995439291000366, acc=0.5833333134651184, loss=1.5995439291000366
train: epoch 39, loss 0.05898873135447502, acc=0.9815555810928345, loss=0.05898873135447502
test: epoch 39, loss 1.2909355163574219, acc=0.6944444179534912, loss=1.2909355163574219
train: epoch 40, loss 0.08320078998804092, acc=0.9746111035346985, loss=0.08320078998804092
test: epoch 40, loss 0.930415689945221, acc=0.7861111164093018, loss=0.930415689945221
train: epoch 41, loss 0.05699765682220459, acc=0.9823889136314392, loss=0.05699765682220459
test: epoch 41, loss 1.3226373195648193, acc=0.7472222447395325, loss=1.3226373195648193
train: epoch 42, loss 0.05494297295808792, acc=0.9831110835075378, loss=0.05494297295808792
test: epoch 42, loss 0.9357927441596985, acc=0.7833333611488342, loss=0.9357927441596985
train: epoch 43, loss 0.0561780221760273, acc=0.9828888773918152, loss=0.0561780221760273
test: epoch 43, loss 0.4867765009403229, acc=0.8166666626930237, loss=0.4867765009403229
train: epoch 44, loss 0.055971503257751465, acc=0.9826111197471619, loss=0.055971503257751465
test: epoch 44, loss 0.650486409664154, acc=0.875, loss=0.650486409664154
train: epoch 45, loss 0.052415426820516586, acc=0.984666645526886, loss=0.052415426820516586
test: epoch 45, loss 1.0641062259674072, acc=0.7972221970558167, loss=1.0641062259674072
train: epoch 46, loss 0.055892519652843475, acc=0.9834444522857666, loss=0.055892519652843475
test: epoch 46, loss 0.9711271524429321, acc=0.824999988079071, loss=0.9711271524429321
train: epoch 47, loss 0.05105523020029068, acc=0.9847221970558167, loss=0.05105523020029068
test: epoch 47, loss 0.6689442992210388, acc=0.8166666626930237, loss=0.6689442992210388
train: epoch 48, loss 0.05025545507669449, acc=0.9852777719497681, loss=0.05025545507669449
test: epoch 48, loss 0.7349274754524231, acc=0.8611111044883728, loss=0.7349274754524231
train: epoch 49, loss 0.04928804934024811, acc=0.9853333234786987, loss=0.04928804934024811
test: epoch 49, loss 0.8927302956581116, acc=0.7972221970558167, loss=0.8927302956581116
train: epoch 50, loss 0.04434599354863167, acc=0.9863333106040955, loss=0.04434599354863167
test: epoch 50, loss 0.41305285692214966, acc=0.894444465637207, loss=0.41305285692214966
train: epoch 51, loss 0.05172307789325714, acc=0.9850000143051147, loss=0.05172307789325714
test: epoch 51, loss 0.3609999716281891, acc=0.9333333373069763, loss=0.3609999716281891
train: epoch 52, loss 0.04479040205478668, acc=0.987333357334137, loss=0.04479040205478668
test: epoch 52, loss 0.3992016315460205, acc=0.8805555701255798, loss=0.3992016315460205
train: epoch 53, loss 0.0384356826543808, acc=0.9893333315849304, loss=0.0384356826543808
test: epoch 53, loss 0.27896055579185486, acc=0.9194444417953491, loss=0.27896055579185486
train: epoch 54, loss 0.04303997382521629, acc=0.9879999756813049, loss=0.04303997382521629
test: epoch 54, loss 0.507672131061554, acc=0.9277777671813965, loss=0.507672131061554
train: epoch 55, loss 0.029948972165584564, acc=0.991777777671814, loss=0.029948972165584564
test: epoch 55, loss 0.26403558254241943, acc=0.9527778029441833, loss=0.26403558254241943
train: epoch 56, loss 0.03315281867980957, acc=0.9893333315849304, loss=0.03315281867980957
test: epoch 56, loss 0.3487708568572998, acc=0.9361110925674438, loss=0.3487708568572998
train: epoch 57, loss 0.037756867706775665, acc=0.9890555739402771, loss=0.037756867706775665
test: epoch 57, loss 0.2905283570289612, acc=0.9416666626930237, loss=0.2905283570289612
train: epoch 58, loss 0.03254145756363869, acc=0.9913333058357239, loss=0.03254145756363869
test: epoch 58, loss 0.2886638641357422, acc=0.9277777671813965, loss=0.2886638641357422
train: epoch 59, loss 0.03983069211244583, acc=0.988444447517395, loss=0.03983069211244583
test: epoch 59, loss 0.25436753034591675, acc=0.9527778029441833, loss=0.25436753034591675
train: epoch 60, loss 0.02681257203221321, acc=0.9920555353164673, loss=0.02681257203221321
test: epoch 60, loss 0.2716673016548157, acc=0.949999988079071, loss=0.2716673016548157
train: epoch 61, loss 0.034587446600198746, acc=0.9900000095367432, loss=0.034587446600198746
test: epoch 61, loss 0.20162811875343323, acc=0.949999988079071, loss=0.20162811875343323
train: epoch 62, loss 0.02724621817469597, acc=0.9922778010368347, loss=0.02724621817469597
test: epoch 62, loss 0.2282644510269165, acc=0.9583333134651184, loss=0.2282644510269165
train: epoch 63, loss 0.019608473405241966, acc=0.9936666488647461, loss=0.019608473405241966
test: epoch 63, loss 0.23376384377479553, acc=0.9611111283302307, loss=0.23376384377479553
train: epoch 64, loss 0.05025564134120941, acc=0.9866111278533936, loss=0.05025564134120941
test: epoch 64, loss 0.20954616367816925, acc=0.9583333134651184, loss=0.20954616367816925
train: epoch 65, loss 0.05052688717842102, acc=0.9858888983726501, loss=0.05052688717842102
test: epoch 65, loss 0.32047614455223083, acc=0.9472222328186035, loss=0.32047614455223083
train: epoch 66, loss 0.030125970020890236, acc=0.9918888807296753, loss=0.030125970020890236
test: epoch 66, loss 0.28992724418640137, acc=0.9472222328186035, loss=0.28992724418640137
train: epoch 67, loss 0.029890630394220352, acc=0.9913333058357239, loss=0.029890630394220352
test: epoch 67, loss 0.20148608088493347, acc=0.9583333134651184, loss=0.20148608088493347
train: epoch 68, loss 0.03241433575749397, acc=0.9917222261428833, loss=0.03241433575749397
test: epoch 68, loss 0.24856148660182953, acc=0.949999988079071, loss=0.24856148660182953
train: epoch 69, loss 0.029415572062134743, acc=0.9908333420753479, loss=0.029415572062134743
test: epoch 69, loss 0.21434175968170166, acc=0.9583333134651184, loss=0.21434175968170166
train: epoch 70, loss 0.032789938151836395, acc=0.9912222027778625, loss=0.032789938151836395
test: epoch 70, loss 0.2435348629951477, acc=0.9583333134651184, loss=0.2435348629951477
train: epoch 71, loss 0.02986758016049862, acc=0.99144446849823, loss=0.02986758016049862
test: epoch 71, loss 0.17881391942501068, acc=0.949999988079071, loss=0.17881391942501068
train: epoch 72, loss 0.033315613865852356, acc=0.9908888936042786, loss=0.033315613865852356
test: epoch 72, loss 0.20315611362457275, acc=0.9583333134651184, loss=0.20315611362457275
train: epoch 73, loss 0.02660427801311016, acc=0.9913889169692993, loss=0.02660427801311016
test: epoch 73, loss 0.22635485231876373, acc=0.949999988079071, loss=0.22635485231876373
train: epoch 74, loss 0.030235450714826584, acc=0.99144446849823, loss=0.030235450714826584
test: epoch 74, loss 0.2484893649816513, acc=0.949999988079071, loss=0.2484893649816513
train: epoch 75, loss 0.021183781325817108, acc=0.992888867855072, loss=0.021183781325817108
test: epoch 75, loss 0.1993228942155838, acc=0.9583333134651184, loss=0.1993228942155838
train: epoch 76, loss 0.02287210337817669, acc=0.9926666617393494, loss=0.02287210337817669
test: epoch 76, loss 0.1838219165802002, acc=0.9527778029441833, loss=0.1838219165802002
train: epoch 77, loss 0.022031040862202644, acc=0.9932222366333008, loss=0.022031040862202644
test: epoch 77, loss 0.2738824188709259, acc=0.9527778029441833, loss=0.2738824188709259
train: epoch 78, loss 0.018778763711452484, acc=0.9936110973358154, loss=0.018778763711452484
test: epoch 78, loss 0.1841224730014801, acc=0.9527778029441833, loss=0.1841224730014801
train: epoch 79, loss 0.02346762828528881, acc=0.9930555820465088, loss=0.02346762828528881
test: epoch 79, loss 0.28898927569389343, acc=0.949999988079071, loss=0.28898927569389343
train: epoch 80, loss 0.03533639758825302, acc=0.9901666641235352, loss=0.03533639758825302
test: epoch 80, loss 0.19982264935970306, acc=0.9611111283302307, loss=0.19982264935970306
train: epoch 81, loss 0.015820158645510674, acc=0.9942222237586975, loss=0.015820158645510674
test: epoch 81, loss 0.2896343767642975, acc=0.9583333134651184, loss=0.2896343767642975
train: epoch 82, loss 0.04030308872461319, acc=0.9910555481910706, loss=0.04030308872461319
test: epoch 82, loss 0.15047241747379303, acc=0.9611111283302307, loss=0.15047241747379303
train: epoch 83, loss 0.01569645293056965, acc=0.9947777986526489, loss=0.01569645293056965
test: epoch 83, loss 0.2740548849105835, acc=0.9611111283302307, loss=0.2740548849105835
train: epoch 84, loss 0.032684575766325, acc=0.9919999837875366, loss=0.032684575766325
test: epoch 84, loss 0.19412431120872498, acc=0.9611111283302307, loss=0.19412431120872498
train: epoch 85, loss 0.01886708475649357, acc=0.9926666617393494, loss=0.01886708475649357
test: epoch 85, loss 0.2817545533180237, acc=0.9583333134651184, loss=0.2817545533180237
train: epoch 86, loss 0.02461996115744114, acc=0.9917222261428833, loss=0.02461996115744114
test: epoch 86, loss 0.1382821500301361, acc=0.9611111283302307, loss=0.1382821500301361
train: epoch 87, loss 0.029539430513978004, acc=0.9913333058357239, loss=0.029539430513978004
test: epoch 87, loss 0.22811436653137207, acc=0.9611111283302307, loss=0.22811436653137207
train: epoch 88, loss 0.02349521592259407, acc=0.9936110973358154, loss=0.02349521592259407
test: epoch 88, loss 0.15386465191841125, acc=0.9611111283302307, loss=0.15386465191841125
train: epoch 89, loss 0.02707953006029129, acc=0.992222249507904, loss=0.02707953006029129
test: epoch 89, loss 0.19944047927856445, acc=0.9611111283302307, loss=0.19944047927856445
train: epoch 90, loss 0.01671559549868107, acc=0.9939444661140442, loss=0.01671559549868107
test: epoch 90, loss 0.24443435668945312, acc=0.9611111283302307, loss=0.24443435668945312
train: epoch 91, loss 0.020853986963629723, acc=0.9941666722297668, loss=0.020853986963629723
test: epoch 91, loss 0.20500081777572632, acc=0.9611111283302307, loss=0.20500081777572632
train: epoch 92, loss 0.037322234362363815, acc=0.9891666769981384, loss=0.037322234362363815
test: epoch 92, loss 0.2768104672431946, acc=0.9555555582046509, loss=0.2768104672431946
train: epoch 93, loss 0.028633134439587593, acc=0.9892777800559998, loss=0.028633134439587593
test: epoch 93, loss 0.18370471894741058, acc=0.9555555582046509, loss=0.18370471894741058
train: epoch 94, loss 0.03268163278698921, acc=0.988444447517395, loss=0.03268163278698921
test: epoch 94, loss 0.2695329189300537, acc=0.9361110925674438, loss=0.2695329189300537
train: epoch 95, loss 0.03092975541949272, acc=0.9888888597488403, loss=0.03092975541949272
test: epoch 95, loss 0.16625294089317322, acc=0.9555555582046509, loss=0.16625294089317322
train: epoch 96, loss 0.024150097742676735, acc=0.9898333549499512, loss=0.024150097742676735
test: epoch 96, loss 0.2521715760231018, acc=0.9555555582046509, loss=0.2521715760231018
train: epoch 97, loss 0.031588125973939896, acc=0.9877777695655823, loss=0.031588125973939896
test: epoch 97, loss 0.14580927789211273, acc=0.9555555582046509, loss=0.14580927789211273
train: epoch 98, loss 0.02691718563437462, acc=0.9892222285270691, loss=0.02691718563437462
test: epoch 98, loss 0.28797340393066406, acc=0.9555555582046509, loss=0.28797340393066406
train: epoch 99, loss 0.04037337750196457, acc=0.9881666898727417, loss=0.04037337750196457
test: epoch 99, loss 0.23252025246620178, acc=0.9555555582046509, loss=0.23252025246620178
train: epoch 100, loss 0.025877680629491806, acc=0.9896110892295837, loss=0.025877680629491806
test: epoch 100, loss 0.1732938438653946, acc=0.9555555582046509, loss=0.1732938438653946
train: epoch 101, loss 0.034459833055734634, acc=0.9887222051620483, loss=0.034459833055734634
test: epoch 101, loss 0.23297899961471558, acc=0.9555555582046509, loss=0.23297899961471558
train: epoch 102, loss 0.031832631677389145, acc=0.9893888831138611, loss=0.031832631677389145
test: epoch 102, loss 0.2249048948287964, acc=0.9583333134651184, loss=0.2249048948287964
train: epoch 103, loss 0.024341175332665443, acc=0.9931111335754395, loss=0.024341175332665443
test: epoch 103, loss 0.16564325988292694, acc=0.9611111283302307, loss=0.16564325988292694
train: epoch 104, loss 0.0238367710262537, acc=0.9943333268165588, loss=0.0238367710262537
test: epoch 104, loss 0.13179796934127808, acc=0.9611111283302307, loss=0.13179796934127808
train: epoch 105, loss 0.026711340993642807, acc=0.9936110973358154, loss=0.026711340993642807
test: epoch 105, loss 0.21710142493247986, acc=0.9611111283302307, loss=0.21710142493247986
train: epoch 106, loss 0.023777775466442108, acc=0.9934444427490234, loss=0.023777775466442108
test: epoch 106, loss 0.15820403397083282, acc=0.9611111283302307, loss=0.15820403397083282
train: epoch 107, loss 0.03475683182477951, acc=0.9905555844306946, loss=0.03475683182477951
test: epoch 107, loss 0.2749244272708893, acc=0.9611111283302307, loss=0.2749244272708893
train: epoch 108, loss 0.019366446882486343, acc=0.9941666722297668, loss=0.019366446882486343
test: epoch 108, loss 0.20658089220523834, acc=0.9611111283302307, loss=0.20658089220523834
train: epoch 109, loss 0.017306579276919365, acc=0.9945555329322815, loss=0.017306579276919365
test: epoch 109, loss 0.20990800857543945, acc=0.9611111283302307, loss=0.20990800857543945
train: epoch 110, loss 0.023936588317155838, acc=0.9926666617393494, loss=0.023936588317155838
test: epoch 110, loss 0.24406632781028748, acc=0.9611111283302307, loss=0.24406632781028748
train: epoch 111, loss 0.024053312838077545, acc=0.9931111335754395, loss=0.024053312838077545
test: epoch 111, loss 0.1655331403017044, acc=0.9611111283302307, loss=0.1655331403017044
train: epoch 112, loss 0.01877150870859623, acc=0.9943888783454895, loss=0.01877150870859623
test: epoch 112, loss 0.296897292137146, acc=0.9611111283302307, loss=0.296897292137146
train: epoch 113, loss 0.0242265984416008, acc=0.9931111335754395, loss=0.0242265984416008
test: epoch 113, loss 0.15521009266376495, acc=0.9611111283302307, loss=0.15521009266376495
train: epoch 114, loss 0.014884598553180695, acc=0.9955000281333923, loss=0.014884598553180695
test: epoch 114, loss 0.19570177793502808, acc=0.9611111283302307, loss=0.19570177793502808
train: epoch 115, loss 0.018364431336522102, acc=0.9937222003936768, loss=0.018364431336522102
test: epoch 115, loss 0.12574410438537598, acc=0.9611111283302307, loss=0.12574410438537598
train: epoch 116, loss 0.025236105546355247, acc=0.9934999942779541, loss=0.025236105546355247
test: epoch 116, loss 0.21042419970035553, acc=0.9583333134651184, loss=0.21042419970035553
train: epoch 117, loss 0.02725534699857235, acc=0.9932777881622314, loss=0.02725534699857235
test: epoch 117, loss 0.15930095314979553, acc=0.9611111283302307, loss=0.15930095314979553
train: epoch 118, loss 0.017803028225898743, acc=0.9944999814033508, loss=0.017803028225898743
test: epoch 118, loss 0.3842782974243164, acc=0.9611111283302307, loss=0.3842782974243164
train: epoch 119, loss 0.019770128652453423, acc=0.9948889017105103, loss=0.019770128652453423
test: epoch 119, loss 0.1838015466928482, acc=0.9611111283302307, loss=0.1838015466928482
train: epoch 120, loss 0.021408475935459137, acc=0.9943333268165588, loss=0.021408475935459137
test: epoch 120, loss 0.3391527533531189, acc=0.9555555582046509, loss=0.3391527533531189
train: epoch 121, loss 0.018317583948373795, acc=0.9948889017105103, loss=0.018317583948373795
test: epoch 121, loss 0.25010401010513306, acc=0.9611111283302307, loss=0.25010401010513306
train: epoch 122, loss 0.028002193197607994, acc=0.9943888783454895, loss=0.028002193197607994
test: epoch 122, loss 0.19804030656814575, acc=0.9638888835906982, loss=0.19804030656814575
train: epoch 123, loss 0.019517481327056885, acc=0.9947777986526489, loss=0.019517481327056885
test: epoch 123, loss 0.16803455352783203, acc=0.9638888835906982, loss=0.16803455352783203
train: epoch 124, loss 0.016129963099956512, acc=0.9952777624130249, loss=0.016129963099956512
test: epoch 124, loss 0.15446996688842773, acc=0.9638888835906982, loss=0.15446996688842773
train: epoch 125, loss 0.018602367490530014, acc=0.9947222471237183, loss=0.018602367490530014
test: epoch 125, loss 0.19694684445858002, acc=0.9638888835906982, loss=0.19694684445858002
train: epoch 126, loss 0.034181270748376846, acc=0.9911110997200012, loss=0.034181270748376846
test: epoch 126, loss 0.30054110288619995, acc=0.9555555582046509, loss=0.30054110288619995
train: epoch 127, loss 0.04035526514053345, acc=0.9863888621330261, loss=0.04035526514053345
test: epoch 127, loss 0.18713627755641937, acc=0.9555555582046509, loss=0.18713627755641937
train: epoch 128, loss 0.02380824089050293, acc=0.9936666488647461, loss=0.02380824089050293
test: epoch 128, loss 0.18892662227153778, acc=0.9638888835906982, loss=0.18892662227153778
train: epoch 129, loss 0.01674078218638897, acc=0.9956111311912537, loss=0.01674078218638897
test: epoch 129, loss 0.19052229821681976, acc=0.9638888835906982, loss=0.19052229821681976
train: epoch 130, loss 0.02392893098294735, acc=0.9933333396911621, loss=0.02392893098294735
test: epoch 130, loss 0.21349473297595978, acc=0.9611111283302307, loss=0.21349473297595978
train: epoch 131, loss 0.02345386892557144, acc=0.9916666746139526, loss=0.02345386892557144
test: epoch 131, loss 0.1443072408437729, acc=0.9611111283302307, loss=0.1443072408437729
train: epoch 132, loss 0.02300177700817585, acc=0.992222249507904, loss=0.02300177700817585
test: epoch 132, loss 0.14552292227745056, acc=0.9611111283302307, loss=0.14552292227745056
train: epoch 133, loss 0.03863029181957245, acc=0.9905555844306946, loss=0.03863029181957245
test: epoch 133, loss 0.19092051684856415, acc=0.9583333134651184, loss=0.19092051684856415
train: epoch 134, loss 0.03217940405011177, acc=0.9888333082199097, loss=0.03217940405011177
test: epoch 134, loss 0.10290784388780594, acc=0.9583333134651184, loss=0.10290784388780594
train: epoch 135, loss 0.028827210888266563, acc=0.988444447517395, loss=0.028827210888266563
test: epoch 135, loss 0.17844656109809875, acc=0.9583333134651184, loss=0.17844656109809875
train: epoch 136, loss 0.029043201357126236, acc=0.9896666407585144, loss=0.029043201357126236
test: epoch 136, loss 0.26655539870262146, acc=0.9583333134651184, loss=0.26655539870262146
train: epoch 137, loss 0.03239592909812927, acc=0.9892777800559998, loss=0.03239592909812927
test: epoch 137, loss 0.15517018735408783, acc=0.9583333134651184, loss=0.15517018735408783
train: epoch 138, loss 0.04128938168287277, acc=0.9872221946716309, loss=0.04128938168287277
test: epoch 138, loss 0.28540754318237305, acc=0.9583333134651184, loss=0.28540754318237305
train: epoch 139, loss 0.034943368285894394, acc=0.9890000224113464, loss=0.034943368285894394
test: epoch 139, loss 0.16867226362228394, acc=0.9583333134651184, loss=0.16867226362228394
train: epoch 140, loss 0.03232679143548012, acc=0.9882222414016724, loss=0.03232679143548012
test: epoch 140, loss 0.1455293744802475, acc=0.9611111283302307, loss=0.1455293744802475
train: epoch 141, loss 0.03316598758101463, acc=0.9919999837875366, loss=0.03316598758101463
test: epoch 141, loss 0.16444271802902222, acc=0.9611111283302307, loss=0.16444271802902222
train: epoch 142, loss 0.030226580798625946, acc=0.9903888702392578, loss=0.030226580798625946
test: epoch 142, loss 0.15074589848518372, acc=0.9583333134651184, loss=0.15074589848518372
train: epoch 143, loss 0.029803728684782982, acc=0.9901666641235352, loss=0.029803728684782982
test: epoch 143, loss 0.17121395468711853, acc=0.9611111283302307, loss=0.17121395468711853
train: epoch 144, loss 0.03318114951252937, acc=0.9905555844306946, loss=0.03318114951252937
test: epoch 144, loss 0.30257031321525574, acc=0.9611111283302307, loss=0.30257031321525574
train: epoch 145, loss 0.024462861940264702, acc=0.9905555844306946, loss=0.024462861940264702
test: epoch 145, loss 0.20758336782455444, acc=0.9583333134651184, loss=0.20758336782455444
train: epoch 146, loss 0.02447754330933094, acc=0.991777777671814, loss=0.02447754330933094
test: epoch 146, loss 0.20761196315288544, acc=0.9611111283302307, loss=0.20761196315288544
train: epoch 147, loss 0.029530858621001244, acc=0.9915000200271606, loss=0.029530858621001244
test: epoch 147, loss 0.14263221621513367, acc=0.9611111283302307, loss=0.14263221621513367
train: epoch 148, loss 0.02808278612792492, acc=0.992555558681488, loss=0.02808278612792492
test: epoch 148, loss 0.19583654403686523, acc=0.9611111283302307, loss=0.19583654403686523
train: epoch 149, loss 0.02411460690200329, acc=0.9911666512489319, loss=0.02411460690200329
test: epoch 149, loss 0.21061912178993225, acc=0.9611111283302307, loss=0.21061912178993225
train: epoch 150, loss 0.02466106228530407, acc=0.9912777543067932, loss=0.02466106228530407
test: epoch 150, loss 0.17781969904899597, acc=0.9611111283302307, loss=0.17781969904899597
