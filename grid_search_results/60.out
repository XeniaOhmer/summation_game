# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=100", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.995", "--one_hot=true", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_summands=2, n_symbols=100, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=980273925, receiver_embed_dim=64, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.453134059906006, acc=0.04916666820645332, loss=3.453134059906006
test: epoch 1, loss 3.371741533279419, acc=0.07500000298023224, loss=3.371741533279419
train: epoch 2, loss 2.432590961456299, acc=0.15755555033683777, loss=2.432590961456299
test: epoch 2, loss 2.1758832931518555, acc=0.14722222089767456, loss=2.1758832931518555
train: epoch 3, loss 1.837735652923584, acc=0.258388876914978, loss=1.837735652923584
test: epoch 3, loss 2.084066390991211, acc=0.16388888657093048, loss=2.084066390991211
train: epoch 4, loss 1.6476556062698364, acc=0.32055556774139404, loss=1.6476556062698364
test: epoch 4, loss 2.0958259105682373, acc=0.1805555522441864, loss=2.0958259105682373
train: epoch 5, loss 1.5543874502182007, acc=0.35249999165534973, loss=1.5543874502182007
test: epoch 5, loss 2.128551959991455, acc=0.1944444477558136, loss=2.128551959991455
train: epoch 6, loss 1.4835220575332642, acc=0.3779444396495819, loss=1.4835220575332642
test: epoch 6, loss 2.170114278793335, acc=0.20000000298023224, loss=2.170114278793335
train: epoch 7, loss 1.4324655532836914, acc=0.3927222192287445, loss=1.4324655532836914
test: epoch 7, loss 2.1185810565948486, acc=0.1944444477558136, loss=2.1185810565948486
train: epoch 8, loss 1.4038649797439575, acc=0.4067777693271637, loss=1.4038649797439575
test: epoch 8, loss 2.1225407123565674, acc=0.19722221791744232, loss=2.1225407123565674
train: epoch 9, loss 1.363979458808899, acc=0.4258333444595337, loss=1.363979458808899
test: epoch 9, loss 2.076807975769043, acc=0.20555555820465088, loss=2.076807975769043
train: epoch 10, loss 1.329171061515808, acc=0.4381110966205597, loss=1.329171061515808
test: epoch 10, loss 2.0820863246917725, acc=0.20000000298023224, loss=2.0820863246917725
train: epoch 11, loss 1.2867826223373413, acc=0.4493333399295807, loss=1.2867826223373413
test: epoch 11, loss 2.051187515258789, acc=0.2361111044883728, loss=2.051187515258789
train: epoch 12, loss 1.2651017904281616, acc=0.4571666717529297, loss=1.2651017904281616
test: epoch 12, loss 2.000370502471924, acc=0.24166665971279144, loss=2.000370502471924
train: epoch 13, loss 1.2441951036453247, acc=0.4740000069141388, loss=1.2441951036453247
test: epoch 13, loss 1.9238734245300293, acc=0.22777777910232544, loss=1.9238734245300293
train: epoch 14, loss 1.199712872505188, acc=0.4883333444595337, loss=1.199712872505188
test: epoch 14, loss 1.9408938884735107, acc=0.23888888955116272, loss=1.9408938884735107
train: epoch 15, loss 1.1754204034805298, acc=0.5011666417121887, loss=1.1754204034805298
test: epoch 15, loss 1.9811557531356812, acc=0.23333333432674408, loss=1.9811557531356812
train: epoch 16, loss 1.1480520963668823, acc=0.5122222304344177, loss=1.1480520963668823
test: epoch 16, loss 1.9264473915100098, acc=0.24166665971279144, loss=1.9264473915100098
train: epoch 17, loss 1.1295884847640991, acc=0.5149444341659546, loss=1.1295884847640991
test: epoch 17, loss 1.9644757509231567, acc=0.24722221493721008, loss=1.9644757509231567
train: epoch 18, loss 1.1196441650390625, acc=0.5208888649940491, loss=1.1196441650390625
test: epoch 18, loss 1.9445886611938477, acc=0.24722221493721008, loss=1.9445886611938477
train: epoch 19, loss 1.092940330505371, acc=0.5265555381774902, loss=1.092940330505371
test: epoch 19, loss 1.974331021308899, acc=0.25555557012557983, loss=1.974331021308899
train: epoch 20, loss 1.0866159200668335, acc=0.5321666598320007, loss=1.0866159200668335
test: epoch 20, loss 2.006391763687134, acc=0.25555557012557983, loss=2.006391763687134
train: epoch 21, loss 1.0758246183395386, acc=0.5371666550636292, loss=1.0758246183395386
test: epoch 21, loss 2.0328760147094727, acc=0.25, loss=2.0328760147094727
train: epoch 22, loss 1.0636398792266846, acc=0.5421110987663269, loss=1.0636398792266846
test: epoch 22, loss 2.1265335083007812, acc=0.25, loss=2.1265335083007812
train: epoch 23, loss 1.0673489570617676, acc=0.5420555472373962, loss=1.0673489570617676
test: epoch 23, loss 2.0354650020599365, acc=0.25555557012557983, loss=2.0354650020599365
train: epoch 24, loss 1.0427985191345215, acc=0.550166666507721, loss=1.0427985191345215
test: epoch 24, loss 1.9858193397521973, acc=0.26944443583488464, loss=1.9858193397521973
train: epoch 25, loss 1.0395519733428955, acc=0.5535555481910706, loss=1.0395519733428955
test: epoch 25, loss 1.9945907592773438, acc=0.2638888955116272, loss=1.9945907592773438
train: epoch 26, loss 1.0290453433990479, acc=0.5588889122009277, loss=1.0290453433990479
test: epoch 26, loss 2.0941336154937744, acc=0.2611111104488373, loss=2.0941336154937744
train: epoch 27, loss 1.0208848714828491, acc=0.5568888783454895, loss=1.0208848714828491
test: epoch 27, loss 2.0916357040405273, acc=0.2611111104488373, loss=2.0916357040405273
train: epoch 28, loss 1.0105228424072266, acc=0.5650555491447449, loss=1.0105228424072266
test: epoch 28, loss 2.008382558822632, acc=0.2666666805744171, loss=2.008382558822632
train: epoch 29, loss 1.015120267868042, acc=0.5633333325386047, loss=1.015120267868042
test: epoch 29, loss 2.0876212120056152, acc=0.2638888955116272, loss=2.0876212120056152
train: epoch 30, loss 1.006876826286316, acc=0.5671111345291138, loss=1.006876826286316
test: epoch 30, loss 1.960435390472412, acc=0.2666666805744171, loss=1.960435390472412
train: epoch 31, loss 0.9876753687858582, acc=0.5751110911369324, loss=0.9876753687858582
test: epoch 31, loss 2.0639760494232178, acc=0.2638888955116272, loss=2.0639760494232178
train: epoch 32, loss 1.000319004058838, acc=0.5733333230018616, loss=1.000319004058838
test: epoch 32, loss 2.007422685623169, acc=0.2666666805744171, loss=2.007422685623169
train: epoch 33, loss 0.9917703866958618, acc=0.5798333287239075, loss=0.9917703866958618
test: epoch 33, loss 1.9694020748138428, acc=0.2777777910232544, loss=1.9694020748138428
train: epoch 34, loss 0.9843345880508423, acc=0.5753889083862305, loss=0.9843345880508423
test: epoch 34, loss 2.154069423675537, acc=0.2666666805744171, loss=2.154069423675537
train: epoch 35, loss 0.994421124458313, acc=0.5790555477142334, loss=0.994421124458313
test: epoch 35, loss 2.059968948364258, acc=0.27222222089767456, loss=2.059968948364258
train: epoch 36, loss 0.9742643237113953, acc=0.5836111307144165, loss=0.9742643237113953
test: epoch 36, loss 1.9741548299789429, acc=0.2805555462837219, loss=1.9741548299789429
train: epoch 37, loss 0.9622933268547058, acc=0.5841666460037231, loss=0.9622933268547058
test: epoch 37, loss 1.954999327659607, acc=0.28611111640930176, loss=1.954999327659607
train: epoch 38, loss 0.9726677536964417, acc=0.5875555276870728, loss=0.9726677536964417
test: epoch 38, loss 2.047809362411499, acc=0.2750000059604645, loss=2.047809362411499
train: epoch 39, loss 0.9684566855430603, acc=0.5892778038978577, loss=0.9684566855430603
test: epoch 39, loss 2.0983657836914062, acc=0.2888889014720917, loss=2.0983657836914062
train: epoch 40, loss 0.9639319181442261, acc=0.5876666903495789, loss=0.9639319181442261
test: epoch 40, loss 2.094180107116699, acc=0.2888889014720917, loss=2.094180107116699
train: epoch 41, loss 0.9527361392974854, acc=0.5950555801391602, loss=0.9527361392974854
test: epoch 41, loss 2.04555082321167, acc=0.2944444417953491, loss=2.04555082321167
train: epoch 42, loss 0.9610837697982788, acc=0.5924444198608398, loss=0.9610837697982788
test: epoch 42, loss 1.974423885345459, acc=0.2916666567325592, loss=1.974423885345459
train: epoch 43, loss 0.9640100002288818, acc=0.5902222394943237, loss=0.9640100002288818
test: epoch 43, loss 2.075888156890869, acc=0.2777777910232544, loss=2.075888156890869
train: epoch 44, loss 0.9557379484176636, acc=0.5957777500152588, loss=0.9557379484176636
test: epoch 44, loss 2.0349059104919434, acc=0.2944444417953491, loss=2.0349059104919434
train: epoch 45, loss 0.9489386081695557, acc=0.5966110825538635, loss=0.9489386081695557
test: epoch 45, loss 2.020686626434326, acc=0.2944444417953491, loss=2.020686626434326
train: epoch 46, loss 0.9494786262512207, acc=0.5967777967453003, loss=0.9494786262512207
test: epoch 46, loss 1.926667332649231, acc=0.30000001192092896, loss=1.926667332649231
train: epoch 47, loss 0.9430927038192749, acc=0.6016666889190674, loss=0.9430927038192749
test: epoch 47, loss 1.9718208312988281, acc=0.29722222685813904, loss=1.9718208312988281
train: epoch 48, loss 0.9491087198257446, acc=0.5983333587646484, loss=0.9491087198257446
test: epoch 48, loss 2.0725512504577637, acc=0.3055555522441864, loss=2.0725512504577637
train: epoch 49, loss 0.9649100303649902, acc=0.5913333296775818, loss=0.9649100303649902
test: epoch 49, loss 2.009678602218628, acc=0.3027777671813965, loss=2.009678602218628
train: epoch 50, loss 0.9431301355361938, acc=0.5972777605056763, loss=0.9431301355361938
test: epoch 50, loss 2.094129800796509, acc=0.3083333373069763, loss=2.094129800796509
train: epoch 51, loss 0.9400191307067871, acc=0.5995555520057678, loss=0.9400191307067871
test: epoch 51, loss 2.102567195892334, acc=0.29722222685813904, loss=2.102567195892334
train: epoch 52, loss 0.9461339712142944, acc=0.5997222065925598, loss=0.9461339712142944
test: epoch 52, loss 1.9809437990188599, acc=0.3083333373069763, loss=1.9809437990188599
train: epoch 53, loss 0.9486626386642456, acc=0.5967777967453003, loss=0.9486626386642456
test: epoch 53, loss 2.018747568130493, acc=0.3027777671813965, loss=2.018747568130493
train: epoch 54, loss 0.9439831972122192, acc=0.5993888974189758, loss=0.9439831972122192
test: epoch 54, loss 1.974357008934021, acc=0.3055555522441864, loss=1.974357008934021
train: epoch 55, loss 0.9415984749794006, acc=0.5997222065925598, loss=0.9415984749794006
test: epoch 55, loss 1.9568480253219604, acc=0.3055555522441864, loss=1.9568480253219604
train: epoch 56, loss 0.9481692314147949, acc=0.5910555720329285, loss=0.9481692314147949
test: epoch 56, loss 2.026740312576294, acc=0.3055555522441864, loss=2.026740312576294
train: epoch 57, loss 0.9321695566177368, acc=0.6015555262565613, loss=0.9321695566177368
test: epoch 57, loss 2.0901143550872803, acc=0.3055555522441864, loss=2.0901143550872803
train: epoch 58, loss 0.9431254267692566, acc=0.6003333330154419, loss=0.9431254267692566
test: epoch 58, loss 2.0505800247192383, acc=0.3083333373069763, loss=2.0505800247192383
train: epoch 59, loss 0.9470899105072021, acc=0.6012222170829773, loss=0.9470899105072021
test: epoch 59, loss 2.1690480709075928, acc=0.30000001192092896, loss=2.1690480709075928
train: epoch 60, loss 0.9394229054450989, acc=0.5974444150924683, loss=0.9394229054450989
test: epoch 60, loss 2.175203323364258, acc=0.3083333373069763, loss=2.175203323364258
train: epoch 61, loss 0.941200315952301, acc=0.5975000262260437, loss=0.941200315952301
test: epoch 61, loss 2.0492138862609863, acc=0.31111112236976624, loss=2.0492138862609863
train: epoch 62, loss 0.9461508393287659, acc=0.5967222452163696, loss=0.9461508393287659
test: epoch 62, loss 2.2027316093444824, acc=0.3055555522441864, loss=2.2027316093444824
train: epoch 63, loss 0.9495463967323303, acc=0.598111093044281, loss=0.9495463967323303
test: epoch 63, loss 1.980880618095398, acc=0.3083333373069763, loss=1.980880618095398
train: epoch 64, loss 0.9439464211463928, acc=0.5998333096504211, loss=0.9439464211463928
test: epoch 64, loss 2.0376408100128174, acc=0.31111112236976624, loss=2.0376408100128174
train: epoch 65, loss 0.9358705878257751, acc=0.6010555624961853, loss=0.9358705878257751
test: epoch 65, loss 2.1295316219329834, acc=0.3083333373069763, loss=2.1295316219329834
train: epoch 66, loss 0.9266660213470459, acc=0.6066111326217651, loss=0.9266660213470459
test: epoch 66, loss 2.0017447471618652, acc=0.3055555522441864, loss=2.0017447471618652
train: epoch 67, loss 0.9413854479789734, acc=0.6010000109672546, loss=0.9413854479789734
test: epoch 67, loss 2.032827615737915, acc=0.3083333373069763, loss=2.032827615737915
train: epoch 68, loss 0.9355584979057312, acc=0.6013888716697693, loss=0.9355584979057312
test: epoch 68, loss 2.0927603244781494, acc=0.3055555522441864, loss=2.0927603244781494
train: epoch 69, loss 0.9431012868881226, acc=0.6014444231987, loss=0.9431012868881226
test: epoch 69, loss 2.0141489505767822, acc=0.3083333373069763, loss=2.0141489505767822
train: epoch 70, loss 0.9261699318885803, acc=0.6064444184303284, loss=0.9261699318885803
test: epoch 70, loss 2.087472677230835, acc=0.3083333373069763, loss=2.087472677230835
train: epoch 71, loss 0.9235713481903076, acc=0.6037222146987915, loss=0.9235713481903076
test: epoch 71, loss 2.0250203609466553, acc=0.31111112236976624, loss=2.0250203609466553
train: epoch 72, loss 0.9360684752464294, acc=0.5981666445732117, loss=0.9360684752464294
test: epoch 72, loss 2.0502171516418457, acc=0.3083333373069763, loss=2.0502171516418457
train: epoch 73, loss 0.9219608902931213, acc=0.6066111326217651, loss=0.9219608902931213
test: epoch 73, loss 2.094085931777954, acc=0.31111112236976624, loss=2.094085931777954
train: epoch 74, loss 0.9221229553222656, acc=0.6025000214576721, loss=0.9221229553222656
test: epoch 74, loss 2.1814653873443604, acc=0.31111112236976624, loss=2.1814653873443604
train: epoch 75, loss 0.9199798703193665, acc=0.6058889031410217, loss=0.9199798703193665
test: epoch 75, loss 2.168984889984131, acc=0.3083333373069763, loss=2.168984889984131
train: epoch 76, loss 0.929253339767456, acc=0.6019444465637207, loss=0.929253339767456
test: epoch 76, loss 2.0230767726898193, acc=0.3083333373069763, loss=2.0230767726898193
train: epoch 77, loss 0.9150528311729431, acc=0.6072221994400024, loss=0.9150528311729431
test: epoch 77, loss 2.211237907409668, acc=0.3055555522441864, loss=2.211237907409668
train: epoch 78, loss 0.917701780796051, acc=0.6068333387374878, loss=0.917701780796051
test: epoch 78, loss 2.0187418460845947, acc=0.31111112236976624, loss=2.0187418460845947
train: epoch 79, loss 0.918684184551239, acc=0.6090555787086487, loss=0.918684184551239
test: epoch 79, loss 2.178941011428833, acc=0.31111112236976624, loss=2.178941011428833
train: epoch 80, loss 0.920024037361145, acc=0.6073333621025085, loss=0.920024037361145
test: epoch 80, loss 2.087749719619751, acc=0.3083333373069763, loss=2.087749719619751
train: epoch 81, loss 0.9137117862701416, acc=0.6077777743339539, loss=0.9137117862701416
test: epoch 81, loss 2.2833261489868164, acc=0.31388887763023376, loss=2.2833261489868164
train: epoch 82, loss 0.919782280921936, acc=0.6075555682182312, loss=0.919782280921936
test: epoch 82, loss 2.1790053844451904, acc=0.31388887763023376, loss=2.1790053844451904
train: epoch 83, loss 0.9156944155693054, acc=0.6038333177566528, loss=0.9156944155693054
test: epoch 83, loss 1.9682615995407104, acc=0.31388887763023376, loss=1.9682615995407104
train: epoch 84, loss 0.9160485863685608, acc=0.6066111326217651, loss=0.9160485863685608
test: epoch 84, loss 2.1514463424682617, acc=0.3055555522441864, loss=2.1514463424682617
train: epoch 85, loss 0.9088042974472046, acc=0.6097777485847473, loss=0.9088042974472046
test: epoch 85, loss 2.037221908569336, acc=0.3166666626930237, loss=2.037221908569336
train: epoch 86, loss 0.9073711633682251, acc=0.6087777614593506, loss=0.9073711633682251
test: epoch 86, loss 2.031924247741699, acc=0.31388887763023376, loss=2.031924247741699
train: epoch 87, loss 0.9049670100212097, acc=0.613777756690979, loss=0.9049670100212097
test: epoch 87, loss 1.989572525024414, acc=0.3166666626930237, loss=1.989572525024414
train: epoch 88, loss 0.9115228056907654, acc=0.6126111149787903, loss=0.9115228056907654
test: epoch 88, loss 1.933699369430542, acc=0.3166666626930237, loss=1.933699369430542
train: epoch 89, loss 0.909292459487915, acc=0.6101111173629761, loss=0.909292459487915
test: epoch 89, loss 2.1265900135040283, acc=0.3194444477558136, loss=2.1265900135040283
train: epoch 90, loss 0.9067683219909668, acc=0.6084444522857666, loss=0.9067683219909668
test: epoch 90, loss 2.0767714977264404, acc=0.31388887763023376, loss=2.0767714977264404
train: epoch 91, loss 0.9086066484451294, acc=0.608222246170044, loss=0.9086066484451294
test: epoch 91, loss 2.0848770141601562, acc=0.31388887763023376, loss=2.0848770141601562
train: epoch 92, loss 0.8970727920532227, acc=0.6150000095367432, loss=0.8970727920532227
test: epoch 92, loss 2.1391305923461914, acc=0.3194444477558136, loss=2.1391305923461914
train: epoch 93, loss 0.896140992641449, acc=0.6152222156524658, loss=0.896140992641449
test: epoch 93, loss 1.9888399839401245, acc=0.3194444477558136, loss=1.9888399839401245
train: epoch 94, loss 0.8902669548988342, acc=0.6173333525657654, loss=0.8902669548988342
test: epoch 94, loss 2.0422744750976562, acc=0.3166666626930237, loss=2.0422744750976562
train: epoch 95, loss 0.8904785513877869, acc=0.6168888807296753, loss=0.8904785513877869
test: epoch 95, loss 2.2775039672851562, acc=0.31388887763023376, loss=2.2775039672851562
train: epoch 96, loss 0.8999872207641602, acc=0.6139444708824158, loss=0.8999872207641602
test: epoch 96, loss 2.197796106338501, acc=0.3194444477558136, loss=2.197796106338501
train: epoch 97, loss 0.9043366312980652, acc=0.6099444627761841, loss=0.9043366312980652
test: epoch 97, loss 2.231790542602539, acc=0.3194444477558136, loss=2.231790542602539
train: epoch 98, loss 0.8984891772270203, acc=0.6111666560173035, loss=0.8984891772270203
test: epoch 98, loss 2.062054395675659, acc=0.3194444477558136, loss=2.062054395675659
train: epoch 99, loss 0.9001018404960632, acc=0.612500011920929, loss=0.9001018404960632
test: epoch 99, loss 1.9213128089904785, acc=0.32499998807907104, loss=1.9213128089904785
train: epoch 100, loss 0.8951297402381897, acc=0.6138888597488403, loss=0.8951297402381897
test: epoch 100, loss 2.1665735244750977, acc=0.3166666626930237, loss=2.1665735244750977
train: epoch 101, loss 0.8896524906158447, acc=0.6192777752876282, loss=0.8896524906158447
test: epoch 101, loss 2.0321552753448486, acc=0.32777777314186096, loss=2.0321552753448486
train: epoch 102, loss 0.8822607398033142, acc=0.6227222084999084, loss=0.8822607398033142
test: epoch 102, loss 2.1742136478424072, acc=0.3222222328186035, loss=2.1742136478424072
train: epoch 103, loss 0.8812159299850464, acc=0.6256666779518127, loss=0.8812159299850464
test: epoch 103, loss 2.0744576454162598, acc=0.32777777314186096, loss=2.0744576454162598
train: epoch 104, loss 0.8907694816589355, acc=0.6151666641235352, loss=0.8907694816589355
test: epoch 104, loss 2.109623432159424, acc=0.32499998807907104, loss=2.109623432159424
train: epoch 105, loss 0.8854220509529114, acc=0.6197777986526489, loss=0.8854220509529114
test: epoch 105, loss 2.0331523418426514, acc=0.32499998807907104, loss=2.0331523418426514
train: epoch 106, loss 0.8929129242897034, acc=0.6192222237586975, loss=0.8929129242897034
test: epoch 106, loss 2.1470839977264404, acc=0.32499998807907104, loss=2.1470839977264404
train: epoch 107, loss 0.8902135491371155, acc=0.6203333139419556, loss=0.8902135491371155
test: epoch 107, loss 2.036618232727051, acc=0.32499998807907104, loss=2.036618232727051
train: epoch 108, loss 0.8853023052215576, acc=0.6201666593551636, loss=0.8853023052215576
test: epoch 108, loss 1.981709361076355, acc=0.32499998807907104, loss=1.981709361076355
train: epoch 109, loss 0.8808846473693848, acc=0.6207777857780457, loss=0.8808846473693848
test: epoch 109, loss 2.2082595825195312, acc=0.3333333432674408, loss=2.2082595825195312
train: epoch 110, loss 0.9059332013130188, acc=0.6193888783454895, loss=0.9059332013130188
test: epoch 110, loss 2.1011996269226074, acc=0.3222222328186035, loss=2.1011996269226074
train: epoch 111, loss 0.8838583827018738, acc=0.621222198009491, loss=0.8838583827018738
test: epoch 111, loss 2.2520084381103516, acc=0.3305555582046509, loss=2.2520084381103516
train: epoch 112, loss 0.8936687707901001, acc=0.620555579662323, loss=0.8936687707901001
test: epoch 112, loss 2.0427825450897217, acc=0.3361110985279083, loss=2.0427825450897217
train: epoch 113, loss 0.8849548697471619, acc=0.6177777647972107, loss=0.8849548697471619
test: epoch 113, loss 2.0272531509399414, acc=0.32777777314186096, loss=2.0272531509399414
train: epoch 114, loss 0.8766373991966248, acc=0.6240555644035339, loss=0.8766373991966248
test: epoch 114, loss 2.046191692352295, acc=0.3361110985279083, loss=2.046191692352295
train: epoch 115, loss 0.8858493566513062, acc=0.6176666617393494, loss=0.8858493566513062
test: epoch 115, loss 2.153818368911743, acc=0.33888888359069824, loss=2.153818368911743
train: epoch 116, loss 0.874976634979248, acc=0.625166654586792, loss=0.874976634979248
test: epoch 116, loss 2.084822654724121, acc=0.3305555582046509, loss=2.084822654724121
train: epoch 117, loss 0.8715953230857849, acc=0.6244444251060486, loss=0.8715953230857849
test: epoch 117, loss 2.0760536193847656, acc=0.3361110985279083, loss=2.0760536193847656
train: epoch 118, loss 0.8738653063774109, acc=0.6279444694519043, loss=0.8738653063774109
test: epoch 118, loss 2.0801477432250977, acc=0.33888888359069824, loss=2.0801477432250977
train: epoch 119, loss 0.8813278675079346, acc=0.6238333582878113, loss=0.8813278675079346
test: epoch 119, loss 2.2156870365142822, acc=0.33888888359069824, loss=2.2156870365142822
train: epoch 120, loss 0.8761430382728577, acc=0.6240000128746033, loss=0.8761430382728577
test: epoch 120, loss 2.0573785305023193, acc=0.32777777314186096, loss=2.0573785305023193
train: epoch 121, loss 0.871806263923645, acc=0.6242777705192566, loss=0.871806263923645
test: epoch 121, loss 2.4322586059570312, acc=0.32499998807907104, loss=2.4322586059570312
train: epoch 122, loss 0.8727781772613525, acc=0.6240555644035339, loss=0.8727781772613525
test: epoch 122, loss 2.2545077800750732, acc=0.3305555582046509, loss=2.2545077800750732
train: epoch 123, loss 0.879607617855072, acc=0.6224444508552551, loss=0.879607617855072
test: epoch 123, loss 2.321087121963501, acc=0.33888888359069824, loss=2.321087121963501
train: epoch 124, loss 0.8819824457168579, acc=0.6233333349227905, loss=0.8819824457168579
test: epoch 124, loss 2.173715829849243, acc=0.3305555582046509, loss=2.173715829849243
train: epoch 125, loss 0.8733437061309814, acc=0.6237778067588806, loss=0.8733437061309814
test: epoch 125, loss 2.1059255599975586, acc=0.33888888359069824, loss=2.1059255599975586
train: epoch 126, loss 0.8802573084831238, acc=0.6234999895095825, loss=0.8802573084831238
test: epoch 126, loss 2.1368496417999268, acc=0.33888888359069824, loss=2.1368496417999268
train: epoch 127, loss 0.8572362661361694, acc=0.6266111135482788, loss=0.8572362661361694
test: epoch 127, loss 2.1061394214630127, acc=0.3333333432674408, loss=2.1061394214630127
train: epoch 128, loss 0.8636110424995422, acc=0.628777801990509, loss=0.8636110424995422
test: epoch 128, loss 2.2406721115112305, acc=0.33888888359069824, loss=2.2406721115112305
train: epoch 129, loss 0.8769374489784241, acc=0.6227777600288391, loss=0.8769374489784241
test: epoch 129, loss 2.011932134628296, acc=0.3333333432674408, loss=2.011932134628296
train: epoch 130, loss 0.85989910364151, acc=0.6270555257797241, loss=0.85989910364151
test: epoch 130, loss 2.2706642150878906, acc=0.33888888359069824, loss=2.2706642150878906
train: epoch 131, loss 0.8746476173400879, acc=0.6223333477973938, loss=0.8746476173400879
test: epoch 131, loss 2.2950026988983154, acc=0.32777777314186096, loss=2.2950026988983154
train: epoch 132, loss 0.8793274760246277, acc=0.6236666440963745, loss=0.8793274760246277
test: epoch 132, loss 2.1348910331726074, acc=0.33888888359069824, loss=2.1348910331726074
train: epoch 133, loss 0.8549526333808899, acc=0.6294999718666077, loss=0.8549526333808899
test: epoch 133, loss 2.291398525238037, acc=0.3333333432674408, loss=2.291398525238037
train: epoch 134, loss 0.8579546809196472, acc=0.6337777972221375, loss=0.8579546809196472
test: epoch 134, loss 2.3281772136688232, acc=0.33888888359069824, loss=2.3281772136688232
train: epoch 135, loss 0.8637538552284241, acc=0.6241666674613953, loss=0.8637538552284241
test: epoch 135, loss 2.0910298824310303, acc=0.32499998807907104, loss=2.0910298824310303
train: epoch 136, loss 0.8637437224388123, acc=0.6271111369132996, loss=0.8637437224388123
test: epoch 136, loss 2.174147367477417, acc=0.34166666865348816, loss=2.174147367477417
train: epoch 137, loss 0.8564796447753906, acc=0.6237778067588806, loss=0.8564796447753906
test: epoch 137, loss 2.1811933517456055, acc=0.34166666865348816, loss=2.1811933517456055
train: epoch 138, loss 0.8711361289024353, acc=0.6249444484710693, loss=0.8711361289024353
test: epoch 138, loss 2.342470645904541, acc=0.33888888359069824, loss=2.342470645904541
train: epoch 139, loss 0.8608248829841614, acc=0.6237221956253052, loss=0.8608248829841614
test: epoch 139, loss 2.201909065246582, acc=0.33888888359069824, loss=2.201909065246582
train: epoch 140, loss 0.8669161796569824, acc=0.6262778043746948, loss=0.8669161796569824
test: epoch 140, loss 2.145181894302368, acc=0.34166666865348816, loss=2.145181894302368
train: epoch 141, loss 0.8630017638206482, acc=0.625, loss=0.8630017638206482
test: epoch 141, loss 2.28371524810791, acc=0.3305555582046509, loss=2.28371524810791
train: epoch 142, loss 0.8537420630455017, acc=0.6250555515289307, loss=0.8537420630455017
test: epoch 142, loss 2.0878865718841553, acc=0.33888888359069824, loss=2.0878865718841553
train: epoch 143, loss 0.8627510666847229, acc=0.6267777681350708, loss=0.8627510666847229
test: epoch 143, loss 2.163250207901001, acc=0.3361110985279083, loss=2.163250207901001
train: epoch 144, loss 0.8565747737884521, acc=0.6293333172798157, loss=0.8565747737884521
test: epoch 144, loss 2.3722620010375977, acc=0.32777777314186096, loss=2.3722620010375977
train: epoch 145, loss 0.8488247394561768, acc=0.6302777528762817, loss=0.8488247394561768
test: epoch 145, loss 2.1865875720977783, acc=0.3333333432674408, loss=2.1865875720977783
train: epoch 146, loss 0.8522101640701294, acc=0.6298888921737671, loss=0.8522101640701294
test: epoch 146, loss 2.056561231613159, acc=0.3333333432674408, loss=2.056561231613159
train: epoch 147, loss 0.8629430532455444, acc=0.6267777681350708, loss=0.8629430532455444
test: epoch 147, loss 2.0227560997009277, acc=0.33888888359069824, loss=2.0227560997009277
train: epoch 148, loss 0.8563423752784729, acc=0.6304444670677185, loss=0.8563423752784729
test: epoch 148, loss 2.3073060512542725, acc=0.3472222089767456, loss=2.3073060512542725
train: epoch 149, loss 0.8509392142295837, acc=0.6349444389343262, loss=0.8509392142295837
test: epoch 149, loss 2.096695899963379, acc=0.3472222089767456, loss=2.096695899963379
train: epoch 150, loss 0.866146445274353, acc=0.628333330154419, loss=0.866146445274353
test: epoch 150, loss 2.207216739654541, acc=0.3472222089767456, loss=2.207216739654541
