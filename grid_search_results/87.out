# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1006011279, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1006011279, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.5365149974823, acc=0.04766666516661644, loss=3.5365149974823
test: epoch 1, loss 3.494269609451294, acc=0.0555555559694767, loss=3.494269609451294
train: epoch 2, loss 3.4614639282226562, acc=0.05249999836087227, loss=3.4614639282226562
test: epoch 2, loss 3.086904287338257, acc=0.08611111342906952, loss=3.086904287338257
train: epoch 3, loss 3.2849555015563965, acc=0.06750000268220901, loss=3.2849555015563965
test: epoch 3, loss 3.9749858379364014, acc=0.05000000074505806, loss=3.9749858379364014
train: epoch 4, loss 2.908449172973633, acc=0.10944444686174393, loss=2.908449172973633
test: epoch 4, loss 5.2486066818237305, acc=0.0416666679084301, loss=5.2486066818237305
train: epoch 5, loss 2.687577486038208, acc=0.1375555545091629, loss=2.687577486038208
test: epoch 5, loss 5.458317756652832, acc=0.03888889029622078, loss=5.458317756652832
train: epoch 6, loss 2.571903944015503, acc=0.15399999916553497, loss=2.571903944015503
test: epoch 6, loss 5.575473785400391, acc=0.03888889029622078, loss=5.575473785400391
train: epoch 7, loss 2.4903228282928467, acc=0.16744443774223328, loss=2.4903228282928467
test: epoch 7, loss 5.495603084564209, acc=0.03055555559694767, loss=5.495603084564209
train: epoch 8, loss 2.4268884658813477, acc=0.1780555546283722, loss=2.4268884658813477
test: epoch 8, loss 5.380130767822266, acc=0.03333333507180214, loss=5.380130767822266
train: epoch 9, loss 2.3743512630462646, acc=0.18916666507720947, loss=2.3743512630462646
test: epoch 9, loss 5.5751471519470215, acc=0.03888889029622078, loss=5.5751471519470215
train: epoch 10, loss 2.3466877937316895, acc=0.19766665995121002, loss=2.3466877937316895
test: epoch 10, loss 5.111046314239502, acc=0.04444444552063942, loss=5.111046314239502
train: epoch 11, loss 2.320023775100708, acc=0.2083333283662796, loss=2.320023775100708
test: epoch 11, loss 5.3336262702941895, acc=0.04722222313284874, loss=5.3336262702941895
train: epoch 12, loss 2.283432960510254, acc=0.21538889408111572, loss=2.283432960510254
test: epoch 12, loss 5.214563846588135, acc=0.05277777835726738, loss=5.214563846588135
train: epoch 13, loss 2.273860216140747, acc=0.21805556118488312, loss=2.273860216140747
test: epoch 13, loss 5.059577941894531, acc=0.06388889253139496, loss=5.059577941894531
train: epoch 14, loss 2.243114471435547, acc=0.22938889265060425, loss=2.243114471435547
test: epoch 14, loss 5.015255451202393, acc=0.06111111119389534, loss=5.015255451202393
train: epoch 15, loss 2.227637529373169, acc=0.22727777063846588, loss=2.227637529373169
test: epoch 15, loss 5.075076580047607, acc=0.07222222536802292, loss=5.075076580047607
train: epoch 16, loss 2.2138378620147705, acc=0.23027777671813965, loss=2.2138378620147705
test: epoch 16, loss 4.871362209320068, acc=0.05833333358168602, loss=4.871362209320068
train: epoch 17, loss 2.183685302734375, acc=0.24088889360427856, loss=2.183685302734375
test: epoch 17, loss 5.0261311531066895, acc=0.06666667014360428, loss=5.0261311531066895
train: epoch 18, loss 2.1753664016723633, acc=0.24238888919353485, loss=2.1753664016723633
test: epoch 18, loss 5.180111885070801, acc=0.07222222536802292, loss=5.180111885070801
train: epoch 19, loss 2.1621410846710205, acc=0.2485000044107437, loss=2.1621410846710205
test: epoch 19, loss 4.97635555267334, acc=0.0694444477558136, loss=4.97635555267334
train: epoch 20, loss 2.160254716873169, acc=0.2516666650772095, loss=2.160254716873169
test: epoch 20, loss 4.8156256675720215, acc=0.06666667014360428, loss=4.8156256675720215
train: epoch 21, loss 2.141529083251953, acc=0.25522223114967346, loss=2.141529083251953
test: epoch 21, loss 4.613154411315918, acc=0.07222222536802292, loss=4.613154411315918
train: epoch 22, loss 2.140486478805542, acc=0.25183331966400146, loss=2.140486478805542
test: epoch 22, loss 4.472227573394775, acc=0.07777778059244156, loss=4.472227573394775
train: epoch 23, loss 2.1159627437591553, acc=0.2644444406032562, loss=2.1159627437591553
test: epoch 23, loss 4.194571495056152, acc=0.07500000298023224, loss=4.194571495056152
train: epoch 24, loss 2.1195595264434814, acc=0.26411110162734985, loss=2.1195595264434814
test: epoch 24, loss 4.018675327301025, acc=0.0972222238779068, loss=4.018675327301025
train: epoch 25, loss 2.1142055988311768, acc=0.26688888669013977, loss=2.1142055988311768
test: epoch 25, loss 3.911559581756592, acc=0.09444444626569748, loss=3.911559581756592
train: epoch 26, loss 2.100829601287842, acc=0.2730555534362793, loss=2.100829601287842
test: epoch 26, loss 3.663285732269287, acc=0.10277777910232544, loss=3.663285732269287
train: epoch 27, loss 2.094766616821289, acc=0.269222229719162, loss=2.094766616821289
test: epoch 27, loss 3.4946823120117188, acc=0.10277777910232544, loss=3.4946823120117188
train: epoch 28, loss 2.0882766246795654, acc=0.2759999930858612, loss=2.0882766246795654
test: epoch 28, loss 3.4816935062408447, acc=0.1111111119389534, loss=3.4816935062408447
train: epoch 29, loss 2.060161590576172, acc=0.28172221779823303, loss=2.060161590576172
test: epoch 29, loss 3.2936043739318848, acc=0.1111111119389534, loss=3.2936043739318848
train: epoch 30, loss 2.048466920852661, acc=0.2792777717113495, loss=2.048466920852661
test: epoch 30, loss 3.1866819858551025, acc=0.11666666716337204, loss=3.1866819858551025
train: epoch 31, loss 2.0599217414855957, acc=0.285277783870697, loss=2.0599217414855957
test: epoch 31, loss 3.1294026374816895, acc=0.11944444477558136, loss=3.1294026374816895
train: epoch 32, loss 2.038541793823242, acc=0.2906111180782318, loss=2.038541793823242
test: epoch 32, loss 2.980379343032837, acc=0.12777778506278992, loss=2.980379343032837
train: epoch 33, loss 2.031081199645996, acc=0.29294443130493164, loss=2.031081199645996
test: epoch 33, loss 2.981917381286621, acc=0.13611111044883728, loss=2.981917381286621
train: epoch 34, loss 1.997532606124878, acc=0.3001111149787903, loss=1.997532606124878
test: epoch 34, loss 2.9275031089782715, acc=0.13611111044883728, loss=2.9275031089782715
train: epoch 35, loss 2.0094234943389893, acc=0.3002777695655823, loss=2.0094234943389893
test: epoch 35, loss 2.8154542446136475, acc=0.1388888955116272, loss=2.8154542446136475
train: epoch 36, loss 1.999478816986084, acc=0.3039444386959076, loss=1.999478816986084
test: epoch 36, loss 2.763538122177124, acc=0.13611111044883728, loss=2.763538122177124
train: epoch 37, loss 1.9757401943206787, acc=0.3050000071525574, loss=1.9757401943206787
test: epoch 37, loss 2.740964889526367, acc=0.14722222089767456, loss=2.740964889526367
train: epoch 38, loss 1.971408724784851, acc=0.308611124753952, loss=1.971408724784851
test: epoch 38, loss 2.6548080444335938, acc=0.1666666716337204, loss=2.6548080444335938
train: epoch 39, loss 1.9717741012573242, acc=0.31288889050483704, loss=1.9717741012573242
test: epoch 39, loss 2.5776734352111816, acc=0.17499999701976776, loss=2.5776734352111816
train: epoch 40, loss 1.948451280593872, acc=0.32172220945358276, loss=1.948451280593872
test: epoch 40, loss 2.578481674194336, acc=0.15833333134651184, loss=2.578481674194336
train: epoch 41, loss 1.9283102750778198, acc=0.3314444422721863, loss=1.9283102750778198
test: epoch 41, loss 2.462785482406616, acc=0.20277777314186096, loss=2.462785482406616
train: epoch 42, loss 1.9273524284362793, acc=0.33455556631088257, loss=1.9273524284362793
test: epoch 42, loss 2.435598850250244, acc=0.19722221791744232, loss=2.435598850250244
train: epoch 43, loss 1.9023476839065552, acc=0.33027777075767517, loss=1.9023476839065552
test: epoch 43, loss 2.3872616291046143, acc=0.20000000298023224, loss=2.3872616291046143
train: epoch 44, loss 1.9108550548553467, acc=0.33294445276260376, loss=1.9108550548553467
test: epoch 44, loss 2.369885206222534, acc=0.21388888359069824, loss=2.369885206222534
train: epoch 45, loss 1.8961530923843384, acc=0.34200000762939453, loss=1.8961530923843384
test: epoch 45, loss 2.327583074569702, acc=0.21944443881511688, loss=2.327583074569702
train: epoch 46, loss 1.8878531455993652, acc=0.3368888795375824, loss=1.8878531455993652
test: epoch 46, loss 2.295060157775879, acc=0.2083333283662796, loss=2.295060157775879
train: epoch 47, loss 1.8655438423156738, acc=0.3495555520057678, loss=1.8655438423156738
test: epoch 47, loss 2.262362003326416, acc=0.2083333283662796, loss=2.262362003326416
train: epoch 48, loss 1.8779761791229248, acc=0.3534444570541382, loss=1.8779761791229248
test: epoch 48, loss 2.239328145980835, acc=0.21388888359069824, loss=2.239328145980835
train: epoch 49, loss 1.8697845935821533, acc=0.3542777895927429, loss=1.8697845935821533
test: epoch 49, loss 2.241891622543335, acc=0.22777777910232544, loss=2.241891622543335
train: epoch 50, loss 1.8242112398147583, acc=0.36149999499320984, loss=1.8242112398147583
test: epoch 50, loss 2.2378690242767334, acc=0.21666666865348816, loss=2.2378690242767334
train: epoch 51, loss 1.824337124824524, acc=0.3641666769981384, loss=1.824337124824524
test: epoch 51, loss 2.201819658279419, acc=0.2083333283662796, loss=2.201819658279419
train: epoch 52, loss 1.8192484378814697, acc=0.367166668176651, loss=1.8192484378814697
test: epoch 52, loss 2.1644887924194336, acc=0.2361111044883728, loss=2.1644887924194336
train: epoch 53, loss 1.8157258033752441, acc=0.37138888239860535, loss=1.8157258033752441
test: epoch 53, loss 2.118842840194702, acc=0.23333333432674408, loss=2.118842840194702
train: epoch 54, loss 1.79816472530365, acc=0.3776666522026062, loss=1.79816472530365
test: epoch 54, loss 2.10597562789917, acc=0.23888888955116272, loss=2.10597562789917
train: epoch 55, loss 1.790102481842041, acc=0.3822222352027893, loss=1.790102481842041
test: epoch 55, loss 2.106842041015625, acc=0.2611111104488373, loss=2.106842041015625
train: epoch 56, loss 1.783869981765747, acc=0.38661110401153564, loss=1.783869981765747
test: epoch 56, loss 2.102969169616699, acc=0.24444444477558136, loss=2.102969169616699
train: epoch 57, loss 1.752689242362976, acc=0.39544445276260376, loss=1.752689242362976
test: epoch 57, loss 2.077746629714966, acc=0.25555557012557983, loss=2.077746629714966
train: epoch 58, loss 1.7499620914459229, acc=0.3991111218929291, loss=1.7499620914459229
test: epoch 58, loss 2.0440144538879395, acc=0.2638888955116272, loss=2.0440144538879395
train: epoch 59, loss 1.7391494512557983, acc=0.3930000066757202, loss=1.7391494512557983
test: epoch 59, loss 2.038421869277954, acc=0.25555557012557983, loss=2.038421869277954
train: epoch 60, loss 1.735584020614624, acc=0.39933332800865173, loss=1.735584020614624
test: epoch 60, loss 1.9324700832366943, acc=0.27222222089767456, loss=1.9324700832366943
train: epoch 61, loss 1.7207551002502441, acc=0.39977777004241943, loss=1.7207551002502441
test: epoch 61, loss 1.9513272047042847, acc=0.27222222089767456, loss=1.9513272047042847
train: epoch 62, loss 1.694340467453003, acc=0.4129444360733032, loss=1.694340467453003
test: epoch 62, loss 1.912990689277649, acc=0.28611111640930176, loss=1.912990689277649
train: epoch 63, loss 1.6946624517440796, acc=0.41144445538520813, loss=1.6946624517440796
test: epoch 63, loss 1.8748291730880737, acc=0.2916666567325592, loss=1.8748291730880737
train: epoch 64, loss 1.6692924499511719, acc=0.41272222995758057, loss=1.6692924499511719
test: epoch 64, loss 1.8563833236694336, acc=0.30000001192092896, loss=1.8563833236694336
train: epoch 65, loss 1.6566988229751587, acc=0.42100000381469727, loss=1.6566988229751587
test: epoch 65, loss 1.8343156576156616, acc=0.30000001192092896, loss=1.8343156576156616
train: epoch 66, loss 1.6417113542556763, acc=0.4314444363117218, loss=1.6417113542556763
test: epoch 66, loss 1.8346856832504272, acc=0.3055555522441864, loss=1.8346856832504272
train: epoch 67, loss 1.6603164672851562, acc=0.4261666536331177, loss=1.6603164672851562
test: epoch 67, loss 1.8199959993362427, acc=0.29722222685813904, loss=1.8199959993362427
train: epoch 68, loss 1.6441290378570557, acc=0.43522220849990845, loss=1.6441290378570557
test: epoch 68, loss 1.8041011095046997, acc=0.3027777671813965, loss=1.8041011095046997
train: epoch 69, loss 1.6081743240356445, acc=0.43994444608688354, loss=1.6081743240356445
test: epoch 69, loss 1.785691738128662, acc=0.3083333373069763, loss=1.785691738128662
train: epoch 70, loss 1.6096508502960205, acc=0.4426666796207428, loss=1.6096508502960205
test: epoch 70, loss 1.7690240144729614, acc=0.30000001192092896, loss=1.7690240144729614
train: epoch 71, loss 1.592787742614746, acc=0.45144444704055786, loss=1.592787742614746
test: epoch 71, loss 1.7560145854949951, acc=0.3027777671813965, loss=1.7560145854949951
train: epoch 72, loss 1.5808141231536865, acc=0.4509444534778595, loss=1.5808141231536865
test: epoch 72, loss 1.7501190900802612, acc=0.29722222685813904, loss=1.7501190900802612
train: epoch 73, loss 1.5436664819717407, acc=0.4580000042915344, loss=1.5436664819717407
test: epoch 73, loss 1.7384752035140991, acc=0.3083333373069763, loss=1.7384752035140991
train: epoch 74, loss 1.534728765487671, acc=0.45883333683013916, loss=1.534728765487671
test: epoch 74, loss 1.7198237180709839, acc=0.31111112236976624, loss=1.7198237180709839
train: epoch 75, loss 1.5404551029205322, acc=0.46638888120651245, loss=1.5404551029205322
test: epoch 75, loss 1.7026522159576416, acc=0.3083333373069763, loss=1.7026522159576416
train: epoch 76, loss 1.5088492631912231, acc=0.4706111252307892, loss=1.5088492631912231
test: epoch 76, loss 1.7098603248596191, acc=0.31388887763023376, loss=1.7098603248596191
train: epoch 77, loss 1.4800539016723633, acc=0.47955554723739624, loss=1.4800539016723633
test: epoch 77, loss 1.6926809549331665, acc=0.3083333373069763, loss=1.6926809549331665
train: epoch 78, loss 1.4989683628082275, acc=0.4797777831554413, loss=1.4989683628082275
test: epoch 78, loss 1.670901894569397, acc=0.31111112236976624, loss=1.670901894569397
train: epoch 79, loss 1.466132402420044, acc=0.49149999022483826, loss=1.466132402420044
test: epoch 79, loss 1.680953025817871, acc=0.3083333373069763, loss=1.680953025817871
train: epoch 80, loss 1.4534363746643066, acc=0.49311110377311707, loss=1.4534363746643066
test: epoch 80, loss 1.672523021697998, acc=0.3166666626930237, loss=1.672523021697998
train: epoch 81, loss 1.4551151990890503, acc=0.4926111102104187, loss=1.4551151990890503
test: epoch 81, loss 1.6401124000549316, acc=0.31111112236976624, loss=1.6401124000549316
train: epoch 82, loss 1.4445960521697998, acc=0.5, loss=1.4445960521697998
test: epoch 82, loss 1.6266679763793945, acc=0.3361110985279083, loss=1.6266679763793945
train: epoch 83, loss 1.419443964958191, acc=0.5086666941642761, loss=1.419443964958191
test: epoch 83, loss 1.6254998445510864, acc=0.3305555582046509, loss=1.6254998445510864
train: epoch 84, loss 1.4039064645767212, acc=0.508222222328186, loss=1.4039064645767212
test: epoch 84, loss 1.6106754541397095, acc=0.3194444477558136, loss=1.6106754541397095
train: epoch 85, loss 1.3880155086517334, acc=0.5166666507720947, loss=1.3880155086517334
test: epoch 85, loss 1.6400374174118042, acc=0.3333333432674408, loss=1.6400374174118042
train: epoch 86, loss 1.3634580373764038, acc=0.5198333263397217, loss=1.3634580373764038
test: epoch 86, loss 1.6093984842300415, acc=0.3333333432674408, loss=1.6093984842300415
train: epoch 87, loss 1.3574844598770142, acc=0.5266666412353516, loss=1.3574844598770142
test: epoch 87, loss 1.6009670495986938, acc=0.3305555582046509, loss=1.6009670495986938
train: epoch 88, loss 1.3400824069976807, acc=0.5301111340522766, loss=1.3400824069976807
test: epoch 88, loss 1.6033779382705688, acc=0.3361110985279083, loss=1.6033779382705688
train: epoch 89, loss 1.3163931369781494, acc=0.5350555777549744, loss=1.3163931369781494
test: epoch 89, loss 1.6059958934783936, acc=0.34166666865348816, loss=1.6059958934783936
train: epoch 90, loss 1.3363231420516968, acc=0.5350000262260437, loss=1.3363231420516968
test: epoch 90, loss 1.603176236152649, acc=0.32499998807907104, loss=1.603176236152649
train: epoch 91, loss 1.3224146366119385, acc=0.5452222228050232, loss=1.3224146366119385
test: epoch 91, loss 1.5571256875991821, acc=0.33888888359069824, loss=1.5571256875991821
train: epoch 92, loss 1.2954914569854736, acc=0.543666660785675, loss=1.2954914569854736
test: epoch 92, loss 1.5654493570327759, acc=0.33888888359069824, loss=1.5654493570327759
train: epoch 93, loss 1.305293083190918, acc=0.5506666898727417, loss=1.305293083190918
test: epoch 93, loss 1.5781430006027222, acc=0.3499999940395355, loss=1.5781430006027222
train: epoch 94, loss 1.304595708847046, acc=0.5506666898727417, loss=1.304595708847046
test: epoch 94, loss 1.5554516315460205, acc=0.3472222089767456, loss=1.5554516315460205
train: epoch 95, loss 1.2725743055343628, acc=0.5572222471237183, loss=1.2725743055343628
test: epoch 95, loss 1.540295124053955, acc=0.35555556416511536, loss=1.540295124053955
train: epoch 96, loss 1.2611981630325317, acc=0.5611666440963745, loss=1.2611981630325317
test: epoch 96, loss 1.5393266677856445, acc=0.3499999940395355, loss=1.5393266677856445
train: epoch 97, loss 1.2541683912277222, acc=0.5644999742507935, loss=1.2541683912277222
test: epoch 97, loss 1.5352095365524292, acc=0.35555556416511536, loss=1.5352095365524292
train: epoch 98, loss 1.2305241823196411, acc=0.5679444670677185, loss=1.2305241823196411
test: epoch 98, loss 1.5227856636047363, acc=0.35555556416511536, loss=1.5227856636047363
train: epoch 99, loss 1.2446925640106201, acc=0.5706111192703247, loss=1.2446925640106201
test: epoch 99, loss 1.530754804611206, acc=0.36666667461395264, loss=1.530754804611206
train: epoch 100, loss 1.2119611501693726, acc=0.5766111016273499, loss=1.2119611501693726
test: epoch 100, loss 1.507999062538147, acc=0.35555556416511536, loss=1.507999062538147
train: epoch 101, loss 1.2243605852127075, acc=0.5763888955116272, loss=1.2243605852127075
test: epoch 101, loss 1.4898898601531982, acc=0.35277777910232544, loss=1.4898898601531982
train: epoch 102, loss 1.199588656425476, acc=0.5816666483879089, loss=1.199588656425476
test: epoch 102, loss 1.498713731765747, acc=0.36944442987442017, loss=1.498713731765747
train: epoch 103, loss 1.1917903423309326, acc=0.5851110816001892, loss=1.1917903423309326
test: epoch 103, loss 1.4855613708496094, acc=0.3638888895511627, loss=1.4855613708496094
train: epoch 104, loss 1.1806648969650269, acc=0.5951666831970215, loss=1.1806648969650269
test: epoch 104, loss 1.5031179189682007, acc=0.36666667461395264, loss=1.5031179189682007
train: epoch 105, loss 1.1828415393829346, acc=0.590833306312561, loss=1.1828415393829346
test: epoch 105, loss 1.4977821111679077, acc=0.36944442987442017, loss=1.4977821111679077
train: epoch 106, loss 1.1673223972320557, acc=0.5984444618225098, loss=1.1673223972320557
test: epoch 106, loss 1.5073670148849487, acc=0.3638888895511627, loss=1.5073670148849487
train: epoch 107, loss 1.160517692565918, acc=0.5995555520057678, loss=1.160517692565918
test: epoch 107, loss 1.4830424785614014, acc=0.3611111044883728, loss=1.4830424785614014
train: epoch 108, loss 1.1395314931869507, acc=0.606333315372467, loss=1.1395314931869507
test: epoch 108, loss 1.4766323566436768, acc=0.35555556416511536, loss=1.4766323566436768
train: epoch 109, loss 1.14503014087677, acc=0.6063888669013977, loss=1.14503014087677
test: epoch 109, loss 1.4873781204223633, acc=0.3722222149372101, loss=1.4873781204223633
train: epoch 110, loss 1.1217703819274902, acc=0.6151111125946045, loss=1.1217703819274902
test: epoch 110, loss 1.4859223365783691, acc=0.35555556416511536, loss=1.4859223365783691
train: epoch 111, loss 1.1256946325302124, acc=0.6226666569709778, loss=1.1256946325302124
test: epoch 111, loss 1.4918255805969238, acc=0.36944442987442017, loss=1.4918255805969238
train: epoch 112, loss 1.1137672662734985, acc=0.6202222108840942, loss=1.1137672662734985
test: epoch 112, loss 1.450628399848938, acc=0.36666667461395264, loss=1.450628399848938
train: epoch 113, loss 1.1061843633651733, acc=0.6206111311912537, loss=1.1061843633651733
test: epoch 113, loss 1.455740213394165, acc=0.3722222149372101, loss=1.455740213394165
train: epoch 114, loss 1.100909948348999, acc=0.6251111030578613, loss=1.100909948348999
test: epoch 114, loss 1.4647058248519897, acc=0.3722222149372101, loss=1.4647058248519897
train: epoch 115, loss 1.1039403676986694, acc=0.6250555515289307, loss=1.1039403676986694
test: epoch 115, loss 1.4527068138122559, acc=0.3722222149372101, loss=1.4527068138122559
train: epoch 116, loss 1.0967131853103638, acc=0.6297222375869751, loss=1.0967131853103638
test: epoch 116, loss 1.4283132553100586, acc=0.36944442987442017, loss=1.4283132553100586
train: epoch 117, loss 1.0743021965026855, acc=0.6361666917800903, loss=1.0743021965026855
test: epoch 117, loss 1.4411289691925049, acc=0.3722222149372101, loss=1.4411289691925049
train: epoch 118, loss 1.0777539014816284, acc=0.6405555605888367, loss=1.0777539014816284
test: epoch 118, loss 1.4354071617126465, acc=0.375, loss=1.4354071617126465
train: epoch 119, loss 1.0748296976089478, acc=0.637499988079071, loss=1.0748296976089478
test: epoch 119, loss 1.4423282146453857, acc=0.36666667461395264, loss=1.4423282146453857
train: epoch 120, loss 1.045750617980957, acc=0.6449999809265137, loss=1.045750617980957
test: epoch 120, loss 1.4222549200057983, acc=0.375, loss=1.4222549200057983
train: epoch 121, loss 1.0422183275222778, acc=0.6495000123977661, loss=1.0422183275222778
test: epoch 121, loss 1.4315420389175415, acc=0.3777777850627899, loss=1.4315420389175415
train: epoch 122, loss 1.0365722179412842, acc=0.6518333554267883, loss=1.0365722179412842
test: epoch 122, loss 1.4214444160461426, acc=0.3722222149372101, loss=1.4214444160461426
train: epoch 123, loss 1.026157259941101, acc=0.6548333168029785, loss=1.026157259941101
test: epoch 123, loss 1.4137645959854126, acc=0.3861111104488373, loss=1.4137645959854126
train: epoch 124, loss 1.0404016971588135, acc=0.6539444327354431, loss=1.0404016971588135
test: epoch 124, loss 1.4103412628173828, acc=0.38333332538604736, loss=1.4103412628173828
train: epoch 125, loss 1.0096416473388672, acc=0.6577777862548828, loss=1.0096416473388672
test: epoch 125, loss 1.4007543325424194, acc=0.3888888955116272, loss=1.4007543325424194
train: epoch 126, loss 1.0068786144256592, acc=0.6627222299575806, loss=1.0068786144256592
test: epoch 126, loss 1.3953454494476318, acc=0.39444443583488464, loss=1.3953454494476318
train: epoch 127, loss 1.005408763885498, acc=0.6613333225250244, loss=1.005408763885498
test: epoch 127, loss 1.4174925088882446, acc=0.3888888955116272, loss=1.4174925088882446
train: epoch 128, loss 1.0017458200454712, acc=0.6660000085830688, loss=1.0017458200454712
test: epoch 128, loss 1.3899232149124146, acc=0.3861111104488373, loss=1.3899232149124146
train: epoch 129, loss 0.9864380955696106, acc=0.670722246170044, loss=0.9864380955696106
test: epoch 129, loss 1.3619834184646606, acc=0.40833333134651184, loss=1.3619834184646606
train: epoch 130, loss 0.9836748242378235, acc=0.674833357334137, loss=0.9836748242378235
test: epoch 130, loss 1.353256106376648, acc=0.39722222089767456, loss=1.353256106376648
train: epoch 131, loss 0.9785749316215515, acc=0.6765555739402771, loss=0.9785749316215515
test: epoch 131, loss 1.3528788089752197, acc=0.4000000059604645, loss=1.3528788089752197
train: epoch 132, loss 0.9548586010932922, acc=0.6815000176429749, loss=0.9548586010932922
test: epoch 132, loss 1.3640007972717285, acc=0.3861111104488373, loss=1.3640007972717285
train: epoch 133, loss 0.9493566751480103, acc=0.6807777881622314, loss=0.9493566751480103
test: epoch 133, loss 1.3457075357437134, acc=0.3916666805744171, loss=1.3457075357437134
train: epoch 134, loss 0.9621882438659668, acc=0.6812777519226074, loss=0.9621882438659668
test: epoch 134, loss 1.3426800966262817, acc=0.4000000059604645, loss=1.3426800966262817
train: epoch 135, loss 0.9654870629310608, acc=0.684166669845581, loss=0.9654870629310608
test: epoch 135, loss 1.3347097635269165, acc=0.39722222089767456, loss=1.3347097635269165
train: epoch 136, loss 0.9501232504844666, acc=0.6886110901832581, loss=0.9501232504844666
test: epoch 136, loss 1.3459149599075317, acc=0.39722222089767456, loss=1.3459149599075317
train: epoch 137, loss 0.9223702549934387, acc=0.6943333148956299, loss=0.9223702549934387
test: epoch 137, loss 1.3273476362228394, acc=0.4027777910232544, loss=1.3273476362228394
train: epoch 138, loss 0.9354564547538757, acc=0.6887778043746948, loss=0.9354564547538757
test: epoch 138, loss 1.3296259641647339, acc=0.39722222089767456, loss=1.3296259641647339
train: epoch 139, loss 0.9504996538162231, acc=0.6900555491447449, loss=0.9504996538162231
test: epoch 139, loss 1.3442527055740356, acc=0.39722222089767456, loss=1.3442527055740356
train: epoch 140, loss 0.9390299320220947, acc=0.6966111063957214, loss=0.9390299320220947
test: epoch 140, loss 1.3534433841705322, acc=0.4055555462837219, loss=1.3534433841705322
train: epoch 141, loss 0.9351391792297363, acc=0.7008888721466064, loss=0.9351391792297363
test: epoch 141, loss 1.3367115259170532, acc=0.4000000059604645, loss=1.3367115259170532
train: epoch 142, loss 0.9080297946929932, acc=0.7052222490310669, loss=0.9080297946929932
test: epoch 142, loss 1.3365434408187866, acc=0.3916666805744171, loss=1.3365434408187866
train: epoch 143, loss 0.9072372317314148, acc=0.7013888955116272, loss=0.9072372317314148
test: epoch 143, loss 1.3234047889709473, acc=0.4027777910232544, loss=1.3234047889709473
train: epoch 144, loss 0.8922589421272278, acc=0.7087777853012085, loss=0.8922589421272278
test: epoch 144, loss 1.330329418182373, acc=0.41111111640930176, loss=1.330329418182373
train: epoch 145, loss 0.9040878415107727, acc=0.7110000252723694, loss=0.9040878415107727
test: epoch 145, loss 1.3096668720245361, acc=0.41111111640930176, loss=1.3096668720245361
train: epoch 146, loss 0.8814973831176758, acc=0.7167778015136719, loss=0.8814973831176758
test: epoch 146, loss 1.3189408779144287, acc=0.41111111640930176, loss=1.3189408779144287
train: epoch 147, loss 0.8729434609413147, acc=0.7149999737739563, loss=0.8729434609413147
test: epoch 147, loss 1.3111753463745117, acc=0.40833333134651184, loss=1.3111753463745117
train: epoch 148, loss 0.8639219999313354, acc=0.7222222089767456, loss=0.8639219999313354
test: epoch 148, loss 1.3042354583740234, acc=0.4138889014720917, loss=1.3042354583740234
train: epoch 149, loss 0.887050449848175, acc=0.7245000004768372, loss=0.887050449848175
test: epoch 149, loss 1.310171365737915, acc=0.40833333134651184, loss=1.310171365737915
train: epoch 150, loss 0.8579889535903931, acc=0.7198333144187927, loss=0.8579889535903931
test: epoch 150, loss 1.2927149534225464, acc=0.4138889014720917, loss=1.2927149534225464
