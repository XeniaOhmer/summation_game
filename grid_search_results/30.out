# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=0", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1790541961, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1790541961, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.2220120429992676, acc=0.07533333450555801, loss=3.2220120429992676
test: epoch 1, loss 4.104166030883789, acc=0.04722222313284874, loss=4.104166030883789
train: epoch 2, loss 2.759936571121216, acc=0.12716667354106903, loss=2.759936571121216
test: epoch 2, loss 4.578324317932129, acc=0.04722222313284874, loss=4.578324317932129
train: epoch 3, loss 2.591728925704956, acc=0.15466666221618652, loss=2.591728925704956
test: epoch 3, loss 4.928478717803955, acc=0.05277777835726738, loss=4.928478717803955
train: epoch 4, loss 2.5082640647888184, acc=0.16261111199855804, loss=2.5082640647888184
test: epoch 4, loss 5.261256217956543, acc=0.05277777835726738, loss=5.261256217956543
train: epoch 5, loss 2.4555373191833496, acc=0.16766667366027832, loss=2.4555373191833496
test: epoch 5, loss 5.47781229019165, acc=0.05277777835726738, loss=5.47781229019165
train: epoch 6, loss 2.4153621196746826, acc=0.17638888955116272, loss=2.4153621196746826
test: epoch 6, loss 5.586518287658691, acc=0.04444444552063942, loss=5.586518287658691
train: epoch 7, loss 2.381103277206421, acc=0.18422222137451172, loss=2.381103277206421
test: epoch 7, loss 5.886310577392578, acc=0.04444444552063942, loss=5.886310577392578
train: epoch 8, loss 2.364811420440674, acc=0.1860000044107437, loss=2.364811420440674
test: epoch 8, loss 5.825010776519775, acc=0.04722222313284874, loss=5.825010776519775
train: epoch 9, loss 2.3366281986236572, acc=0.20016667246818542, loss=2.3366281986236572
test: epoch 9, loss 6.009939193725586, acc=0.04444444552063942, loss=6.009939193725586
train: epoch 10, loss 2.332303047180176, acc=0.1908888816833496, loss=2.332303047180176
test: epoch 10, loss 6.100309371948242, acc=0.04444444552063942, loss=6.100309371948242
train: epoch 11, loss 2.3195693492889404, acc=0.19705554842948914, loss=2.3195693492889404
test: epoch 11, loss 5.7906951904296875, acc=0.04444444552063942, loss=5.7906951904296875
train: epoch 12, loss 2.294426441192627, acc=0.19838888943195343, loss=2.294426441192627
test: epoch 12, loss 6.048943042755127, acc=0.04722222313284874, loss=6.048943042755127
train: epoch 13, loss 2.2852346897125244, acc=0.20027777552604675, loss=2.2852346897125244
test: epoch 13, loss 5.942493438720703, acc=0.0416666679084301, loss=5.942493438720703
train: epoch 14, loss 2.2715861797332764, acc=0.2076111137866974, loss=2.2715861797332764
test: epoch 14, loss 5.670775890350342, acc=0.04444444552063942, loss=5.670775890350342
train: epoch 15, loss 2.2784011363983154, acc=0.20305556058883667, loss=2.2784011363983154
test: epoch 15, loss 5.6557159423828125, acc=0.04444444552063942, loss=5.6557159423828125
train: epoch 16, loss 2.2616636753082275, acc=0.20933333039283752, loss=2.2616636753082275
test: epoch 16, loss 5.524491310119629, acc=0.0416666679084301, loss=5.524491310119629
train: epoch 17, loss 2.256227731704712, acc=0.21105556190013885, loss=2.256227731704712
test: epoch 17, loss 5.655455112457275, acc=0.03888889029622078, loss=5.655455112457275
train: epoch 18, loss 2.25907564163208, acc=0.2080555558204651, loss=2.25907564163208
test: epoch 18, loss 5.442756652832031, acc=0.03611111268401146, loss=5.442756652832031
train: epoch 19, loss 2.2525105476379395, acc=0.20916666090488434, loss=2.2525105476379395
test: epoch 19, loss 5.618175506591797, acc=0.0416666679084301, loss=5.618175506591797
train: epoch 20, loss 2.257201910018921, acc=0.21311111748218536, loss=2.257201910018921
test: epoch 20, loss 5.532778263092041, acc=0.0416666679084301, loss=5.532778263092041
train: epoch 21, loss 2.244112730026245, acc=0.21549999713897705, loss=2.244112730026245
test: epoch 21, loss 5.39762020111084, acc=0.0416666679084301, loss=5.39762020111084
train: epoch 22, loss 2.2430593967437744, acc=0.21477778255939484, loss=2.2430593967437744
test: epoch 22, loss 5.346618175506592, acc=0.03888889029622078, loss=5.346618175506592
train: epoch 23, loss 2.258115291595459, acc=0.21566666662693024, loss=2.258115291595459
test: epoch 23, loss 5.366876125335693, acc=0.04722222313284874, loss=5.366876125335693
train: epoch 24, loss 2.2303097248077393, acc=0.2175000011920929, loss=2.2303097248077393
test: epoch 24, loss 5.308618545532227, acc=0.0416666679084301, loss=5.308618545532227
train: epoch 25, loss 2.2464258670806885, acc=0.21716666221618652, loss=2.2464258670806885
test: epoch 25, loss 5.190155029296875, acc=0.05000000074505806, loss=5.190155029296875
train: epoch 26, loss 2.237227439880371, acc=0.21988889575004578, loss=2.237227439880371
test: epoch 26, loss 5.21636438369751, acc=0.04444444552063942, loss=5.21636438369751
train: epoch 27, loss 2.247163772583008, acc=0.2152777761220932, loss=2.247163772583008
test: epoch 27, loss 4.992403507232666, acc=0.0416666679084301, loss=4.992403507232666
train: epoch 28, loss 2.250258684158325, acc=0.2191111147403717, loss=2.250258684158325
test: epoch 28, loss 4.932774543762207, acc=0.05277777835726738, loss=4.932774543762207
train: epoch 29, loss 2.2414770126342773, acc=0.21755555272102356, loss=2.2414770126342773
test: epoch 29, loss 4.828122615814209, acc=0.0555555559694767, loss=4.828122615814209
train: epoch 30, loss 2.2320258617401123, acc=0.22011111676692963, loss=2.2320258617401123
test: epoch 30, loss 4.871591091156006, acc=0.04722222313284874, loss=4.871591091156006
train: epoch 31, loss 2.243227243423462, acc=0.21755555272102356, loss=2.243227243423462
test: epoch 31, loss 4.701938629150391, acc=0.04722222313284874, loss=4.701938629150391
train: epoch 32, loss 2.231142520904541, acc=0.2182222157716751, loss=2.231142520904541
test: epoch 32, loss 4.912741661071777, acc=0.04444444552063942, loss=4.912741661071777
train: epoch 33, loss 2.250427722930908, acc=0.22022221982479095, loss=2.250427722930908
test: epoch 33, loss 4.63149881362915, acc=0.0416666679084301, loss=4.63149881362915
train: epoch 34, loss 2.2499682903289795, acc=0.21649999916553497, loss=2.2499682903289795
test: epoch 34, loss 4.577032566070557, acc=0.03611111268401146, loss=4.577032566070557
train: epoch 35, loss 2.2619805335998535, acc=0.21461111307144165, loss=2.2619805335998535
test: epoch 35, loss 4.686534404754639, acc=0.0416666679084301, loss=4.686534404754639
train: epoch 36, loss 2.2436490058898926, acc=0.22138889133930206, loss=2.2436490058898926
test: epoch 36, loss 4.533057689666748, acc=0.0416666679084301, loss=4.533057689666748
train: epoch 37, loss 2.2534124851226807, acc=0.21238888800144196, loss=2.2534124851226807
test: epoch 37, loss 4.542785167694092, acc=0.04444444552063942, loss=4.542785167694092
train: epoch 38, loss 2.2584524154663086, acc=0.21316666901111603, loss=2.2584524154663086
test: epoch 38, loss 4.492066383361816, acc=0.04722222313284874, loss=4.492066383361816
train: epoch 39, loss 2.258047103881836, acc=0.21505555510520935, loss=2.258047103881836
test: epoch 39, loss 4.365477085113525, acc=0.04722222313284874, loss=4.365477085113525
train: epoch 40, loss 2.262432336807251, acc=0.21666666865348816, loss=2.262432336807251
test: epoch 40, loss 4.404184818267822, acc=0.05277777835726738, loss=4.404184818267822
train: epoch 41, loss 2.258521556854248, acc=0.2117222249507904, loss=2.258521556854248
test: epoch 41, loss 4.099329948425293, acc=0.04444444552063942, loss=4.099329948425293
train: epoch 42, loss 2.279658555984497, acc=0.21066667139530182, loss=2.279658555984497
test: epoch 42, loss 4.2224297523498535, acc=0.0416666679084301, loss=4.2224297523498535
train: epoch 43, loss 2.266462802886963, acc=0.2136666625738144, loss=2.266462802886963
test: epoch 43, loss 4.030698776245117, acc=0.05000000074505806, loss=4.030698776245117
train: epoch 44, loss 2.271627426147461, acc=0.2114444375038147, loss=2.271627426147461
test: epoch 44, loss 4.099302768707275, acc=0.04722222313284874, loss=4.099302768707275
train: epoch 45, loss 2.283026933670044, acc=0.21255555748939514, loss=2.283026933670044
test: epoch 45, loss 4.124350547790527, acc=0.04444444552063942, loss=4.124350547790527
train: epoch 46, loss 2.2923688888549805, acc=0.21005555987358093, loss=2.2923688888549805
test: epoch 46, loss 3.797802209854126, acc=0.04722222313284874, loss=3.797802209854126
train: epoch 47, loss 2.29071307182312, acc=0.2118888944387436, loss=2.29071307182312
test: epoch 47, loss 3.9071621894836426, acc=0.06388889253139496, loss=3.9071621894836426
train: epoch 48, loss 2.2880594730377197, acc=0.20677778124809265, loss=2.2880594730377197
test: epoch 48, loss 3.8289577960968018, acc=0.0555555559694767, loss=3.8289577960968018
train: epoch 49, loss 2.3057122230529785, acc=0.20288889110088348, loss=2.3057122230529785
test: epoch 49, loss 3.9086663722991943, acc=0.04722222313284874, loss=3.9086663722991943
train: epoch 50, loss 2.3028314113616943, acc=0.2071666717529297, loss=2.3028314113616943
test: epoch 50, loss 3.772136926651001, acc=0.06111111119389534, loss=3.772136926651001
train: epoch 51, loss 2.3139381408691406, acc=0.20622222125530243, loss=2.3139381408691406
test: epoch 51, loss 3.8472819328308105, acc=0.04444444552063942, loss=3.8472819328308105
train: epoch 52, loss 2.3099799156188965, acc=0.2036111056804657, loss=2.3099799156188965
test: epoch 52, loss 3.679626941680908, acc=0.06666667014360428, loss=3.679626941680908
train: epoch 53, loss 2.3249309062957764, acc=0.20105555653572083, loss=2.3249309062957764
test: epoch 53, loss 3.6735494136810303, acc=0.06666667014360428, loss=3.6735494136810303
train: epoch 54, loss 2.3268191814422607, acc=0.20294444262981415, loss=2.3268191814422607
test: epoch 54, loss 3.5787642002105713, acc=0.04722222313284874, loss=3.5787642002105713
train: epoch 55, loss 2.31744122505188, acc=0.2007777839899063, loss=2.31744122505188
test: epoch 55, loss 3.5804224014282227, acc=0.06666667014360428, loss=3.5804224014282227
train: epoch 56, loss 2.3257668018341064, acc=0.19927777349948883, loss=2.3257668018341064
test: epoch 56, loss 3.5634922981262207, acc=0.0694444477558136, loss=3.5634922981262207
train: epoch 57, loss 2.3288614749908447, acc=0.19544444978237152, loss=2.3288614749908447
test: epoch 57, loss 3.5993103981018066, acc=0.06388889253139496, loss=3.5993103981018066
train: epoch 58, loss 2.326991081237793, acc=0.1956111043691635, loss=2.326991081237793
test: epoch 58, loss 3.534320831298828, acc=0.06388889253139496, loss=3.534320831298828
train: epoch 59, loss 2.338366985321045, acc=0.19244444370269775, loss=2.338366985321045
test: epoch 59, loss 3.5304112434387207, acc=0.06388889253139496, loss=3.5304112434387207
train: epoch 60, loss 2.335880994796753, acc=0.19455555081367493, loss=2.335880994796753
test: epoch 60, loss 3.5687806606292725, acc=0.06388889253139496, loss=3.5687806606292725
train: epoch 61, loss 2.3312957286834717, acc=0.1965000033378601, loss=2.3312957286834717
test: epoch 61, loss 3.4775547981262207, acc=0.06388889253139496, loss=3.4775547981262207
train: epoch 62, loss 2.3242180347442627, acc=0.19705554842948914, loss=2.3242180347442627
test: epoch 62, loss 3.477613687515259, acc=0.06388889253139496, loss=3.477613687515259
train: epoch 63, loss 2.348039388656616, acc=0.18888889253139496, loss=2.348039388656616
test: epoch 63, loss 3.4643898010253906, acc=0.05833333358168602, loss=3.4643898010253906
train: epoch 64, loss 2.3560891151428223, acc=0.18516667187213898, loss=2.3560891151428223
test: epoch 64, loss 3.390286684036255, acc=0.06666667014360428, loss=3.390286684036255
train: epoch 65, loss 2.3458125591278076, acc=0.19083333015441895, loss=2.3458125591278076
test: epoch 65, loss 3.4168691635131836, acc=0.06111111119389534, loss=3.4168691635131836
train: epoch 66, loss 2.348583459854126, acc=0.18488888442516327, loss=2.348583459854126
test: epoch 66, loss 3.2548415660858154, acc=0.07777778059244156, loss=3.2548415660858154
train: epoch 67, loss 2.373943567276001, acc=0.18400000035762787, loss=2.373943567276001
test: epoch 67, loss 3.2945950031280518, acc=0.06666667014360428, loss=3.2945950031280518
train: epoch 68, loss 2.3648788928985596, acc=0.18833333253860474, loss=2.3648788928985596
test: epoch 68, loss 3.30300235748291, acc=0.05833333358168602, loss=3.30300235748291
train: epoch 69, loss 2.3659560680389404, acc=0.18222221732139587, loss=2.3659560680389404
test: epoch 69, loss 3.3852486610412598, acc=0.05000000074505806, loss=3.3852486610412598
train: epoch 70, loss 2.3696916103363037, acc=0.17861111462116241, loss=2.3696916103363037
test: epoch 70, loss 3.3308181762695312, acc=0.07777778059244156, loss=3.3308181762695312
train: epoch 71, loss 2.3693974018096924, acc=0.1798333376646042, loss=2.3693974018096924
test: epoch 71, loss 3.3680224418640137, acc=0.06388889253139496, loss=3.3680224418640137
train: epoch 72, loss 2.3635051250457764, acc=0.1805555522441864, loss=2.3635051250457764
test: epoch 72, loss 3.374709129333496, acc=0.07777778059244156, loss=3.374709129333496
train: epoch 73, loss 2.362506151199341, acc=0.17838889360427856, loss=2.362506151199341
test: epoch 73, loss 3.3666470050811768, acc=0.06388889253139496, loss=3.3666470050811768
train: epoch 74, loss 2.37131929397583, acc=0.18422222137451172, loss=2.37131929397583
test: epoch 74, loss 3.282917022705078, acc=0.06111111119389534, loss=3.282917022705078
train: epoch 75, loss 2.3475496768951416, acc=0.18122221529483795, loss=2.3475496768951416
test: epoch 75, loss 3.3493361473083496, acc=0.07222222536802292, loss=3.3493361473083496
train: epoch 76, loss 2.3504116535186768, acc=0.18344444036483765, loss=2.3504116535186768
test: epoch 76, loss 3.3108344078063965, acc=0.0694444477558136, loss=3.3108344078063965
train: epoch 77, loss 2.362840175628662, acc=0.18355555832386017, loss=2.362840175628662
test: epoch 77, loss 3.226869821548462, acc=0.08611111342906952, loss=3.226869821548462
train: epoch 78, loss 2.349515438079834, acc=0.18772222101688385, loss=2.349515438079834
test: epoch 78, loss 3.1820759773254395, acc=0.08055555820465088, loss=3.1820759773254395
train: epoch 79, loss 2.354357957839966, acc=0.18888889253139496, loss=2.354357957839966
test: epoch 79, loss 3.2510364055633545, acc=0.07222222536802292, loss=3.2510364055633545
train: epoch 80, loss 2.367541551589966, acc=0.18583333492279053, loss=2.367541551589966
test: epoch 80, loss 3.1994097232818604, acc=0.08055555820465088, loss=3.1994097232818604
train: epoch 81, loss 2.349915027618408, acc=0.18433333933353424, loss=2.349915027618408
test: epoch 81, loss 3.2177863121032715, acc=0.07500000298023224, loss=3.2177863121032715
train: epoch 82, loss 2.3606607913970947, acc=0.18538889288902283, loss=2.3606607913970947
test: epoch 82, loss 3.1782546043395996, acc=0.08055555820465088, loss=3.1782546043395996
train: epoch 83, loss 2.3402655124664307, acc=0.1904444396495819, loss=2.3402655124664307
test: epoch 83, loss 3.2715001106262207, acc=0.07500000298023224, loss=3.2715001106262207
train: epoch 84, loss 2.3450586795806885, acc=0.18383333086967468, loss=2.3450586795806885
test: epoch 84, loss 3.2482802867889404, acc=0.07500000298023224, loss=3.2482802867889404
train: epoch 85, loss 2.3347418308258057, acc=0.1854444444179535, loss=2.3347418308258057
test: epoch 85, loss 3.266277551651001, acc=0.0694444477558136, loss=3.266277551651001
train: epoch 86, loss 2.3557207584381104, acc=0.18522222340106964, loss=2.3557207584381104
test: epoch 86, loss 3.2180092334747314, acc=0.08611111342906952, loss=3.2180092334747314
train: epoch 87, loss 2.337975263595581, acc=0.18566666543483734, loss=2.337975263595581
test: epoch 87, loss 3.1818764209747314, acc=0.07222222536802292, loss=3.1818764209747314
train: epoch 88, loss 2.335470199584961, acc=0.1850000023841858, loss=2.335470199584961
test: epoch 88, loss 3.187873125076294, acc=0.05277777835726738, loss=3.187873125076294
train: epoch 89, loss 2.3260293006896973, acc=0.19033333659172058, loss=2.3260293006896973
test: epoch 89, loss 3.192005157470703, acc=0.06666667014360428, loss=3.192005157470703
train: epoch 90, loss 2.332993268966675, acc=0.1913333386182785, loss=2.332993268966675
test: epoch 90, loss 3.086963653564453, acc=0.07777778059244156, loss=3.086963653564453
train: epoch 91, loss 2.3238446712493896, acc=0.18961110711097717, loss=2.3238446712493896
test: epoch 91, loss 3.1223349571228027, acc=0.07222222536802292, loss=3.1223349571228027
train: epoch 92, loss 2.322232961654663, acc=0.18816666305065155, loss=2.322232961654663
test: epoch 92, loss 3.310985803604126, acc=0.07500000298023224, loss=3.310985803604126
train: epoch 93, loss 2.342864513397217, acc=0.18666666746139526, loss=2.342864513397217
test: epoch 93, loss 3.2524173259735107, acc=0.07500000298023224, loss=3.2524173259735107
train: epoch 94, loss 2.321619987487793, acc=0.18905556201934814, loss=2.321619987487793
test: epoch 94, loss 3.129194498062134, acc=0.07500000298023224, loss=3.129194498062134
train: epoch 95, loss 2.3129489421844482, acc=0.19300000369548798, loss=2.3129489421844482
test: epoch 95, loss 3.2742843627929688, acc=0.0694444477558136, loss=3.2742843627929688
train: epoch 96, loss 2.325822353363037, acc=0.1942777782678604, loss=2.325822353363037
test: epoch 96, loss 3.2584426403045654, acc=0.0694444477558136, loss=3.2584426403045654
train: epoch 97, loss 2.3188910484313965, acc=0.19483333826065063, loss=2.3188910484313965
test: epoch 97, loss 3.2196085453033447, acc=0.07222222536802292, loss=3.2196085453033447
train: epoch 98, loss 2.3109543323516846, acc=0.19455555081367493, loss=2.3109543323516846
test: epoch 98, loss 3.1967835426330566, acc=0.05277777835726738, loss=3.1967835426330566
train: epoch 99, loss 2.307360887527466, acc=0.1951666623353958, loss=2.307360887527466
test: epoch 99, loss 3.180168628692627, acc=0.07222222536802292, loss=3.180168628692627
train: epoch 100, loss 2.3168675899505615, acc=0.19183333218097687, loss=2.3168675899505615
test: epoch 100, loss 3.246201753616333, acc=0.0694444477558136, loss=3.246201753616333
train: epoch 101, loss 2.2955739498138428, acc=0.19633333384990692, loss=2.2955739498138428
test: epoch 101, loss 3.2605016231536865, acc=0.07222222536802292, loss=3.2605016231536865
train: epoch 102, loss 2.3069863319396973, acc=0.19433332979679108, loss=2.3069863319396973
test: epoch 102, loss 3.137073516845703, acc=0.0694444477558136, loss=3.137073516845703
train: epoch 103, loss 2.309008836746216, acc=0.18961110711097717, loss=2.309008836746216
test: epoch 103, loss 3.0823476314544678, acc=0.07777778059244156, loss=3.0823476314544678
train: epoch 104, loss 2.282292127609253, acc=0.19772222638130188, loss=2.282292127609253
test: epoch 104, loss 3.092426061630249, acc=0.0694444477558136, loss=3.092426061630249
train: epoch 105, loss 2.3023624420166016, acc=0.1910555511713028, loss=2.3023624420166016
test: epoch 105, loss 3.135374069213867, acc=0.07500000298023224, loss=3.135374069213867
train: epoch 106, loss 2.302558660507202, acc=0.19466666877269745, loss=2.302558660507202
test: epoch 106, loss 3.171584129333496, acc=0.07777778059244156, loss=3.171584129333496
train: epoch 107, loss 2.296797752380371, acc=0.1906111091375351, loss=2.296797752380371
test: epoch 107, loss 3.1281425952911377, acc=0.0833333358168602, loss=3.1281425952911377
train: epoch 108, loss 2.3063364028930664, acc=0.19866666197776794, loss=2.3063364028930664
test: epoch 108, loss 3.129807710647583, acc=0.07777778059244156, loss=3.129807710647583
train: epoch 109, loss 2.2892725467681885, acc=0.20127777755260468, loss=2.2892725467681885
test: epoch 109, loss 3.0351903438568115, acc=0.07500000298023224, loss=3.0351903438568115
train: epoch 110, loss 2.2898311614990234, acc=0.1962222158908844, loss=2.2898311614990234
test: epoch 110, loss 3.1028151512145996, acc=0.07500000298023224, loss=3.1028151512145996
train: epoch 111, loss 2.2996413707733154, acc=0.1965000033378601, loss=2.2996413707733154
test: epoch 111, loss 3.148942708969116, acc=0.07500000298023224, loss=3.148942708969116
train: epoch 112, loss 2.291950225830078, acc=0.19911110401153564, loss=2.291950225830078
test: epoch 112, loss 3.165973663330078, acc=0.07222222536802292, loss=3.165973663330078
train: epoch 113, loss 2.302212715148926, acc=0.19794444739818573, loss=2.302212715148926
test: epoch 113, loss 3.1039252281188965, acc=0.06666667014360428, loss=3.1039252281188965
train: epoch 114, loss 2.295963764190674, acc=0.19683332741260529, loss=2.295963764190674
test: epoch 114, loss 3.2204723358154297, acc=0.05833333358168602, loss=3.2204723358154297
train: epoch 115, loss 2.280722141265869, acc=0.20061111450195312, loss=2.280722141265869
test: epoch 115, loss 3.162235975265503, acc=0.07500000298023224, loss=3.162235975265503
train: epoch 116, loss 2.2728655338287354, acc=0.20550000667572021, loss=2.2728655338287354
test: epoch 116, loss 3.2139642238616943, acc=0.08055555820465088, loss=3.2139642238616943
train: epoch 117, loss 2.288045883178711, acc=0.20000000298023224, loss=2.288045883178711
test: epoch 117, loss 3.166219711303711, acc=0.06666667014360428, loss=3.166219711303711
train: epoch 118, loss 2.282729148864746, acc=0.20383332669734955, loss=2.282729148864746
test: epoch 118, loss 3.142176389694214, acc=0.07222222536802292, loss=3.142176389694214
train: epoch 119, loss 2.275686502456665, acc=0.20177777111530304, loss=2.275686502456665
test: epoch 119, loss 3.10699462890625, acc=0.09444444626569748, loss=3.10699462890625
train: epoch 120, loss 2.279776096343994, acc=0.19766665995121002, loss=2.279776096343994
test: epoch 120, loss 3.135209321975708, acc=0.0694444477558136, loss=3.135209321975708
train: epoch 121, loss 2.283051013946533, acc=0.2047777771949768, loss=2.283051013946533
test: epoch 121, loss 3.1408166885375977, acc=0.0833333358168602, loss=3.1408166885375977
train: epoch 122, loss 2.2623484134674072, acc=0.19922222197055817, loss=2.2623484134674072
test: epoch 122, loss 3.143754243850708, acc=0.07777778059244156, loss=3.143754243850708
train: epoch 123, loss 2.269176483154297, acc=0.20294444262981415, loss=2.269176483154297
test: epoch 123, loss 3.115725517272949, acc=0.08611111342906952, loss=3.115725517272949
train: epoch 124, loss 2.271411895751953, acc=0.20000000298023224, loss=2.271411895751953
test: epoch 124, loss 3.1330788135528564, acc=0.08611111342906952, loss=3.1330788135528564
train: epoch 125, loss 2.2562761306762695, acc=0.19794444739818573, loss=2.2562761306762695
test: epoch 125, loss 3.0273337364196777, acc=0.08611111342906952, loss=3.0273337364196777
train: epoch 126, loss 2.266878366470337, acc=0.20172221958637238, loss=2.266878366470337
test: epoch 126, loss 3.1056885719299316, acc=0.08611111342906952, loss=3.1056885719299316
train: epoch 127, loss 2.263465404510498, acc=0.20527777075767517, loss=2.263465404510498
test: epoch 127, loss 3.129293441772461, acc=0.08611111342906952, loss=3.129293441772461
train: epoch 128, loss 2.2654614448547363, acc=0.20366667211055756, loss=2.2654614448547363
test: epoch 128, loss 3.191941976547241, acc=0.07500000298023224, loss=3.191941976547241
train: epoch 129, loss 2.2614338397979736, acc=0.20222222805023193, loss=2.2614338397979736
test: epoch 129, loss 3.0668742656707764, acc=0.07777778059244156, loss=3.0668742656707764
train: epoch 130, loss 2.2723658084869385, acc=0.20016667246818542, loss=2.2723658084869385
test: epoch 130, loss 3.0310099124908447, acc=0.07777778059244156, loss=3.0310099124908447
train: epoch 131, loss 2.2459750175476074, acc=0.2092222273349762, loss=2.2459750175476074
test: epoch 131, loss 3.176516532897949, acc=0.07500000298023224, loss=3.176516532897949
train: epoch 132, loss 2.250516176223755, acc=0.2018333375453949, loss=2.250516176223755
test: epoch 132, loss 3.191523551940918, acc=0.0694444477558136, loss=3.191523551940918
train: epoch 133, loss 2.257457971572876, acc=0.2043333351612091, loss=2.257457971572876
test: epoch 133, loss 3.1115736961364746, acc=0.07500000298023224, loss=3.1115736961364746
train: epoch 134, loss 2.248086929321289, acc=0.20644444227218628, loss=2.248086929321289
test: epoch 134, loss 3.128077983856201, acc=0.08888889104127884, loss=3.128077983856201
train: epoch 135, loss 2.2581608295440674, acc=0.2034444510936737, loss=2.2581608295440674
test: epoch 135, loss 3.1163833141326904, acc=0.09166666865348816, loss=3.1163833141326904
train: epoch 136, loss 2.2382092475891113, acc=0.201222226023674, loss=2.2382092475891113
test: epoch 136, loss 3.1139886379241943, acc=0.08611111342906952, loss=3.1139886379241943
train: epoch 137, loss 2.239203929901123, acc=0.20533333718776703, loss=2.239203929901123
test: epoch 137, loss 3.1898274421691895, acc=0.07222222536802292, loss=3.1898274421691895
train: epoch 138, loss 2.2395315170288086, acc=0.20961111783981323, loss=2.2395315170288086
test: epoch 138, loss 3.2707114219665527, acc=0.07500000298023224, loss=3.2707114219665527
train: epoch 139, loss 2.2516045570373535, acc=0.20677778124809265, loss=2.2516045570373535
test: epoch 139, loss 3.1526401042938232, acc=0.08611111342906952, loss=3.1526401042938232
train: epoch 140, loss 2.2291998863220215, acc=0.20688888430595398, loss=2.2291998863220215
test: epoch 140, loss 3.200965166091919, acc=0.08055555820465088, loss=3.200965166091919
train: epoch 141, loss 2.2466838359832764, acc=0.20550000667572021, loss=2.2466838359832764
test: epoch 141, loss 3.1691603660583496, acc=0.07777778059244156, loss=3.1691603660583496
train: epoch 142, loss 2.237398862838745, acc=0.21027778089046478, loss=2.237398862838745
test: epoch 142, loss 3.1812541484832764, acc=0.08888889104127884, loss=3.1812541484832764
train: epoch 143, loss 2.2236242294311523, acc=0.20783333480358124, loss=2.2236242294311523
test: epoch 143, loss 3.0827691555023193, acc=0.07500000298023224, loss=3.0827691555023193
train: epoch 144, loss 2.2185475826263428, acc=0.212777778506279, loss=2.2185475826263428
test: epoch 144, loss 3.113499641418457, acc=0.09444444626569748, loss=3.113499641418457
train: epoch 145, loss 2.219022035598755, acc=0.21283333003520966, loss=2.219022035598755
test: epoch 145, loss 3.078428268432617, acc=0.09166666865348816, loss=3.078428268432617
train: epoch 146, loss 2.236232280731201, acc=0.20783333480358124, loss=2.236232280731201
test: epoch 146, loss 3.058124303817749, acc=0.07777778059244156, loss=3.058124303817749
train: epoch 147, loss 2.218811511993408, acc=0.20644444227218628, loss=2.218811511993408
test: epoch 147, loss 3.144181966781616, acc=0.0833333358168602, loss=3.144181966781616
train: epoch 148, loss 2.2289347648620605, acc=0.2092222273349762, loss=2.2289347648620605
test: epoch 148, loss 3.0017457008361816, acc=0.09166666865348816, loss=3.0017457008361816
train: epoch 149, loss 2.234078884124756, acc=0.2101111114025116, loss=2.234078884124756
test: epoch 149, loss 2.947735071182251, acc=0.09444444626569748, loss=2.947735071182251
train: epoch 150, loss 2.224104642868042, acc=0.21183332800865173, loss=2.224104642868042
test: epoch 150, loss 3.0288684368133545, acc=0.07777778059244156, loss=3.0288684368133545
