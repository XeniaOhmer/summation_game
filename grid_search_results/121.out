# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.99", "--one_hot=0", "--n_layers=2"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1901972844, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=2, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1901972844, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.8452200889587402, acc=0.08916666358709335, loss=2.8452200889587402
test: epoch 1, loss 2.9485833644866943, acc=0.08611111342906952, loss=2.9485833644866943
train: epoch 2, loss 2.3608222007751465, acc=0.15711110830307007, loss=2.3608222007751465
test: epoch 2, loss 2.6627347469329834, acc=0.10833333432674408, loss=2.6627347469329834
train: epoch 3, loss 2.1633970737457275, acc=0.18388888239860535, loss=2.1633970737457275
test: epoch 3, loss 2.5882821083068848, acc=0.14166666567325592, loss=2.5882821083068848
train: epoch 4, loss 2.0199878215789795, acc=0.21877777576446533, loss=2.0199878215789795
test: epoch 4, loss 2.5393853187561035, acc=0.13055555522441864, loss=2.5393853187561035
train: epoch 5, loss 1.9318190813064575, acc=0.2387777715921402, loss=1.9318190813064575
test: epoch 5, loss 2.326056718826294, acc=0.14444445073604584, loss=2.326056718826294
train: epoch 6, loss 1.8622074127197266, acc=0.2584444582462311, loss=1.8622074127197266
test: epoch 6, loss 2.3509037494659424, acc=0.16944444179534912, loss=2.3509037494659424
train: epoch 7, loss 1.8199272155761719, acc=0.27594444155693054, loss=1.8199272155761719
test: epoch 7, loss 2.2794296741485596, acc=0.17499999701976776, loss=2.2794296741485596
train: epoch 8, loss 1.7615454196929932, acc=0.2990555465221405, loss=1.7615454196929932
test: epoch 8, loss 2.3297009468078613, acc=0.17777778208255768, loss=2.3297009468078613
train: epoch 9, loss 1.7266948223114014, acc=0.30772221088409424, loss=1.7266948223114014
test: epoch 9, loss 2.320526361465454, acc=0.15833333134651184, loss=2.320526361465454
train: epoch 10, loss 1.6850082874298096, acc=0.3231111168861389, loss=1.6850082874298096
test: epoch 10, loss 2.2609004974365234, acc=0.19166666269302368, loss=2.2609004974365234
train: epoch 11, loss 1.640403389930725, acc=0.3387777805328369, loss=1.640403389930725
test: epoch 11, loss 2.22245192527771, acc=0.1805555522441864, loss=2.22245192527771
train: epoch 12, loss 1.622465968132019, acc=0.3440000116825104, loss=1.622465968132019
test: epoch 12, loss 2.2439639568328857, acc=0.18888889253139496, loss=2.2439639568328857
train: epoch 13, loss 1.59340238571167, acc=0.35394445061683655, loss=1.59340238571167
test: epoch 13, loss 2.2915806770324707, acc=0.17777778208255768, loss=2.2915806770324707
train: epoch 14, loss 1.552344799041748, acc=0.3717777729034424, loss=1.552344799041748
test: epoch 14, loss 2.2102339267730713, acc=0.19722221791744232, loss=2.2102339267730713
train: epoch 15, loss 1.5538119077682495, acc=0.37316668033599854, loss=1.5538119077682495
test: epoch 15, loss 2.0898678302764893, acc=0.21944443881511688, loss=2.0898678302764893
train: epoch 16, loss 1.5202323198318481, acc=0.38316667079925537, loss=1.5202323198318481
test: epoch 16, loss 2.227187395095825, acc=0.2222222238779068, loss=2.227187395095825
train: epoch 17, loss 1.5063456296920776, acc=0.38927778601646423, loss=1.5063456296920776
test: epoch 17, loss 1.994241714477539, acc=0.2222222238779068, loss=1.994241714477539
train: epoch 18, loss 1.4778457880020142, acc=0.39622223377227783, loss=1.4778457880020142
test: epoch 18, loss 2.1539804935455322, acc=0.18333333730697632, loss=2.1539804935455322
train: epoch 19, loss 1.4680951833724976, acc=0.4028888940811157, loss=1.4680951833724976
test: epoch 19, loss 1.982957363128662, acc=0.21944443881511688, loss=1.982957363128662
train: epoch 20, loss 1.4504939317703247, acc=0.40538889169692993, loss=1.4504939317703247
test: epoch 20, loss 1.9602612257003784, acc=0.1944444477558136, loss=1.9602612257003784
train: epoch 21, loss 1.436892032623291, acc=0.4116666615009308, loss=1.436892032623291
test: epoch 21, loss 2.0918612480163574, acc=0.22777777910232544, loss=2.0918612480163574
train: epoch 22, loss 1.41962468624115, acc=0.42277777194976807, loss=1.41962468624115
test: epoch 22, loss 2.0374691486358643, acc=0.22777777910232544, loss=2.0374691486358643
train: epoch 23, loss 1.419844627380371, acc=0.42238888144493103, loss=1.419844627380371
test: epoch 23, loss 2.085555076599121, acc=0.25, loss=2.085555076599121
train: epoch 24, loss 1.406332015991211, acc=0.4269999861717224, loss=1.406332015991211
test: epoch 24, loss 2.0171260833740234, acc=0.23888888955116272, loss=2.0171260833740234
train: epoch 25, loss 1.378290057182312, acc=0.4377777874469757, loss=1.378290057182312
test: epoch 25, loss 1.9911807775497437, acc=0.20000000298023224, loss=1.9911807775497437
train: epoch 26, loss 1.379611849784851, acc=0.4396111071109772, loss=1.379611849784851
test: epoch 26, loss 2.1342053413391113, acc=0.20555555820465088, loss=2.1342053413391113
train: epoch 27, loss 1.3556525707244873, acc=0.449055552482605, loss=1.3556525707244873
test: epoch 27, loss 1.9347693920135498, acc=0.23888888955116272, loss=1.9347693920135498
train: epoch 28, loss 1.347856044769287, acc=0.44761112332344055, loss=1.347856044769287
test: epoch 28, loss 1.9019086360931396, acc=0.28333333134651184, loss=1.9019086360931396
train: epoch 29, loss 1.3309662342071533, acc=0.45322221517562866, loss=1.3309662342071533
test: epoch 29, loss 2.08410906791687, acc=0.25833332538604736, loss=2.08410906791687
train: epoch 30, loss 1.3320221900939941, acc=0.4546111226081848, loss=1.3320221900939941
test: epoch 30, loss 1.9733338356018066, acc=0.23055554926395416, loss=1.9733338356018066
train: epoch 31, loss 1.3105796575546265, acc=0.464555561542511, loss=1.3105796575546265
test: epoch 31, loss 2.105949878692627, acc=0.20000000298023224, loss=2.105949878692627
train: epoch 32, loss 1.3020614385604858, acc=0.46488890051841736, loss=1.3020614385604858
test: epoch 32, loss 1.9618228673934937, acc=0.25555557012557983, loss=1.9618228673934937
train: epoch 33, loss 1.2883952856063843, acc=0.46950000524520874, loss=1.2883952856063843
test: epoch 33, loss 1.9355543851852417, acc=0.22777777910232544, loss=1.9355543851852417
train: epoch 34, loss 1.2732720375061035, acc=0.47894445061683655, loss=1.2732720375061035
test: epoch 34, loss 1.9577369689941406, acc=0.3194444477558136, loss=1.9577369689941406
train: epoch 35, loss 1.2524148225784302, acc=0.4873333275318146, loss=1.2524148225784302
test: epoch 35, loss 1.8413872718811035, acc=0.2750000059604645, loss=1.8413872718811035
train: epoch 36, loss 1.2404249906539917, acc=0.49505555629730225, loss=1.2404249906539917
test: epoch 36, loss 1.9366434812545776, acc=0.2638888955116272, loss=1.9366434812545776
train: epoch 37, loss 1.2509973049163818, acc=0.49061110615730286, loss=1.2509973049163818
test: epoch 37, loss 1.8689337968826294, acc=0.2777777910232544, loss=1.8689337968826294
train: epoch 38, loss 1.2439862489700317, acc=0.49149999022483826, loss=1.2439862489700317
test: epoch 38, loss 1.9200284481048584, acc=0.26944443583488464, loss=1.9200284481048584
train: epoch 39, loss 1.2213605642318726, acc=0.5023888945579529, loss=1.2213605642318726
test: epoch 39, loss 1.9047930240631104, acc=0.25833332538604736, loss=1.9047930240631104
train: epoch 40, loss 1.205165982246399, acc=0.5079444646835327, loss=1.205165982246399
test: epoch 40, loss 1.7384308576583862, acc=0.3055555522441864, loss=1.7384308576583862
train: epoch 41, loss 1.1909980773925781, acc=0.5137222409248352, loss=1.1909980773925781
test: epoch 41, loss 1.7601349353790283, acc=0.3083333373069763, loss=1.7601349353790283
train: epoch 42, loss 1.1834087371826172, acc=0.5141666531562805, loss=1.1834087371826172
test: epoch 42, loss 1.901459813117981, acc=0.3055555522441864, loss=1.901459813117981
train: epoch 43, loss 1.1875585317611694, acc=0.5183333158493042, loss=1.1875585317611694
test: epoch 43, loss 1.6963770389556885, acc=0.33888888359069824, loss=1.6963770389556885
train: epoch 44, loss 1.1681379079818726, acc=0.5218889117240906, loss=1.1681379079818726
test: epoch 44, loss 1.8183754682540894, acc=0.2916666567325592, loss=1.8183754682540894
train: epoch 45, loss 1.1505638360977173, acc=0.535277783870697, loss=1.1505638360977173
test: epoch 45, loss 1.9866724014282227, acc=0.2666666805744171, loss=1.9866724014282227
train: epoch 46, loss 1.1501091718673706, acc=0.5329999923706055, loss=1.1501091718673706
test: epoch 46, loss 1.9325436353683472, acc=0.28611111640930176, loss=1.9325436353683472
train: epoch 47, loss 1.143027424812317, acc=0.5347777605056763, loss=1.143027424812317
test: epoch 47, loss 1.7415132522583008, acc=0.26944443583488464, loss=1.7415132522583008
train: epoch 48, loss 1.1343841552734375, acc=0.5388333201408386, loss=1.1343841552734375
test: epoch 48, loss 1.787940502166748, acc=0.2916666567325592, loss=1.787940502166748
train: epoch 49, loss 1.1276954412460327, acc=0.5364444255828857, loss=1.1276954412460327
test: epoch 49, loss 1.8533705472946167, acc=0.3083333373069763, loss=1.8533705472946167
train: epoch 50, loss 1.1282533407211304, acc=0.5436111092567444, loss=1.1282533407211304
test: epoch 50, loss 1.9867119789123535, acc=0.25833332538604736, loss=1.9867119789123535
train: epoch 51, loss 1.1185131072998047, acc=0.5447221994400024, loss=1.1185131072998047
test: epoch 51, loss 1.8743445873260498, acc=0.3055555522441864, loss=1.8743445873260498
train: epoch 52, loss 1.0974860191345215, acc=0.5574444532394409, loss=1.0974860191345215
test: epoch 52, loss 1.8993059396743774, acc=0.27222222089767456, loss=1.8993059396743774
train: epoch 53, loss 1.0984565019607544, acc=0.5510555505752563, loss=1.0984565019607544
test: epoch 53, loss 1.7921760082244873, acc=0.2916666567325592, loss=1.7921760082244873
train: epoch 54, loss 1.095081090927124, acc=0.5513888597488403, loss=1.095081090927124
test: epoch 54, loss 2.031970977783203, acc=0.2805555462837219, loss=2.031970977783203
train: epoch 55, loss 1.080580472946167, acc=0.5627222061157227, loss=1.080580472946167
test: epoch 55, loss 1.710667610168457, acc=0.3222222328186035, loss=1.710667610168457
train: epoch 56, loss 1.0791126489639282, acc=0.566611111164093, loss=1.0791126489639282
test: epoch 56, loss 1.8520102500915527, acc=0.31111112236976624, loss=1.8520102500915527
train: epoch 57, loss 1.0773130655288696, acc=0.5659444332122803, loss=1.0773130655288696
test: epoch 57, loss 1.8394907712936401, acc=0.2805555462837219, loss=1.8394907712936401
train: epoch 58, loss 1.0661457777023315, acc=0.5643333196640015, loss=1.0661457777023315
test: epoch 58, loss 1.842069149017334, acc=0.31388887763023376, loss=1.842069149017334
train: epoch 59, loss 1.058018684387207, acc=0.5720000267028809, loss=1.058018684387207
test: epoch 59, loss 1.733443260192871, acc=0.3055555522441864, loss=1.733443260192871
train: epoch 60, loss 1.0488649606704712, acc=0.5733888745307922, loss=1.0488649606704712
test: epoch 60, loss 1.8409236669540405, acc=0.3055555522441864, loss=1.8409236669540405
train: epoch 61, loss 1.0407453775405884, acc=0.5793889164924622, loss=1.0407453775405884
test: epoch 61, loss 1.6707264184951782, acc=0.34166666865348816, loss=1.6707264184951782
train: epoch 62, loss 1.0453439950942993, acc=0.5767222046852112, loss=1.0453439950942993
test: epoch 62, loss 1.7844278812408447, acc=0.3472222089767456, loss=1.7844278812408447
train: epoch 63, loss 1.0312579870224, acc=0.5789444446563721, loss=1.0312579870224
test: epoch 63, loss 1.8168681859970093, acc=0.3166666626930237, loss=1.8168681859970093
train: epoch 64, loss 1.0318145751953125, acc=0.581166684627533, loss=1.0318145751953125
test: epoch 64, loss 1.8112529516220093, acc=0.3472222089767456, loss=1.8112529516220093
train: epoch 65, loss 1.0184897184371948, acc=0.5888888835906982, loss=1.0184897184371948
test: epoch 65, loss 1.623388409614563, acc=0.3027777671813965, loss=1.623388409614563
train: epoch 66, loss 1.0100408792495728, acc=0.5931666493415833, loss=1.0100408792495728
test: epoch 66, loss 1.7699941396713257, acc=0.34166666865348816, loss=1.7699941396713257
train: epoch 67, loss 1.001387596130371, acc=0.5902777910232544, loss=1.001387596130371
test: epoch 67, loss 1.8748770952224731, acc=0.32499998807907104, loss=1.8748770952224731
train: epoch 68, loss 0.9985857009887695, acc=0.5965555310249329, loss=0.9985857009887695
test: epoch 68, loss 1.7557851076126099, acc=0.3333333432674408, loss=1.7557851076126099
train: epoch 69, loss 0.9973427653312683, acc=0.597777783870697, loss=0.9973427653312683
test: epoch 69, loss 1.7520884275436401, acc=0.29722222685813904, loss=1.7520884275436401
train: epoch 70, loss 0.9854404926300049, acc=0.6025000214576721, loss=0.9854404926300049
test: epoch 70, loss 1.7924832105636597, acc=0.2527777850627899, loss=1.7924832105636597
train: epoch 71, loss 0.9920535087585449, acc=0.600944459438324, loss=0.9920535087585449
test: epoch 71, loss 1.6304677724838257, acc=0.32777777314186096, loss=1.6304677724838257
train: epoch 72, loss 0.977902889251709, acc=0.6085555553436279, loss=0.977902889251709
test: epoch 72, loss 1.8045732975006104, acc=0.3444444537162781, loss=1.8045732975006104
train: epoch 73, loss 0.9846923351287842, acc=0.604888916015625, loss=0.9846923351287842
test: epoch 73, loss 1.7103374004364014, acc=0.32499998807907104, loss=1.7103374004364014
train: epoch 74, loss 0.9804278016090393, acc=0.6061111092567444, loss=0.9804278016090393
test: epoch 74, loss 1.6970168352127075, acc=0.3027777671813965, loss=1.6970168352127075
train: epoch 75, loss 0.9573611617088318, acc=0.6148889064788818, loss=0.9573611617088318
test: epoch 75, loss 1.700530767440796, acc=0.34166666865348816, loss=1.700530767440796
train: epoch 76, loss 0.9505149126052856, acc=0.6175000071525574, loss=0.9505149126052856
test: epoch 76, loss 1.6234138011932373, acc=0.36666667461395264, loss=1.6234138011932373
train: epoch 77, loss 0.9534637928009033, acc=0.6182777881622314, loss=0.9534637928009033
test: epoch 77, loss 1.7659024000167847, acc=0.38055557012557983, loss=1.7659024000167847
train: epoch 78, loss 0.9421461820602417, acc=0.6205000281333923, loss=0.9421461820602417
test: epoch 78, loss 1.5958404541015625, acc=0.38055557012557983, loss=1.5958404541015625
train: epoch 79, loss 0.9349820613861084, acc=0.6278333067893982, loss=0.9349820613861084
test: epoch 79, loss 1.8715394735336304, acc=0.29722222685813904, loss=1.8715394735336304
train: epoch 80, loss 0.9232222437858582, acc=0.6354444622993469, loss=0.9232222437858582
test: epoch 80, loss 1.6712620258331299, acc=0.3305555582046509, loss=1.6712620258331299
train: epoch 81, loss 0.9244107604026794, acc=0.6312222480773926, loss=0.9244107604026794
test: epoch 81, loss 1.6071933507919312, acc=0.3499999940395355, loss=1.6071933507919312
train: epoch 82, loss 0.913018524646759, acc=0.6370000243186951, loss=0.913018524646759
test: epoch 82, loss 2.0742039680480957, acc=0.36666667461395264, loss=2.0742039680480957
train: epoch 83, loss 0.912947952747345, acc=0.6372222304344177, loss=0.912947952747345
test: epoch 83, loss 1.7884159088134766, acc=0.31388887763023376, loss=1.7884159088134766
train: epoch 84, loss 0.9196903109550476, acc=0.629444420337677, loss=0.9196903109550476
test: epoch 84, loss 1.7656562328338623, acc=0.3444444537162781, loss=1.7656562328338623
train: epoch 85, loss 0.9087672233581543, acc=0.6341666579246521, loss=0.9087672233581543
test: epoch 85, loss 1.7937836647033691, acc=0.3583333194255829, loss=1.7937836647033691
train: epoch 86, loss 0.9093360900878906, acc=0.6364444494247437, loss=0.9093360900878906
test: epoch 86, loss 1.7494432926177979, acc=0.3333333432674408, loss=1.7494432926177979
train: epoch 87, loss 0.9041029810905457, acc=0.6388333439826965, loss=0.9041029810905457
test: epoch 87, loss 1.7297877073287964, acc=0.3444444537162781, loss=1.7297877073287964
train: epoch 88, loss 0.8836671710014343, acc=0.6459444165229797, loss=0.8836671710014343
test: epoch 88, loss 1.7061175107955933, acc=0.3722222149372101, loss=1.7061175107955933
train: epoch 89, loss 0.8871802687644958, acc=0.6445000171661377, loss=0.8871802687644958
test: epoch 89, loss 1.9103456735610962, acc=0.38055557012557983, loss=1.9103456735610962
train: epoch 90, loss 0.8967717289924622, acc=0.6396111249923706, loss=0.8967717289924622
test: epoch 90, loss 1.7175180912017822, acc=0.3333333432674408, loss=1.7175180912017822
train: epoch 91, loss 0.8913518786430359, acc=0.640500009059906, loss=0.8913518786430359
test: epoch 91, loss 1.9595580101013184, acc=0.3027777671813965, loss=1.9595580101013184
train: epoch 92, loss 0.8574621081352234, acc=0.6508333086967468, loss=0.8574621081352234
test: epoch 92, loss 1.7508540153503418, acc=0.34166666865348816, loss=1.7508540153503418
train: epoch 93, loss 0.8673569560050964, acc=0.64811110496521, loss=0.8673569560050964
test: epoch 93, loss 1.8405873775482178, acc=0.36666667461395264, loss=1.8405873775482178
train: epoch 94, loss 0.8696326613426208, acc=0.6466666460037231, loss=0.8696326613426208
test: epoch 94, loss 1.816604495048523, acc=0.3499999940395355, loss=1.816604495048523
train: epoch 95, loss 0.8734740614891052, acc=0.6518333554267883, loss=0.8734740614891052
test: epoch 95, loss 1.9463905096054077, acc=0.3611111044883728, loss=1.9463905096054077
train: epoch 96, loss 0.8689242601394653, acc=0.6495555639266968, loss=0.8689242601394653
test: epoch 96, loss 1.8173109292984009, acc=0.36666667461395264, loss=1.8173109292984009
train: epoch 97, loss 0.8540368676185608, acc=0.6554444432258606, loss=0.8540368676185608
test: epoch 97, loss 1.7522931098937988, acc=0.3638888895511627, loss=1.7522931098937988
train: epoch 98, loss 0.8517311215400696, acc=0.6525555849075317, loss=0.8517311215400696
test: epoch 98, loss 1.681604027748108, acc=0.36944442987442017, loss=1.681604027748108
train: epoch 99, loss 0.8465211391448975, acc=0.6572777628898621, loss=0.8465211391448975
test: epoch 99, loss 1.780001163482666, acc=0.34166666865348816, loss=1.780001163482666
train: epoch 100, loss 0.8295049667358398, acc=0.6648889183998108, loss=0.8295049667358398
test: epoch 100, loss 1.7203662395477295, acc=0.3583333194255829, loss=1.7203662395477295
train: epoch 101, loss 0.8454870581626892, acc=0.6596666574478149, loss=0.8454870581626892
test: epoch 101, loss 1.836531400680542, acc=0.3222222328186035, loss=1.836531400680542
train: epoch 102, loss 0.8565934300422668, acc=0.6603333353996277, loss=0.8565934300422668
test: epoch 102, loss 1.7942131757736206, acc=0.375, loss=1.7942131757736206
train: epoch 103, loss 0.8415424823760986, acc=0.6629999876022339, loss=0.8415424823760986
test: epoch 103, loss 1.6483012437820435, acc=0.375, loss=1.6483012437820435
train: epoch 104, loss 0.8320961594581604, acc=0.6638888716697693, loss=0.8320961594581604
test: epoch 104, loss 1.884427547454834, acc=0.3611111044883728, loss=1.884427547454834
train: epoch 105, loss 0.8294953107833862, acc=0.6682222485542297, loss=0.8294953107833862
test: epoch 105, loss 1.9335968494415283, acc=0.35555556416511536, loss=1.9335968494415283
train: epoch 106, loss 0.825910747051239, acc=0.6720555424690247, loss=0.825910747051239
test: epoch 106, loss 1.8166931867599487, acc=0.3777777850627899, loss=1.8166931867599487
train: epoch 107, loss 0.8242328763008118, acc=0.6672222018241882, loss=0.8242328763008118
test: epoch 107, loss 1.8815515041351318, acc=0.32777777314186096, loss=1.8815515041351318
train: epoch 108, loss 0.8310438394546509, acc=0.6698889136314392, loss=0.8310438394546509
test: epoch 108, loss 1.895918846130371, acc=0.3611111044883728, loss=1.895918846130371
train: epoch 109, loss 0.8157326579093933, acc=0.6773889064788818, loss=0.8157326579093933
test: epoch 109, loss 1.9771268367767334, acc=0.3638888895511627, loss=1.9771268367767334
train: epoch 110, loss 0.8097925186157227, acc=0.6809999942779541, loss=0.8097925186157227
test: epoch 110, loss 1.791176438331604, acc=0.3638888895511627, loss=1.791176438331604
train: epoch 111, loss 0.8265589475631714, acc=0.671833336353302, loss=0.8265589475631714
test: epoch 111, loss 1.9382400512695312, acc=0.32777777314186096, loss=1.9382400512695312
train: epoch 112, loss 0.8098810315132141, acc=0.6817777752876282, loss=0.8098810315132141
test: epoch 112, loss 1.8232489824295044, acc=0.3222222328186035, loss=1.8232489824295044
train: epoch 113, loss 0.8094967603683472, acc=0.6803333163261414, loss=0.8094967603683472
test: epoch 113, loss 1.9188563823699951, acc=0.32777777314186096, loss=1.9188563823699951
train: epoch 114, loss 0.8165154457092285, acc=0.6765555739402771, loss=0.8165154457092285
test: epoch 114, loss 2.0570151805877686, acc=0.3333333432674408, loss=2.0570151805877686
train: epoch 115, loss 0.8077244758605957, acc=0.6812222003936768, loss=0.8077244758605957
test: epoch 115, loss 1.8828967809677124, acc=0.3722222149372101, loss=1.8828967809677124
train: epoch 116, loss 0.8035696148872375, acc=0.683555543422699, loss=0.8035696148872375
test: epoch 116, loss 1.6983304023742676, acc=0.375, loss=1.6983304023742676
train: epoch 117, loss 0.8026724457740784, acc=0.6851666569709778, loss=0.8026724457740784
test: epoch 117, loss 1.7997289896011353, acc=0.3361110985279083, loss=1.7997289896011353
train: epoch 118, loss 0.8107911944389343, acc=0.6824444532394409, loss=0.8107911944389343
test: epoch 118, loss 1.8236401081085205, acc=0.3166666626930237, loss=1.8236401081085205
train: epoch 119, loss 0.7900757193565369, acc=0.6884999871253967, loss=0.7900757193565369
test: epoch 119, loss 1.83346426486969, acc=0.36666667461395264, loss=1.83346426486969
train: epoch 120, loss 0.8081939220428467, acc=0.6856666803359985, loss=0.8081939220428467
test: epoch 120, loss 1.9186893701553345, acc=0.3222222328186035, loss=1.9186893701553345
train: epoch 121, loss 0.79253089427948, acc=0.6855555772781372, loss=0.79253089427948
test: epoch 121, loss 1.8377139568328857, acc=0.3222222328186035, loss=1.8377139568328857
train: epoch 122, loss 0.7972625494003296, acc=0.684499979019165, loss=0.7972625494003296
test: epoch 122, loss 1.8882215023040771, acc=0.3777777850627899, loss=1.8882215023040771
train: epoch 123, loss 0.7842116355895996, acc=0.6905555725097656, loss=0.7842116355895996
test: epoch 123, loss 1.9088023900985718, acc=0.32499998807907104, loss=1.9088023900985718
train: epoch 124, loss 0.7825731039047241, acc=0.6895555257797241, loss=0.7825731039047241
test: epoch 124, loss 1.6737995147705078, acc=0.3777777850627899, loss=1.6737995147705078
train: epoch 125, loss 0.7805017828941345, acc=0.6913333535194397, loss=0.7805017828941345
test: epoch 125, loss 1.8827393054962158, acc=0.36944442987442017, loss=1.8827393054962158
train: epoch 126, loss 0.7743750810623169, acc=0.6961666941642761, loss=0.7743750810623169
test: epoch 126, loss 1.7884629964828491, acc=0.3722222149372101, loss=1.7884629964828491
train: epoch 127, loss 0.7876473665237427, acc=0.6872777938842773, loss=0.7876473665237427
test: epoch 127, loss 1.554848551750183, acc=0.3722222149372101, loss=1.554848551750183
train: epoch 128, loss 0.7805764079093933, acc=0.6949999928474426, loss=0.7805764079093933
test: epoch 128, loss 1.961733341217041, acc=0.36944442987442017, loss=1.961733341217041
train: epoch 129, loss 0.7806074619293213, acc=0.6965555548667908, loss=0.7806074619293213
test: epoch 129, loss 1.9402809143066406, acc=0.32777777314186096, loss=1.9402809143066406
train: epoch 130, loss 0.7932186126708984, acc=0.6877777576446533, loss=0.7932186126708984
test: epoch 130, loss 1.8829774856567383, acc=0.3222222328186035, loss=1.8829774856567383
train: epoch 131, loss 0.7658734917640686, acc=0.6971666812896729, loss=0.7658734917640686
test: epoch 131, loss 1.7651138305664062, acc=0.32777777314186096, loss=1.7651138305664062
train: epoch 132, loss 0.7783912420272827, acc=0.6948888897895813, loss=0.7783912420272827
test: epoch 132, loss 1.88019859790802, acc=0.35555556416511536, loss=1.88019859790802
train: epoch 133, loss 0.7593408823013306, acc=0.7023333311080933, loss=0.7593408823013306
test: epoch 133, loss 1.8143924474716187, acc=0.35555556416511536, loss=1.8143924474716187
train: epoch 134, loss 0.7605025172233582, acc=0.7001110911369324, loss=0.7605025172233582
test: epoch 134, loss 1.9520050287246704, acc=0.3583333194255829, loss=1.9520050287246704
train: epoch 135, loss 0.7852773070335388, acc=0.6962777972221375, loss=0.7852773070335388
test: epoch 135, loss 1.7154024839401245, acc=0.3638888895511627, loss=1.7154024839401245
train: epoch 136, loss 0.7859131693840027, acc=0.6945000290870667, loss=0.7859131693840027
test: epoch 136, loss 1.7260935306549072, acc=0.36666667461395264, loss=1.7260935306549072
train: epoch 137, loss 0.7658082842826843, acc=0.7003889083862305, loss=0.7658082842826843
test: epoch 137, loss 1.7649832963943481, acc=0.3611111044883728, loss=1.7649832963943481
train: epoch 138, loss 0.7703392505645752, acc=0.6958333253860474, loss=0.7703392505645752
test: epoch 138, loss 1.8676499128341675, acc=0.38333332538604736, loss=1.8676499128341675
train: epoch 139, loss 0.7706717848777771, acc=0.6972777843475342, loss=0.7706717848777771
test: epoch 139, loss 1.8256561756134033, acc=0.3722222149372101, loss=1.8256561756134033
train: epoch 140, loss 0.7768385410308838, acc=0.6963889002799988, loss=0.7768385410308838
test: epoch 140, loss 2.1085293292999268, acc=0.3333333432674408, loss=2.1085293292999268
train: epoch 141, loss 0.7615399360656738, acc=0.7001110911369324, loss=0.7615399360656738
test: epoch 141, loss 1.860422134399414, acc=0.3638888895511627, loss=1.860422134399414
train: epoch 142, loss 0.7620266675949097, acc=0.7016111016273499, loss=0.7620266675949097
test: epoch 142, loss 1.9005259275436401, acc=0.375, loss=1.9005259275436401
train: epoch 143, loss 0.76760333776474, acc=0.6966111063957214, loss=0.76760333776474
test: epoch 143, loss 1.8554677963256836, acc=0.32499998807907104, loss=1.8554677963256836
train: epoch 144, loss 0.7342763543128967, acc=0.7123888731002808, loss=0.7342763543128967
test: epoch 144, loss 1.6679128408432007, acc=0.3583333194255829, loss=1.6679128408432007
train: epoch 145, loss 0.764492928981781, acc=0.6983333230018616, loss=0.764492928981781
test: epoch 145, loss 1.7110481262207031, acc=0.3638888895511627, loss=1.7110481262207031
train: epoch 146, loss 0.7597673535346985, acc=0.6997777819633484, loss=0.7597673535346985
test: epoch 146, loss 1.8983385562896729, acc=0.38333332538604736, loss=1.8983385562896729
train: epoch 147, loss 0.7749366164207458, acc=0.6945555806159973, loss=0.7749366164207458
test: epoch 147, loss 1.8638331890106201, acc=0.3777777850627899, loss=1.8638331890106201
train: epoch 148, loss 0.7610257267951965, acc=0.7045555710792542, loss=0.7610257267951965
test: epoch 148, loss 1.8422837257385254, acc=0.33888888359069824, loss=1.8422837257385254
train: epoch 149, loss 0.7533088326454163, acc=0.7064444422721863, loss=0.7533088326454163
test: epoch 149, loss 1.9218465089797974, acc=0.3194444477558136, loss=1.9218465089797974
train: epoch 150, loss 0.762627363204956, acc=0.7009999752044678, loss=0.762627363204956
test: epoch 150, loss 1.7551883459091187, acc=0.375, loss=1.7551883459091187
