# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1.5", "--temp_decay=0.995", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=959123966, receiver_embed_dim=128, save_run=0, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=959123966, receiver_embed_dim=128, save_run=False, temp_decay=0.995, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.525909185409546, acc=0.04722222313284874, loss=3.525909185409546
test: epoch 1, loss 3.4292216300964355, acc=0.05833333358168602, loss=3.4292216300964355
train: epoch 2, loss 3.4631714820861816, acc=0.05372222140431404, loss=3.4631714820861816
test: epoch 2, loss 3.0814907550811768, acc=0.1111111119389534, loss=3.0814907550811768
train: epoch 3, loss 3.278378963470459, acc=0.06827777624130249, loss=3.278378963470459
test: epoch 3, loss 4.891077995300293, acc=0.04722222313284874, loss=4.891077995300293
train: epoch 4, loss 2.878329038619995, acc=0.11344444751739502, loss=2.878329038619995
test: epoch 4, loss 6.277170181274414, acc=0.03055555559694767, loss=6.277170181274414
train: epoch 5, loss 2.6532485485076904, acc=0.1440555602312088, loss=2.6532485485076904
test: epoch 5, loss 6.5963544845581055, acc=0.0416666679084301, loss=6.5963544845581055
train: epoch 6, loss 2.521221160888672, acc=0.16200000047683716, loss=2.521221160888672
test: epoch 6, loss 6.2984185218811035, acc=0.03888889029622078, loss=6.2984185218811035
train: epoch 7, loss 2.433790922164917, acc=0.1742222160100937, loss=2.433790922164917
test: epoch 7, loss 6.128531455993652, acc=0.05000000074505806, loss=6.128531455993652
train: epoch 8, loss 2.3726367950439453, acc=0.1908888816833496, loss=2.3726367950439453
test: epoch 8, loss 6.027812480926514, acc=0.04722222313284874, loss=6.027812480926514
train: epoch 9, loss 2.310053586959839, acc=0.20305556058883667, loss=2.310053586959839
test: epoch 9, loss 6.2068376541137695, acc=0.06666667014360428, loss=6.2068376541137695
train: epoch 10, loss 2.274232864379883, acc=0.2103888839483261, loss=2.274232864379883
test: epoch 10, loss 6.107419490814209, acc=0.05000000074505806, loss=6.107419490814209
train: epoch 11, loss 2.242018938064575, acc=0.21983332931995392, loss=2.242018938064575
test: epoch 11, loss 6.029057025909424, acc=0.04444444552063942, loss=6.029057025909424
train: epoch 12, loss 2.1920957565307617, acc=0.2349444478750229, loss=2.1920957565307617
test: epoch 12, loss 5.9799909591674805, acc=0.0555555559694767, loss=5.9799909591674805
train: epoch 13, loss 2.174086093902588, acc=0.24077777564525604, loss=2.174086093902588
test: epoch 13, loss 6.012680530548096, acc=0.06388889253139496, loss=6.012680530548096
train: epoch 14, loss 2.1465985774993896, acc=0.24983333051204681, loss=2.1465985774993896
test: epoch 14, loss 5.833895206451416, acc=0.05833333358168602, loss=5.833895206451416
train: epoch 15, loss 2.129225492477417, acc=0.24988888204097748, loss=2.129225492477417
test: epoch 15, loss 5.940191745758057, acc=0.0694444477558136, loss=5.940191745758057
train: epoch 16, loss 2.106232166290283, acc=0.25511109828948975, loss=2.106232166290283
test: epoch 16, loss 5.701610088348389, acc=0.06666667014360428, loss=5.701610088348389
train: epoch 17, loss 2.08146595954895, acc=0.273499995470047, loss=2.08146595954895
test: epoch 17, loss 5.641750335693359, acc=0.07500000298023224, loss=5.641750335693359
train: epoch 18, loss 2.065884828567505, acc=0.2740555703639984, loss=2.065884828567505
test: epoch 18, loss 5.3805952072143555, acc=0.07222222536802292, loss=5.3805952072143555
train: epoch 19, loss 2.028615713119507, acc=0.28183332085609436, loss=2.028615713119507
test: epoch 19, loss 5.140562057495117, acc=0.07222222536802292, loss=5.140562057495117
train: epoch 20, loss 2.0231480598449707, acc=0.28672221302986145, loss=2.0231480598449707
test: epoch 20, loss 5.051103591918945, acc=0.06666667014360428, loss=5.051103591918945
train: epoch 21, loss 2.0033955574035645, acc=0.2946110963821411, loss=2.0033955574035645
test: epoch 21, loss 4.877500057220459, acc=0.09166666865348816, loss=4.877500057220459
train: epoch 22, loss 1.9913289546966553, acc=0.2955000102519989, loss=1.9913289546966553
test: epoch 22, loss 4.3751091957092285, acc=0.0833333358168602, loss=4.3751091957092285
train: epoch 23, loss 1.965826392173767, acc=0.30638888478279114, loss=1.965826392173767
test: epoch 23, loss 4.35073709487915, acc=0.0972222238779068, loss=4.35073709487915
train: epoch 24, loss 1.9363023042678833, acc=0.31450000405311584, loss=1.9363023042678833
test: epoch 24, loss 4.081297397613525, acc=0.09444444626569748, loss=4.081297397613525
train: epoch 25, loss 1.92490816116333, acc=0.31672221422195435, loss=1.92490816116333
test: epoch 25, loss 3.7139248847961426, acc=0.11388888955116272, loss=3.7139248847961426
train: epoch 26, loss 1.8995712995529175, acc=0.3225555419921875, loss=1.8995712995529175
test: epoch 26, loss 3.538658380508423, acc=0.13611111044883728, loss=3.538658380508423
train: epoch 27, loss 1.890717625617981, acc=0.32838889956474304, loss=1.890717625617981
test: epoch 27, loss 3.5397591590881348, acc=0.1388888955116272, loss=3.5397591590881348
train: epoch 28, loss 1.8461447954177856, acc=0.3405555486679077, loss=1.8461447954177856
test: epoch 28, loss 3.4210591316223145, acc=0.125, loss=3.4210591316223145
train: epoch 29, loss 1.8348252773284912, acc=0.34716665744781494, loss=1.8348252773284912
test: epoch 29, loss 3.248811960220337, acc=0.1388888955116272, loss=3.248811960220337
train: epoch 30, loss 1.8072047233581543, acc=0.35305556654930115, loss=1.8072047233581543
test: epoch 30, loss 3.212061882019043, acc=0.12222222238779068, loss=3.212061882019043
train: epoch 31, loss 1.805859923362732, acc=0.35822221636772156, loss=1.805859923362732
test: epoch 31, loss 3.0614511966705322, acc=0.13333334028720856, loss=3.0614511966705322
train: epoch 32, loss 1.7859773635864258, acc=0.36238887906074524, loss=1.7859773635864258
test: epoch 32, loss 2.998206377029419, acc=0.13611111044883728, loss=2.998206377029419
train: epoch 33, loss 1.771358847618103, acc=0.37283334136009216, loss=1.771358847618103
test: epoch 33, loss 2.9179041385650635, acc=0.17222222685813904, loss=2.9179041385650635
train: epoch 34, loss 1.7655603885650635, acc=0.37950000166893005, loss=1.7655603885650635
test: epoch 34, loss 2.9365251064300537, acc=0.17222222685813904, loss=2.9365251064300537
train: epoch 35, loss 1.7332615852355957, acc=0.39100000262260437, loss=1.7332615852355957
test: epoch 35, loss 2.79215407371521, acc=0.1805555522441864, loss=2.79215407371521
train: epoch 36, loss 1.7227445840835571, acc=0.39844444394111633, loss=1.7227445840835571
test: epoch 36, loss 2.6656627655029297, acc=0.20277777314186096, loss=2.6656627655029297
train: epoch 37, loss 1.698624849319458, acc=0.4076666533946991, loss=1.698624849319458
test: epoch 37, loss 2.618882656097412, acc=0.18611110746860504, loss=2.618882656097412
train: epoch 38, loss 1.6872211694717407, acc=0.4092777669429779, loss=1.6872211694717407
test: epoch 38, loss 2.6370601654052734, acc=0.16944444179534912, loss=2.6370601654052734
train: epoch 39, loss 1.6619904041290283, acc=0.41966667771339417, loss=1.6619904041290283
test: epoch 39, loss 2.616274833679199, acc=0.18333333730697632, loss=2.616274833679199
train: epoch 40, loss 1.6334376335144043, acc=0.4293888807296753, loss=1.6334376335144043
test: epoch 40, loss 2.537506103515625, acc=0.18888889253139496, loss=2.537506103515625
train: epoch 41, loss 1.6159799098968506, acc=0.4339999854564667, loss=1.6159799098968506
test: epoch 41, loss 2.5879902839660645, acc=0.17499999701976776, loss=2.5879902839660645
train: epoch 42, loss 1.595399022102356, acc=0.44377776980400085, loss=1.595399022102356
test: epoch 42, loss 2.488434314727783, acc=0.18888889253139496, loss=2.488434314727783
train: epoch 43, loss 1.5851606130599976, acc=0.4537777900695801, loss=1.5851606130599976
test: epoch 43, loss 2.38016676902771, acc=0.18611110746860504, loss=2.38016676902771
train: epoch 44, loss 1.5610085725784302, acc=0.4597777724266052, loss=1.5610085725784302
test: epoch 44, loss 2.4624922275543213, acc=0.19722221791744232, loss=2.4624922275543213
train: epoch 45, loss 1.5587185621261597, acc=0.4657222330570221, loss=1.5587185621261597
test: epoch 45, loss 2.424532890319824, acc=0.19166666269302368, loss=2.424532890319824
train: epoch 46, loss 1.5343263149261475, acc=0.47566667199134827, loss=1.5343263149261475
test: epoch 46, loss 2.4039835929870605, acc=0.21388888359069824, loss=2.4039835929870605
train: epoch 47, loss 1.5280412435531616, acc=0.47761112451553345, loss=1.5280412435531616
test: epoch 47, loss 2.3823437690734863, acc=0.21666666865348816, loss=2.3823437690734863
train: epoch 48, loss 1.5045655965805054, acc=0.48577776551246643, loss=1.5045655965805054
test: epoch 48, loss 2.320544719696045, acc=0.21666666865348816, loss=2.320544719696045
train: epoch 49, loss 1.4747809171676636, acc=0.5015555620193481, loss=1.4747809171676636
test: epoch 49, loss 2.2984254360198975, acc=0.2083333283662796, loss=2.2984254360198975
train: epoch 50, loss 1.4598439931869507, acc=0.5031111240386963, loss=1.4598439931869507
test: epoch 50, loss 2.30759596824646, acc=0.2222222238779068, loss=2.30759596824646
train: epoch 51, loss 1.4356838464736938, acc=0.5209444165229797, loss=1.4356838464736938
test: epoch 51, loss 2.2567131519317627, acc=0.2222222238779068, loss=2.2567131519317627
train: epoch 52, loss 1.4041171073913574, acc=0.5183888673782349, loss=1.4041171073913574
test: epoch 52, loss 2.226334571838379, acc=0.21944443881511688, loss=2.226334571838379
train: epoch 53, loss 1.3960201740264893, acc=0.5332221984863281, loss=1.3960201740264893
test: epoch 53, loss 2.284083843231201, acc=0.21944443881511688, loss=2.284083843231201
train: epoch 54, loss 1.389355182647705, acc=0.5366111397743225, loss=1.389355182647705
test: epoch 54, loss 2.2281599044799805, acc=0.2083333283662796, loss=2.2281599044799805
train: epoch 55, loss 1.370509147644043, acc=0.5420555472373962, loss=1.370509147644043
test: epoch 55, loss 2.2428958415985107, acc=0.22777777910232544, loss=2.2428958415985107
train: epoch 56, loss 1.3559447526931763, acc=0.555055558681488, loss=1.3559447526931763
test: epoch 56, loss 2.1734466552734375, acc=0.21944443881511688, loss=2.1734466552734375
train: epoch 57, loss 1.337942361831665, acc=0.562666654586792, loss=1.337942361831665
test: epoch 57, loss 2.1844282150268555, acc=0.22499999403953552, loss=2.1844282150268555
train: epoch 58, loss 1.2978293895721436, acc=0.5729444622993469, loss=1.2978293895721436
test: epoch 58, loss 2.136016607284546, acc=0.2361111044883728, loss=2.136016607284546
train: epoch 59, loss 1.2930079698562622, acc=0.5772222280502319, loss=1.2930079698562622
test: epoch 59, loss 2.1574110984802246, acc=0.21944443881511688, loss=2.1574110984802246
train: epoch 60, loss 1.270179271697998, acc=0.5858888626098633, loss=1.270179271697998
test: epoch 60, loss 2.175229072570801, acc=0.24166665971279144, loss=2.175229072570801
train: epoch 61, loss 1.2627235651016235, acc=0.590499997138977, loss=1.2627235651016235
test: epoch 61, loss 2.1444454193115234, acc=0.2361111044883728, loss=2.1444454193115234
train: epoch 62, loss 1.247414231300354, acc=0.5977222323417664, loss=1.247414231300354
test: epoch 62, loss 2.1310606002807617, acc=0.22499999403953552, loss=2.1310606002807617
train: epoch 63, loss 1.2423018217086792, acc=0.6049444675445557, loss=1.2423018217086792
test: epoch 63, loss 2.132711410522461, acc=0.24166665971279144, loss=2.132711410522461
train: epoch 64, loss 1.205998182296753, acc=0.6118888854980469, loss=1.205998182296753
test: epoch 64, loss 2.0715765953063965, acc=0.25555557012557983, loss=2.0715765953063965
train: epoch 65, loss 1.1937886476516724, acc=0.6263889074325562, loss=1.1937886476516724
test: epoch 65, loss 2.1177120208740234, acc=0.25, loss=2.1177120208740234
train: epoch 66, loss 1.2068191766738892, acc=0.6274444460868835, loss=1.2068191766738892
test: epoch 66, loss 2.0540342330932617, acc=0.25, loss=2.0540342330932617
train: epoch 67, loss 1.1794968843460083, acc=0.6283888816833496, loss=1.1794968843460083
test: epoch 67, loss 2.058295488357544, acc=0.25833332538604736, loss=2.058295488357544
train: epoch 68, loss 1.147407054901123, acc=0.6452777981758118, loss=1.147407054901123
test: epoch 68, loss 2.0300445556640625, acc=0.25, loss=2.0300445556640625
train: epoch 69, loss 1.1651802062988281, acc=0.6485555768013, loss=1.1651802062988281
test: epoch 69, loss 2.046619415283203, acc=0.2666666805744171, loss=2.046619415283203
train: epoch 70, loss 1.1187901496887207, acc=0.6582221984863281, loss=1.1187901496887207
test: epoch 70, loss 2.014799118041992, acc=0.2750000059604645, loss=2.014799118041992
train: epoch 71, loss 1.1088615655899048, acc=0.6565555334091187, loss=1.1088615655899048
test: epoch 71, loss 2.0391972064971924, acc=0.26944443583488464, loss=2.0391972064971924
train: epoch 72, loss 1.1156409978866577, acc=0.6655555367469788, loss=1.1156409978866577
test: epoch 72, loss 2.0110573768615723, acc=0.2527777850627899, loss=2.0110573768615723
train: epoch 73, loss 1.0811023712158203, acc=0.6714444160461426, loss=1.0811023712158203
test: epoch 73, loss 1.9894955158233643, acc=0.25833332538604736, loss=1.9894955158233643
train: epoch 74, loss 1.0704452991485596, acc=0.6747778058052063, loss=1.0704452991485596
test: epoch 74, loss 2.0000901222229004, acc=0.2750000059604645, loss=2.0000901222229004
train: epoch 75, loss 1.0695468187332153, acc=0.6819999814033508, loss=1.0695468187332153
test: epoch 75, loss 1.9891974925994873, acc=0.2750000059604645, loss=1.9891974925994873
train: epoch 76, loss 1.054645299911499, acc=0.6836110949516296, loss=1.054645299911499
test: epoch 76, loss 1.9817157983779907, acc=0.27222222089767456, loss=1.9817157983779907
train: epoch 77, loss 1.0157604217529297, acc=0.7005555629730225, loss=1.0157604217529297
test: epoch 77, loss 1.9662213325500488, acc=0.2611111104488373, loss=1.9662213325500488
train: epoch 78, loss 1.0389764308929443, acc=0.6959444284439087, loss=1.0389764308929443
test: epoch 78, loss 1.964292049407959, acc=0.27222222089767456, loss=1.964292049407959
train: epoch 79, loss 1.016274094581604, acc=0.6977777481079102, loss=1.016274094581604
test: epoch 79, loss 1.8888977766036987, acc=0.2805555462837219, loss=1.8888977766036987
train: epoch 80, loss 0.990900993347168, acc=0.707111120223999, loss=0.990900993347168
test: epoch 80, loss 1.8980751037597656, acc=0.2805555462837219, loss=1.8980751037597656
train: epoch 81, loss 0.9859583377838135, acc=0.7099444270133972, loss=0.9859583377838135
test: epoch 81, loss 1.8901296854019165, acc=0.28611111640930176, loss=1.8901296854019165
train: epoch 82, loss 0.9924206137657166, acc=0.7166110873222351, loss=0.9924206137657166
test: epoch 82, loss 1.8769257068634033, acc=0.2805555462837219, loss=1.8769257068634033
train: epoch 83, loss 0.9731675386428833, acc=0.7217222452163696, loss=0.9731675386428833
test: epoch 83, loss 1.8644616603851318, acc=0.2944444417953491, loss=1.8644616603851318
train: epoch 84, loss 0.9304736256599426, acc=0.7296666502952576, loss=0.9304736256599426
test: epoch 84, loss 1.854797124862671, acc=0.26944443583488464, loss=1.854797124862671
train: epoch 85, loss 0.9336150288581848, acc=0.7252222299575806, loss=0.9336150288581848
test: epoch 85, loss 1.788527250289917, acc=0.2750000059604645, loss=1.788527250289917
train: epoch 86, loss 0.9358474016189575, acc=0.7315000295639038, loss=0.9358474016189575
test: epoch 86, loss 1.7741411924362183, acc=0.31388887763023376, loss=1.7741411924362183
train: epoch 87, loss 0.9312991499900818, acc=0.7295555472373962, loss=0.9312991499900818
test: epoch 87, loss 1.7423095703125, acc=0.3055555522441864, loss=1.7423095703125
train: epoch 88, loss 0.9255467653274536, acc=0.7384999990463257, loss=0.9255467653274536
test: epoch 88, loss 1.7285569906234741, acc=0.2916666567325592, loss=1.7285569906234741
train: epoch 89, loss 0.9062614440917969, acc=0.7465555667877197, loss=0.9062614440917969
test: epoch 89, loss 1.7145682573318481, acc=0.3083333373069763, loss=1.7145682573318481
train: epoch 90, loss 0.8763710856437683, acc=0.7493888735771179, loss=0.8763710856437683
test: epoch 90, loss 1.7053076028823853, acc=0.3166666626930237, loss=1.7053076028823853
train: epoch 91, loss 0.8912487626075745, acc=0.749666690826416, loss=0.8912487626075745
test: epoch 91, loss 1.6979581117630005, acc=0.3305555582046509, loss=1.6979581117630005
train: epoch 92, loss 0.8945529460906982, acc=0.7512778043746948, loss=0.8945529460906982
test: epoch 92, loss 1.709468126296997, acc=0.31111112236976624, loss=1.709468126296997
train: epoch 93, loss 0.8754077553749084, acc=0.7563333511352539, loss=0.8754077553749084
test: epoch 93, loss 1.7282134294509888, acc=0.3194444477558136, loss=1.7282134294509888
train: epoch 94, loss 0.8718230128288269, acc=0.7586666941642761, loss=0.8718230128288269
test: epoch 94, loss 1.713420033454895, acc=0.31111112236976624, loss=1.713420033454895
train: epoch 95, loss 0.8570884466171265, acc=0.761388897895813, loss=0.8570884466171265
test: epoch 95, loss 1.6474721431732178, acc=0.35277777910232544, loss=1.6474721431732178
train: epoch 96, loss 0.8628628253936768, acc=0.762333333492279, loss=0.8628628253936768
test: epoch 96, loss 1.6706629991531372, acc=0.3361110985279083, loss=1.6706629991531372
train: epoch 97, loss 0.8308137655258179, acc=0.7765555381774902, loss=0.8308137655258179
test: epoch 97, loss 1.7063015699386597, acc=0.33888888359069824, loss=1.7063015699386597
train: epoch 98, loss 0.8137948513031006, acc=0.7709444165229797, loss=0.8137948513031006
test: epoch 98, loss 1.660599946975708, acc=0.3361110985279083, loss=1.660599946975708
train: epoch 99, loss 0.8041820526123047, acc=0.7760000228881836, loss=0.8041820526123047
test: epoch 99, loss 1.6436960697174072, acc=0.3499999940395355, loss=1.6436960697174072
train: epoch 100, loss 0.8029492497444153, acc=0.7845555543899536, loss=0.8029492497444153
test: epoch 100, loss 1.6129201650619507, acc=0.3472222089767456, loss=1.6129201650619507
train: epoch 101, loss 0.8296791315078735, acc=0.7803888916969299, loss=0.8296791315078735
test: epoch 101, loss 1.6403639316558838, acc=0.3472222089767456, loss=1.6403639316558838
train: epoch 102, loss 0.7905106544494629, acc=0.781166672706604, loss=0.7905106544494629
test: epoch 102, loss 1.5977027416229248, acc=0.3583333194255829, loss=1.5977027416229248
train: epoch 103, loss 0.802751362323761, acc=0.7808333039283752, loss=0.802751362323761
test: epoch 103, loss 1.566986083984375, acc=0.36666667461395264, loss=1.566986083984375
train: epoch 104, loss 0.7990034818649292, acc=0.7821666598320007, loss=0.7990034818649292
test: epoch 104, loss 1.5485433340072632, acc=0.3499999940395355, loss=1.5485433340072632
train: epoch 105, loss 0.7806787490844727, acc=0.7858889102935791, loss=0.7806787490844727
test: epoch 105, loss 1.5586060285568237, acc=0.3583333194255829, loss=1.5586060285568237
train: epoch 106, loss 0.7954052090644836, acc=0.7876111268997192, loss=0.7954052090644836
test: epoch 106, loss 1.5889346599578857, acc=0.3638888895511627, loss=1.5889346599578857
train: epoch 107, loss 0.7891404628753662, acc=0.7868333458900452, loss=0.7891404628753662
test: epoch 107, loss 1.5864536762237549, acc=0.375, loss=1.5864536762237549
train: epoch 108, loss 0.7908371686935425, acc=0.7952222228050232, loss=0.7908371686935425
test: epoch 108, loss 1.5242373943328857, acc=0.3722222149372101, loss=1.5242373943328857
train: epoch 109, loss 0.7808715105056763, acc=0.7943888902664185, loss=0.7808715105056763
test: epoch 109, loss 1.544248104095459, acc=0.38055557012557983, loss=1.544248104095459
train: epoch 110, loss 0.7663306593894958, acc=0.7975555658340454, loss=0.7663306593894958
test: epoch 110, loss 1.5235036611557007, acc=0.3916666805744171, loss=1.5235036611557007
train: epoch 111, loss 0.7579736113548279, acc=0.7950000166893005, loss=0.7579736113548279
test: epoch 111, loss 1.5092241764068604, acc=0.39444443583488464, loss=1.5092241764068604
train: epoch 112, loss 0.7763661742210388, acc=0.7949444651603699, loss=0.7763661742210388
test: epoch 112, loss 1.471657156944275, acc=0.39444443583488464, loss=1.471657156944275
train: epoch 113, loss 0.760050892829895, acc=0.8016666769981384, loss=0.760050892829895
test: epoch 113, loss 1.4951603412628174, acc=0.3888888955116272, loss=1.4951603412628174
train: epoch 114, loss 0.7404131889343262, acc=0.8043888807296753, loss=0.7404131889343262
test: epoch 114, loss 1.48908269405365, acc=0.3916666805744171, loss=1.48908269405365
train: epoch 115, loss 0.7538707256317139, acc=0.8042222261428833, loss=0.7538707256317139
test: epoch 115, loss 1.4566078186035156, acc=0.39722222089767456, loss=1.4566078186035156
train: epoch 116, loss 0.736028790473938, acc=0.8058333396911621, loss=0.736028790473938
test: epoch 116, loss 1.4743549823760986, acc=0.4000000059604645, loss=1.4743549823760986
train: epoch 117, loss 0.7397509217262268, acc=0.8059444427490234, loss=0.7397509217262268
test: epoch 117, loss 1.4629744291305542, acc=0.39444443583488464, loss=1.4629744291305542
train: epoch 118, loss 0.7316222190856934, acc=0.8090000152587891, loss=0.7316222190856934
test: epoch 118, loss 1.4650923013687134, acc=0.39722222089767456, loss=1.4650923013687134
train: epoch 119, loss 0.7342736721038818, acc=0.808222234249115, loss=0.7342736721038818
test: epoch 119, loss 1.441735863685608, acc=0.39444443583488464, loss=1.441735863685608
train: epoch 120, loss 0.7337311506271362, acc=0.8114444613456726, loss=0.7337311506271362
test: epoch 120, loss 1.4566653966903687, acc=0.4166666567325592, loss=1.4566653966903687
train: epoch 121, loss 0.729897141456604, acc=0.8136110901832581, loss=0.729897141456604
test: epoch 121, loss 1.4372555017471313, acc=0.39444443583488464, loss=1.4372555017471313
train: epoch 122, loss 0.7034964561462402, acc=0.8133333325386047, loss=0.7034964561462402
test: epoch 122, loss 1.4409563541412354, acc=0.4138889014720917, loss=1.4409563541412354
train: epoch 123, loss 0.6986587643623352, acc=0.8131111264228821, loss=0.6986587643623352
test: epoch 123, loss 1.4419704675674438, acc=0.4194444417953491, loss=1.4419704675674438
train: epoch 124, loss 0.7265896201133728, acc=0.8152222037315369, loss=0.7265896201133728
test: epoch 124, loss 1.4376295804977417, acc=0.4000000059604645, loss=1.4376295804977417
train: epoch 125, loss 0.7232679724693298, acc=0.8096110820770264, loss=0.7232679724693298
test: epoch 125, loss 1.4330646991729736, acc=0.4194444417953491, loss=1.4330646991729736
train: epoch 126, loss 0.7083709239959717, acc=0.8115555644035339, loss=0.7083709239959717
test: epoch 126, loss 1.4626994132995605, acc=0.4055555462837219, loss=1.4626994132995605
train: epoch 127, loss 0.691628098487854, acc=0.8222222328186035, loss=0.691628098487854
test: epoch 127, loss 1.4329456090927124, acc=0.4055555462837219, loss=1.4329456090927124
train: epoch 128, loss 0.6954408884048462, acc=0.8168888688087463, loss=0.6954408884048462
test: epoch 128, loss 1.4027127027511597, acc=0.4194444417953491, loss=1.4027127027511597
train: epoch 129, loss 0.682691752910614, acc=0.8211666941642761, loss=0.682691752910614
test: epoch 129, loss 1.422678828239441, acc=0.42500001192092896, loss=1.422678828239441
train: epoch 130, loss 0.693195641040802, acc=0.8184444308280945, loss=0.693195641040802
test: epoch 130, loss 1.3847453594207764, acc=0.41111111640930176, loss=1.3847453594207764
train: epoch 131, loss 0.704201340675354, acc=0.8176666498184204, loss=0.704201340675354
test: epoch 131, loss 1.406870722770691, acc=0.4333333373069763, loss=1.406870722770691
train: epoch 132, loss 0.7036493420600891, acc=0.8188333511352539, loss=0.7036493420600891
test: epoch 132, loss 1.3779264688491821, acc=0.4138889014720917, loss=1.3779264688491821
train: epoch 133, loss 0.6912850737571716, acc=0.8218333125114441, loss=0.6912850737571716
test: epoch 133, loss 1.3963425159454346, acc=0.42500001192092896, loss=1.3963425159454346
train: epoch 134, loss 0.6988574266433716, acc=0.8168333172798157, loss=0.6988574266433716
test: epoch 134, loss 1.3798049688339233, acc=0.4305555522441864, loss=1.3798049688339233
train: epoch 135, loss 0.6730746030807495, acc=0.823722243309021, loss=0.6730746030807495
test: epoch 135, loss 1.3542252779006958, acc=0.4305555522441864, loss=1.3542252779006958
train: epoch 136, loss 0.6681135892868042, acc=0.8184999823570251, loss=0.6681135892868042
test: epoch 136, loss 1.3565415143966675, acc=0.43611112236976624, loss=1.3565415143966675
train: epoch 137, loss 0.6663827300071716, acc=0.823888897895813, loss=0.6663827300071716
test: epoch 137, loss 1.3537355661392212, acc=0.4416666626930237, loss=1.3537355661392212
train: epoch 138, loss 0.7280322313308716, acc=0.8174444437026978, loss=0.7280322313308716
test: epoch 138, loss 1.3684407472610474, acc=0.4444444477558136, loss=1.3684407472610474
train: epoch 139, loss 0.7002442479133606, acc=0.8184444308280945, loss=0.7002442479133606
test: epoch 139, loss 1.3429311513900757, acc=0.4472222328186035, loss=1.3429311513900757
train: epoch 140, loss 0.6571009755134583, acc=0.8246666789054871, loss=0.6571009755134583
test: epoch 140, loss 1.3512721061706543, acc=0.4444444477558136, loss=1.3512721061706543
train: epoch 141, loss 0.6703261137008667, acc=0.8266111016273499, loss=0.6703261137008667
test: epoch 141, loss 1.3512699604034424, acc=0.43888887763023376, loss=1.3512699604034424
train: epoch 142, loss 0.6489676237106323, acc=0.8310555815696716, loss=0.6489676237106323
test: epoch 142, loss 1.330148696899414, acc=0.4416666626930237, loss=1.330148696899414
train: epoch 143, loss 0.6597579121589661, acc=0.8250555396080017, loss=0.6597579121589661
test: epoch 143, loss 1.3092437982559204, acc=0.45277777314186096, loss=1.3092437982559204
train: epoch 144, loss 0.6565282344818115, acc=0.8272222280502319, loss=0.6565282344818115
test: epoch 144, loss 1.346234679222107, acc=0.4416666626930237, loss=1.346234679222107
train: epoch 145, loss 0.6633615493774414, acc=0.8271666765213013, loss=0.6633615493774414
test: epoch 145, loss 1.300331950187683, acc=0.4555555582046509, loss=1.300331950187683
train: epoch 146, loss 0.640473484992981, acc=0.8305000066757202, loss=0.640473484992981
test: epoch 146, loss 1.301458477973938, acc=0.4611110985279083, loss=1.301458477973938
train: epoch 147, loss 0.6442552804946899, acc=0.8334444165229797, loss=0.6442552804946899
test: epoch 147, loss 1.2683522701263428, acc=0.45277777314186096, loss=1.2683522701263428
train: epoch 148, loss 0.6432448029518127, acc=0.8320555686950684, loss=0.6432448029518127
test: epoch 148, loss 1.2936925888061523, acc=0.4722222089767456, loss=1.2936925888061523
train: epoch 149, loss 0.6136581301689148, acc=0.8326666951179504, loss=0.6136581301689148
test: epoch 149, loss 1.2955756187438965, acc=0.4583333432674408, loss=1.2955756187438965
train: epoch 150, loss 0.6124122738838196, acc=0.8369444608688354, loss=0.6124122738838196
test: epoch 150, loss 1.2910614013671875, acc=0.4694444537162781, loss=1.2910614013671875
