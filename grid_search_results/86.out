# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=1.5", "--temp_decay=0.99", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=81354876, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=81354876, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=1.5, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.515735626220703, acc=0.14477777481079102, loss=2.515735626220703
test: epoch 1, loss 5.426448822021484, acc=0.07777778059244156, loss=5.426448822021484
train: epoch 2, loss 1.7880988121032715, acc=0.2817777693271637, loss=1.7880988121032715
test: epoch 2, loss 4.284915447235107, acc=0.11388888955116272, loss=4.284915447235107
train: epoch 3, loss 1.5659127235412598, acc=0.36122220754623413, loss=1.5659127235412598
test: epoch 3, loss 3.2211928367614746, acc=0.16111111640930176, loss=3.2211928367614746
train: epoch 4, loss 1.4114460945129395, acc=0.42338889837265015, loss=1.4114460945129395
test: epoch 4, loss 2.855767250061035, acc=0.20555555820465088, loss=2.855767250061035
train: epoch 5, loss 1.2882996797561646, acc=0.4779999852180481, loss=1.2882996797561646
test: epoch 5, loss 2.7804136276245117, acc=0.20277777314186096, loss=2.7804136276245117
train: epoch 6, loss 1.2198413610458374, acc=0.5007777810096741, loss=1.2198413610458374
test: epoch 6, loss 2.896177291870117, acc=0.21666666865348816, loss=2.896177291870117
train: epoch 7, loss 1.1266635656356812, acc=0.5448333621025085, loss=1.1266635656356812
test: epoch 7, loss 2.2919163703918457, acc=0.2916666567325592, loss=2.2919163703918457
train: epoch 8, loss 1.0934619903564453, acc=0.5565000176429749, loss=1.0934619903564453
test: epoch 8, loss 2.0141208171844482, acc=0.30000001192092896, loss=2.0141208171844482
train: epoch 9, loss 1.0266261100769043, acc=0.5830555558204651, loss=1.0266261100769043
test: epoch 9, loss 2.3050999641418457, acc=0.2638888955116272, loss=2.3050999641418457
train: epoch 10, loss 0.9882319569587708, acc=0.6014999747276306, loss=0.9882319569587708
test: epoch 10, loss 2.2507247924804688, acc=0.3083333373069763, loss=2.2507247924804688
train: epoch 11, loss 0.9597596526145935, acc=0.6133888959884644, loss=0.9597596526145935
test: epoch 11, loss 1.9122071266174316, acc=0.26944443583488464, loss=1.9122071266174316
train: epoch 12, loss 0.9273332953453064, acc=0.6288889050483704, loss=0.9273332953453064
test: epoch 12, loss 1.970473289489746, acc=0.30000001192092896, loss=1.970473289489746
train: epoch 13, loss 0.8820974826812744, acc=0.6486111283302307, loss=0.8820974826812744
test: epoch 13, loss 2.317448854446411, acc=0.29722222685813904, loss=2.317448854446411
train: epoch 14, loss 0.8753045797348022, acc=0.6537222266197205, loss=0.8753045797348022
test: epoch 14, loss 1.6939704418182373, acc=0.39722222089767456, loss=1.6939704418182373
train: epoch 15, loss 0.8485614061355591, acc=0.6608333587646484, loss=0.8485614061355591
test: epoch 15, loss 1.827866792678833, acc=0.41111111640930176, loss=1.827866792678833
train: epoch 16, loss 0.816790759563446, acc=0.6756666898727417, loss=0.816790759563446
test: epoch 16, loss 1.87235689163208, acc=0.3305555582046509, loss=1.87235689163208
train: epoch 17, loss 0.7952910661697388, acc=0.6822777986526489, loss=0.7952910661697388
test: epoch 17, loss 1.5608628988265991, acc=0.4194444417953491, loss=1.5608628988265991
train: epoch 18, loss 0.7949541211128235, acc=0.683722198009491, loss=0.7949541211128235
test: epoch 18, loss 1.3878979682922363, acc=0.4305555522441864, loss=1.3878979682922363
train: epoch 19, loss 0.7337929606437683, acc=0.7003889083862305, loss=0.7337929606437683
test: epoch 19, loss 1.7013646364212036, acc=0.38055557012557983, loss=1.7013646364212036
train: epoch 20, loss 0.7632640600204468, acc=0.6919999718666077, loss=0.7632640600204468
test: epoch 20, loss 2.1589882373809814, acc=0.30000001192092896, loss=2.1589882373809814
train: epoch 21, loss 0.7381547093391418, acc=0.7057222127914429, loss=0.7381547093391418
test: epoch 21, loss 1.7407517433166504, acc=0.35277777910232544, loss=1.7407517433166504
train: epoch 22, loss 0.7009075880050659, acc=0.7178888916969299, loss=0.7009075880050659
test: epoch 22, loss 1.6671744585037231, acc=0.43611112236976624, loss=1.6671744585037231
train: epoch 23, loss 0.7067534327507019, acc=0.7166110873222351, loss=0.7067534327507019
test: epoch 23, loss 1.4228923320770264, acc=0.4444444477558136, loss=1.4228923320770264
train: epoch 24, loss 0.6994168758392334, acc=0.7191666960716248, loss=0.6994168758392334
test: epoch 24, loss 1.5349220037460327, acc=0.4138889014720917, loss=1.5349220037460327
train: epoch 25, loss 0.693422794342041, acc=0.7191666960716248, loss=0.693422794342041
test: epoch 25, loss 1.5960097312927246, acc=0.41111111640930176, loss=1.5960097312927246
train: epoch 26, loss 0.6846285462379456, acc=0.7293333411216736, loss=0.6846285462379456
test: epoch 26, loss 1.515723705291748, acc=0.4166666567325592, loss=1.515723705291748
train: epoch 27, loss 0.6767409443855286, acc=0.7244444489479065, loss=0.6767409443855286
test: epoch 27, loss 1.7539715766906738, acc=0.3333333432674408, loss=1.7539715766906738
train: epoch 28, loss 0.6602709293365479, acc=0.733222246170044, loss=0.6602709293365479
test: epoch 28, loss 1.546032190322876, acc=0.45277777314186096, loss=1.546032190322876
train: epoch 29, loss 0.6434848308563232, acc=0.7422778010368347, loss=0.6434848308563232
test: epoch 29, loss 1.6968146562576294, acc=0.4194444417953491, loss=1.6968146562576294
train: epoch 30, loss 0.6378611922264099, acc=0.7374444603919983, loss=0.6378611922264099
test: epoch 30, loss 1.6584243774414062, acc=0.3583333194255829, loss=1.6584243774414062
train: epoch 31, loss 0.6314299702644348, acc=0.7435555458068848, loss=0.6314299702644348
test: epoch 31, loss 1.577796459197998, acc=0.3777777850627899, loss=1.577796459197998
train: epoch 32, loss 0.6386327147483826, acc=0.7444444298744202, loss=0.6386327147483826
test: epoch 32, loss 1.3250612020492554, acc=0.4444444477558136, loss=1.3250612020492554
train: epoch 33, loss 0.6276475191116333, acc=0.7438889145851135, loss=0.6276475191116333
test: epoch 33, loss 1.2886959314346313, acc=0.5111111402511597, loss=1.2886959314346313
train: epoch 34, loss 0.6151637434959412, acc=0.754277765750885, loss=0.6151637434959412
test: epoch 34, loss 1.5480438470840454, acc=0.4583333432674408, loss=1.5480438470840454
train: epoch 35, loss 0.6113567352294922, acc=0.7490000128746033, loss=0.6113567352294922
test: epoch 35, loss 1.5219781398773193, acc=0.3888888955116272, loss=1.5219781398773193
train: epoch 36, loss 0.6287738680839539, acc=0.745555579662323, loss=0.6287738680839539
test: epoch 36, loss 1.3575522899627686, acc=0.5111111402511597, loss=1.3575522899627686
train: epoch 37, loss 0.5977228879928589, acc=0.7603889107704163, loss=0.5977228879928589
test: epoch 37, loss 1.3433822393417358, acc=0.4333333373069763, loss=1.3433822393417358
train: epoch 38, loss 0.6020188927650452, acc=0.7597222328186035, loss=0.6020188927650452
test: epoch 38, loss 1.5351545810699463, acc=0.4888888895511627, loss=1.5351545810699463
train: epoch 39, loss 0.601408839225769, acc=0.7597777843475342, loss=0.601408839225769
test: epoch 39, loss 1.297173261642456, acc=0.5166666507720947, loss=1.297173261642456
train: epoch 40, loss 0.5906099081039429, acc=0.7632777690887451, loss=0.5906099081039429
test: epoch 40, loss 1.1404149532318115, acc=0.5138888955116272, loss=1.1404149532318115
train: epoch 41, loss 0.5819337964057922, acc=0.7705555558204651, loss=0.5819337964057922
test: epoch 41, loss 1.1780319213867188, acc=0.49166667461395264, loss=1.1780319213867188
train: epoch 42, loss 0.5887058973312378, acc=0.761388897895813, loss=0.5887058973312378
test: epoch 42, loss 1.2616233825683594, acc=0.46666666865348816, loss=1.2616233825683594
train: epoch 43, loss 0.6003143787384033, acc=0.7627778053283691, loss=0.6003143787384033
test: epoch 43, loss 1.3464782238006592, acc=0.4749999940395355, loss=1.3464782238006592
train: epoch 44, loss 0.5791405439376831, acc=0.7672777771949768, loss=0.5791405439376831
test: epoch 44, loss 0.9810452461242676, acc=0.605555534362793, loss=0.9810452461242676
train: epoch 45, loss 0.5684501528739929, acc=0.7737777829170227, loss=0.5684501528739929
test: epoch 45, loss 1.2812539339065552, acc=0.5, loss=1.2812539339065552
train: epoch 46, loss 0.5539118647575378, acc=0.7796666622161865, loss=0.5539118647575378
test: epoch 46, loss 1.2062373161315918, acc=0.5555555820465088, loss=1.2062373161315918
train: epoch 47, loss 0.5835760235786438, acc=0.7643888592720032, loss=0.5835760235786438
test: epoch 47, loss 1.2555893659591675, acc=0.519444465637207, loss=1.2555893659591675
train: epoch 48, loss 0.559795618057251, acc=0.7743889093399048, loss=0.559795618057251
test: epoch 48, loss 1.1258165836334229, acc=0.5305555462837219, loss=1.1258165836334229
train: epoch 49, loss 0.5440331697463989, acc=0.7826111316680908, loss=0.5440331697463989
test: epoch 49, loss 1.2177268266677856, acc=0.5416666865348816, loss=1.2177268266677856
train: epoch 50, loss 0.5843992233276367, acc=0.7641111016273499, loss=0.5843992233276367
test: epoch 50, loss 1.2226409912109375, acc=0.5833333134651184, loss=1.2226409912109375
train: epoch 51, loss 0.5622797608375549, acc=0.7784444689750671, loss=0.5622797608375549
test: epoch 51, loss 1.2412337064743042, acc=0.5111111402511597, loss=1.2412337064743042
train: epoch 52, loss 0.5651850700378418, acc=0.772777795791626, loss=0.5651850700378418
test: epoch 52, loss 0.815690815448761, acc=0.6222222447395325, loss=0.815690815448761
train: epoch 53, loss 0.5838630795478821, acc=0.7666110992431641, loss=0.5838630795478821
test: epoch 53, loss 1.4023383855819702, acc=0.46388888359069824, loss=1.4023383855819702
train: epoch 54, loss 0.5343817472457886, acc=0.7831666469573975, loss=0.5343817472457886
test: epoch 54, loss 0.9185299277305603, acc=0.5666666626930237, loss=0.9185299277305603
train: epoch 55, loss 0.5650150179862976, acc=0.7718889117240906, loss=0.5650150179862976
test: epoch 55, loss 1.145066738128662, acc=0.5305555462837219, loss=1.145066738128662
train: epoch 56, loss 0.566074788570404, acc=0.7690555453300476, loss=0.566074788570404
test: epoch 56, loss 0.998199462890625, acc=0.5583333373069763, loss=0.998199462890625
train: epoch 57, loss 0.5573256015777588, acc=0.7710555791854858, loss=0.5573256015777588
test: epoch 57, loss 0.9155545830726624, acc=0.6361111402511597, loss=0.9155545830726624
train: epoch 58, loss 0.5541374683380127, acc=0.773722231388092, loss=0.5541374683380127
test: epoch 58, loss 1.0217658281326294, acc=0.5638889074325562, loss=1.0217658281326294
train: epoch 59, loss 0.5596669316291809, acc=0.7777222394943237, loss=0.5596669316291809
test: epoch 59, loss 0.8997589349746704, acc=0.5861111283302307, loss=0.8997589349746704
train: epoch 60, loss 0.5404036045074463, acc=0.781000018119812, loss=0.5404036045074463
test: epoch 60, loss 1.0723297595977783, acc=0.5861111283302307, loss=1.0723297595977783
train: epoch 61, loss 0.5327620506286621, acc=0.7814444303512573, loss=0.5327620506286621
test: epoch 61, loss 0.9092400074005127, acc=0.5916666388511658, loss=0.9092400074005127
train: epoch 62, loss 0.5762960910797119, acc=0.7663333415985107, loss=0.5762960910797119
test: epoch 62, loss 0.8346039056777954, acc=0.6111111044883728, loss=0.8346039056777954
train: epoch 63, loss 0.5378450751304626, acc=0.7813888788223267, loss=0.5378450751304626
test: epoch 63, loss 0.8936035633087158, acc=0.5972222089767456, loss=0.8936035633087158
train: epoch 64, loss 0.5347256064414978, acc=0.7864444255828857, loss=0.5347256064414978
test: epoch 64, loss 0.9208825826644897, acc=0.6166666746139526, loss=0.9208825826644897
train: epoch 65, loss 0.5216990113258362, acc=0.7945555448532104, loss=0.5216990113258362
test: epoch 65, loss 0.8865621089935303, acc=0.5472221970558167, loss=0.8865621089935303
train: epoch 66, loss 0.5440696477890015, acc=0.7818889021873474, loss=0.5440696477890015
test: epoch 66, loss 1.1095157861709595, acc=0.5805555582046509, loss=1.1095157861709595
train: epoch 67, loss 0.5292001962661743, acc=0.7901111245155334, loss=0.5292001962661743
test: epoch 67, loss 0.8120878338813782, acc=0.6111111044883728, loss=0.8120878338813782
train: epoch 68, loss 0.5572359561920166, acc=0.7803888916969299, loss=0.5572359561920166
test: epoch 68, loss 1.0226110219955444, acc=0.6361111402511597, loss=1.0226110219955444
train: epoch 69, loss 0.5434151291847229, acc=0.785611093044281, loss=0.5434151291847229
test: epoch 69, loss 0.9069437980651855, acc=0.6388888955116272, loss=0.9069437980651855
train: epoch 70, loss 0.5302821397781372, acc=0.7939444184303284, loss=0.5302821397781372
test: epoch 70, loss 0.6943828463554382, acc=0.6916666626930237, loss=0.6943828463554382
train: epoch 71, loss 0.5683107376098633, acc=0.773888885974884, loss=0.5683107376098633
test: epoch 71, loss 0.7882197499275208, acc=0.625, loss=0.7882197499275208
train: epoch 72, loss 0.5586018562316895, acc=0.7774999737739563, loss=0.5586018562316895
test: epoch 72, loss 0.8669290542602539, acc=0.6472222208976746, loss=0.8669290542602539
train: epoch 73, loss 0.4938688576221466, acc=0.801277756690979, loss=0.4938688576221466
test: epoch 73, loss 1.0180424451828003, acc=0.6499999761581421, loss=1.0180424451828003
train: epoch 74, loss 0.538624107837677, acc=0.7876666784286499, loss=0.538624107837677
test: epoch 74, loss 0.7697533965110779, acc=0.675000011920929, loss=0.7697533965110779
train: epoch 75, loss 0.5151808261871338, acc=0.7894999980926514, loss=0.5151808261871338
test: epoch 75, loss 0.888031542301178, acc=0.6000000238418579, loss=0.888031542301178
train: epoch 76, loss 0.5268608927726746, acc=0.7885555624961853, loss=0.5268608927726746
test: epoch 76, loss 0.8826575875282288, acc=0.6083333492279053, loss=0.8826575875282288
train: epoch 77, loss 0.4962809383869171, acc=0.8036110997200012, loss=0.4962809383869171
test: epoch 77, loss 0.8148927092552185, acc=0.644444465637207, loss=0.8148927092552185
train: epoch 78, loss 0.5082002878189087, acc=0.7959444522857666, loss=0.5082002878189087
test: epoch 78, loss 0.754630982875824, acc=0.6472222208976746, loss=0.754630982875824
train: epoch 79, loss 0.5438040494918823, acc=0.7816110849380493, loss=0.5438040494918823
test: epoch 79, loss 1.0049405097961426, acc=0.5777778029441833, loss=1.0049405097961426
train: epoch 80, loss 0.4649399220943451, acc=0.8146666884422302, loss=0.4649399220943451
test: epoch 80, loss 0.9576351642608643, acc=0.6111111044883728, loss=0.9576351642608643
train: epoch 81, loss 0.4760703146457672, acc=0.8105000257492065, loss=0.4760703146457672
test: epoch 81, loss 1.10569167137146, acc=0.5944444537162781, loss=1.10569167137146
train: epoch 82, loss 0.5112069845199585, acc=0.7994444370269775, loss=0.5112069845199585
test: epoch 82, loss 0.8636106848716736, acc=0.6222222447395325, loss=0.8636106848716736
train: epoch 83, loss 0.4798565208911896, acc=0.8083333373069763, loss=0.4798565208911896
test: epoch 83, loss 0.9372774362564087, acc=0.6416666507720947, loss=0.9372774362564087
train: epoch 84, loss 0.4743726849555969, acc=0.8131111264228821, loss=0.4743726849555969
test: epoch 84, loss 1.053013801574707, acc=0.5611110925674438, loss=1.053013801574707
train: epoch 85, loss 0.5831586718559265, acc=0.7771111130714417, loss=0.5831586718559265
test: epoch 85, loss 0.8816735148429871, acc=0.5972222089767456, loss=0.8816735148429871
train: epoch 86, loss 0.5407816171646118, acc=0.7917777895927429, loss=0.5407816171646118
test: epoch 86, loss 1.0373494625091553, acc=0.6111111044883728, loss=1.0373494625091553
train: epoch 87, loss 0.5648965239524841, acc=0.7789999842643738, loss=0.5648965239524841
test: epoch 87, loss 0.9031416177749634, acc=0.6222222447395325, loss=0.9031416177749634
train: epoch 88, loss 0.5224974155426025, acc=0.7940555810928345, loss=0.5224974155426025
test: epoch 88, loss 0.8881257176399231, acc=0.625, loss=0.8881257176399231
train: epoch 89, loss 0.4913066625595093, acc=0.8033333420753479, loss=0.4913066625595093
test: epoch 89, loss 0.9238206148147583, acc=0.6361111402511597, loss=0.9238206148147583
train: epoch 90, loss 0.4798685908317566, acc=0.8071666955947876, loss=0.4798685908317566
test: epoch 90, loss 0.8000054359436035, acc=0.6416666507720947, loss=0.8000054359436035
train: epoch 91, loss 0.4953489303588867, acc=0.8006666898727417, loss=0.4953489303588867
test: epoch 91, loss 0.8190292716026306, acc=0.6555555462837219, loss=0.8190292716026306
train: epoch 92, loss 0.4978877902030945, acc=0.8015000224113464, loss=0.4978877902030945
test: epoch 92, loss 0.7942811250686646, acc=0.6416666507720947, loss=0.7942811250686646
train: epoch 93, loss 0.4652585983276367, acc=0.8157777786254883, loss=0.4652585983276367
test: epoch 93, loss 0.9969234466552734, acc=0.5583333373069763, loss=0.9969234466552734
train: epoch 94, loss 0.4415554106235504, acc=0.829277753829956, loss=0.4415554106235504
test: epoch 94, loss 0.7807490825653076, acc=0.675000011920929, loss=0.7807490825653076
train: epoch 95, loss 0.47649502754211426, acc=0.8153889179229736, loss=0.47649502754211426
test: epoch 95, loss 0.7588448524475098, acc=0.6472222208976746, loss=0.7588448524475098
train: epoch 96, loss 0.4689673185348511, acc=0.8161666393280029, loss=0.4689673185348511
test: epoch 96, loss 0.9466345310211182, acc=0.644444465637207, loss=0.9466345310211182
train: epoch 97, loss 0.49364325404167175, acc=0.8074444532394409, loss=0.49364325404167175
test: epoch 97, loss 0.813116192817688, acc=0.6861110925674438, loss=0.813116192817688
train: epoch 98, loss 0.45889294147491455, acc=0.8258888721466064, loss=0.45889294147491455
test: epoch 98, loss 0.851076602935791, acc=0.6694444417953491, loss=0.851076602935791
train: epoch 99, loss 0.5162075757980347, acc=0.8004999756813049, loss=0.5162075757980347
test: epoch 99, loss 1.0496538877487183, acc=0.625, loss=1.0496538877487183
train: epoch 100, loss 0.43007588386535645, acc=0.8311111330986023, loss=0.43007588386535645
test: epoch 100, loss 0.9390611052513123, acc=0.6166666746139526, loss=0.9390611052513123
train: epoch 101, loss 0.46281322836875916, acc=0.815500020980835, loss=0.46281322836875916
test: epoch 101, loss 0.947573721408844, acc=0.5888888835906982, loss=0.947573721408844
train: epoch 102, loss 0.454047828912735, acc=0.812333345413208, loss=0.454047828912735
test: epoch 102, loss 0.7833853363990784, acc=0.6777777671813965, loss=0.7833853363990784
train: epoch 103, loss 0.4938725233078003, acc=0.7986666560173035, loss=0.4938725233078003
test: epoch 103, loss 0.9111770987510681, acc=0.6138888597488403, loss=0.9111770987510681
train: epoch 104, loss 0.4591474235057831, acc=0.8157222270965576, loss=0.4591474235057831
test: epoch 104, loss 0.9420675039291382, acc=0.6083333492279053, loss=0.9420675039291382
train: epoch 105, loss 0.4474790692329407, acc=0.820722222328186, loss=0.4474790692329407
test: epoch 105, loss 0.8169260621070862, acc=0.6472222208976746, loss=0.8169260621070862
train: epoch 106, loss 0.4919756352901459, acc=0.8096110820770264, loss=0.4919756352901459
test: epoch 106, loss 0.9032192826271057, acc=0.6194444298744202, loss=0.9032192826271057
train: epoch 107, loss 0.44310104846954346, acc=0.8268888592720032, loss=0.44310104846954346
test: epoch 107, loss 0.7696295976638794, acc=0.6861110925674438, loss=0.7696295976638794
train: epoch 108, loss 0.48172685503959656, acc=0.8091111183166504, loss=0.48172685503959656
test: epoch 108, loss 1.1598516702651978, acc=0.5333333611488342, loss=1.1598516702651978
train: epoch 109, loss 0.44915229082107544, acc=0.8237777948379517, loss=0.44915229082107544
test: epoch 109, loss 0.9825667142868042, acc=0.6833333373069763, loss=0.9825667142868042
train: epoch 110, loss 0.4336230158805847, acc=0.8288888931274414, loss=0.4336230158805847
test: epoch 110, loss 0.675399899482727, acc=0.6944444179534912, loss=0.675399899482727
train: epoch 111, loss 0.4537964165210724, acc=0.824833333492279, loss=0.4537964165210724
test: epoch 111, loss 0.9721700549125671, acc=0.6000000238418579, loss=0.9721700549125671
train: epoch 112, loss 0.4814525842666626, acc=0.8170555830001831, loss=0.4814525842666626
test: epoch 112, loss 0.773490846157074, acc=0.6972222328186035, loss=0.773490846157074
train: epoch 113, loss 0.46193742752075195, acc=0.8157777786254883, loss=0.46193742752075195
test: epoch 113, loss 0.6898228526115417, acc=0.675000011920929, loss=0.6898228526115417
train: epoch 114, loss 0.4309590458869934, acc=0.82833331823349, loss=0.4309590458869934
test: epoch 114, loss 0.7526324987411499, acc=0.7222222089767456, loss=0.7526324987411499
train: epoch 115, loss 0.4609706699848175, acc=0.8183888792991638, loss=0.4609706699848175
test: epoch 115, loss 0.7440767288208008, acc=0.6833333373069763, loss=0.7440767288208008
train: epoch 116, loss 0.42100924253463745, acc=0.8317221999168396, loss=0.42100924253463745
test: epoch 116, loss 0.8193914890289307, acc=0.6666666865348816, loss=0.8193914890289307
train: epoch 117, loss 0.4641495943069458, acc=0.8148888945579529, loss=0.4641495943069458
test: epoch 117, loss 0.7830843925476074, acc=0.6944444179534912, loss=0.7830843925476074
train: epoch 118, loss 0.4148855209350586, acc=0.8302778005599976, loss=0.4148855209350586
test: epoch 118, loss 0.6900373101234436, acc=0.6972222328186035, loss=0.6900373101234436
train: epoch 119, loss 0.42340871691703796, acc=0.8350555300712585, loss=0.42340871691703796
test: epoch 119, loss 0.7616785168647766, acc=0.6972222328186035, loss=0.7616785168647766
train: epoch 120, loss 0.4535519778728485, acc=0.8213333487510681, loss=0.4535519778728485
test: epoch 120, loss 0.7756055593490601, acc=0.699999988079071, loss=0.7756055593490601
train: epoch 121, loss 0.41288840770721436, acc=0.8419444561004639, loss=0.41288840770721436
test: epoch 121, loss 0.8054730892181396, acc=0.6972222328186035, loss=0.8054730892181396
train: epoch 122, loss 0.5182437896728516, acc=0.8026111125946045, loss=0.5182437896728516
test: epoch 122, loss 1.6229852437973022, acc=0.574999988079071, loss=1.6229852437973022
train: epoch 123, loss 0.4366190731525421, acc=0.8289444446563721, loss=0.4366190731525421
test: epoch 123, loss 1.068366289138794, acc=0.5722222328186035, loss=1.068366289138794
train: epoch 124, loss 0.4819202721118927, acc=0.7953333258628845, loss=0.4819202721118927
test: epoch 124, loss 1.3203420639038086, acc=0.574999988079071, loss=1.3203420639038086
train: epoch 125, loss 0.49242082238197327, acc=0.80522221326828, loss=0.49242082238197327
test: epoch 125, loss 1.235355257987976, acc=0.625, loss=1.235355257987976
train: epoch 126, loss 0.5370957851409912, acc=0.7883333563804626, loss=0.5370957851409912
test: epoch 126, loss 0.7835296988487244, acc=0.6333333253860474, loss=0.7835296988487244
train: epoch 127, loss 0.4384186565876007, acc=0.8327777981758118, loss=0.4384186565876007
test: epoch 127, loss 0.7879965901374817, acc=0.6638888716697693, loss=0.7879965901374817
train: epoch 128, loss 0.3982735574245453, acc=0.8428888916969299, loss=0.3982735574245453
test: epoch 128, loss 0.9233191609382629, acc=0.6694444417953491, loss=0.9233191609382629
train: epoch 129, loss 0.455595463514328, acc=0.8233888745307922, loss=0.455595463514328
test: epoch 129, loss 0.8084977865219116, acc=0.6722221970558167, loss=0.8084977865219116
train: epoch 130, loss 0.4233033061027527, acc=0.8356666564941406, loss=0.4233033061027527
test: epoch 130, loss 0.7498051524162292, acc=0.6888889074325562, loss=0.7498051524162292
train: epoch 131, loss 0.4351539611816406, acc=0.8307222127914429, loss=0.4351539611816406
test: epoch 131, loss 0.6901340484619141, acc=0.7111111283302307, loss=0.6901340484619141
train: epoch 132, loss 0.42928099632263184, acc=0.831944465637207, loss=0.42928099632263184
test: epoch 132, loss 0.6696869730949402, acc=0.7333333492279053, loss=0.6696869730949402
train: epoch 133, loss 0.41303592920303345, acc=0.83561110496521, loss=0.41303592920303345
test: epoch 133, loss 0.7335166335105896, acc=0.6833333373069763, loss=0.7335166335105896
train: epoch 134, loss 0.453372061252594, acc=0.8222777843475342, loss=0.453372061252594
test: epoch 134, loss 0.7980368733406067, acc=0.644444465637207, loss=0.7980368733406067
train: epoch 135, loss 0.4606473445892334, acc=0.82105553150177, loss=0.4606473445892334
test: epoch 135, loss 0.7083399891853333, acc=0.6916666626930237, loss=0.7083399891853333
train: epoch 136, loss 0.4132726490497589, acc=0.8338888883590698, loss=0.4132726490497589
test: epoch 136, loss 0.7051599621772766, acc=0.7083333134651184, loss=0.7051599621772766
train: epoch 137, loss 0.48567822575569153, acc=0.8145555257797241, loss=0.48567822575569153
test: epoch 137, loss 1.028831124305725, acc=0.6194444298744202, loss=1.028831124305725
train: epoch 138, loss 0.4541126489639282, acc=0.8223888874053955, loss=0.4541126489639282
test: epoch 138, loss 0.6662114858627319, acc=0.7027778029441833, loss=0.6662114858627319
train: epoch 139, loss 0.42387205362319946, acc=0.8313888907432556, loss=0.42387205362319946
test: epoch 139, loss 0.6859786510467529, acc=0.699999988079071, loss=0.6859786510467529
train: epoch 140, loss 0.4526364207267761, acc=0.8101111054420471, loss=0.4526364207267761
test: epoch 140, loss 0.693364679813385, acc=0.6944444179534912, loss=0.693364679813385
train: epoch 141, loss 0.485467791557312, acc=0.7983888983726501, loss=0.485467791557312
test: epoch 141, loss 0.6289512515068054, acc=0.7388888597488403, loss=0.6289512515068054
train: epoch 142, loss 0.5206855535507202, acc=0.7972777485847473, loss=0.5206855535507202
test: epoch 142, loss 0.8582929372787476, acc=0.6499999761581421, loss=0.8582929372787476
train: epoch 143, loss 0.40660360455513, acc=0.8402777910232544, loss=0.40660360455513
test: epoch 143, loss 0.8138344883918762, acc=0.6805555820465088, loss=0.8138344883918762
train: epoch 144, loss 0.45608922839164734, acc=0.8192222118377686, loss=0.45608922839164734
test: epoch 144, loss 0.7469533681869507, acc=0.6861110925674438, loss=0.7469533681869507
train: epoch 145, loss 0.4727552533149719, acc=0.8172222375869751, loss=0.4727552533149719
test: epoch 145, loss 0.8915770053863525, acc=0.625, loss=0.8915770053863525
train: epoch 146, loss 0.49333757162094116, acc=0.8077222108840942, loss=0.49333757162094116
test: epoch 146, loss 0.8851827383041382, acc=0.6555555462837219, loss=0.8851827383041382
train: epoch 147, loss 0.4971790015697479, acc=0.8092222213745117, loss=0.4971790015697479
test: epoch 147, loss 0.9009343385696411, acc=0.6416666507720947, loss=0.9009343385696411
train: epoch 148, loss 0.4883350729942322, acc=0.8073333501815796, loss=0.4883350729942322
test: epoch 148, loss 0.8234483599662781, acc=0.6777777671813965, loss=0.8234483599662781
train: epoch 149, loss 0.5541538596153259, acc=0.7868888974189758, loss=0.5541538596153259
test: epoch 149, loss 0.8435487151145935, acc=0.6777777671813965, loss=0.8435487151145935
train: epoch 150, loss 0.4873902201652527, acc=0.8074444532394409, loss=0.4873902201652527
test: epoch 150, loss 1.1595464944839478, acc=0.6027777791023254, loss=1.1595464944839478
