# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=100", "--receiver_embed_dim=64", "--temperature=1", "--temp_decay=0.99", "--one_hot=false", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_summands=2, n_symbols=100, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=213679550, receiver_embed_dim=64, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.1023709774017334, acc=0.07527777552604675, loss=3.1023709774017334
test: epoch 1, loss 2.7274980545043945, acc=0.0972222238779068, loss=2.7274980545043945
train: epoch 2, loss 2.151066780090332, acc=0.21449999511241913, loss=2.151066780090332
test: epoch 2, loss 2.320863962173462, acc=0.1666666716337204, loss=2.320863962173462
train: epoch 3, loss 1.6334172487258911, acc=0.3485555648803711, loss=1.6334172487258911
test: epoch 3, loss 2.0951640605926514, acc=0.23888888955116272, loss=2.0951640605926514
train: epoch 4, loss 1.376421570777893, acc=0.425944447517395, loss=1.376421570777893
test: epoch 4, loss 1.857062816619873, acc=0.2638888955116272, loss=1.857062816619873
train: epoch 5, loss 1.2374879121780396, acc=0.4821110963821411, loss=1.2374879121780396
test: epoch 5, loss 1.8122453689575195, acc=0.2611111104488373, loss=1.8122453689575195
train: epoch 6, loss 1.1397285461425781, acc=0.515500009059906, loss=1.1397285461425781
test: epoch 6, loss 1.6757093667984009, acc=0.26944443583488464, loss=1.6757093667984009
train: epoch 7, loss 1.0643450021743774, acc=0.5412222146987915, loss=1.0643450021743774
test: epoch 7, loss 1.835587739944458, acc=0.2888889014720917, loss=1.835587739944458
train: epoch 8, loss 1.0151759386062622, acc=0.5558888912200928, loss=1.0151759386062622
test: epoch 8, loss 1.6850420236587524, acc=0.3333333432674408, loss=1.6850420236587524
train: epoch 9, loss 0.955660879611969, acc=0.5817777514457703, loss=0.955660879611969
test: epoch 9, loss 1.6872409582138062, acc=0.3333333432674408, loss=1.6872409582138062
train: epoch 10, loss 0.9537773132324219, acc=0.5756666660308838, loss=0.9537773132324219
test: epoch 10, loss 1.6969281435012817, acc=0.3638888895511627, loss=1.6969281435012817
train: epoch 11, loss 0.915273904800415, acc=0.5922777652740479, loss=0.915273904800415
test: epoch 11, loss 1.712355613708496, acc=0.3722222149372101, loss=1.712355613708496
train: epoch 12, loss 0.8964940905570984, acc=0.5937777757644653, loss=0.8964940905570984
test: epoch 12, loss 1.6765574216842651, acc=0.36944442987442017, loss=1.6765574216842651
train: epoch 13, loss 0.8757686614990234, acc=0.6040555834770203, loss=0.8757686614990234
test: epoch 13, loss 1.6448945999145508, acc=0.35277777910232544, loss=1.6448945999145508
train: epoch 14, loss 0.8463006019592285, acc=0.6165000200271606, loss=0.8463006019592285
test: epoch 14, loss 1.5954151153564453, acc=0.3499999940395355, loss=1.5954151153564453
train: epoch 15, loss 0.8220133781433105, acc=0.6366111040115356, loss=0.8220133781433105
test: epoch 15, loss 1.534865379333496, acc=0.39722222089767456, loss=1.534865379333496
train: epoch 16, loss 0.7955305576324463, acc=0.6412222385406494, loss=0.7955305576324463
test: epoch 16, loss 1.5454868078231812, acc=0.38055557012557983, loss=1.5454868078231812
train: epoch 17, loss 0.769235372543335, acc=0.6493889093399048, loss=0.769235372543335
test: epoch 17, loss 1.5938928127288818, acc=0.41111111640930176, loss=1.5938928127288818
train: epoch 18, loss 0.7495363354682922, acc=0.6603333353996277, loss=0.7495363354682922
test: epoch 18, loss 1.5904244184494019, acc=0.36666667461395264, loss=1.5904244184494019
train: epoch 19, loss 0.7586520314216614, acc=0.6529444456100464, loss=0.7586520314216614
test: epoch 19, loss 1.6335629224777222, acc=0.39722222089767456, loss=1.6335629224777222
train: epoch 20, loss 0.7382614016532898, acc=0.6605555415153503, loss=0.7382614016532898
test: epoch 20, loss 1.4474775791168213, acc=0.4416666626930237, loss=1.4474775791168213
train: epoch 21, loss 0.7263176441192627, acc=0.6615555286407471, loss=0.7263176441192627
test: epoch 21, loss 1.6286075115203857, acc=0.4305555522441864, loss=1.6286075115203857
train: epoch 22, loss 0.7332113981246948, acc=0.6588888764381409, loss=0.7332113981246948
test: epoch 22, loss 1.508231282234192, acc=0.43888887763023376, loss=1.508231282234192
train: epoch 23, loss 0.7116018533706665, acc=0.6652777791023254, loss=0.7116018533706665
test: epoch 23, loss 1.4927290678024292, acc=0.42500001192092896, loss=1.4927290678024292
train: epoch 24, loss 0.704157829284668, acc=0.6708889007568359, loss=0.704157829284668
test: epoch 24, loss 1.5212258100509644, acc=0.43611112236976624, loss=1.5212258100509644
train: epoch 25, loss 0.6974104046821594, acc=0.6788889169692993, loss=0.6974104046821594
test: epoch 25, loss 1.3768529891967773, acc=0.43888887763023376, loss=1.3768529891967773
train: epoch 26, loss 0.6654152274131775, acc=0.6867777705192566, loss=0.6654152274131775
test: epoch 26, loss 1.5900977849960327, acc=0.4166666567325592, loss=1.5900977849960327
train: epoch 27, loss 0.6738777160644531, acc=0.6826666593551636, loss=0.6738777160644531
test: epoch 27, loss 1.6272413730621338, acc=0.4333333373069763, loss=1.6272413730621338
train: epoch 28, loss 0.6631035208702087, acc=0.695388913154602, loss=0.6631035208702087
test: epoch 28, loss 1.442805290222168, acc=0.46666666865348816, loss=1.442805290222168
train: epoch 29, loss 0.6675302982330322, acc=0.6980000138282776, loss=0.6675302982330322
test: epoch 29, loss 1.4172085523605347, acc=0.4472222328186035, loss=1.4172085523605347
train: epoch 30, loss 0.6523382067680359, acc=0.7024999856948853, loss=0.6523382067680359
test: epoch 30, loss 1.5366777181625366, acc=0.4444444477558136, loss=1.5366777181625366
train: epoch 31, loss 0.6404370069503784, acc=0.7078333497047424, loss=0.6404370069503784
test: epoch 31, loss 1.3931527137756348, acc=0.4305555522441864, loss=1.3931527137756348
train: epoch 32, loss 0.6366569995880127, acc=0.7055000066757202, loss=0.6366569995880127
test: epoch 32, loss 1.3669489622116089, acc=0.4444444477558136, loss=1.3669489622116089
train: epoch 33, loss 0.6337065696716309, acc=0.7077222466468811, loss=0.6337065696716309
test: epoch 33, loss 1.6500678062438965, acc=0.46388888359069824, loss=1.6500678062438965
train: epoch 34, loss 0.620124101638794, acc=0.7101666927337646, loss=0.620124101638794
test: epoch 34, loss 1.4240167140960693, acc=0.4472222328186035, loss=1.4240167140960693
train: epoch 35, loss 0.6091799139976501, acc=0.7138333320617676, loss=0.6091799139976501
test: epoch 35, loss 1.534828782081604, acc=0.46388888359069824, loss=1.534828782081604
train: epoch 36, loss 0.6051326990127563, acc=0.7181666493415833, loss=0.6051326990127563
test: epoch 36, loss 1.5192935466766357, acc=0.4861111044883728, loss=1.5192935466766357
train: epoch 37, loss 0.6070907711982727, acc=0.715499997138977, loss=0.6070907711982727
test: epoch 37, loss 1.4870884418487549, acc=0.4722222089767456, loss=1.4870884418487549
train: epoch 38, loss 0.6036739349365234, acc=0.7157777547836304, loss=0.6036739349365234
test: epoch 38, loss 1.4704958200454712, acc=0.47777777910232544, loss=1.4704958200454712
train: epoch 39, loss 0.6060861945152283, acc=0.7152222394943237, loss=0.6060861945152283
test: epoch 39, loss 1.4575012922286987, acc=0.47777777910232544, loss=1.4575012922286987
train: epoch 40, loss 0.5872081518173218, acc=0.7265555262565613, loss=0.5872081518173218
test: epoch 40, loss 1.2341773509979248, acc=0.47777777910232544, loss=1.2341773509979248
train: epoch 41, loss 0.5854827761650085, acc=0.7269444465637207, loss=0.5854827761650085
test: epoch 41, loss 1.6347707509994507, acc=0.43611112236976624, loss=1.6347707509994507
train: epoch 42, loss 0.5728648900985718, acc=0.7318333387374878, loss=0.5728648900985718
test: epoch 42, loss 1.5263381004333496, acc=0.46388888359069824, loss=1.5263381004333496
train: epoch 43, loss 0.5757709741592407, acc=0.7322777509689331, loss=0.5757709741592407
test: epoch 43, loss 1.4131718873977661, acc=0.4833333194255829, loss=1.4131718873977661
train: epoch 44, loss 0.5913466811180115, acc=0.7231666445732117, loss=0.5913466811180115
test: epoch 44, loss 1.5504599809646606, acc=0.48055556416511536, loss=1.5504599809646606
train: epoch 45, loss 0.5946000218391418, acc=0.7196111083030701, loss=0.5946000218391418
test: epoch 45, loss 1.3443633317947388, acc=0.48055556416511536, loss=1.3443633317947388
train: epoch 46, loss 0.566510021686554, acc=0.7345555424690247, loss=0.566510021686554
test: epoch 46, loss 1.387811541557312, acc=0.4722222089767456, loss=1.387811541557312
train: epoch 47, loss 0.5639411211013794, acc=0.7318333387374878, loss=0.5639411211013794
test: epoch 47, loss 1.5181807279586792, acc=0.48055556416511536, loss=1.5181807279586792
train: epoch 48, loss 0.5912415981292725, acc=0.7246111035346985, loss=0.5912415981292725
test: epoch 48, loss 1.3739655017852783, acc=0.48055556416511536, loss=1.3739655017852783
train: epoch 49, loss 0.5711938738822937, acc=0.7278888821601868, loss=0.5711938738822937
test: epoch 49, loss 1.4311847686767578, acc=0.47777777910232544, loss=1.4311847686767578
train: epoch 50, loss 0.5715133547782898, acc=0.7286666631698608, loss=0.5715133547782898
test: epoch 50, loss 1.519690990447998, acc=0.4722222089767456, loss=1.519690990447998
train: epoch 51, loss 0.5716652274131775, acc=0.7288333177566528, loss=0.5716652274131775
test: epoch 51, loss 1.480871319770813, acc=0.4749999940395355, loss=1.480871319770813
train: epoch 52, loss 0.577488899230957, acc=0.7279999852180481, loss=0.577488899230957
test: epoch 52, loss 1.5474873781204224, acc=0.4722222089767456, loss=1.5474873781204224
train: epoch 53, loss 0.5602397322654724, acc=0.7353888750076294, loss=0.5602397322654724
test: epoch 53, loss 1.2608264684677124, acc=0.48055556416511536, loss=1.2608264684677124
train: epoch 54, loss 0.5651453137397766, acc=0.7331666946411133, loss=0.5651453137397766
test: epoch 54, loss 1.3731911182403564, acc=0.4722222089767456, loss=1.3731911182403564
train: epoch 55, loss 0.5415068864822388, acc=0.738777756690979, loss=0.5415068864822388
test: epoch 55, loss 1.4659568071365356, acc=0.4749999940395355, loss=1.4659568071365356
train: epoch 56, loss 0.5441552400588989, acc=0.7397778034210205, loss=0.5441552400588989
test: epoch 56, loss 1.548414945602417, acc=0.4611110985279083, loss=1.548414945602417
train: epoch 57, loss 0.5531618595123291, acc=0.7358888983726501, loss=0.5531618595123291
test: epoch 57, loss 1.4139484167099, acc=0.48055556416511536, loss=1.4139484167099
train: epoch 58, loss 0.548996090888977, acc=0.7389444708824158, loss=0.548996090888977
test: epoch 58, loss 1.5609707832336426, acc=0.48055556416511536, loss=1.5609707832336426
train: epoch 59, loss 0.5661979913711548, acc=0.7352777719497681, loss=0.5661979913711548
test: epoch 59, loss 1.3641068935394287, acc=0.46388888359069824, loss=1.3641068935394287
train: epoch 60, loss 0.5442565679550171, acc=0.7404444217681885, loss=0.5442565679550171
test: epoch 60, loss 1.4348102807998657, acc=0.49444442987442017, loss=1.4348102807998657
train: epoch 61, loss 0.5380604267120361, acc=0.7468888759613037, loss=0.5380604267120361
test: epoch 61, loss 1.3768906593322754, acc=0.49444442987442017, loss=1.3768906593322754
train: epoch 62, loss 0.540317714214325, acc=0.7379999756813049, loss=0.540317714214325
test: epoch 62, loss 1.4328824281692505, acc=0.49444442987442017, loss=1.4328824281692505
train: epoch 63, loss 0.5305845141410828, acc=0.7512221932411194, loss=0.5305845141410828
test: epoch 63, loss 1.4568188190460205, acc=0.49444442987442017, loss=1.4568188190460205
train: epoch 64, loss 0.5489088296890259, acc=0.7408888936042786, loss=0.5489088296890259
test: epoch 64, loss 1.4458621740341187, acc=0.49444442987442017, loss=1.4458621740341187
train: epoch 65, loss 0.5401557683944702, acc=0.7434444427490234, loss=0.5401557683944702
test: epoch 65, loss 1.440899133682251, acc=0.4972222149372101, loss=1.440899133682251
train: epoch 66, loss 0.5306405425071716, acc=0.7465555667877197, loss=0.5306405425071716
test: epoch 66, loss 1.4145797491073608, acc=0.4972222149372101, loss=1.4145797491073608
train: epoch 67, loss 0.5176968574523926, acc=0.7548333406448364, loss=0.5176968574523926
test: epoch 67, loss 1.540952205657959, acc=0.4749999940395355, loss=1.540952205657959
train: epoch 68, loss 0.5493174195289612, acc=0.7406111359596252, loss=0.5493174195289612
test: epoch 68, loss 1.3641204833984375, acc=0.4972222149372101, loss=1.3641204833984375
train: epoch 69, loss 0.5285131931304932, acc=0.746055543422699, loss=0.5285131931304932
test: epoch 69, loss 1.3082431554794312, acc=0.4972222149372101, loss=1.3082431554794312
train: epoch 70, loss 0.524124026298523, acc=0.7474444508552551, loss=0.524124026298523
test: epoch 70, loss 1.535910964012146, acc=0.5, loss=1.535910964012146
train: epoch 71, loss 0.5231037735939026, acc=0.7472222447395325, loss=0.5231037735939026
test: epoch 71, loss 1.421943187713623, acc=0.4888888895511627, loss=1.421943187713623
train: epoch 72, loss 0.5181096196174622, acc=0.7480555772781372, loss=0.5181096196174622
test: epoch 72, loss 1.3762497901916504, acc=0.5, loss=1.3762497901916504
train: epoch 73, loss 0.5333265662193298, acc=0.7451111078262329, loss=0.5333265662193298
test: epoch 73, loss 1.344728946685791, acc=0.4972222149372101, loss=1.344728946685791
train: epoch 74, loss 0.5278254747390747, acc=0.7474444508552551, loss=0.5278254747390747
test: epoch 74, loss 1.2711204290390015, acc=0.5, loss=1.2711204290390015
train: epoch 75, loss 0.5139071941375732, acc=0.7478888630867004, loss=0.5139071941375732
test: epoch 75, loss 1.3937581777572632, acc=0.49444442987442017, loss=1.3937581777572632
train: epoch 76, loss 0.5010512471199036, acc=0.7521111369132996, loss=0.5010512471199036
test: epoch 76, loss 1.3539502620697021, acc=0.4972222149372101, loss=1.3539502620697021
train: epoch 77, loss 0.5160797238349915, acc=0.7493333220481873, loss=0.5160797238349915
test: epoch 77, loss 1.2844160795211792, acc=0.49444442987442017, loss=1.2844160795211792
train: epoch 78, loss 0.5128790140151978, acc=0.749833345413208, loss=0.5128790140151978
test: epoch 78, loss 1.546080231666565, acc=0.4861111044883728, loss=1.546080231666565
train: epoch 79, loss 0.5141389966011047, acc=0.7522222399711609, loss=0.5141389966011047
test: epoch 79, loss 1.3578542470932007, acc=0.4972222149372101, loss=1.3578542470932007
train: epoch 80, loss 0.49883103370666504, acc=0.7595555782318115, loss=0.49883103370666504
test: epoch 80, loss 1.2646695375442505, acc=0.5, loss=1.2646695375442505
train: epoch 81, loss 0.506718099117279, acc=0.7568888664245605, loss=0.506718099117279
test: epoch 81, loss 1.4422848224639893, acc=0.5, loss=1.4422848224639893
train: epoch 82, loss 0.5219259262084961, acc=0.7456111311912537, loss=0.5219259262084961
test: epoch 82, loss 1.397372841835022, acc=0.5027777552604675, loss=1.397372841835022
train: epoch 83, loss 0.5045489072799683, acc=0.7518333196640015, loss=0.5045489072799683
test: epoch 83, loss 1.3215399980545044, acc=0.5, loss=1.3215399980545044
train: epoch 84, loss 0.513475775718689, acc=0.753777801990509, loss=0.513475775718689
test: epoch 84, loss 1.2733373641967773, acc=0.49444442987442017, loss=1.2733373641967773
train: epoch 85, loss 0.4964161217212677, acc=0.7556666731834412, loss=0.4964161217212677
test: epoch 85, loss 1.6045804023742676, acc=0.5, loss=1.6045804023742676
train: epoch 86, loss 0.49996331334114075, acc=0.7560555338859558, loss=0.49996331334114075
test: epoch 86, loss 1.4025996923446655, acc=0.5055555701255798, loss=1.4025996923446655
train: epoch 87, loss 0.48962050676345825, acc=0.7613333463668823, loss=0.48962050676345825
test: epoch 87, loss 1.4953761100769043, acc=0.5, loss=1.4953761100769043
train: epoch 88, loss 0.4987249970436096, acc=0.761555552482605, loss=0.4987249970436096
test: epoch 88, loss 1.4110426902770996, acc=0.5, loss=1.4110426902770996
train: epoch 89, loss 0.5201777219772339, acc=0.75, loss=0.5201777219772339
test: epoch 89, loss 1.5014899969100952, acc=0.4749999940395355, loss=1.5014899969100952
train: epoch 90, loss 0.4989023506641388, acc=0.758055567741394, loss=0.4989023506641388
test: epoch 90, loss 1.3943371772766113, acc=0.5, loss=1.3943371772766113
train: epoch 91, loss 0.4931710660457611, acc=0.754111111164093, loss=0.4931710660457611
test: epoch 91, loss 1.6280666589736938, acc=0.46666666865348816, loss=1.6280666589736938
train: epoch 92, loss 0.4974331259727478, acc=0.7570555806159973, loss=0.4974331259727478
test: epoch 92, loss 1.3206104040145874, acc=0.5, loss=1.3206104040145874
train: epoch 93, loss 0.49835124611854553, acc=0.758388876914978, loss=0.49835124611854553
test: epoch 93, loss 1.4435503482818604, acc=0.4972222149372101, loss=1.4435503482818604
train: epoch 94, loss 0.5038109421730042, acc=0.7586110830307007, loss=0.5038109421730042
test: epoch 94, loss 1.3867846727371216, acc=0.4972222149372101, loss=1.3867846727371216
train: epoch 95, loss 0.4995592534542084, acc=0.7561110854148865, loss=0.4995592534542084
test: epoch 95, loss 1.526802897453308, acc=0.49166667461395264, loss=1.526802897453308
train: epoch 96, loss 0.4781448245048523, acc=0.7619444727897644, loss=0.4781448245048523
test: epoch 96, loss 1.6084961891174316, acc=0.48055556416511536, loss=1.6084961891174316
train: epoch 97, loss 0.4791366755962372, acc=0.7609444260597229, loss=0.4791366755962372
test: epoch 97, loss 1.5733988285064697, acc=0.5, loss=1.5733988285064697
train: epoch 98, loss 0.5099719166755676, acc=0.753166675567627, loss=0.5099719166755676
test: epoch 98, loss 1.4295367002487183, acc=0.4972222149372101, loss=1.4295367002487183
train: epoch 99, loss 0.49645334482192993, acc=0.7572222352027893, loss=0.49645334482192993
test: epoch 99, loss 1.332987666130066, acc=0.5, loss=1.332987666130066
train: epoch 100, loss 0.4894979000091553, acc=0.7586666941642761, loss=0.4894979000091553
test: epoch 100, loss 1.31157386302948, acc=0.4972222149372101, loss=1.31157386302948
train: epoch 101, loss 0.47795072197914124, acc=0.7620000243186951, loss=0.47795072197914124
test: epoch 101, loss 1.4491078853607178, acc=0.5055555701255798, loss=1.4491078853607178
train: epoch 102, loss 0.4943118095397949, acc=0.757444441318512, loss=0.4943118095397949
test: epoch 102, loss 1.5020915269851685, acc=0.4972222149372101, loss=1.5020915269851685
train: epoch 103, loss 0.48664990067481995, acc=0.7617777585983276, loss=0.48664990067481995
test: epoch 103, loss 1.5416603088378906, acc=0.49166667461395264, loss=1.5416603088378906
train: epoch 104, loss 0.48390284180641174, acc=0.7633333206176758, loss=0.48390284180641174
test: epoch 104, loss 1.4351115226745605, acc=0.5, loss=1.4351115226745605
train: epoch 105, loss 0.49326401948928833, acc=0.7592777609825134, loss=0.49326401948928833
test: epoch 105, loss 1.3670623302459717, acc=0.5, loss=1.3670623302459717
train: epoch 106, loss 0.48342928290367126, acc=0.7595555782318115, loss=0.48342928290367126
test: epoch 106, loss 1.4030359983444214, acc=0.5, loss=1.4030359983444214
train: epoch 107, loss 0.4890579581260681, acc=0.7566666603088379, loss=0.4890579581260681
test: epoch 107, loss 1.301485538482666, acc=0.5, loss=1.301485538482666
train: epoch 108, loss 0.4869138300418854, acc=0.7587777972221375, loss=0.4869138300418854
test: epoch 108, loss 1.4166436195373535, acc=0.5, loss=1.4166436195373535
train: epoch 109, loss 0.4810108244419098, acc=0.7589444518089294, loss=0.4810108244419098
test: epoch 109, loss 1.3102883100509644, acc=0.49166667461395264, loss=1.3102883100509644
train: epoch 110, loss 0.47908517718315125, acc=0.7628889083862305, loss=0.47908517718315125
test: epoch 110, loss 1.527833104133606, acc=0.49166667461395264, loss=1.527833104133606
train: epoch 111, loss 0.45560523867607117, acc=0.7654444575309753, loss=0.45560523867607117
test: epoch 111, loss 1.5013442039489746, acc=0.5, loss=1.5013442039489746
train: epoch 112, loss 0.4932520389556885, acc=0.7593333125114441, loss=0.4932520389556885
test: epoch 112, loss 1.3783155679702759, acc=0.5, loss=1.3783155679702759
train: epoch 113, loss 0.4798310399055481, acc=0.7548333406448364, loss=0.4798310399055481
test: epoch 113, loss 1.4165771007537842, acc=0.5, loss=1.4165771007537842
train: epoch 114, loss 0.48875126242637634, acc=0.7560555338859558, loss=0.48875126242637634
test: epoch 114, loss 1.371658205986023, acc=0.5, loss=1.371658205986023
train: epoch 115, loss 0.4847104251384735, acc=0.7616666555404663, loss=0.4847104251384735
test: epoch 115, loss 1.5014818906784058, acc=0.5, loss=1.5014818906784058
train: epoch 116, loss 0.47514578700065613, acc=0.7660555839538574, loss=0.47514578700065613
test: epoch 116, loss 1.4273909330368042, acc=0.5, loss=1.4273909330368042
train: epoch 117, loss 0.49009665846824646, acc=0.7680555582046509, loss=0.49009665846824646
test: epoch 117, loss 1.4718221426010132, acc=0.4972222149372101, loss=1.4718221426010132
train: epoch 118, loss 0.4842020869255066, acc=0.7641111016273499, loss=0.4842020869255066
test: epoch 118, loss 1.3891820907592773, acc=0.4972222149372101, loss=1.3891820907592773
train: epoch 119, loss 0.4797739088535309, acc=0.7666110992431641, loss=0.4797739088535309
test: epoch 119, loss 1.4734901189804077, acc=0.5, loss=1.4734901189804077
train: epoch 120, loss 0.4777359366416931, acc=0.7664444446563721, loss=0.4777359366416931
test: epoch 120, loss 1.4517885446548462, acc=0.4722222089767456, loss=1.4517885446548462
train: epoch 121, loss 0.4766627252101898, acc=0.7668333053588867, loss=0.4766627252101898
test: epoch 121, loss 1.4629805088043213, acc=0.49166667461395264, loss=1.4629805088043213
train: epoch 122, loss 0.47181829810142517, acc=0.7634444236755371, loss=0.47181829810142517
test: epoch 122, loss 1.4427363872528076, acc=0.5027777552604675, loss=1.4427363872528076
train: epoch 123, loss 0.4939596951007843, acc=0.7548888921737671, loss=0.4939596951007843
test: epoch 123, loss 1.3337465524673462, acc=0.5, loss=1.3337465524673462
train: epoch 124, loss 0.47340255975723267, acc=0.7700555324554443, loss=0.47340255975723267
test: epoch 124, loss 1.4449515342712402, acc=0.5027777552604675, loss=1.4449515342712402
train: epoch 125, loss 0.47780197858810425, acc=0.7709444165229797, loss=0.47780197858810425
test: epoch 125, loss 1.5076438188552856, acc=0.4972222149372101, loss=1.5076438188552856
train: epoch 126, loss 0.4792218804359436, acc=0.7668333053588867, loss=0.4792218804359436
test: epoch 126, loss 1.595328450202942, acc=0.49166667461395264, loss=1.595328450202942
train: epoch 127, loss 0.48526203632354736, acc=0.7650555372238159, loss=0.48526203632354736
test: epoch 127, loss 1.3553425073623657, acc=0.5, loss=1.3553425073623657
train: epoch 128, loss 0.4650501608848572, acc=0.769444465637207, loss=0.4650501608848572
test: epoch 128, loss 1.4206463098526, acc=0.5027777552604675, loss=1.4206463098526
train: epoch 129, loss 0.4753838777542114, acc=0.7642777562141418, loss=0.4753838777542114
test: epoch 129, loss 1.6251047849655151, acc=0.5027777552604675, loss=1.6251047849655151
train: epoch 130, loss 0.5001887083053589, acc=0.7565555572509766, loss=0.5001887083053589
test: epoch 130, loss 1.3033922910690308, acc=0.49166667461395264, loss=1.3033922910690308
train: epoch 131, loss 0.48201102018356323, acc=0.7608333230018616, loss=0.48201102018356323
test: epoch 131, loss 1.4316707849502563, acc=0.5027777552604675, loss=1.4316707849502563
train: epoch 132, loss 0.47969162464141846, acc=0.7626110911369324, loss=0.47969162464141846
test: epoch 132, loss 1.3794015645980835, acc=0.5055555701255798, loss=1.3794015645980835
train: epoch 133, loss 0.4847109317779541, acc=0.7629444599151611, loss=0.4847109317779541
test: epoch 133, loss 1.3601856231689453, acc=0.5, loss=1.3601856231689453
train: epoch 134, loss 0.474963515996933, acc=0.7677222490310669, loss=0.474963515996933
test: epoch 134, loss 1.3498055934906006, acc=0.5, loss=1.3498055934906006
train: epoch 135, loss 0.46591198444366455, acc=0.7706666588783264, loss=0.46591198444366455
test: epoch 135, loss 1.5740561485290527, acc=0.4972222149372101, loss=1.5740561485290527
train: epoch 136, loss 0.4559127986431122, acc=0.7714444398880005, loss=0.4559127986431122
test: epoch 136, loss 1.5367710590362549, acc=0.4888888895511627, loss=1.5367710590362549
train: epoch 137, loss 0.4857467710971832, acc=0.7672777771949768, loss=0.4857467710971832
test: epoch 137, loss 1.3538249731063843, acc=0.5, loss=1.3538249731063843
train: epoch 138, loss 0.47404858469963074, acc=0.7711666822433472, loss=0.47404858469963074
test: epoch 138, loss 1.5058513879776, acc=0.5027777552604675, loss=1.5058513879776
train: epoch 139, loss 0.4640570878982544, acc=0.766777753829956, loss=0.4640570878982544
test: epoch 139, loss 1.399817705154419, acc=0.5, loss=1.399817705154419
train: epoch 140, loss 0.4657650887966156, acc=0.7709444165229797, loss=0.4657650887966156
test: epoch 140, loss 1.500087857246399, acc=0.4972222149372101, loss=1.500087857246399
train: epoch 141, loss 0.47244560718536377, acc=0.769444465637207, loss=0.47244560718536377
test: epoch 141, loss 1.418471336364746, acc=0.49444442987442017, loss=1.418471336364746
train: epoch 142, loss 0.48673197627067566, acc=0.7659444212913513, loss=0.48673197627067566
test: epoch 142, loss 1.3853404521942139, acc=0.5027777552604675, loss=1.3853404521942139
train: epoch 143, loss 0.4928220510482788, acc=0.7558333277702332, loss=0.4928220510482788
test: epoch 143, loss 1.4950453042984009, acc=0.5027777552604675, loss=1.4950453042984009
train: epoch 144, loss 0.4715997576713562, acc=0.765999972820282, loss=0.4715997576713562
test: epoch 144, loss 1.438118815422058, acc=0.5, loss=1.438118815422058
train: epoch 145, loss 0.45779675245285034, acc=0.777388870716095, loss=0.45779675245285034
test: epoch 145, loss 1.3656032085418701, acc=0.5, loss=1.3656032085418701
train: epoch 146, loss 0.46724504232406616, acc=0.7716666460037231, loss=0.46724504232406616
test: epoch 146, loss 1.4765962362289429, acc=0.4861111044883728, loss=1.4765962362289429
train: epoch 147, loss 0.4699365496635437, acc=0.7693889141082764, loss=0.4699365496635437
test: epoch 147, loss 1.332170009613037, acc=0.5027777552604675, loss=1.332170009613037
train: epoch 148, loss 0.45569661259651184, acc=0.7724444270133972, loss=0.45569661259651184
test: epoch 148, loss 1.4119551181793213, acc=0.5, loss=1.4119551181793213
train: epoch 149, loss 0.4647836983203888, acc=0.7722222208976746, loss=0.4647836983203888
test: epoch 149, loss 1.3783583641052246, acc=0.5027777552604675, loss=1.3783583641052246
train: epoch 150, loss 0.4920194149017334, acc=0.7616111040115356, loss=0.4920194149017334
test: epoch 150, loss 1.3564180135726929, acc=0.4972222149372101, loss=1.3564180135726929
