# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.995", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=416500575, receiver_embed_dim=128, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=416500575, receiver_embed_dim=128, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.561124563217163, acc=0.04738888889551163, loss=3.561124563217163
test: epoch 1, loss 3.4763448238372803, acc=0.05000000074505806, loss=3.4763448238372803
train: epoch 2, loss 3.5024092197418213, acc=0.050111111253499985, loss=3.5024092197418213
test: epoch 2, loss 3.359926462173462, acc=0.05277777835726738, loss=3.359926462173462
train: epoch 3, loss 3.4847288131713867, acc=0.050111111253499985, loss=3.4847288131713867
test: epoch 3, loss 3.1572868824005127, acc=0.09166666865348816, loss=3.1572868824005127
train: epoch 4, loss 3.4324803352355957, acc=0.05227777734398842, loss=3.4324803352355957
test: epoch 4, loss 3.1884658336639404, acc=0.0555555559694767, loss=3.1884658336639404
train: epoch 5, loss 3.2915847301483154, acc=0.06366666406393051, loss=3.2915847301483154
test: epoch 5, loss 3.532797336578369, acc=0.05833333358168602, loss=3.532797336578369
train: epoch 6, loss 3.1200945377349854, acc=0.08172222226858139, loss=3.1200945377349854
test: epoch 6, loss 3.6676032543182373, acc=0.06666667014360428, loss=3.6676032543182373
train: epoch 7, loss 2.9927382469177246, acc=0.09505555778741837, loss=2.9927382469177246
test: epoch 7, loss 3.728419303894043, acc=0.0555555559694767, loss=3.728419303894043
train: epoch 8, loss 2.8889718055725098, acc=0.10249999910593033, loss=2.8889718055725098
test: epoch 8, loss 3.756464958190918, acc=0.06388889253139496, loss=3.756464958190918
train: epoch 9, loss 2.8156352043151855, acc=0.11266666650772095, loss=2.8156352043151855
test: epoch 9, loss 3.7672107219696045, acc=0.06111111119389534, loss=3.7672107219696045
train: epoch 10, loss 2.7600669860839844, acc=0.12133333086967468, loss=2.7600669860839844
test: epoch 10, loss 3.7034707069396973, acc=0.05833333358168602, loss=3.7034707069396973
train: epoch 11, loss 2.714698314666748, acc=0.12777778506278992, loss=2.714698314666748
test: epoch 11, loss 3.6168997287750244, acc=0.05000000074505806, loss=3.6168997287750244
train: epoch 12, loss 2.6529428958892822, acc=0.1397777795791626, loss=2.6529428958892822
test: epoch 12, loss 3.6131951808929443, acc=0.06666667014360428, loss=3.6131951808929443
train: epoch 13, loss 2.6345760822296143, acc=0.13966666162014008, loss=2.6345760822296143
test: epoch 13, loss 3.5710952281951904, acc=0.06666667014360428, loss=3.5710952281951904
train: epoch 14, loss 2.6016933917999268, acc=0.14622221887111664, loss=2.6016933917999268
test: epoch 14, loss 3.5233025550842285, acc=0.06111111119389534, loss=3.5233025550842285
train: epoch 15, loss 2.567676544189453, acc=0.1480555534362793, loss=2.567676544189453
test: epoch 15, loss 3.4706480503082275, acc=0.0555555559694767, loss=3.4706480503082275
train: epoch 16, loss 2.55637788772583, acc=0.15194444358348846, loss=2.55637788772583
test: epoch 16, loss 3.497026205062866, acc=0.05277777835726738, loss=3.497026205062866
train: epoch 17, loss 2.5268361568450928, acc=0.1558888852596283, loss=2.5268361568450928
test: epoch 17, loss 3.444096565246582, acc=0.05833333358168602, loss=3.444096565246582
train: epoch 18, loss 2.5155575275421143, acc=0.15966667234897614, loss=2.5155575275421143
test: epoch 18, loss 3.385777473449707, acc=0.07777778059244156, loss=3.385777473449707
train: epoch 19, loss 2.499480962753296, acc=0.16349999606609344, loss=2.499480962753296
test: epoch 19, loss 3.3429625034332275, acc=0.07222222536802292, loss=3.3429625034332275
train: epoch 20, loss 2.4805245399475098, acc=0.17172221839427948, loss=2.4805245399475098
test: epoch 20, loss 3.367074966430664, acc=0.0694444477558136, loss=3.367074966430664
train: epoch 21, loss 2.4560298919677734, acc=0.17088888585567474, loss=2.4560298919677734
test: epoch 21, loss 3.3373332023620605, acc=0.06666667014360428, loss=3.3373332023620605
train: epoch 22, loss 2.4507148265838623, acc=0.17694444954395294, loss=2.4507148265838623
test: epoch 22, loss 3.324638605117798, acc=0.09444444626569748, loss=3.324638605117798
train: epoch 23, loss 2.444178342819214, acc=0.17255555093288422, loss=2.444178342819214
test: epoch 23, loss 3.220146417617798, acc=0.08055555820465088, loss=3.220146417617798
train: epoch 24, loss 2.4301023483276367, acc=0.17149999737739563, loss=2.4301023483276367
test: epoch 24, loss 3.2350456714630127, acc=0.08888889104127884, loss=3.2350456714630127
train: epoch 25, loss 2.4132797718048096, acc=0.1812777817249298, loss=2.4132797718048096
test: epoch 25, loss 3.1949644088745117, acc=0.08055555820465088, loss=3.1949644088745117
train: epoch 26, loss 2.409080743789673, acc=0.17677778005599976, loss=2.409080743789673
test: epoch 26, loss 3.101119041442871, acc=0.08055555820465088, loss=3.101119041442871
train: epoch 27, loss 2.3784127235412598, acc=0.1882222294807434, loss=2.3784127235412598
test: epoch 27, loss 3.13330078125, acc=0.08055555820465088, loss=3.13330078125
train: epoch 28, loss 2.380258798599243, acc=0.18744444847106934, loss=2.380258798599243
test: epoch 28, loss 3.1085739135742188, acc=0.0972222238779068, loss=3.1085739135742188
train: epoch 29, loss 2.3618276119232178, acc=0.1914999932050705, loss=2.3618276119232178
test: epoch 29, loss 3.1021077632904053, acc=0.10000000149011612, loss=3.1021077632904053
train: epoch 30, loss 2.346148729324341, acc=0.19277778267860413, loss=2.346148729324341
test: epoch 30, loss 3.0488080978393555, acc=0.09444444626569748, loss=3.0488080978393555
train: epoch 31, loss 2.3393304347991943, acc=0.19733333587646484, loss=2.3393304347991943
test: epoch 31, loss 2.9936394691467285, acc=0.11666666716337204, loss=2.9936394691467285
train: epoch 32, loss 2.3186330795288086, acc=0.1996111124753952, loss=2.3186330795288086
test: epoch 32, loss 2.8925673961639404, acc=0.11944444477558136, loss=2.8925673961639404
train: epoch 33, loss 2.2992043495178223, acc=0.20455555617809296, loss=2.2992043495178223
test: epoch 33, loss 2.9091169834136963, acc=0.11666666716337204, loss=2.9091169834136963
train: epoch 34, loss 2.3029892444610596, acc=0.20355555415153503, loss=2.3029892444610596
test: epoch 34, loss 2.8525173664093018, acc=0.12222222238779068, loss=2.8525173664093018
train: epoch 35, loss 2.283142566680908, acc=0.21061110496520996, loss=2.283142566680908
test: epoch 35, loss 2.7965991497039795, acc=0.11388888955116272, loss=2.7965991497039795
train: epoch 36, loss 2.2733049392700195, acc=0.21183332800865173, loss=2.2733049392700195
test: epoch 36, loss 2.7395899295806885, acc=0.11666666716337204, loss=2.7395899295806885
train: epoch 37, loss 2.2465102672576904, acc=0.218833327293396, loss=2.2465102672576904
test: epoch 37, loss 2.7538793087005615, acc=0.11944444477558136, loss=2.7538793087005615
train: epoch 38, loss 2.243637800216675, acc=0.21755555272102356, loss=2.243637800216675
test: epoch 38, loss 2.6833677291870117, acc=0.13611111044883728, loss=2.6833677291870117
train: epoch 39, loss 2.2129669189453125, acc=0.2240000069141388, loss=2.2129669189453125
test: epoch 39, loss 2.638056993484497, acc=0.14166666567325592, loss=2.638056993484497
train: epoch 40, loss 2.2019784450531006, acc=0.22450000047683716, loss=2.2019784450531006
test: epoch 40, loss 2.672304630279541, acc=0.14166666567325592, loss=2.672304630279541
train: epoch 41, loss 2.1826140880584717, acc=0.23116666078567505, loss=2.1826140880584717
test: epoch 41, loss 2.616109609603882, acc=0.14444445073604584, loss=2.616109609603882
train: epoch 42, loss 2.1557466983795166, acc=0.2378888875246048, loss=2.1557466983795166
test: epoch 42, loss 2.6013028621673584, acc=0.14444445073604584, loss=2.6013028621673584
train: epoch 43, loss 2.1481895446777344, acc=0.24294444918632507, loss=2.1481895446777344
test: epoch 43, loss 2.4679880142211914, acc=0.15555556118488312, loss=2.4679880142211914
train: epoch 44, loss 2.140721082687378, acc=0.24161110818386078, loss=2.140721082687378
test: epoch 44, loss 2.4482240676879883, acc=0.1527777761220932, loss=2.4482240676879883
train: epoch 45, loss 2.127730131149292, acc=0.24844443798065186, loss=2.127730131149292
test: epoch 45, loss 2.4317872524261475, acc=0.15833333134651184, loss=2.4317872524261475
train: epoch 46, loss 2.1160659790039062, acc=0.2442222237586975, loss=2.1160659790039062
test: epoch 46, loss 2.3802425861358643, acc=0.15833333134651184, loss=2.3802425861358643
train: epoch 47, loss 2.098055362701416, acc=0.2535000145435333, loss=2.098055362701416
test: epoch 47, loss 2.3538217544555664, acc=0.15833333134651184, loss=2.3538217544555664
train: epoch 48, loss 2.072935104370117, acc=0.2606111168861389, loss=2.072935104370117
test: epoch 48, loss 2.335273027420044, acc=0.16388888657093048, loss=2.335273027420044
train: epoch 49, loss 2.0486278533935547, acc=0.2659444510936737, loss=2.0486278533935547
test: epoch 49, loss 2.3273489475250244, acc=0.15555556118488312, loss=2.3273489475250244
train: epoch 50, loss 2.039705753326416, acc=0.26422223448753357, loss=2.039705753326416
test: epoch 50, loss 2.274662971496582, acc=0.16388888657093048, loss=2.274662971496582
train: epoch 51, loss 2.033926486968994, acc=0.26972222328186035, loss=2.033926486968994
test: epoch 51, loss 2.243567705154419, acc=0.17222222685813904, loss=2.243567705154419
train: epoch 52, loss 2.0117135047912598, acc=0.27594444155693054, loss=2.0117135047912598
test: epoch 52, loss 2.248774528503418, acc=0.1666666716337204, loss=2.248774528503418
train: epoch 53, loss 2.009722948074341, acc=0.2715555429458618, loss=2.009722948074341
test: epoch 53, loss 2.2803471088409424, acc=0.1666666716337204, loss=2.2803471088409424
train: epoch 54, loss 1.9906476736068726, acc=0.27622222900390625, loss=1.9906476736068726
test: epoch 54, loss 2.211652994155884, acc=0.17222222685813904, loss=2.211652994155884
train: epoch 55, loss 1.9778177738189697, acc=0.28733333945274353, loss=1.9778177738189697
test: epoch 55, loss 2.1959478855133057, acc=0.17499999701976776, loss=2.1959478855133057
train: epoch 56, loss 1.9576513767242432, acc=0.285055547952652, loss=1.9576513767242432
test: epoch 56, loss 2.2128782272338867, acc=0.17499999701976776, loss=2.2128782272338867
train: epoch 57, loss 1.9354848861694336, acc=0.29472222924232483, loss=1.9354848861694336
test: epoch 57, loss 2.1780037879943848, acc=0.1805555522441864, loss=2.1780037879943848
train: epoch 58, loss 1.957533836364746, acc=0.2971666753292084, loss=1.957533836364746
test: epoch 58, loss 2.1847541332244873, acc=0.18611110746860504, loss=2.1847541332244873
train: epoch 59, loss 1.909754991531372, acc=0.304500013589859, loss=1.909754991531372
test: epoch 59, loss 2.1596643924713135, acc=0.18611110746860504, loss=2.1596643924713135
train: epoch 60, loss 1.905747652053833, acc=0.301111102104187, loss=1.905747652053833
test: epoch 60, loss 2.16021466255188, acc=0.1805555522441864, loss=2.16021466255188
train: epoch 61, loss 1.9050074815750122, acc=0.30383333563804626, loss=1.9050074815750122
test: epoch 61, loss 2.1511054039001465, acc=0.1805555522441864, loss=2.1511054039001465
train: epoch 62, loss 1.8935736417770386, acc=0.31033334136009216, loss=1.8935736417770386
test: epoch 62, loss 2.128157377243042, acc=0.18333333730697632, loss=2.128157377243042
train: epoch 63, loss 1.8772389888763428, acc=0.3144444525241852, loss=1.8772389888763428
test: epoch 63, loss 2.147127389907837, acc=0.18333333730697632, loss=2.147127389907837
train: epoch 64, loss 1.866182565689087, acc=0.31672221422195435, loss=1.866182565689087
test: epoch 64, loss 2.1423115730285645, acc=0.18611110746860504, loss=2.1423115730285645
train: epoch 65, loss 1.8524415493011475, acc=0.32022222876548767, loss=1.8524415493011475
test: epoch 65, loss 2.1324124336242676, acc=0.18888889253139496, loss=2.1324124336242676
train: epoch 66, loss 1.8313331604003906, acc=0.32627779245376587, loss=1.8313331604003906
test: epoch 66, loss 2.1481447219848633, acc=0.18611110746860504, loss=2.1481447219848633
train: epoch 67, loss 1.8242173194885254, acc=0.3236111104488373, loss=1.8242173194885254
test: epoch 67, loss 2.119786024093628, acc=0.18888889253139496, loss=2.119786024093628
train: epoch 68, loss 1.830566644668579, acc=0.3319999873638153, loss=1.830566644668579
test: epoch 68, loss 2.099287271499634, acc=0.18611110746860504, loss=2.099287271499634
train: epoch 69, loss 1.803194522857666, acc=0.33088889718055725, loss=1.803194522857666
test: epoch 69, loss 2.076063871383667, acc=0.19722221791744232, loss=2.076063871383667
train: epoch 70, loss 1.7832859754562378, acc=0.33694443106651306, loss=1.7832859754562378
test: epoch 70, loss 2.092282772064209, acc=0.18333333730697632, loss=2.092282772064209
train: epoch 71, loss 1.7672533988952637, acc=0.34627777338027954, loss=1.7672533988952637
test: epoch 71, loss 2.0956268310546875, acc=0.18888889253139496, loss=2.0956268310546875
train: epoch 72, loss 1.7666109800338745, acc=0.3457222282886505, loss=1.7666109800338745
test: epoch 72, loss 2.0526621341705322, acc=0.19722221791744232, loss=2.0526621341705322
train: epoch 73, loss 1.7597486972808838, acc=0.3517777919769287, loss=1.7597486972808838
test: epoch 73, loss 2.1026577949523926, acc=0.20555555820465088, loss=2.1026577949523926
train: epoch 74, loss 1.7376314401626587, acc=0.3508888781070709, loss=1.7376314401626587
test: epoch 74, loss 2.0436294078826904, acc=0.20000000298023224, loss=2.0436294078826904
train: epoch 75, loss 1.7373608350753784, acc=0.3550555408000946, loss=1.7373608350753784
test: epoch 75, loss 1.9906867742538452, acc=0.20555555820465088, loss=1.9906867742538452
train: epoch 76, loss 1.7221547365188599, acc=0.3572777807712555, loss=1.7221547365188599
test: epoch 76, loss 2.0822834968566895, acc=0.2083333283662796, loss=2.0822834968566895
train: epoch 77, loss 1.7048288583755493, acc=0.3631666600704193, loss=1.7048288583755493
test: epoch 77, loss 2.0330991744995117, acc=0.20000000298023224, loss=2.0330991744995117
train: epoch 78, loss 1.69287109375, acc=0.36516666412353516, loss=1.69287109375
test: epoch 78, loss 2.0389766693115234, acc=0.20555555820465088, loss=2.0389766693115234
train: epoch 79, loss 1.676368236541748, acc=0.36638888716697693, loss=1.676368236541748
test: epoch 79, loss 2.034421920776367, acc=0.2083333283662796, loss=2.034421920776367
train: epoch 80, loss 1.6709387302398682, acc=0.37549999356269836, loss=1.6709387302398682
test: epoch 80, loss 2.0427889823913574, acc=0.21388888359069824, loss=2.0427889823913574
train: epoch 81, loss 1.6674917936325073, acc=0.3689444363117218, loss=1.6674917936325073
test: epoch 81, loss 2.0434985160827637, acc=0.2083333283662796, loss=2.0434985160827637
train: epoch 82, loss 1.6508848667144775, acc=0.3732222318649292, loss=1.6508848667144775
test: epoch 82, loss 2.030874252319336, acc=0.21944443881511688, loss=2.030874252319336
train: epoch 83, loss 1.6602234840393066, acc=0.37905555963516235, loss=1.6602234840393066
test: epoch 83, loss 2.0630757808685303, acc=0.2083333283662796, loss=2.0630757808685303
train: epoch 84, loss 1.6500840187072754, acc=0.3811666667461395, loss=1.6500840187072754
test: epoch 84, loss 2.022972583770752, acc=0.21111111342906952, loss=2.022972583770752
train: epoch 85, loss 1.6338094472885132, acc=0.3858333230018616, loss=1.6338094472885132
test: epoch 85, loss 2.013700485229492, acc=0.21388888359069824, loss=2.013700485229492
train: epoch 86, loss 1.6355924606323242, acc=0.3874444365501404, loss=1.6355924606323242
test: epoch 86, loss 1.9913986921310425, acc=0.20000000298023224, loss=1.9913986921310425
train: epoch 87, loss 1.6237077713012695, acc=0.38822221755981445, loss=1.6237077713012695
test: epoch 87, loss 1.9731881618499756, acc=0.2222222238779068, loss=1.9731881618499756
train: epoch 88, loss 1.614457130432129, acc=0.39133334159851074, loss=1.614457130432129
test: epoch 88, loss 1.9882338047027588, acc=0.21944443881511688, loss=1.9882338047027588
train: epoch 89, loss 1.6028943061828613, acc=0.3958333432674408, loss=1.6028943061828613
test: epoch 89, loss 1.9880061149597168, acc=0.2083333283662796, loss=1.9880061149597168
train: epoch 90, loss 1.6099122762680054, acc=0.40227776765823364, loss=1.6099122762680054
test: epoch 90, loss 1.9626843929290771, acc=0.22499999403953552, loss=1.9626843929290771
train: epoch 91, loss 1.6031819581985474, acc=0.39605554938316345, loss=1.6031819581985474
test: epoch 91, loss 1.9697883129119873, acc=0.22499999403953552, loss=1.9697883129119873
train: epoch 92, loss 1.5772985219955444, acc=0.4091666638851166, loss=1.5772985219955444
test: epoch 92, loss 1.9362674951553345, acc=0.22777777910232544, loss=1.9362674951553345
train: epoch 93, loss 1.5770231485366821, acc=0.40655556321144104, loss=1.5770231485366821
test: epoch 93, loss 1.9505122900009155, acc=0.2222222238779068, loss=1.9505122900009155
train: epoch 94, loss 1.5632802248001099, acc=0.40638887882232666, loss=1.5632802248001099
test: epoch 94, loss 1.976935863494873, acc=0.21388888359069824, loss=1.976935863494873
train: epoch 95, loss 1.554179072380066, acc=0.41616666316986084, loss=1.554179072380066
test: epoch 95, loss 1.9653009176254272, acc=0.21944443881511688, loss=1.9653009176254272
train: epoch 96, loss 1.536203145980835, acc=0.41538888216018677, loss=1.536203145980835
test: epoch 96, loss 1.9371215105056763, acc=0.23333333432674408, loss=1.9371215105056763
train: epoch 97, loss 1.5452206134796143, acc=0.4168333411216736, loss=1.5452206134796143
test: epoch 97, loss 1.9770433902740479, acc=0.23055554926395416, loss=1.9770433902740479
train: epoch 98, loss 1.558003544807434, acc=0.4194999933242798, loss=1.558003544807434
test: epoch 98, loss 1.9597091674804688, acc=0.23333333432674408, loss=1.9597091674804688
train: epoch 99, loss 1.5331306457519531, acc=0.42561110854148865, loss=1.5331306457519531
test: epoch 99, loss 1.9632916450500488, acc=0.23055554926395416, loss=1.9632916450500488
train: epoch 100, loss 1.5148189067840576, acc=0.4277777671813965, loss=1.5148189067840576
test: epoch 100, loss 1.9532946348190308, acc=0.22777777910232544, loss=1.9532946348190308
train: epoch 101, loss 1.5126746892929077, acc=0.4237777888774872, loss=1.5126746892929077
test: epoch 101, loss 1.971482515335083, acc=0.2222222238779068, loss=1.971482515335083
train: epoch 102, loss 1.496695637702942, acc=0.4358888864517212, loss=1.496695637702942
test: epoch 102, loss 1.984574556350708, acc=0.23333333432674408, loss=1.984574556350708
train: epoch 103, loss 1.5072352886199951, acc=0.4342777729034424, loss=1.5072352886199951
test: epoch 103, loss 1.9518908262252808, acc=0.2222222238779068, loss=1.9518908262252808
train: epoch 104, loss 1.4985886812210083, acc=0.43755555152893066, loss=1.4985886812210083
test: epoch 104, loss 1.9413808584213257, acc=0.22777777910232544, loss=1.9413808584213257
train: epoch 105, loss 1.4851839542388916, acc=0.4388333261013031, loss=1.4851839542388916
test: epoch 105, loss 1.9472132921218872, acc=0.22777777910232544, loss=1.9472132921218872
train: epoch 106, loss 1.473707914352417, acc=0.44261109828948975, loss=1.473707914352417
test: epoch 106, loss 1.9101271629333496, acc=0.23888888955116272, loss=1.9101271629333496
train: epoch 107, loss 1.4806101322174072, acc=0.4425555467605591, loss=1.4806101322174072
test: epoch 107, loss 1.9434360265731812, acc=0.22777777910232544, loss=1.9434360265731812
train: epoch 108, loss 1.4607471227645874, acc=0.453166663646698, loss=1.4607471227645874
test: epoch 108, loss 1.9554554224014282, acc=0.2361111044883728, loss=1.9554554224014282
train: epoch 109, loss 1.4425604343414307, acc=0.4508333206176758, loss=1.4425604343414307
test: epoch 109, loss 1.8923968076705933, acc=0.24166665971279144, loss=1.8923968076705933
train: epoch 110, loss 1.4558812379837036, acc=0.45172223448753357, loss=1.4558812379837036
test: epoch 110, loss 1.9177460670471191, acc=0.24166665971279144, loss=1.9177460670471191
train: epoch 111, loss 1.4303934574127197, acc=0.45438888669013977, loss=1.4303934574127197
test: epoch 111, loss 1.919575572013855, acc=0.24166665971279144, loss=1.919575572013855
train: epoch 112, loss 1.452441692352295, acc=0.45927777886390686, loss=1.452441692352295
test: epoch 112, loss 1.9161168336868286, acc=0.24722221493721008, loss=1.9161168336868286
train: epoch 113, loss 1.4144964218139648, acc=0.4577777683734894, loss=1.4144964218139648
test: epoch 113, loss 1.9397417306900024, acc=0.24722221493721008, loss=1.9397417306900024
train: epoch 114, loss 1.4301121234893799, acc=0.46211111545562744, loss=1.4301121234893799
test: epoch 114, loss 1.9431922435760498, acc=0.23333333432674408, loss=1.9431922435760498
train: epoch 115, loss 1.4282578229904175, acc=0.4646666646003723, loss=1.4282578229904175
test: epoch 115, loss 1.9338301420211792, acc=0.24722221493721008, loss=1.9338301420211792
train: epoch 116, loss 1.412663221359253, acc=0.4692777693271637, loss=1.412663221359253
test: epoch 116, loss 1.9245234727859497, acc=0.25, loss=1.9245234727859497
train: epoch 117, loss 1.4059401750564575, acc=0.4726666808128357, loss=1.4059401750564575
test: epoch 117, loss 1.9132853746414185, acc=0.2361111044883728, loss=1.9132853746414185
train: epoch 118, loss 1.3994829654693604, acc=0.47361111640930176, loss=1.3994829654693604
test: epoch 118, loss 1.9010952711105347, acc=0.24166665971279144, loss=1.9010952711105347
train: epoch 119, loss 1.3826603889465332, acc=0.47422221302986145, loss=1.3826603889465332
test: epoch 119, loss 1.9084954261779785, acc=0.24166665971279144, loss=1.9084954261779785
train: epoch 120, loss 1.37836754322052, acc=0.4831666648387909, loss=1.37836754322052
test: epoch 120, loss 1.8972405195236206, acc=0.2638888955116272, loss=1.8972405195236206
train: epoch 121, loss 1.3838728666305542, acc=0.4831666648387909, loss=1.3838728666305542
test: epoch 121, loss 1.860807180404663, acc=0.2638888955116272, loss=1.860807180404663
train: epoch 122, loss 1.3663798570632935, acc=0.48438888788223267, loss=1.3663798570632935
test: epoch 122, loss 1.8799389600753784, acc=0.25, loss=1.8799389600753784
train: epoch 123, loss 1.381962537765503, acc=0.4823888838291168, loss=1.381962537765503
test: epoch 123, loss 1.8847676515579224, acc=0.25833332538604736, loss=1.8847676515579224
train: epoch 124, loss 1.3612895011901855, acc=0.49194443225860596, loss=1.3612895011901855
test: epoch 124, loss 1.9085880517959595, acc=0.2527777850627899, loss=1.9085880517959595
train: epoch 125, loss 1.3515582084655762, acc=0.4875555634498596, loss=1.3515582084655762
test: epoch 125, loss 1.8558733463287354, acc=0.2638888955116272, loss=1.8558733463287354
train: epoch 126, loss 1.3452293872833252, acc=0.496055543422699, loss=1.3452293872833252
test: epoch 126, loss 1.9033702611923218, acc=0.2638888955116272, loss=1.9033702611923218
train: epoch 127, loss 1.3377951383590698, acc=0.4955555498600006, loss=1.3377951383590698
test: epoch 127, loss 1.875460147857666, acc=0.26944443583488464, loss=1.875460147857666
train: epoch 128, loss 1.3318455219268799, acc=0.5006666779518127, loss=1.3318455219268799
test: epoch 128, loss 1.8069926500320435, acc=0.2666666805744171, loss=1.8069926500320435
train: epoch 129, loss 1.336232304573059, acc=0.5016666650772095, loss=1.336232304573059
test: epoch 129, loss 1.827954649925232, acc=0.26944443583488464, loss=1.827954649925232
train: epoch 130, loss 1.326102614402771, acc=0.5053889155387878, loss=1.326102614402771
test: epoch 130, loss 1.807381272315979, acc=0.2750000059604645, loss=1.807381272315979
train: epoch 131, loss 1.3155356645584106, acc=0.5079444646835327, loss=1.3155356645584106
test: epoch 131, loss 1.8179335594177246, acc=0.27222222089767456, loss=1.8179335594177246
train: epoch 132, loss 1.322169303894043, acc=0.5096111297607422, loss=1.322169303894043
test: epoch 132, loss 1.8313931226730347, acc=0.27222222089767456, loss=1.8313931226730347
train: epoch 133, loss 1.3048772811889648, acc=0.5092777609825134, loss=1.3048772811889648
test: epoch 133, loss 1.7659727334976196, acc=0.2777777910232544, loss=1.7659727334976196
train: epoch 134, loss 1.3061001300811768, acc=0.5135555267333984, loss=1.3061001300811768
test: epoch 134, loss 1.791612982749939, acc=0.27222222089767456, loss=1.791612982749939
train: epoch 135, loss 1.3083369731903076, acc=0.5141111016273499, loss=1.3083369731903076
test: epoch 135, loss 1.7643582820892334, acc=0.2777777910232544, loss=1.7643582820892334
train: epoch 136, loss 1.2951686382293701, acc=0.5157777667045593, loss=1.2951686382293701
test: epoch 136, loss 1.7686008214950562, acc=0.2805555462837219, loss=1.7686008214950562
train: epoch 137, loss 1.295116662979126, acc=0.5162777900695801, loss=1.295116662979126
test: epoch 137, loss 1.7929133176803589, acc=0.2805555462837219, loss=1.7929133176803589
train: epoch 138, loss 1.2784584760665894, acc=0.5234444737434387, loss=1.2784584760665894
test: epoch 138, loss 1.7877048254013062, acc=0.2750000059604645, loss=1.7877048254013062
train: epoch 139, loss 1.2833657264709473, acc=0.5207777619361877, loss=1.2833657264709473
test: epoch 139, loss 1.7661333084106445, acc=0.2916666567325592, loss=1.7661333084106445
train: epoch 140, loss 1.2636038064956665, acc=0.5267778038978577, loss=1.2636038064956665
test: epoch 140, loss 1.7980200052261353, acc=0.2916666567325592, loss=1.7980200052261353
train: epoch 141, loss 1.261459469795227, acc=0.5264444351196289, loss=1.261459469795227
test: epoch 141, loss 1.7859042882919312, acc=0.28611111640930176, loss=1.7859042882919312
train: epoch 142, loss 1.2715933322906494, acc=0.5302777886390686, loss=1.2715933322906494
test: epoch 142, loss 1.7770622968673706, acc=0.2888889014720917, loss=1.7770622968673706
train: epoch 143, loss 1.258386254310608, acc=0.5306110978126526, loss=1.258386254310608
test: epoch 143, loss 1.7411028146743774, acc=0.2888889014720917, loss=1.7411028146743774
train: epoch 144, loss 1.2556170225143433, acc=0.5323333144187927, loss=1.2556170225143433
test: epoch 144, loss 1.7603360414505005, acc=0.28333333134651184, loss=1.7603360414505005
train: epoch 145, loss 1.2477483749389648, acc=0.5404999852180481, loss=1.2477483749389648
test: epoch 145, loss 1.7470815181732178, acc=0.2944444417953491, loss=1.7470815181732178
train: epoch 146, loss 1.2430695295333862, acc=0.5347222089767456, loss=1.2430695295333862
test: epoch 146, loss 1.7124263048171997, acc=0.2888889014720917, loss=1.7124263048171997
train: epoch 147, loss 1.2382903099060059, acc=0.5410000085830688, loss=1.2382903099060059
test: epoch 147, loss 1.7357215881347656, acc=0.29722222685813904, loss=1.7357215881347656
train: epoch 148, loss 1.239911675453186, acc=0.5432778000831604, loss=1.239911675453186
test: epoch 148, loss 1.7370555400848389, acc=0.30000001192092896, loss=1.7370555400848389
train: epoch 149, loss 1.2424193620681763, acc=0.5429999828338623, loss=1.2424193620681763
test: epoch 149, loss 1.7201433181762695, acc=0.30000001192092896, loss=1.7201433181762695
train: epoch 150, loss 1.226430058479309, acc=0.546500027179718, loss=1.226430058479309
test: epoch 150, loss 1.6893538236618042, acc=0.3027777671813965, loss=1.6893538236618042
