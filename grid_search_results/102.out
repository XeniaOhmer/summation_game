# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=64", "--temperature=2.0", "--temp_decay=0.99", "--one_hot=0", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=1338834744, receiver_embed_dim=64, save_run=0, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=1338834744, receiver_embed_dim=64, save_run=False, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.0250980854034424, acc=0.09966666996479034, loss=3.0250980854034424
test: epoch 1, loss 5.32574462890625, acc=0.03888889029622078, loss=5.32574462890625
train: epoch 2, loss 2.5099382400512695, acc=0.17427778244018555, loss=2.5099382400512695
test: epoch 2, loss 6.8280792236328125, acc=0.03055555559694767, loss=6.8280792236328125
train: epoch 3, loss 2.3476767539978027, acc=0.19583334028720856, loss=2.3476767539978027
test: epoch 3, loss 7.5982489585876465, acc=0.03333333507180214, loss=7.5982489585876465
train: epoch 4, loss 2.2717549800872803, acc=0.2108333259820938, loss=2.2717549800872803
test: epoch 4, loss 8.114646911621094, acc=0.03611111268401146, loss=8.114646911621094
train: epoch 5, loss 2.2029662132263184, acc=0.22638888657093048, loss=2.2029662132263184
test: epoch 5, loss 8.645051956176758, acc=0.03888889029622078, loss=8.645051956176758
train: epoch 6, loss 2.1531450748443604, acc=0.2353888899087906, loss=2.1531450748443604
test: epoch 6, loss 9.014819145202637, acc=0.03888889029622078, loss=9.014819145202637
train: epoch 7, loss 2.116509199142456, acc=0.24655555188655853, loss=2.116509199142456
test: epoch 7, loss 9.366236686706543, acc=0.03888889029622078, loss=9.366236686706543
train: epoch 8, loss 2.0857315063476562, acc=0.24755555391311646, loss=2.0857315063476562
test: epoch 8, loss 9.46878719329834, acc=0.0416666679084301, loss=9.46878719329834
train: epoch 9, loss 2.066584587097168, acc=0.24899999797344208, loss=2.066584587097168
test: epoch 9, loss 10.046738624572754, acc=0.03611111268401146, loss=10.046738624572754
train: epoch 10, loss 2.04042911529541, acc=0.25555557012557983, loss=2.04042911529541
test: epoch 10, loss 10.087060928344727, acc=0.03611111268401146, loss=10.087060928344727
train: epoch 11, loss 2.0194857120513916, acc=0.2605000138282776, loss=2.0194857120513916
test: epoch 11, loss 10.047281265258789, acc=0.03611111268401146, loss=10.047281265258789
train: epoch 12, loss 2.0104613304138184, acc=0.2598888874053955, loss=2.0104613304138184
test: epoch 12, loss 10.252912521362305, acc=0.03888889029622078, loss=10.252912521362305
train: epoch 13, loss 1.994118332862854, acc=0.2598888874053955, loss=1.994118332862854
test: epoch 13, loss 10.165855407714844, acc=0.0416666679084301, loss=10.165855407714844
train: epoch 14, loss 1.9889776706695557, acc=0.26188889145851135, loss=1.9889776706695557
test: epoch 14, loss 10.343160629272461, acc=0.0416666679084301, loss=10.343160629272461
train: epoch 15, loss 1.9803905487060547, acc=0.27477777004241943, loss=1.9803905487060547
test: epoch 15, loss 10.103198051452637, acc=0.04444444552063942, loss=10.103198051452637
train: epoch 16, loss 1.9675673246383667, acc=0.2722777724266052, loss=1.9675673246383667
test: epoch 16, loss 10.121376037597656, acc=0.04444444552063942, loss=10.121376037597656
train: epoch 17, loss 1.958101749420166, acc=0.27105554938316345, loss=1.958101749420166
test: epoch 17, loss 9.928569793701172, acc=0.03611111268401146, loss=9.928569793701172
train: epoch 18, loss 1.9603952169418335, acc=0.26866665482521057, loss=1.9603952169418335
test: epoch 18, loss 9.509957313537598, acc=0.0416666679084301, loss=9.509957313537598
train: epoch 19, loss 1.959191918373108, acc=0.2801666557788849, loss=1.959191918373108
test: epoch 19, loss 9.876325607299805, acc=0.0416666679084301, loss=9.876325607299805
train: epoch 20, loss 1.9494329690933228, acc=0.2742222249507904, loss=1.9494329690933228
test: epoch 20, loss 9.630376815795898, acc=0.03888889029622078, loss=9.630376815795898
train: epoch 21, loss 1.956044316291809, acc=0.27844443917274475, loss=1.956044316291809
test: epoch 21, loss 9.575291633605957, acc=0.04444444552063942, loss=9.575291633605957
train: epoch 22, loss 1.9566056728363037, acc=0.28033334016799927, loss=1.9566056728363037
test: epoch 22, loss 9.21339225769043, acc=0.03611111268401146, loss=9.21339225769043
train: epoch 23, loss 1.9381235837936401, acc=0.28111112117767334, loss=1.9381235837936401
test: epoch 23, loss 9.175597190856934, acc=0.03888889029622078, loss=9.175597190856934
train: epoch 24, loss 1.945007085800171, acc=0.2841666638851166, loss=1.945007085800171
test: epoch 24, loss 9.13679313659668, acc=0.03055555559694767, loss=9.13679313659668
train: epoch 25, loss 1.938562273979187, acc=0.2789444327354431, loss=1.938562273979187
test: epoch 25, loss 9.286370277404785, acc=0.03333333507180214, loss=9.286370277404785
train: epoch 26, loss 1.9433526992797852, acc=0.2817777693271637, loss=1.9433526992797852
test: epoch 26, loss 9.192768096923828, acc=0.03611111268401146, loss=9.192768096923828
train: epoch 27, loss 1.93830406665802, acc=0.28272223472595215, loss=1.93830406665802
test: epoch 27, loss 9.284594535827637, acc=0.03055555559694767, loss=9.284594535827637
train: epoch 28, loss 1.9490474462509155, acc=0.2801666557788849, loss=1.9490474462509155
test: epoch 28, loss 8.85293960571289, acc=0.03055555559694767, loss=8.85293960571289
train: epoch 29, loss 1.942522644996643, acc=0.2791111171245575, loss=1.942522644996643
test: epoch 29, loss 8.449363708496094, acc=0.03611111268401146, loss=8.449363708496094
train: epoch 30, loss 1.9424054622650146, acc=0.2841111123561859, loss=1.9424054622650146
test: epoch 30, loss 8.182568550109863, acc=0.03888889029622078, loss=8.182568550109863
train: epoch 31, loss 1.952478051185608, acc=0.27988889813423157, loss=1.952478051185608
test: epoch 31, loss 8.095519065856934, acc=0.04444444552063942, loss=8.095519065856934
train: epoch 32, loss 1.956603765487671, acc=0.28244444727897644, loss=1.956603765487671
test: epoch 32, loss 8.162440299987793, acc=0.03055555559694767, loss=8.162440299987793
train: epoch 33, loss 1.9439600706100464, acc=0.2841111123561859, loss=1.9439600706100464
test: epoch 33, loss 8.359258651733398, acc=0.03055555559694767, loss=8.359258651733398
train: epoch 34, loss 1.9560514688491821, acc=0.2823888957500458, loss=1.9560514688491821
test: epoch 34, loss 8.110252380371094, acc=0.03611111268401146, loss=8.110252380371094
train: epoch 35, loss 1.9428917169570923, acc=0.28033334016799927, loss=1.9428917169570923
test: epoch 35, loss 7.7068281173706055, acc=0.03611111268401146, loss=7.7068281173706055
train: epoch 36, loss 1.9506440162658691, acc=0.27888888120651245, loss=1.9506440162658691
test: epoch 36, loss 7.669479846954346, acc=0.03611111268401146, loss=7.669479846954346
train: epoch 37, loss 1.954835057258606, acc=0.28272223472595215, loss=1.954835057258606
test: epoch 37, loss 7.334801197052002, acc=0.04722222313284874, loss=7.334801197052002
train: epoch 38, loss 1.9443212747573853, acc=0.2839444577693939, loss=1.9443212747573853
test: epoch 38, loss 7.214173316955566, acc=0.03611111268401146, loss=7.214173316955566
train: epoch 39, loss 1.977494716644287, acc=0.2763333320617676, loss=1.977494716644287
test: epoch 39, loss 7.360412120819092, acc=0.03611111268401146, loss=7.360412120819092
train: epoch 40, loss 1.968738079071045, acc=0.28216665983200073, loss=1.968738079071045
test: epoch 40, loss 6.875175952911377, acc=0.03611111268401146, loss=6.875175952911377
train: epoch 41, loss 1.9570460319519043, acc=0.28138887882232666, loss=1.9570460319519043
test: epoch 41, loss 6.940649032592773, acc=0.03611111268401146, loss=6.940649032592773
train: epoch 42, loss 1.9822896718978882, acc=0.2801111042499542, loss=1.9822896718978882
test: epoch 42, loss 6.737334728240967, acc=0.03611111268401146, loss=6.737334728240967
train: epoch 43, loss 1.9854174852371216, acc=0.2761666774749756, loss=1.9854174852371216
test: epoch 43, loss 6.55349063873291, acc=0.03611111268401146, loss=6.55349063873291
train: epoch 44, loss 1.985330581665039, acc=0.2762777805328369, loss=1.985330581665039
test: epoch 44, loss 6.5751471519470215, acc=0.03611111268401146, loss=6.5751471519470215
train: epoch 45, loss 1.9849098920822144, acc=0.2758888900279999, loss=1.9849098920822144
test: epoch 45, loss 6.372037887573242, acc=0.03888889029622078, loss=6.372037887573242
train: epoch 46, loss 1.9999887943267822, acc=0.27344444394111633, loss=1.9999887943267822
test: epoch 46, loss 6.332118034362793, acc=0.03611111268401146, loss=6.332118034362793
train: epoch 47, loss 1.996451735496521, acc=0.26866665482521057, loss=1.996451735496521
test: epoch 47, loss 6.389248371124268, acc=0.03611111268401146, loss=6.389248371124268
train: epoch 48, loss 2.0081748962402344, acc=0.27772220969200134, loss=2.0081748962402344
test: epoch 48, loss 6.240726947784424, acc=0.03888889029622078, loss=6.240726947784424
train: epoch 49, loss 1.996341586112976, acc=0.2718888819217682, loss=1.996341586112976
test: epoch 49, loss 6.139749050140381, acc=0.03611111268401146, loss=6.139749050140381
train: epoch 50, loss 2.0060553550720215, acc=0.2755555510520935, loss=2.0060553550720215
test: epoch 50, loss 6.214081764221191, acc=0.03611111268401146, loss=6.214081764221191
train: epoch 51, loss 2.014106512069702, acc=0.2701111137866974, loss=2.014106512069702
test: epoch 51, loss 6.0377888679504395, acc=0.03611111268401146, loss=6.0377888679504395
train: epoch 52, loss 2.0175697803497314, acc=0.26883333921432495, loss=2.0175697803497314
test: epoch 52, loss 5.830328464508057, acc=0.0416666679084301, loss=5.830328464508057
train: epoch 53, loss 2.0237374305725098, acc=0.2717222273349762, loss=2.0237374305725098
test: epoch 53, loss 5.6332221031188965, acc=0.03888889029622078, loss=5.6332221031188965
train: epoch 54, loss 2.0369393825531006, acc=0.2680555582046509, loss=2.0369393825531006
test: epoch 54, loss 5.762980937957764, acc=0.03611111268401146, loss=5.762980937957764
train: epoch 55, loss 2.02856183052063, acc=0.2627222239971161, loss=2.02856183052063
test: epoch 55, loss 5.621159076690674, acc=0.05000000074505806, loss=5.621159076690674
train: epoch 56, loss 2.049518585205078, acc=0.26527777314186096, loss=2.049518585205078
test: epoch 56, loss 5.55381965637207, acc=0.03611111268401146, loss=5.55381965637207
train: epoch 57, loss 2.047260284423828, acc=0.26811110973358154, loss=2.047260284423828
test: epoch 57, loss 5.4032816886901855, acc=0.03611111268401146, loss=5.4032816886901855
train: epoch 58, loss 2.0460026264190674, acc=0.26561111211776733, loss=2.0460026264190674
test: epoch 58, loss 5.30355978012085, acc=0.0416666679084301, loss=5.30355978012085
train: epoch 59, loss 2.0533649921417236, acc=0.26294443011283875, loss=2.0533649921417236
test: epoch 59, loss 5.242506980895996, acc=0.03611111268401146, loss=5.242506980895996
train: epoch 60, loss 2.045823812484741, acc=0.2636111080646515, loss=2.045823812484741
test: epoch 60, loss 5.06544828414917, acc=0.03611111268401146, loss=5.06544828414917
train: epoch 61, loss 2.0488967895507812, acc=0.26277777552604675, loss=2.0488967895507812
test: epoch 61, loss 5.119236469268799, acc=0.03611111268401146, loss=5.119236469268799
train: epoch 62, loss 2.0759055614471436, acc=0.2567777633666992, loss=2.0759055614471436
test: epoch 62, loss 4.906644344329834, acc=0.03333333507180214, loss=4.906644344329834
train: epoch 63, loss 2.0739991664886475, acc=0.25727778673171997, loss=2.0739991664886475
test: epoch 63, loss 4.729775905609131, acc=0.03611111268401146, loss=4.729775905609131
train: epoch 64, loss 2.0893139839172363, acc=0.25183331966400146, loss=2.0893139839172363
test: epoch 64, loss 4.732031345367432, acc=0.03888889029622078, loss=4.732031345367432
train: epoch 65, loss 2.087151288986206, acc=0.25155556201934814, loss=2.087151288986206
test: epoch 65, loss 4.686054706573486, acc=0.03888889029622078, loss=4.686054706573486
train: epoch 66, loss 2.090179443359375, acc=0.2585555613040924, loss=2.090179443359375
test: epoch 66, loss 4.653634548187256, acc=0.03611111268401146, loss=4.653634548187256
train: epoch 67, loss 2.111880302429199, acc=0.24722221493721008, loss=2.111880302429199
test: epoch 67, loss 4.6705427169799805, acc=0.03611111268401146, loss=4.6705427169799805
train: epoch 68, loss 2.1120221614837646, acc=0.24788889288902283, loss=2.1120221614837646
test: epoch 68, loss 4.578261852264404, acc=0.05000000074505806, loss=4.578261852264404
train: epoch 69, loss 2.1242682933807373, acc=0.2457222193479538, loss=2.1242682933807373
test: epoch 69, loss 4.553521156311035, acc=0.03611111268401146, loss=4.553521156311035
train: epoch 70, loss 2.1277103424072266, acc=0.2485000044107437, loss=2.1277103424072266
test: epoch 70, loss 4.29397439956665, acc=0.03888889029622078, loss=4.29397439956665
train: epoch 71, loss 2.1148622035980225, acc=0.24211111664772034, loss=2.1148622035980225
test: epoch 71, loss 4.313869953155518, acc=0.03888889029622078, loss=4.313869953155518
train: epoch 72, loss 2.1424777507781982, acc=0.23483332991600037, loss=2.1424777507781982
test: epoch 72, loss 4.158931732177734, acc=0.0416666679084301, loss=4.158931732177734
train: epoch 73, loss 2.1362359523773193, acc=0.24449999630451202, loss=2.1362359523773193
test: epoch 73, loss 4.321221351623535, acc=0.0416666679084301, loss=4.321221351623535
train: epoch 74, loss 2.154083728790283, acc=0.242166668176651, loss=2.154083728790283
test: epoch 74, loss 4.085113525390625, acc=0.05000000074505806, loss=4.085113525390625
train: epoch 75, loss 2.1486854553222656, acc=0.242166668176651, loss=2.1486854553222656
test: epoch 75, loss 4.054690837860107, acc=0.04722222313284874, loss=4.054690837860107
train: epoch 76, loss 2.1630730628967285, acc=0.23366667330265045, loss=2.1630730628967285
test: epoch 76, loss 3.9720754623413086, acc=0.05000000074505806, loss=3.9720754623413086
train: epoch 77, loss 2.173909902572632, acc=0.23205555975437164, loss=2.173909902572632
test: epoch 77, loss 3.934481143951416, acc=0.04444444552063942, loss=3.934481143951416
train: epoch 78, loss 2.168489933013916, acc=0.23444443941116333, loss=2.168489933013916
test: epoch 78, loss 3.8888983726501465, acc=0.05277777835726738, loss=3.8888983726501465
train: epoch 79, loss 2.1842751502990723, acc=0.22927777469158173, loss=2.1842751502990723
test: epoch 79, loss 3.8936851024627686, acc=0.05833333358168602, loss=3.8936851024627686
train: epoch 80, loss 2.1851489543914795, acc=0.22494444251060486, loss=2.1851489543914795
test: epoch 80, loss 3.919253349304199, acc=0.05000000074505806, loss=3.919253349304199
train: epoch 81, loss 2.197589874267578, acc=0.22805555164813995, loss=2.197589874267578
test: epoch 81, loss 3.8729615211486816, acc=0.0555555559694767, loss=3.8729615211486816
train: epoch 82, loss 2.194042921066284, acc=0.22538888454437256, loss=2.194042921066284
test: epoch 82, loss 3.7088751792907715, acc=0.05000000074505806, loss=3.7088751792907715
train: epoch 83, loss 2.2157390117645264, acc=0.2207222282886505, loss=2.2157390117645264
test: epoch 83, loss 3.5920310020446777, acc=0.04722222313284874, loss=3.5920310020446777
train: epoch 84, loss 2.196180582046509, acc=0.22216667234897614, loss=2.196180582046509
test: epoch 84, loss 3.7670235633850098, acc=0.04722222313284874, loss=3.7670235633850098
train: epoch 85, loss 2.209174394607544, acc=0.22322222590446472, loss=2.209174394607544
test: epoch 85, loss 3.797288656234741, acc=0.04444444552063942, loss=3.797288656234741
train: epoch 86, loss 2.218257427215576, acc=0.2132222205400467, loss=2.218257427215576
test: epoch 86, loss 3.740943193435669, acc=0.05000000074505806, loss=3.740943193435669
train: epoch 87, loss 2.244755506515503, acc=0.21372222900390625, loss=2.244755506515503
test: epoch 87, loss 3.6918821334838867, acc=0.05277777835726738, loss=3.6918821334838867
train: epoch 88, loss 2.2297003269195557, acc=0.2130555510520935, loss=2.2297003269195557
test: epoch 88, loss 3.420961856842041, acc=0.05000000074505806, loss=3.420961856842041
train: epoch 89, loss 2.2356724739074707, acc=0.20794443786144257, loss=2.2356724739074707
test: epoch 89, loss 3.415863037109375, acc=0.06388889253139496, loss=3.415863037109375
train: epoch 90, loss 2.240271806716919, acc=0.21150000393390656, loss=2.240271806716919
test: epoch 90, loss 3.4597718715667725, acc=0.0555555559694767, loss=3.4597718715667725
train: epoch 91, loss 2.254843235015869, acc=0.20944444835186005, loss=2.254843235015869
test: epoch 91, loss 3.489180564880371, acc=0.05277777835726738, loss=3.489180564880371
train: epoch 92, loss 2.247843027114868, acc=0.2047777771949768, loss=2.247843027114868
test: epoch 92, loss 3.5492913722991943, acc=0.05277777835726738, loss=3.5492913722991943
train: epoch 93, loss 2.271210193634033, acc=0.2043333351612091, loss=2.271210193634033
test: epoch 93, loss 3.427645206451416, acc=0.0833333358168602, loss=3.427645206451416
train: epoch 94, loss 2.2825510501861572, acc=0.20383332669734955, loss=2.2825510501861572
test: epoch 94, loss 3.4398820400238037, acc=0.06666667014360428, loss=3.4398820400238037
train: epoch 95, loss 2.288004159927368, acc=0.20366667211055756, loss=2.288004159927368
test: epoch 95, loss 3.45751690864563, acc=0.04444444552063942, loss=3.45751690864563
train: epoch 96, loss 2.2563183307647705, acc=0.20149999856948853, loss=2.2563183307647705
test: epoch 96, loss 3.2977793216705322, acc=0.05000000074505806, loss=3.2977793216705322
train: epoch 97, loss 2.27797269821167, acc=0.19905555248260498, loss=2.27797269821167
test: epoch 97, loss 3.2830615043640137, acc=0.06666667014360428, loss=3.2830615043640137
train: epoch 98, loss 2.277010202407837, acc=0.1987222284078598, loss=2.277010202407837
test: epoch 98, loss 3.3084330558776855, acc=0.07777778059244156, loss=3.3084330558776855
train: epoch 99, loss 2.2877578735351562, acc=0.19900000095367432, loss=2.2877578735351562
test: epoch 99, loss 3.211118698120117, acc=0.06666667014360428, loss=3.211118698120117
train: epoch 100, loss 2.304744243621826, acc=0.1987222284078598, loss=2.304744243621826
test: epoch 100, loss 3.3888099193573, acc=0.06666667014360428, loss=3.3888099193573
train: epoch 101, loss 2.285722255706787, acc=0.19349999725818634, loss=2.285722255706787
test: epoch 101, loss 3.344489336013794, acc=0.07777778059244156, loss=3.344489336013794
train: epoch 102, loss 2.2864866256713867, acc=0.19627778232097626, loss=2.2864866256713867
test: epoch 102, loss 3.358999490737915, acc=0.06111111119389534, loss=3.358999490737915
train: epoch 103, loss 2.282818078994751, acc=0.19699999690055847, loss=2.282818078994751
test: epoch 103, loss 3.3774542808532715, acc=0.06666667014360428, loss=3.3774542808532715
train: epoch 104, loss 2.278501510620117, acc=0.19677777588367462, loss=2.278501510620117
test: epoch 104, loss 3.26119327545166, acc=0.0694444477558136, loss=3.26119327545166
train: epoch 105, loss 2.2889444828033447, acc=0.19550000131130219, loss=2.2889444828033447
test: epoch 105, loss 3.2089943885803223, acc=0.09444444626569748, loss=3.2089943885803223
train: epoch 106, loss 2.29573655128479, acc=0.19716666638851166, loss=2.29573655128479
test: epoch 106, loss 3.156374216079712, acc=0.08611111342906952, loss=3.156374216079712
train: epoch 107, loss 2.290684700012207, acc=0.1991666704416275, loss=2.290684700012207
test: epoch 107, loss 3.228970766067505, acc=0.0833333358168602, loss=3.228970766067505
train: epoch 108, loss 2.273390054702759, acc=0.19672222435474396, loss=2.273390054702759
test: epoch 108, loss 3.199145555496216, acc=0.07222222536802292, loss=3.199145555496216
train: epoch 109, loss 2.269990921020508, acc=0.20100000500679016, loss=2.269990921020508
test: epoch 109, loss 3.256540298461914, acc=0.07500000298023224, loss=3.256540298461914
train: epoch 110, loss 2.2776169776916504, acc=0.20016667246818542, loss=2.2776169776916504
test: epoch 110, loss 3.2673943042755127, acc=0.07500000298023224, loss=3.2673943042755127
train: epoch 111, loss 2.2842531204223633, acc=0.19661110639572144, loss=2.2842531204223633
test: epoch 111, loss 3.1835544109344482, acc=0.08611111342906952, loss=3.1835544109344482
train: epoch 112, loss 2.272084951400757, acc=0.19750000536441803, loss=2.272084951400757
test: epoch 112, loss 3.150210380554199, acc=0.06666667014360428, loss=3.150210380554199
train: epoch 113, loss 2.2744810581207275, acc=0.1984444409608841, loss=2.2744810581207275
test: epoch 113, loss 3.152285575866699, acc=0.08055555820465088, loss=3.152285575866699
train: epoch 114, loss 2.270005702972412, acc=0.19455555081367493, loss=2.270005702972412
test: epoch 114, loss 3.1833314895629883, acc=0.07500000298023224, loss=3.1833314895629883
train: epoch 115, loss 2.2621495723724365, acc=0.19722221791744232, loss=2.2621495723724365
test: epoch 115, loss 3.1953511238098145, acc=0.07777778059244156, loss=3.1953511238098145
train: epoch 116, loss 2.2753725051879883, acc=0.19855555891990662, loss=2.2753725051879883
test: epoch 116, loss 3.2086663246154785, acc=0.0833333358168602, loss=3.2086663246154785
train: epoch 117, loss 2.2668955326080322, acc=0.20333333313465118, loss=2.2668955326080322
test: epoch 117, loss 3.1477842330932617, acc=0.08611111342906952, loss=3.1477842330932617
train: epoch 118, loss 2.268728733062744, acc=0.20461110770702362, loss=2.268728733062744
test: epoch 118, loss 3.2169275283813477, acc=0.06666667014360428, loss=3.2169275283813477
train: epoch 119, loss 2.252088785171509, acc=0.201222226023674, loss=2.252088785171509
test: epoch 119, loss 3.170142412185669, acc=0.08888889104127884, loss=3.170142412185669
train: epoch 120, loss 2.268840789794922, acc=0.19733333587646484, loss=2.268840789794922
test: epoch 120, loss 3.1730358600616455, acc=0.06111111119389534, loss=3.1730358600616455
train: epoch 121, loss 2.257272958755493, acc=0.20377777516841888, loss=2.257272958755493
test: epoch 121, loss 3.309375762939453, acc=0.06666667014360428, loss=3.309375762939453
train: epoch 122, loss 2.2610347270965576, acc=0.20116665959358215, loss=2.2610347270965576
test: epoch 122, loss 3.2071127891540527, acc=0.07500000298023224, loss=3.2071127891540527
train: epoch 123, loss 2.2734344005584717, acc=0.20644444227218628, loss=2.2734344005584717
test: epoch 123, loss 3.075078010559082, acc=0.06388889253139496, loss=3.075078010559082
train: epoch 124, loss 2.260545015335083, acc=0.19688889384269714, loss=2.260545015335083
test: epoch 124, loss 3.1725592613220215, acc=0.08055555820465088, loss=3.1725592613220215
train: epoch 125, loss 2.2518270015716553, acc=0.2011111080646515, loss=2.2518270015716553
test: epoch 125, loss 3.1917569637298584, acc=0.07222222536802292, loss=3.1917569637298584
train: epoch 126, loss 2.254746437072754, acc=0.20305556058883667, loss=2.254746437072754
test: epoch 126, loss 3.2007853984832764, acc=0.08611111342906952, loss=3.2007853984832764
train: epoch 127, loss 2.2489264011383057, acc=0.2034444510936737, loss=2.2489264011383057
test: epoch 127, loss 3.215379476547241, acc=0.07777778059244156, loss=3.215379476547241
train: epoch 128, loss 2.2503771781921387, acc=0.19922222197055817, loss=2.2503771781921387
test: epoch 128, loss 3.2371203899383545, acc=0.0694444477558136, loss=3.2371203899383545
train: epoch 129, loss 2.25087308883667, acc=0.20444443821907043, loss=2.25087308883667
test: epoch 129, loss 3.172041893005371, acc=0.07777778059244156, loss=3.172041893005371
train: epoch 130, loss 2.2438805103302, acc=0.20455555617809296, loss=2.2438805103302
test: epoch 130, loss 3.1029934883117676, acc=0.09166666865348816, loss=3.1029934883117676
train: epoch 131, loss 2.2306461334228516, acc=0.20383332669734955, loss=2.2306461334228516
test: epoch 131, loss 3.2222769260406494, acc=0.05833333358168602, loss=3.2222769260406494
train: epoch 132, loss 2.254434585571289, acc=0.20333333313465118, loss=2.254434585571289
test: epoch 132, loss 3.2694315910339355, acc=0.07777778059244156, loss=3.2694315910339355
train: epoch 133, loss 2.240591287612915, acc=0.2078888863325119, loss=2.240591287612915
test: epoch 133, loss 3.2246406078338623, acc=0.06111111119389534, loss=3.2246406078338623
train: epoch 134, loss 2.252669334411621, acc=0.20283333957195282, loss=2.252669334411621
test: epoch 134, loss 3.175501823425293, acc=0.0694444477558136, loss=3.175501823425293
train: epoch 135, loss 2.257524251937866, acc=0.2027222216129303, loss=2.257524251937866
test: epoch 135, loss 3.2390239238739014, acc=0.06111111119389534, loss=3.2390239238739014
train: epoch 136, loss 2.228139877319336, acc=0.20749999582767487, loss=2.228139877319336
test: epoch 136, loss 3.1867520809173584, acc=0.08055555820465088, loss=3.1867520809173584
train: epoch 137, loss 2.231395959854126, acc=0.20872221887111664, loss=2.231395959854126
test: epoch 137, loss 3.1436548233032227, acc=0.07222222536802292, loss=3.1436548233032227
train: epoch 138, loss 2.240213632583618, acc=0.20561110973358154, loss=2.240213632583618
test: epoch 138, loss 3.159740686416626, acc=0.06388889253139496, loss=3.159740686416626
train: epoch 139, loss 2.217534065246582, acc=0.20811110734939575, loss=2.217534065246582
test: epoch 139, loss 3.32198166847229, acc=0.06388889253139496, loss=3.32198166847229
train: epoch 140, loss 2.2138588428497314, acc=0.20827777683734894, loss=2.2138588428497314
test: epoch 140, loss 3.1677327156066895, acc=0.07777778059244156, loss=3.1677327156066895
train: epoch 141, loss 2.2467522621154785, acc=0.20638889074325562, loss=2.2467522621154785
test: epoch 141, loss 3.066960334777832, acc=0.0833333358168602, loss=3.066960334777832
train: epoch 142, loss 2.264324188232422, acc=0.20600000023841858, loss=2.264324188232422
test: epoch 142, loss 3.0910706520080566, acc=0.06111111119389534, loss=3.0910706520080566
train: epoch 143, loss 2.242546558380127, acc=0.20749999582767487, loss=2.242546558380127
test: epoch 143, loss 3.082761764526367, acc=0.08055555820465088, loss=3.082761764526367
train: epoch 144, loss 2.236908197402954, acc=0.2053888887166977, loss=2.236908197402954
test: epoch 144, loss 3.1924540996551514, acc=0.06388889253139496, loss=3.1924540996551514
train: epoch 145, loss 2.2281556129455566, acc=0.21294444799423218, loss=2.2281556129455566
test: epoch 145, loss 3.198742151260376, acc=0.08055555820465088, loss=3.198742151260376
train: epoch 146, loss 2.223952293395996, acc=0.2065555602312088, loss=2.223952293395996
test: epoch 146, loss 3.1689867973327637, acc=0.0833333358168602, loss=3.1689867973327637
train: epoch 147, loss 2.2364554405212402, acc=0.2061111181974411, loss=2.2364554405212402
test: epoch 147, loss 3.0977909564971924, acc=0.07777778059244156, loss=3.0977909564971924
train: epoch 148, loss 2.227376699447632, acc=0.21161110699176788, loss=2.227376699447632
test: epoch 148, loss 3.291761636734009, acc=0.0694444477558136, loss=3.291761636734009
train: epoch 149, loss 2.229656219482422, acc=0.20888888835906982, loss=2.229656219482422
test: epoch 149, loss 3.360029458999634, acc=0.0694444477558136, loss=3.360029458999634
train: epoch 150, loss 2.2302539348602295, acc=0.20888888835906982, loss=2.2302539348602295
test: epoch 150, loss 3.1907315254211426, acc=0.06111111119389534, loss=3.1907315254211426
