# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.99", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1019575887, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1019575887, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.6212594509124756, acc=0.03938888758420944, loss=3.6212594509124756
test: epoch 1, loss 3.501371383666992, acc=0.05277777835726738, loss=3.501371383666992
train: epoch 2, loss 3.5100581645965576, acc=0.048055555671453476, loss=3.5100581645965576
test: epoch 2, loss 3.4691271781921387, acc=0.04722222313284874, loss=3.4691271781921387
train: epoch 3, loss 3.482332706451416, acc=0.05066666752099991, loss=3.482332706451416
test: epoch 3, loss 3.2652885913848877, acc=0.05833333358168602, loss=3.2652885913848877
train: epoch 4, loss 3.4500370025634766, acc=0.05294444411993027, loss=3.4500370025634766
test: epoch 4, loss 3.059221029281616, acc=0.09444444626569748, loss=3.059221029281616
train: epoch 5, loss 3.343646764755249, acc=0.058888889849185944, loss=3.343646764755249
test: epoch 5, loss 3.2748003005981445, acc=0.05833333358168602, loss=3.2748003005981445
train: epoch 6, loss 3.1567535400390625, acc=0.07822222262620926, loss=3.1567535400390625
test: epoch 6, loss 3.828049421310425, acc=0.04444444552063942, loss=3.828049421310425
train: epoch 7, loss 3.009101152420044, acc=0.09261111170053482, loss=3.009101152420044
test: epoch 7, loss 3.8820579051971436, acc=0.0416666679084301, loss=3.8820579051971436
train: epoch 8, loss 2.912492513656616, acc=0.10483333468437195, loss=2.912492513656616
test: epoch 8, loss 3.771355628967285, acc=0.04722222313284874, loss=3.771355628967285
train: epoch 9, loss 2.8376669883728027, acc=0.10966666787862778, loss=2.8376669883728027
test: epoch 9, loss 3.6631572246551514, acc=0.05277777835726738, loss=3.6631572246551514
train: epoch 10, loss 2.7688002586364746, acc=0.11727777868509293, loss=2.7688002586364746
test: epoch 10, loss 3.6520602703094482, acc=0.04444444552063942, loss=3.6520602703094482
train: epoch 11, loss 2.727994918823242, acc=0.12472222000360489, loss=2.727994918823242
test: epoch 11, loss 3.6254730224609375, acc=0.05277777835726738, loss=3.6254730224609375
train: epoch 12, loss 2.691105842590332, acc=0.1317777782678604, loss=2.691105842590332
test: epoch 12, loss 3.52477765083313, acc=0.05833333358168602, loss=3.52477765083313
train: epoch 13, loss 2.6650025844573975, acc=0.1335555613040924, loss=2.6650025844573975
test: epoch 13, loss 3.5223095417022705, acc=0.05833333358168602, loss=3.5223095417022705
train: epoch 14, loss 2.6355059146881104, acc=0.13449999690055847, loss=2.6355059146881104
test: epoch 14, loss 3.51708722114563, acc=0.0555555559694767, loss=3.51708722114563
train: epoch 15, loss 2.6158387660980225, acc=0.1396111100912094, loss=2.6158387660980225
test: epoch 15, loss 3.4258878231048584, acc=0.0555555559694767, loss=3.4258878231048584
train: epoch 16, loss 2.5942177772521973, acc=0.1449444442987442, loss=2.5942177772521973
test: epoch 16, loss 3.414050340652466, acc=0.06111111119389534, loss=3.414050340652466
train: epoch 17, loss 2.5761237144470215, acc=0.14149999618530273, loss=2.5761237144470215
test: epoch 17, loss 3.4054524898529053, acc=0.06111111119389534, loss=3.4054524898529053
train: epoch 18, loss 2.5603952407836914, acc=0.14911110699176788, loss=2.5603952407836914
test: epoch 18, loss 3.396984338760376, acc=0.05833333358168602, loss=3.396984338760376
train: epoch 19, loss 2.549666166305542, acc=0.1548333317041397, loss=2.549666166305542
test: epoch 19, loss 3.3146421909332275, acc=0.05833333358168602, loss=3.3146421909332275
train: epoch 20, loss 2.5348894596099854, acc=0.15194444358348846, loss=2.5348894596099854
test: epoch 20, loss 3.3487184047698975, acc=0.06388889253139496, loss=3.3487184047698975
train: epoch 21, loss 2.525465965270996, acc=0.15627777576446533, loss=2.525465965270996
test: epoch 21, loss 3.313748359680176, acc=0.06388889253139496, loss=3.313748359680176
train: epoch 22, loss 2.5127158164978027, acc=0.156333327293396, loss=2.5127158164978027
test: epoch 22, loss 3.2933099269866943, acc=0.06666667014360428, loss=3.2933099269866943
train: epoch 23, loss 2.5060384273529053, acc=0.15516667068004608, loss=2.5060384273529053
test: epoch 23, loss 3.255268096923828, acc=0.06388889253139496, loss=3.255268096923828
train: epoch 24, loss 2.490648031234741, acc=0.16050000488758087, loss=2.490648031234741
test: epoch 24, loss 3.2462234497070312, acc=0.07777778059244156, loss=3.2462234497070312
train: epoch 25, loss 2.4927172660827637, acc=0.16261111199855804, loss=2.4927172660827637
test: epoch 25, loss 3.2930610179901123, acc=0.0694444477558136, loss=3.2930610179901123
train: epoch 26, loss 2.4796230792999268, acc=0.15916666388511658, loss=2.4796230792999268
test: epoch 26, loss 3.251516819000244, acc=0.07777778059244156, loss=3.251516819000244
train: epoch 27, loss 2.472417116165161, acc=0.1657777726650238, loss=2.472417116165161
test: epoch 27, loss 3.2743189334869385, acc=0.08055555820465088, loss=3.2743189334869385
train: epoch 28, loss 2.471212148666382, acc=0.16883333027362823, loss=2.471212148666382
test: epoch 28, loss 3.2416064739227295, acc=0.0833333358168602, loss=3.2416064739227295
train: epoch 29, loss 2.4463372230529785, acc=0.16899999976158142, loss=2.4463372230529785
test: epoch 29, loss 3.173954963684082, acc=0.09166666865348816, loss=3.173954963684082
train: epoch 30, loss 2.442317485809326, acc=0.16822221875190735, loss=2.442317485809326
test: epoch 30, loss 3.121985912322998, acc=0.09166666865348816, loss=3.121985912322998
train: epoch 31, loss 2.4163811206817627, acc=0.17166666686534882, loss=2.4163811206817627
test: epoch 31, loss 3.19233775138855, acc=0.09166666865348816, loss=3.19233775138855
train: epoch 32, loss 2.416792631149292, acc=0.17299999296665192, loss=2.416792631149292
test: epoch 32, loss 3.111265182495117, acc=0.0972222238779068, loss=3.111265182495117
train: epoch 33, loss 2.4053103923797607, acc=0.17572222650051117, loss=2.4053103923797607
test: epoch 33, loss 3.0805745124816895, acc=0.10000000149011612, loss=3.0805745124816895
train: epoch 34, loss 2.3872947692871094, acc=0.18283332884311676, loss=2.3872947692871094
test: epoch 34, loss 3.0397109985351562, acc=0.10000000149011612, loss=3.0397109985351562
train: epoch 35, loss 2.3771426677703857, acc=0.18344444036483765, loss=2.3771426677703857
test: epoch 35, loss 3.0526115894317627, acc=0.10555555671453476, loss=3.0526115894317627
train: epoch 36, loss 2.364941358566284, acc=0.18294444680213928, loss=2.364941358566284
test: epoch 36, loss 3.003146171569824, acc=0.11388888955116272, loss=3.003146171569824
train: epoch 37, loss 2.3389816284179688, acc=0.18861110508441925, loss=2.3389816284179688
test: epoch 37, loss 2.9669241905212402, acc=0.11944444477558136, loss=2.9669241905212402
train: epoch 38, loss 2.324124813079834, acc=0.1910555511713028, loss=2.324124813079834
test: epoch 38, loss 2.8491642475128174, acc=0.12222222238779068, loss=2.8491642475128174
train: epoch 39, loss 2.316370725631714, acc=0.19722221791744232, loss=2.316370725631714
test: epoch 39, loss 2.7739341259002686, acc=0.12777778506278992, loss=2.7739341259002686
train: epoch 40, loss 2.290616989135742, acc=0.1975555568933487, loss=2.290616989135742
test: epoch 40, loss 2.7540018558502197, acc=0.13055555522441864, loss=2.7540018558502197
train: epoch 41, loss 2.28402042388916, acc=0.20422221720218658, loss=2.28402042388916
test: epoch 41, loss 2.5984508991241455, acc=0.14722222089767456, loss=2.5984508991241455
train: epoch 42, loss 2.252136707305908, acc=0.21244443953037262, loss=2.252136707305908
test: epoch 42, loss 2.498340606689453, acc=0.16111111640930176, loss=2.498340606689453
train: epoch 43, loss 2.256646156311035, acc=0.21133333444595337, loss=2.256646156311035
test: epoch 43, loss 2.394401788711548, acc=0.16111111640930176, loss=2.394401788711548
train: epoch 44, loss 2.231072425842285, acc=0.20933333039283752, loss=2.231072425842285
test: epoch 44, loss 2.347951889038086, acc=0.16944444179534912, loss=2.347951889038086
train: epoch 45, loss 2.202737331390381, acc=0.21666666865348816, loss=2.202737331390381
test: epoch 45, loss 2.2791008949279785, acc=0.17777778208255768, loss=2.2791008949279785
train: epoch 46, loss 2.187506914138794, acc=0.21622222661972046, loss=2.187506914138794
test: epoch 46, loss 2.246457576751709, acc=0.17499999701976776, loss=2.246457576751709
train: epoch 47, loss 2.179380416870117, acc=0.22333332896232605, loss=2.179380416870117
test: epoch 47, loss 2.164212942123413, acc=0.17777778208255768, loss=2.164212942123413
train: epoch 48, loss 2.154489517211914, acc=0.22994443774223328, loss=2.154489517211914
test: epoch 48, loss 2.14337158203125, acc=0.1805555522441864, loss=2.14337158203125
train: epoch 49, loss 2.146130084991455, acc=0.2290000021457672, loss=2.146130084991455
test: epoch 49, loss 2.12971830368042, acc=0.18333333730697632, loss=2.12971830368042
train: epoch 50, loss 2.1276121139526367, acc=0.23038889467716217, loss=2.1276121139526367
test: epoch 50, loss 2.1217544078826904, acc=0.17777778208255768, loss=2.1217544078826904
train: epoch 51, loss 2.1143290996551514, acc=0.2363888919353485, loss=2.1143290996551514
test: epoch 51, loss 2.0828840732574463, acc=0.18333333730697632, loss=2.0828840732574463
train: epoch 52, loss 2.1033413410186768, acc=0.23933333158493042, loss=2.1033413410186768
test: epoch 52, loss 2.0885679721832275, acc=0.18611110746860504, loss=2.0885679721832275
train: epoch 53, loss 2.0936028957366943, acc=0.24238888919353485, loss=2.0936028957366943
test: epoch 53, loss 2.0689098834991455, acc=0.1805555522441864, loss=2.0689098834991455
train: epoch 54, loss 2.075831651687622, acc=0.24988888204097748, loss=2.075831651687622
test: epoch 54, loss 2.061408281326294, acc=0.18333333730697632, loss=2.061408281326294
train: epoch 55, loss 2.0358035564422607, acc=0.25083333253860474, loss=2.0358035564422607
test: epoch 55, loss 2.0544397830963135, acc=0.18611110746860504, loss=2.0544397830963135
train: epoch 56, loss 2.0343844890594482, acc=0.25477778911590576, loss=2.0343844890594482
test: epoch 56, loss 2.0439140796661377, acc=0.18888889253139496, loss=2.0439140796661377
train: epoch 57, loss 2.029245376586914, acc=0.25822222232818604, loss=2.029245376586914
test: epoch 57, loss 2.017068386077881, acc=0.19722221791744232, loss=2.017068386077881
train: epoch 58, loss 2.020313262939453, acc=0.25627776980400085, loss=2.020313262939453
test: epoch 58, loss 2.0075676441192627, acc=0.19722221791744232, loss=2.0075676441192627
train: epoch 59, loss 1.9964834451675415, acc=0.2625555694103241, loss=1.9964834451675415
test: epoch 59, loss 2.008280038833618, acc=0.20277777314186096, loss=2.008280038833618
train: epoch 60, loss 1.978975772857666, acc=0.2689444422721863, loss=1.978975772857666
test: epoch 60, loss 2.0030758380889893, acc=0.20555555820465088, loss=2.0030758380889893
train: epoch 61, loss 1.9754066467285156, acc=0.2691666781902313, loss=1.9754066467285156
test: epoch 61, loss 1.979820966720581, acc=0.20277777314186096, loss=1.979820966720581
train: epoch 62, loss 1.9552297592163086, acc=0.2724444568157196, loss=1.9552297592163086
test: epoch 62, loss 1.9641177654266357, acc=0.2083333283662796, loss=1.9641177654266357
train: epoch 63, loss 1.947375774383545, acc=0.27105554938316345, loss=1.947375774383545
test: epoch 63, loss 1.9437659978866577, acc=0.21388888359069824, loss=1.9437659978866577
train: epoch 64, loss 1.9313042163848877, acc=0.2778888940811157, loss=1.9313042163848877
test: epoch 64, loss 1.9537925720214844, acc=0.2083333283662796, loss=1.9537925720214844
train: epoch 65, loss 1.930390477180481, acc=0.2863333225250244, loss=1.930390477180481
test: epoch 65, loss 1.9514724016189575, acc=0.21388888359069824, loss=1.9514724016189575
train: epoch 66, loss 1.9109638929367065, acc=0.28733333945274353, loss=1.9109638929367065
test: epoch 66, loss 1.9401485919952393, acc=0.2222222238779068, loss=1.9401485919952393
train: epoch 67, loss 1.8935363292694092, acc=0.28777778148651123, loss=1.8935363292694092
test: epoch 67, loss 1.9238064289093018, acc=0.21666666865348816, loss=1.9238064289093018
train: epoch 68, loss 1.896068811416626, acc=0.29233333468437195, loss=1.896068811416626
test: epoch 68, loss 1.9090274572372437, acc=0.21388888359069824, loss=1.9090274572372437
train: epoch 69, loss 1.870306134223938, acc=0.2937222123146057, loss=1.870306134223938
test: epoch 69, loss 1.9167572259902954, acc=0.21944443881511688, loss=1.9167572259902954
train: epoch 70, loss 1.8555045127868652, acc=0.29511111974716187, loss=1.8555045127868652
test: epoch 70, loss 1.9083178043365479, acc=0.21111111342906952, loss=1.9083178043365479
train: epoch 71, loss 1.8513394594192505, acc=0.30566665530204773, loss=1.8513394594192505
test: epoch 71, loss 1.89593505859375, acc=0.21111111342906952, loss=1.89593505859375
train: epoch 72, loss 1.8495912551879883, acc=0.3055555522441864, loss=1.8495912551879883
test: epoch 72, loss 1.8673216104507446, acc=0.21944443881511688, loss=1.8673216104507446
train: epoch 73, loss 1.8385051488876343, acc=0.3050000071525574, loss=1.8385051488876343
test: epoch 73, loss 1.852974534034729, acc=0.2222222238779068, loss=1.852974534034729
train: epoch 74, loss 1.813767671585083, acc=0.308555543422699, loss=1.813767671585083
test: epoch 74, loss 1.8545600175857544, acc=0.22777777910232544, loss=1.8545600175857544
train: epoch 75, loss 1.818705677986145, acc=0.31405556201934814, loss=1.818705677986145
test: epoch 75, loss 1.8565179109573364, acc=0.21944443881511688, loss=1.8565179109573364
train: epoch 76, loss 1.789249062538147, acc=0.3200555443763733, loss=1.789249062538147
test: epoch 76, loss 1.8551993370056152, acc=0.21944443881511688, loss=1.8551993370056152
train: epoch 77, loss 1.8034741878509521, acc=0.31949999928474426, loss=1.8034741878509521
test: epoch 77, loss 1.8607940673828125, acc=0.21944443881511688, loss=1.8607940673828125
train: epoch 78, loss 1.793845772743225, acc=0.3191666603088379, loss=1.793845772743225
test: epoch 78, loss 1.8621069192886353, acc=0.2222222238779068, loss=1.8621069192886353
train: epoch 79, loss 1.784453272819519, acc=0.324611097574234, loss=1.784453272819519
test: epoch 79, loss 1.8453483581542969, acc=0.22499999403953552, loss=1.8453483581542969
train: epoch 80, loss 1.767755150794983, acc=0.324055552482605, loss=1.767755150794983
test: epoch 80, loss 1.8411065340042114, acc=0.21944443881511688, loss=1.8411065340042114
train: epoch 81, loss 1.7528780698776245, acc=0.32938888669013977, loss=1.7528780698776245
test: epoch 81, loss 1.8275450468063354, acc=0.22777777910232544, loss=1.8275450468063354
train: epoch 82, loss 1.7466027736663818, acc=0.3371666669845581, loss=1.7466027736663818
test: epoch 82, loss 1.8190807104110718, acc=0.2222222238779068, loss=1.8190807104110718
train: epoch 83, loss 1.7561683654785156, acc=0.33149999380111694, loss=1.7561683654785156
test: epoch 83, loss 1.823425054550171, acc=0.2222222238779068, loss=1.823425054550171
train: epoch 84, loss 1.7377147674560547, acc=0.3364444375038147, loss=1.7377147674560547
test: epoch 84, loss 1.8245582580566406, acc=0.2222222238779068, loss=1.8245582580566406
train: epoch 85, loss 1.719807744026184, acc=0.3392777740955353, loss=1.719807744026184
test: epoch 85, loss 1.812897801399231, acc=0.23055554926395416, loss=1.812897801399231
train: epoch 86, loss 1.7079027891159058, acc=0.3449999988079071, loss=1.7079027891159058
test: epoch 86, loss 1.7915886640548706, acc=0.22777777910232544, loss=1.7915886640548706
train: epoch 87, loss 1.7138158082962036, acc=0.3444444537162781, loss=1.7138158082962036
test: epoch 87, loss 1.7990405559539795, acc=0.22777777910232544, loss=1.7990405559539795
train: epoch 88, loss 1.687074899673462, acc=0.35233333706855774, loss=1.687074899673462
test: epoch 88, loss 1.7989749908447266, acc=0.23055554926395416, loss=1.7989749908447266
train: epoch 89, loss 1.6896361112594604, acc=0.35066667199134827, loss=1.6896361112594604
test: epoch 89, loss 1.78220796585083, acc=0.23333333432674408, loss=1.78220796585083
train: epoch 90, loss 1.690814733505249, acc=0.3490000069141388, loss=1.690814733505249
test: epoch 90, loss 1.7937175035476685, acc=0.22777777910232544, loss=1.7937175035476685
train: epoch 91, loss 1.668562412261963, acc=0.3564999997615814, loss=1.668562412261963
test: epoch 91, loss 1.791115641593933, acc=0.22777777910232544, loss=1.791115641593933
train: epoch 92, loss 1.6629949808120728, acc=0.3606666624546051, loss=1.6629949808120728
test: epoch 92, loss 1.7776681184768677, acc=0.22777777910232544, loss=1.7776681184768677
train: epoch 93, loss 1.651230812072754, acc=0.35955554246902466, loss=1.651230812072754
test: epoch 93, loss 1.7860721349716187, acc=0.22777777910232544, loss=1.7860721349716187
train: epoch 94, loss 1.6525799036026, acc=0.36016666889190674, loss=1.6525799036026
test: epoch 94, loss 1.7844218015670776, acc=0.2361111044883728, loss=1.7844218015670776
train: epoch 95, loss 1.6523832082748413, acc=0.3684999942779541, loss=1.6523832082748413
test: epoch 95, loss 1.7532545328140259, acc=0.23333333432674408, loss=1.7532545328140259
train: epoch 96, loss 1.6375325918197632, acc=0.3711666762828827, loss=1.6375325918197632
test: epoch 96, loss 1.7651137113571167, acc=0.23333333432674408, loss=1.7651137113571167
train: epoch 97, loss 1.6184265613555908, acc=0.37166666984558105, loss=1.6184265613555908
test: epoch 97, loss 1.7615400552749634, acc=0.2361111044883728, loss=1.7615400552749634
train: epoch 98, loss 1.6285957098007202, acc=0.3736666738986969, loss=1.6285957098007202
test: epoch 98, loss 1.7650058269500732, acc=0.22777777910232544, loss=1.7650058269500732
train: epoch 99, loss 1.6290394067764282, acc=0.3733333349227905, loss=1.6290394067764282
test: epoch 99, loss 1.7657095193862915, acc=0.23055554926395416, loss=1.7657095193862915
train: epoch 100, loss 1.626615285873413, acc=0.37327778339385986, loss=1.626615285873413
test: epoch 100, loss 1.7527040243148804, acc=0.23333333432674408, loss=1.7527040243148804
train: epoch 101, loss 1.613243579864502, acc=0.3819444477558136, loss=1.613243579864502
test: epoch 101, loss 1.763115644454956, acc=0.23888888955116272, loss=1.763115644454956
train: epoch 102, loss 1.605172038078308, acc=0.3831111192703247, loss=1.605172038078308
test: epoch 102, loss 1.75387442111969, acc=0.2361111044883728, loss=1.75387442111969
train: epoch 103, loss 1.6026049852371216, acc=0.38688889145851135, loss=1.6026049852371216
test: epoch 103, loss 1.7474077939987183, acc=0.24444444477558136, loss=1.7474077939987183
train: epoch 104, loss 1.5915734767913818, acc=0.3813333213329315, loss=1.5915734767913818
test: epoch 104, loss 1.7451155185699463, acc=0.24722221493721008, loss=1.7451155185699463
train: epoch 105, loss 1.5947809219360352, acc=0.38777777552604675, loss=1.5947809219360352
test: epoch 105, loss 1.7482378482818604, acc=0.2527777850627899, loss=1.7482378482818604
train: epoch 106, loss 1.571832537651062, acc=0.3925555646419525, loss=1.571832537651062
test: epoch 106, loss 1.765485167503357, acc=0.24722221493721008, loss=1.765485167503357
train: epoch 107, loss 1.5590611696243286, acc=0.3943333327770233, loss=1.5590611696243286
test: epoch 107, loss 1.7486275434494019, acc=0.24444444477558136, loss=1.7486275434494019
train: epoch 108, loss 1.561181902885437, acc=0.3921666741371155, loss=1.561181902885437
test: epoch 108, loss 1.7626802921295166, acc=0.24722221493721008, loss=1.7626802921295166
train: epoch 109, loss 1.5607540607452393, acc=0.39705556631088257, loss=1.5607540607452393
test: epoch 109, loss 1.7443257570266724, acc=0.24722221493721008, loss=1.7443257570266724
train: epoch 110, loss 1.5613138675689697, acc=0.39811110496520996, loss=1.5613138675689697
test: epoch 110, loss 1.7326430082321167, acc=0.25, loss=1.7326430082321167
train: epoch 111, loss 1.5653154850006104, acc=0.39827778935432434, loss=1.5653154850006104
test: epoch 111, loss 1.7314327955245972, acc=0.2527777850627899, loss=1.7314327955245972
train: epoch 112, loss 1.5463188886642456, acc=0.40299999713897705, loss=1.5463188886642456
test: epoch 112, loss 1.7143256664276123, acc=0.25555557012557983, loss=1.7143256664276123
train: epoch 113, loss 1.5411196947097778, acc=0.4069444537162781, loss=1.5411196947097778
test: epoch 113, loss 1.7042210102081299, acc=0.2527777850627899, loss=1.7042210102081299
train: epoch 114, loss 1.5241245031356812, acc=0.40700000524520874, loss=1.5241245031356812
test: epoch 114, loss 1.7208166122436523, acc=0.2527777850627899, loss=1.7208166122436523
train: epoch 115, loss 1.526633381843567, acc=0.40361112356185913, loss=1.526633381843567
test: epoch 115, loss 1.7100424766540527, acc=0.2527777850627899, loss=1.7100424766540527
train: epoch 116, loss 1.5004559755325317, acc=0.4138333201408386, loss=1.5004559755325317
test: epoch 116, loss 1.702522873878479, acc=0.25555557012557983, loss=1.702522873878479
train: epoch 117, loss 1.5117793083190918, acc=0.41438889503479004, loss=1.5117793083190918
test: epoch 117, loss 1.7017358541488647, acc=0.2611111104488373, loss=1.7017358541488647
train: epoch 118, loss 1.5176082849502563, acc=0.41144445538520813, loss=1.5176082849502563
test: epoch 118, loss 1.6950777769088745, acc=0.2666666805744171, loss=1.6950777769088745
train: epoch 119, loss 1.502140998840332, acc=0.4202222228050232, loss=1.502140998840332
test: epoch 119, loss 1.711810827255249, acc=0.25833332538604736, loss=1.711810827255249
train: epoch 120, loss 1.5106847286224365, acc=0.41438889503479004, loss=1.5106847286224365
test: epoch 120, loss 1.6974141597747803, acc=0.2611111104488373, loss=1.6974141597747803
train: epoch 121, loss 1.484511137008667, acc=0.42072221636772156, loss=1.484511137008667
test: epoch 121, loss 1.6979613304138184, acc=0.2611111104488373, loss=1.6979613304138184
train: epoch 122, loss 1.4985295534133911, acc=0.4216666519641876, loss=1.4985295534133911
test: epoch 122, loss 1.7065637111663818, acc=0.2638888955116272, loss=1.7065637111663818
train: epoch 123, loss 1.4779731035232544, acc=0.42633333802223206, loss=1.4779731035232544
test: epoch 123, loss 1.70751953125, acc=0.2638888955116272, loss=1.70751953125
train: epoch 124, loss 1.4868273735046387, acc=0.42472222447395325, loss=1.4868273735046387
test: epoch 124, loss 1.7090275287628174, acc=0.2638888955116272, loss=1.7090275287628174
train: epoch 125, loss 1.4833078384399414, acc=0.42572221159935, loss=1.4833078384399414
test: epoch 125, loss 1.6921025514602661, acc=0.2666666805744171, loss=1.6921025514602661
train: epoch 126, loss 1.4775996208190918, acc=0.42933332920074463, loss=1.4775996208190918
test: epoch 126, loss 1.6919628381729126, acc=0.2666666805744171, loss=1.6919628381729126
train: epoch 127, loss 1.4625778198242188, acc=0.4318888783454895, loss=1.4625778198242188
test: epoch 127, loss 1.7019141912460327, acc=0.2666666805744171, loss=1.7019141912460327
train: epoch 128, loss 1.4583072662353516, acc=0.43105554580688477, loss=1.4583072662353516
test: epoch 128, loss 1.6897213459014893, acc=0.2666666805744171, loss=1.6897213459014893
train: epoch 129, loss 1.4589372873306274, acc=0.4307222366333008, loss=1.4589372873306274
test: epoch 129, loss 1.6972650289535522, acc=0.2666666805744171, loss=1.6972650289535522
train: epoch 130, loss 1.4640954732894897, acc=0.4362777769565582, loss=1.4640954732894897
test: epoch 130, loss 1.6969355344772339, acc=0.2666666805744171, loss=1.6969355344772339
train: epoch 131, loss 1.458268642425537, acc=0.4345555603504181, loss=1.458268642425537
test: epoch 131, loss 1.6960757970809937, acc=0.2638888955116272, loss=1.6960757970809937
train: epoch 132, loss 1.443910002708435, acc=0.4359999895095825, loss=1.443910002708435
test: epoch 132, loss 1.6975890398025513, acc=0.2666666805744171, loss=1.6975890398025513
train: epoch 133, loss 1.4437355995178223, acc=0.44111111760139465, loss=1.4437355995178223
test: epoch 133, loss 1.6988998651504517, acc=0.2638888955116272, loss=1.6988998651504517
train: epoch 134, loss 1.430608868598938, acc=0.44305557012557983, loss=1.430608868598938
test: epoch 134, loss 1.6926478147506714, acc=0.2638888955116272, loss=1.6926478147506714
train: epoch 135, loss 1.4376815557479858, acc=0.44438889622688293, loss=1.4376815557479858
test: epoch 135, loss 1.6997158527374268, acc=0.2638888955116272, loss=1.6997158527374268
train: epoch 136, loss 1.4499298334121704, acc=0.448888897895813, loss=1.4499298334121704
test: epoch 136, loss 1.6861470937728882, acc=0.2666666805744171, loss=1.6861470937728882
train: epoch 137, loss 1.4310559034347534, acc=0.44894444942474365, loss=1.4310559034347534
test: epoch 137, loss 1.684384822845459, acc=0.2666666805744171, loss=1.684384822845459
train: epoch 138, loss 1.4264014959335327, acc=0.4481111168861389, loss=1.4264014959335327
test: epoch 138, loss 1.682194709777832, acc=0.2666666805744171, loss=1.682194709777832
train: epoch 139, loss 1.4197591543197632, acc=0.45222222805023193, loss=1.4197591543197632
test: epoch 139, loss 1.6818854808807373, acc=0.26944443583488464, loss=1.6818854808807373
train: epoch 140, loss 1.404837965965271, acc=0.4503333270549774, loss=1.404837965965271
test: epoch 140, loss 1.6885197162628174, acc=0.27222222089767456, loss=1.6885197162628174
train: epoch 141, loss 1.4108309745788574, acc=0.4487222135066986, loss=1.4108309745788574
test: epoch 141, loss 1.6846232414245605, acc=0.27222222089767456, loss=1.6846232414245605
train: epoch 142, loss 1.4003227949142456, acc=0.45872223377227783, loss=1.4003227949142456
test: epoch 142, loss 1.6936074495315552, acc=0.27222222089767456, loss=1.6936074495315552
train: epoch 143, loss 1.3771008253097534, acc=0.456888884305954, loss=1.3771008253097534
test: epoch 143, loss 1.6950558423995972, acc=0.2750000059604645, loss=1.6950558423995972
train: epoch 144, loss 1.3903050422668457, acc=0.4565555453300476, loss=1.3903050422668457
test: epoch 144, loss 1.6984751224517822, acc=0.27222222089767456, loss=1.6984751224517822
train: epoch 145, loss 1.3937872648239136, acc=0.4593888819217682, loss=1.3937872648239136
test: epoch 145, loss 1.6974595785140991, acc=0.27222222089767456, loss=1.6974595785140991
train: epoch 146, loss 1.3965286016464233, acc=0.4549444317817688, loss=1.3965286016464233
test: epoch 146, loss 1.6866135597229004, acc=0.2750000059604645, loss=1.6866135597229004
train: epoch 147, loss 1.394309639930725, acc=0.4602222144603729, loss=1.394309639930725
test: epoch 147, loss 1.6858158111572266, acc=0.2777777910232544, loss=1.6858158111572266
train: epoch 148, loss 1.379970908164978, acc=0.46511110663414, loss=1.379970908164978
test: epoch 148, loss 1.7028703689575195, acc=0.27222222089767456, loss=1.7028703689575195
train: epoch 149, loss 1.3677617311477661, acc=0.4651666581630707, loss=1.3677617311477661
test: epoch 149, loss 1.684340000152588, acc=0.2805555462837219, loss=1.684340000152588
train: epoch 150, loss 1.3718582391738892, acc=0.4643888771533966, loss=1.3718582391738892
test: epoch 150, loss 1.6786233186721802, acc=0.28333333134651184, loss=1.6786233186721802
