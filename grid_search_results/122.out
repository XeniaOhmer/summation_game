# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=100", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.99", "--one_hot=true", "--n_layers=4"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=4, n_summands=2, n_symbols=100, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=588787755, receiver_embed_dim=128, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.933431625366211, acc=0.086888886988163, loss=2.933431625366211
test: epoch 1, loss 2.4093074798583984, acc=0.12777778506278992, loss=2.4093074798583984
train: epoch 2, loss 2.013028144836426, acc=0.21783334016799927, loss=2.013028144836426
test: epoch 2, loss 1.9073586463928223, acc=0.2527777850627899, loss=1.9073586463928223
train: epoch 3, loss 1.5706228017807007, acc=0.320166677236557, loss=1.5706228017807007
test: epoch 3, loss 1.6503052711486816, acc=0.3055555522441864, loss=1.6503052711486816
train: epoch 4, loss 1.4254155158996582, acc=0.3695000112056732, loss=1.4254155158996582
test: epoch 4, loss 1.650206208229065, acc=0.34166666865348816, loss=1.650206208229065
train: epoch 5, loss 1.3523430824279785, acc=0.3968888819217682, loss=1.3523430824279785
test: epoch 5, loss 1.5058917999267578, acc=0.3472222089767456, loss=1.5058917999267578
train: epoch 6, loss 1.2877131700515747, acc=0.426111102104187, loss=1.2877131700515747
test: epoch 6, loss 1.618308424949646, acc=0.3333333432674408, loss=1.618308424949646
train: epoch 7, loss 1.2722924947738647, acc=0.43761110305786133, loss=1.2722924947738647
test: epoch 7, loss 1.561928153038025, acc=0.35277777910232544, loss=1.561928153038025
train: epoch 8, loss 1.2408312559127808, acc=0.4565555453300476, loss=1.2408312559127808
test: epoch 8, loss 1.6734650135040283, acc=0.375, loss=1.6734650135040283
train: epoch 9, loss 1.2440173625946045, acc=0.46266666054725647, loss=1.2440173625946045
test: epoch 9, loss 1.396445631980896, acc=0.3777777850627899, loss=1.396445631980896
train: epoch 10, loss 1.1993621587753296, acc=0.4779999852180481, loss=1.1993621587753296
test: epoch 10, loss 1.6062660217285156, acc=0.375, loss=1.6062660217285156
train: epoch 11, loss 1.175626516342163, acc=0.4905555546283722, loss=1.175626516342163
test: epoch 11, loss 1.5560503005981445, acc=0.3777777850627899, loss=1.5560503005981445
train: epoch 12, loss 1.1364916563034058, acc=0.5096666812896729, loss=1.1364916563034058
test: epoch 12, loss 1.6117421388626099, acc=0.38055557012557983, loss=1.6117421388626099
train: epoch 13, loss 1.0796103477478027, acc=0.534333348274231, loss=1.0796103477478027
test: epoch 13, loss 1.5497328042984009, acc=0.42222222685813904, loss=1.5497328042984009
train: epoch 14, loss 1.0787254571914673, acc=0.5321666598320007, loss=1.0787254571914673
test: epoch 14, loss 1.3553756475448608, acc=0.42222222685813904, loss=1.3553756475448608
train: epoch 15, loss 1.0191656351089478, acc=0.5515555739402771, loss=1.0191656351089478
test: epoch 15, loss 1.4414191246032715, acc=0.42222222685813904, loss=1.4414191246032715
train: epoch 16, loss 1.0049145221710205, acc=0.5479999780654907, loss=1.0049145221710205
test: epoch 16, loss 1.382918357849121, acc=0.4166666567325592, loss=1.382918357849121
train: epoch 17, loss 0.9864537119865417, acc=0.5633333325386047, loss=0.9864537119865417
test: epoch 17, loss 1.4023528099060059, acc=0.4194444417953491, loss=1.4023528099060059
train: epoch 18, loss 0.9749671220779419, acc=0.5574444532394409, loss=0.9749671220779419
test: epoch 18, loss 1.4629374742507935, acc=0.42222222685813904, loss=1.4629374742507935
train: epoch 19, loss 0.9375827312469482, acc=0.5810555815696716, loss=0.9375827312469482
test: epoch 19, loss 1.466506004333496, acc=0.4166666567325592, loss=1.466506004333496
train: epoch 20, loss 0.9437044858932495, acc=0.5771666765213013, loss=0.9437044858932495
test: epoch 20, loss 1.5203437805175781, acc=0.42222222685813904, loss=1.5203437805175781
train: epoch 21, loss 0.9131060242652893, acc=0.6022777557373047, loss=0.9131060242652893
test: epoch 21, loss 1.4484686851501465, acc=0.4194444417953491, loss=1.4484686851501465
train: epoch 22, loss 0.9227067232131958, acc=0.5932777523994446, loss=0.9227067232131958
test: epoch 22, loss 1.3643763065338135, acc=0.4166666567325592, loss=1.3643763065338135
train: epoch 23, loss 0.9074974060058594, acc=0.6013888716697693, loss=0.9074974060058594
test: epoch 23, loss 1.4047406911849976, acc=0.42500001192092896, loss=1.4047406911849976
train: epoch 24, loss 0.8875086307525635, acc=0.6058889031410217, loss=0.8875086307525635
test: epoch 24, loss 1.4899431467056274, acc=0.4166666567325592, loss=1.4899431467056274
train: epoch 25, loss 0.8479426503181458, acc=0.6183333396911621, loss=0.8479426503181458
test: epoch 25, loss 1.494657278060913, acc=0.42222222685813904, loss=1.494657278060913
train: epoch 26, loss 0.8662899136543274, acc=0.6163889169692993, loss=0.8662899136543274
test: epoch 26, loss 1.5206358432769775, acc=0.4194444417953491, loss=1.5206358432769775
train: epoch 27, loss 0.8939949870109558, acc=0.6089444160461426, loss=0.8939949870109558
test: epoch 27, loss 1.4639008045196533, acc=0.42222222685813904, loss=1.4639008045196533
train: epoch 28, loss 0.8470199108123779, acc=0.6244444251060486, loss=0.8470199108123779
test: epoch 28, loss 1.519112467765808, acc=0.42500001192092896, loss=1.519112467765808
train: epoch 29, loss 0.8452883958816528, acc=0.6262221932411194, loss=0.8452883958816528
test: epoch 29, loss 1.9144599437713623, acc=0.3722222149372101, loss=1.9144599437713623
train: epoch 30, loss 0.848446786403656, acc=0.6242777705192566, loss=0.848446786403656
test: epoch 30, loss 1.5151768922805786, acc=0.42222222685813904, loss=1.5151768922805786
train: epoch 31, loss 0.8644643425941467, acc=0.621055543422699, loss=0.8644643425941467
test: epoch 31, loss 1.6637722253799438, acc=0.4027777910232544, loss=1.6637722253799438
train: epoch 32, loss 0.8562623858451843, acc=0.6226666569709778, loss=0.8562623858451843
test: epoch 32, loss 1.5217734575271606, acc=0.43888887763023376, loss=1.5217734575271606
train: epoch 33, loss 0.8459770679473877, acc=0.6225000023841858, loss=0.8459770679473877
test: epoch 33, loss 1.4688793420791626, acc=0.45277777314186096, loss=1.4688793420791626
train: epoch 34, loss 0.8725865483283997, acc=0.6058889031410217, loss=0.8725865483283997
test: epoch 34, loss 1.2730612754821777, acc=0.45277777314186096, loss=1.2730612754821777
train: epoch 35, loss 0.7896863222122192, acc=0.6282777786254883, loss=0.7896863222122192
test: epoch 35, loss 1.4205290079116821, acc=0.4611110985279083, loss=1.4205290079116821
train: epoch 36, loss 0.8037638664245605, acc=0.6265555620193481, loss=0.8037638664245605
test: epoch 36, loss 1.1953991651535034, acc=0.4611110985279083, loss=1.1953991651535034
train: epoch 37, loss 0.7941693663597107, acc=0.6353333592414856, loss=0.7941693663597107
test: epoch 37, loss 1.243830680847168, acc=0.46388888359069824, loss=1.243830680847168
train: epoch 38, loss 0.8010731935501099, acc=0.6355000138282776, loss=0.8010731935501099
test: epoch 38, loss 1.4944071769714355, acc=0.43888887763023376, loss=1.4944071769714355
train: epoch 39, loss 0.794223427772522, acc=0.6398888826370239, loss=0.794223427772522
test: epoch 39, loss 1.4004347324371338, acc=0.4416666626930237, loss=1.4004347324371338
train: epoch 40, loss 0.7427225112915039, acc=0.656166672706604, loss=0.7427225112915039
test: epoch 40, loss 1.1926372051239014, acc=0.48055556416511536, loss=1.1926372051239014
train: epoch 41, loss 0.7205081582069397, acc=0.6612222194671631, loss=0.7205081582069397
test: epoch 41, loss 1.246652364730835, acc=0.5111111402511597, loss=1.246652364730835
train: epoch 42, loss 0.7099597454071045, acc=0.6709444522857666, loss=0.7099597454071045
test: epoch 42, loss 1.2182414531707764, acc=0.5166666507720947, loss=1.2182414531707764
train: epoch 43, loss 0.7228652238845825, acc=0.6722777485847473, loss=0.7228652238845825
test: epoch 43, loss 1.3274295330047607, acc=0.5111111402511597, loss=1.3274295330047607
train: epoch 44, loss 0.7207069396972656, acc=0.6767222285270691, loss=0.7207069396972656
test: epoch 44, loss 1.1993167400360107, acc=0.5333333611488342, loss=1.1993167400360107
train: epoch 45, loss 0.6942883133888245, acc=0.6809444427490234, loss=0.6942883133888245
test: epoch 45, loss 1.2023547887802124, acc=0.5138888955116272, loss=1.2023547887802124
train: epoch 46, loss 0.7122052311897278, acc=0.6750555634498596, loss=0.7122052311897278
test: epoch 46, loss 1.222806692123413, acc=0.5222222208976746, loss=1.222806692123413
train: epoch 47, loss 0.6838222146034241, acc=0.6818333268165588, loss=0.6838222146034241
test: epoch 47, loss 1.1738396883010864, acc=0.5305555462837219, loss=1.1738396883010864
train: epoch 48, loss 0.6676091551780701, acc=0.683388888835907, loss=0.6676091551780701
test: epoch 48, loss 1.2492302656173706, acc=0.5333333611488342, loss=1.2492302656173706
train: epoch 49, loss 0.6661772727966309, acc=0.6887221932411194, loss=0.6661772727966309
test: epoch 49, loss 1.2595425844192505, acc=0.5333333611488342, loss=1.2595425844192505
train: epoch 50, loss 0.6932191848754883, acc=0.6827222108840942, loss=0.6932191848754883
test: epoch 50, loss 1.32065749168396, acc=0.5111111402511597, loss=1.32065749168396
train: epoch 51, loss 0.6985791921615601, acc=0.6728888750076294, loss=0.6985791921615601
test: epoch 51, loss 1.2226905822753906, acc=0.5361111164093018, loss=1.2226905822753906
train: epoch 52, loss 0.7043296694755554, acc=0.6740000247955322, loss=0.7043296694755554
test: epoch 52, loss 1.0785695314407349, acc=0.5333333611488342, loss=1.0785695314407349
train: epoch 53, loss 0.6376715898513794, acc=0.6948333382606506, loss=0.6376715898513794
test: epoch 53, loss 1.16753089427948, acc=0.5416666865348816, loss=1.16753089427948
train: epoch 54, loss 0.671127438545227, acc=0.683388888835907, loss=0.671127438545227
test: epoch 54, loss 1.1405510902404785, acc=0.5361111164093018, loss=1.1405510902404785
train: epoch 55, loss 0.6554135680198669, acc=0.6819444298744202, loss=0.6554135680198669
test: epoch 55, loss 1.228703260421753, acc=0.5277777910232544, loss=1.228703260421753
train: epoch 56, loss 0.6378142237663269, acc=0.694944441318512, loss=0.6378142237663269
test: epoch 56, loss 1.1162059307098389, acc=0.574999988079071, loss=1.1162059307098389
train: epoch 57, loss 0.5995842218399048, acc=0.7044444680213928, loss=0.5995842218399048
test: epoch 57, loss 1.0945183038711548, acc=0.5805555582046509, loss=1.0945183038711548
train: epoch 58, loss 0.6341661214828491, acc=0.6924999952316284, loss=0.6341661214828491
test: epoch 58, loss 1.0738961696624756, acc=0.5777778029441833, loss=1.0738961696624756
train: epoch 59, loss 0.5883122682571411, acc=0.6918333172798157, loss=0.5883122682571411
test: epoch 59, loss 1.0862020254135132, acc=0.5805555582046509, loss=1.0862020254135132
train: epoch 60, loss 0.5936020612716675, acc=0.6883888840675354, loss=0.5936020612716675
test: epoch 60, loss 1.073310375213623, acc=0.5805555582046509, loss=1.073310375213623
train: epoch 61, loss 0.5894017219543457, acc=0.6896666884422302, loss=0.5894017219543457
test: epoch 61, loss 1.039221167564392, acc=0.6083333492279053, loss=1.039221167564392
train: epoch 62, loss 0.621680498123169, acc=0.6892222166061401, loss=0.621680498123169
test: epoch 62, loss 0.9542000889778137, acc=0.605555534362793, loss=0.9542000889778137
train: epoch 63, loss 0.6079067587852478, acc=0.7021111249923706, loss=0.6079067587852478
test: epoch 63, loss 0.9549104571342468, acc=0.605555534362793, loss=0.9549104571342468
train: epoch 64, loss 0.6455980539321899, acc=0.6911110877990723, loss=0.6455980539321899
test: epoch 64, loss 0.9635303020477295, acc=0.6000000238418579, loss=0.9635303020477295
train: epoch 65, loss 0.670590341091156, acc=0.6817222237586975, loss=0.670590341091156
test: epoch 65, loss 0.9880898594856262, acc=0.5861111283302307, loss=0.9880898594856262
train: epoch 66, loss 0.6341865658760071, acc=0.6901111006736755, loss=0.6341865658760071
test: epoch 66, loss 0.9079975485801697, acc=0.6277777552604675, loss=0.9079975485801697
train: epoch 67, loss 0.6088514924049377, acc=0.6990000009536743, loss=0.6088514924049377
test: epoch 67, loss 0.8264067769050598, acc=0.6555555462837219, loss=0.8264067769050598
train: epoch 68, loss 0.5649292469024658, acc=0.7093889117240906, loss=0.5649292469024658
test: epoch 68, loss 0.778679609298706, acc=0.6666666865348816, loss=0.778679609298706
train: epoch 69, loss 0.5480254888534546, acc=0.7088333368301392, loss=0.5480254888534546
test: epoch 69, loss 0.7810174226760864, acc=0.6638888716697693, loss=0.7810174226760864
train: epoch 70, loss 0.5606381893157959, acc=0.7122777700424194, loss=0.5606381893157959
test: epoch 70, loss 0.7236817479133606, acc=0.6666666865348816, loss=0.7236817479133606
train: epoch 71, loss 0.54920893907547, acc=0.7173333168029785, loss=0.54920893907547
test: epoch 71, loss 0.7937966585159302, acc=0.6638888716697693, loss=0.7937966585159302
train: epoch 72, loss 0.5739617347717285, acc=0.7171111106872559, loss=0.5739617347717285
test: epoch 72, loss 0.808711588382721, acc=0.6861110925674438, loss=0.808711588382721
train: epoch 73, loss 0.563528299331665, acc=0.7162777781486511, loss=0.563528299331665
test: epoch 73, loss 0.7641839385032654, acc=0.6888889074325562, loss=0.7641839385032654
train: epoch 74, loss 0.520870566368103, acc=0.7235555648803711, loss=0.520870566368103
test: epoch 74, loss 0.6407829523086548, acc=0.699999988079071, loss=0.6407829523086548
train: epoch 75, loss 0.5391314029693604, acc=0.7166666388511658, loss=0.5391314029693604
test: epoch 75, loss 0.6581771373748779, acc=0.6944444179534912, loss=0.6581771373748779
train: epoch 76, loss 0.5186798572540283, acc=0.7186111211776733, loss=0.5186798572540283
test: epoch 76, loss 0.6427022218704224, acc=0.6888889074325562, loss=0.6427022218704224
train: epoch 77, loss 0.5071037411689758, acc=0.7252222299575806, loss=0.5071037411689758
test: epoch 77, loss 0.6061822175979614, acc=0.7027778029441833, loss=0.6061822175979614
train: epoch 78, loss 0.4964841604232788, acc=0.7273889183998108, loss=0.4964841604232788
test: epoch 78, loss 0.5891865491867065, acc=0.7027778029441833, loss=0.5891865491867065
train: epoch 79, loss 0.503566563129425, acc=0.7289999723434448, loss=0.503566563129425
test: epoch 79, loss 0.7691262364387512, acc=0.6916666626930237, loss=0.7691262364387512
train: epoch 80, loss 0.5324583649635315, acc=0.725777804851532, loss=0.5324583649635315
test: epoch 80, loss 0.5986112952232361, acc=0.7055555582046509, loss=0.5986112952232361
train: epoch 81, loss 0.49338462948799133, acc=0.7292222380638123, loss=0.49338462948799133
test: epoch 81, loss 0.6699467897415161, acc=0.7055555582046509, loss=0.6699467897415161
train: epoch 82, loss 0.4976850748062134, acc=0.7264999747276306, loss=0.4976850748062134
test: epoch 82, loss 0.5994937419891357, acc=0.7055555582046509, loss=0.5994937419891357
train: epoch 83, loss 0.6067031621932983, acc=0.7141110897064209, loss=0.6067031621932983
test: epoch 83, loss 0.6104685664176941, acc=0.7083333134651184, loss=0.6104685664176941
train: epoch 84, loss 0.5274540185928345, acc=0.718500018119812, loss=0.5274540185928345
test: epoch 84, loss 0.5885710716247559, acc=0.7138888835906982, loss=0.5885710716247559
train: epoch 85, loss 0.5176978707313538, acc=0.7236111164093018, loss=0.5176978707313538
test: epoch 85, loss 0.625706136226654, acc=0.7138888835906982, loss=0.625706136226654
train: epoch 86, loss 0.5339870452880859, acc=0.7262222170829773, loss=0.5339870452880859
test: epoch 86, loss 0.6244879961013794, acc=0.6888889074325562, loss=0.6244879961013794
train: epoch 87, loss 0.5210721492767334, acc=0.7350555658340454, loss=0.5210721492767334
test: epoch 87, loss 0.5978827476501465, acc=0.7194444537162781, loss=0.5978827476501465
train: epoch 88, loss 0.5013839602470398, acc=0.7424444556236267, loss=0.5013839602470398
test: epoch 88, loss 0.6806924343109131, acc=0.7222222089767456, loss=0.6806924343109131
train: epoch 89, loss 0.49303874373435974, acc=0.7441666722297668, loss=0.49303874373435974
test: epoch 89, loss 0.60091632604599, acc=0.730555534362793, loss=0.60091632604599
train: epoch 90, loss 0.5402835607528687, acc=0.7321110963821411, loss=0.5402835607528687
test: epoch 90, loss 0.6051221489906311, acc=0.7222222089767456, loss=0.6051221489906311
train: epoch 91, loss 0.5334151387214661, acc=0.7338888645172119, loss=0.5334151387214661
test: epoch 91, loss 0.6364351511001587, acc=0.7138888835906982, loss=0.6364351511001587
train: epoch 92, loss 0.5472449064254761, acc=0.738111138343811, loss=0.5472449064254761
test: epoch 92, loss 0.5492615699768066, acc=0.7277777791023254, loss=0.5492615699768066
train: epoch 93, loss 0.5394269824028015, acc=0.7356111407279968, loss=0.5394269824028015
test: epoch 93, loss 0.5934419631958008, acc=0.7277777791023254, loss=0.5934419631958008
train: epoch 94, loss 0.513908863067627, acc=0.7370555400848389, loss=0.513908863067627
test: epoch 94, loss 0.576006293296814, acc=0.7277777791023254, loss=0.576006293296814
train: epoch 95, loss 0.5137988924980164, acc=0.7366111278533936, loss=0.5137988924980164
test: epoch 95, loss 0.5839173793792725, acc=0.7277777791023254, loss=0.5839173793792725
train: epoch 96, loss 0.5982710719108582, acc=0.7203888893127441, loss=0.5982710719108582
test: epoch 96, loss 0.6377208232879639, acc=0.6944444179534912, loss=0.6377208232879639
train: epoch 97, loss 0.6143353581428528, acc=0.7049444317817688, loss=0.6143353581428528
test: epoch 97, loss 0.5854311585426331, acc=0.7083333134651184, loss=0.5854311585426331
train: epoch 98, loss 0.5907143950462341, acc=0.7170000076293945, loss=0.5907143950462341
test: epoch 98, loss 0.6182374954223633, acc=0.7083333134651184, loss=0.6182374954223633
train: epoch 99, loss 0.5545251965522766, acc=0.7221666574478149, loss=0.5545251965522766
test: epoch 99, loss 0.5785028338432312, acc=0.7166666388511658, loss=0.5785028338432312
train: epoch 100, loss 0.5447531342506409, acc=0.7246111035346985, loss=0.5447531342506409
test: epoch 100, loss 0.6089081168174744, acc=0.7166666388511658, loss=0.6089081168174744
train: epoch 101, loss 0.5273205041885376, acc=0.7286666631698608, loss=0.5273205041885376
test: epoch 101, loss 0.5910595655441284, acc=0.7194444537162781, loss=0.5910595655441284
train: epoch 102, loss 0.5219282507896423, acc=0.7289444208145142, loss=0.5219282507896423
test: epoch 102, loss 0.6022705435752869, acc=0.7194444537162781, loss=0.6022705435752869
train: epoch 103, loss 0.5304979681968689, acc=0.7287222146987915, loss=0.5304979681968689
test: epoch 103, loss 0.5918089151382446, acc=0.7166666388511658, loss=0.5918089151382446
train: epoch 104, loss 0.5902115106582642, acc=0.7204444408416748, loss=0.5902115106582642
test: epoch 104, loss 0.5486787557601929, acc=0.7222222089767456, loss=0.5486787557601929
train: epoch 105, loss 0.5411696434020996, acc=0.7238333225250244, loss=0.5411696434020996
test: epoch 105, loss 0.6140202283859253, acc=0.7194444537162781, loss=0.6140202283859253
train: epoch 106, loss 0.5469149351119995, acc=0.721833348274231, loss=0.5469149351119995
test: epoch 106, loss 0.607827365398407, acc=0.7222222089767456, loss=0.607827365398407
train: epoch 107, loss 0.5748539566993713, acc=0.711555540561676, loss=0.5748539566993713
test: epoch 107, loss 0.5794175863265991, acc=0.7166666388511658, loss=0.5794175863265991
train: epoch 108, loss 0.5745642185211182, acc=0.7128333449363708, loss=0.5745642185211182
test: epoch 108, loss 0.5647006630897522, acc=0.7222222089767456, loss=0.5647006630897522
train: epoch 109, loss 0.5416660904884338, acc=0.7194444537162781, loss=0.5416660904884338
test: epoch 109, loss 0.5884429812431335, acc=0.7250000238418579, loss=0.5884429812431335
train: epoch 110, loss 0.5318080186843872, acc=0.726722240447998, loss=0.5318080186843872
test: epoch 110, loss 0.5809241533279419, acc=0.7333333492279053, loss=0.5809241533279419
train: epoch 111, loss 0.5722446441650391, acc=0.7333889007568359, loss=0.5722446441650391
test: epoch 111, loss 0.7120125889778137, acc=0.7027778029441833, loss=0.7120125889778137
train: epoch 112, loss 0.6223141551017761, acc=0.7092777490615845, loss=0.6223141551017761
test: epoch 112, loss 0.7142188549041748, acc=0.6722221970558167, loss=0.7142188549041748
train: epoch 113, loss 0.7056630253791809, acc=0.6858333349227905, loss=0.7056630253791809
test: epoch 113, loss 0.7332612872123718, acc=0.6722221970558167, loss=0.7332612872123718
train: epoch 114, loss 0.6298489570617676, acc=0.6977221965789795, loss=0.6298489570617676
test: epoch 114, loss 0.6959666013717651, acc=0.6888889074325562, loss=0.6959666013717651
train: epoch 115, loss 0.633867084980011, acc=0.6967222094535828, loss=0.633867084980011
test: epoch 115, loss 0.6317794322967529, acc=0.7027778029441833, loss=0.6317794322967529
train: epoch 116, loss 0.6378018260002136, acc=0.6969444155693054, loss=0.6378018260002136
test: epoch 116, loss 0.6121070384979248, acc=0.7083333134651184, loss=0.6121070384979248
train: epoch 117, loss 0.6100553274154663, acc=0.7021111249923706, loss=0.6100553274154663
test: epoch 117, loss 0.6015517711639404, acc=0.7111111283302307, loss=0.6015517711639404
train: epoch 118, loss 0.6050347089767456, acc=0.7032777667045593, loss=0.6050347089767456
test: epoch 118, loss 0.6019487380981445, acc=0.7111111283302307, loss=0.6019487380981445
train: epoch 119, loss 0.6062288880348206, acc=0.70333331823349, loss=0.6062288880348206
test: epoch 119, loss 0.6011301875114441, acc=0.7111111283302307, loss=0.6011301875114441
train: epoch 120, loss 0.8194265365600586, acc=0.6571666598320007, loss=0.8194265365600586
test: epoch 120, loss 0.7713425159454346, acc=0.6527777910232544, loss=0.7713425159454346
train: epoch 121, loss 0.9666796922683716, acc=0.6077222228050232, loss=0.9666796922683716
test: epoch 121, loss 0.8933195471763611, acc=0.6222222447395325, loss=0.8933195471763611
train: epoch 122, loss 0.8837428689002991, acc=0.616777777671814, loss=0.8837428689002991
test: epoch 122, loss 0.8450788855552673, acc=0.6388888955116272, loss=0.8450788855552673
train: epoch 123, loss 0.8517566323280334, acc=0.6281111240386963, loss=0.8517566323280334
test: epoch 123, loss 0.8465422987937927, acc=0.6361111402511597, loss=0.8465422987937927
train: epoch 124, loss 0.8426311016082764, acc=0.6287222504615784, loss=0.8426311016082764
test: epoch 124, loss 0.8082970976829529, acc=0.6416666507720947, loss=0.8082970976829529
train: epoch 125, loss 0.8110095262527466, acc=0.6188333630561829, loss=0.8110095262527466
test: epoch 125, loss 0.7967185378074646, acc=0.6416666507720947, loss=0.7967185378074646
train: epoch 126, loss 0.8016464114189148, acc=0.6288333535194397, loss=0.8016464114189148
test: epoch 126, loss 0.7793135643005371, acc=0.6527777910232544, loss=0.7793135643005371
train: epoch 127, loss 0.856909453868866, acc=0.6232777833938599, loss=0.856909453868866
test: epoch 127, loss 0.8030012249946594, acc=0.6388888955116272, loss=0.8030012249946594
train: epoch 128, loss 0.7694668173789978, acc=0.6466666460037231, loss=0.7694668173789978
test: epoch 128, loss 0.7457084059715271, acc=0.6555555462837219, loss=0.7457084059715271
train: epoch 129, loss 0.7598965764045715, acc=0.6502222418785095, loss=0.7598965764045715
test: epoch 129, loss 0.8958105444908142, acc=0.6333333253860474, loss=0.8958105444908142
train: epoch 130, loss 0.7813808917999268, acc=0.6613888740539551, loss=0.7813808917999268
test: epoch 130, loss 0.7656741738319397, acc=0.6499999761581421, loss=0.7656741738319397
train: epoch 131, loss 0.7512125372886658, acc=0.6704999804496765, loss=0.7512125372886658
test: epoch 131, loss 0.7250640988349915, acc=0.6777777671813965, loss=0.7250640988349915
train: epoch 132, loss 0.7307549118995667, acc=0.6683889031410217, loss=0.7307549118995667
test: epoch 132, loss 0.7246997356414795, acc=0.6777777671813965, loss=0.7246997356414795
train: epoch 133, loss 0.7223023176193237, acc=0.6718888878822327, loss=0.7223023176193237
test: epoch 133, loss 0.7094348073005676, acc=0.6833333373069763, loss=0.7094348073005676
train: epoch 134, loss 0.71369469165802, acc=0.6729444265365601, loss=0.71369469165802
test: epoch 134, loss 0.7091956734657288, acc=0.6833333373069763, loss=0.7091956734657288
train: epoch 135, loss 0.7133470773696899, acc=0.6730555295944214, loss=0.7133470773696899
test: epoch 135, loss 0.7091115713119507, acc=0.6833333373069763, loss=0.7091115713119507
train: epoch 136, loss 0.7130506634712219, acc=0.6729999780654907, loss=0.7130506634712219
test: epoch 136, loss 0.7090445160865784, acc=0.6833333373069763, loss=0.7090445160865784
train: epoch 137, loss 0.7128688097000122, acc=0.6735555529594421, loss=0.7128688097000122
test: epoch 137, loss 0.7090136408805847, acc=0.6833333373069763, loss=0.7090136408805847
train: epoch 138, loss 0.7127074003219604, acc=0.6741111278533936, loss=0.7127074003219604
test: epoch 138, loss 0.7089837193489075, acc=0.6833333373069763, loss=0.7089837193489075
train: epoch 139, loss 0.7131262421607971, acc=0.6741666793823242, loss=0.7131262421607971
test: epoch 139, loss 0.7073472738265991, acc=0.6833333373069763, loss=0.7073472738265991
train: epoch 140, loss 0.7175425887107849, acc=0.6737222075462341, loss=0.7175425887107849
test: epoch 140, loss 0.7069240212440491, acc=0.6833333373069763, loss=0.7069240212440491
train: epoch 141, loss 0.7104859352111816, acc=0.6742222309112549, loss=0.7104859352111816
test: epoch 141, loss 0.7067916989326477, acc=0.6833333373069763, loss=0.7067916989326477
train: epoch 142, loss 0.710324227809906, acc=0.6743333339691162, loss=0.710324227809906
test: epoch 142, loss 0.7068012356758118, acc=0.6833333373069763, loss=0.7068012356758118
train: epoch 143, loss 0.710212230682373, acc=0.6743333339691162, loss=0.710212230682373
test: epoch 143, loss 0.7067725658416748, acc=0.6833333373069763, loss=0.7067725658416748
train: epoch 144, loss 0.8755287528038025, acc=0.6317777633666992, loss=0.8755287528038025
test: epoch 144, loss 1.0444786548614502, acc=0.5666666626930237, loss=1.0444786548614502
train: epoch 145, loss 1.0405040979385376, acc=0.5755000114440918, loss=1.0405040979385376
test: epoch 145, loss 0.9726778268814087, acc=0.5833333134651184, loss=0.9726778268814087
train: epoch 146, loss 0.9967272281646729, acc=0.5872222185134888, loss=0.9967272281646729
test: epoch 146, loss 0.935924232006073, acc=0.6166666746139526, loss=0.935924232006073
train: epoch 147, loss 0.9477747678756714, acc=0.6089444160461426, loss=0.9477747678756714
test: epoch 147, loss 0.9622172117233276, acc=0.6111111044883728, loss=0.9622172117233276
train: epoch 148, loss 0.9257314205169678, acc=0.6183333396911621, loss=0.9257314205169678
test: epoch 148, loss 0.8990874886512756, acc=0.6138888597488403, loss=0.8990874886512756
train: epoch 149, loss 0.8681763410568237, acc=0.6258333325386047, loss=0.8681763410568237
test: epoch 149, loss 0.8286722898483276, acc=0.6388888955116272, loss=0.8286722898483276
train: epoch 150, loss 0.8878984451293945, acc=0.6342777609825134, loss=0.8878984451293945
test: epoch 150, loss 0.8540632724761963, acc=0.6388888955116272, loss=0.8540632724761963
