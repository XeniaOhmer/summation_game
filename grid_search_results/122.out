# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=1", "--temp_decay=0.99", "--one_hot=0", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=0, optimizer='adam', preemptable=False, random_seed=574429022, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=False, optimizer='adam', preemptable=False, random_seed=574429022, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.748274087905884, acc=0.10655555874109268, loss=2.748274087905884
test: epoch 1, loss 2.99291729927063, acc=0.10277777910232544, loss=2.99291729927063
train: epoch 2, loss 2.060765266418457, acc=0.21183332800865173, loss=2.060765266418457
test: epoch 2, loss 2.221205234527588, acc=0.20277777314186096, loss=2.221205234527588
train: epoch 3, loss 1.7479034662246704, acc=0.29499998688697815, loss=1.7479034662246704
test: epoch 3, loss 2.0276405811309814, acc=0.21944443881511688, loss=2.0276405811309814
train: epoch 4, loss 1.5906418561935425, acc=0.34555554389953613, loss=1.5906418561935425
test: epoch 4, loss 2.020693063735962, acc=0.23333333432674408, loss=2.020693063735962
train: epoch 5, loss 1.4534984827041626, acc=0.3995000123977661, loss=1.4534984827041626
test: epoch 5, loss 2.0322940349578857, acc=0.21944443881511688, loss=2.0322940349578857
train: epoch 6, loss 1.348453164100647, acc=0.43211111426353455, loss=1.348453164100647
test: epoch 6, loss 1.9047777652740479, acc=0.25833332538604736, loss=1.9047777652740479
train: epoch 7, loss 1.2946066856384277, acc=0.46266666054725647, loss=1.2946066856384277
test: epoch 7, loss 1.9670166969299316, acc=0.23055554926395416, loss=1.9670166969299316
train: epoch 8, loss 1.2428646087646484, acc=0.48027777671813965, loss=1.2428646087646484
test: epoch 8, loss 2.014479637145996, acc=0.24166665971279144, loss=2.014479637145996
train: epoch 9, loss 1.184312343597412, acc=0.5035555362701416, loss=1.184312343597412
test: epoch 9, loss 1.615625023841858, acc=0.3361110985279083, loss=1.615625023841858
train: epoch 10, loss 1.145390510559082, acc=0.5198333263397217, loss=1.145390510559082
test: epoch 10, loss 1.7634650468826294, acc=0.32499998807907104, loss=1.7634650468826294
train: epoch 11, loss 1.1114246845245361, acc=0.5303888916969299, loss=1.1114246845245361
test: epoch 11, loss 1.6162782907485962, acc=0.3361110985279083, loss=1.6162782907485962
train: epoch 12, loss 1.0880461931228638, acc=0.5435000061988831, loss=1.0880461931228638
test: epoch 12, loss 1.532698392868042, acc=0.3499999940395355, loss=1.532698392868042
train: epoch 13, loss 1.0554404258728027, acc=0.5568888783454895, loss=1.0554404258728027
test: epoch 13, loss 1.4381167888641357, acc=0.4027777910232544, loss=1.4381167888641357
train: epoch 14, loss 1.0359596014022827, acc=0.5636110901832581, loss=1.0359596014022827
test: epoch 14, loss 1.6637994050979614, acc=0.33888888359069824, loss=1.6637994050979614
train: epoch 15, loss 1.0107113122940063, acc=0.577833354473114, loss=1.0107113122940063
test: epoch 15, loss 1.7090556621551514, acc=0.3333333432674408, loss=1.7090556621551514
train: epoch 16, loss 0.9777652025222778, acc=0.5902222394943237, loss=0.9777652025222778
test: epoch 16, loss 1.4594722986221313, acc=0.41111111640930176, loss=1.4594722986221313
train: epoch 17, loss 0.9623508453369141, acc=0.5976666808128357, loss=0.9623508453369141
test: epoch 17, loss 1.6796038150787354, acc=0.24444444477558136, loss=1.6796038150787354
train: epoch 18, loss 0.9280529022216797, acc=0.6174444556236267, loss=0.9280529022216797
test: epoch 18, loss 1.408609390258789, acc=0.45277777314186096, loss=1.408609390258789
train: epoch 19, loss 0.9783421754837036, acc=0.586722195148468, loss=0.9783421754837036
test: epoch 19, loss 1.3090906143188477, acc=0.39444443583488464, loss=1.3090906143188477
train: epoch 20, loss 0.8890712261199951, acc=0.6269444227218628, loss=0.8890712261199951
test: epoch 20, loss 1.6526985168457031, acc=0.43611112236976624, loss=1.6526985168457031
train: epoch 21, loss 0.9090061187744141, acc=0.6221666932106018, loss=0.9090061187744141
test: epoch 21, loss 1.2574113607406616, acc=0.49444442987442017, loss=1.2574113607406616
train: epoch 22, loss 0.9203276634216309, acc=0.6202222108840942, loss=0.9203276634216309
test: epoch 22, loss 1.307389736175537, acc=0.4749999940395355, loss=1.307389736175537
train: epoch 23, loss 0.8737109899520874, acc=0.6372777819633484, loss=0.8737109899520874
test: epoch 23, loss 1.5586826801300049, acc=0.36944442987442017, loss=1.5586826801300049
train: epoch 24, loss 0.867162823677063, acc=0.643666684627533, loss=0.867162823677063
test: epoch 24, loss 1.387004017829895, acc=0.4555555582046509, loss=1.387004017829895
train: epoch 25, loss 0.8686046004295349, acc=0.6393333077430725, loss=0.8686046004295349
test: epoch 25, loss 1.232709527015686, acc=0.44999998807907104, loss=1.232709527015686
train: epoch 26, loss 0.8581827282905579, acc=0.6448888778686523, loss=0.8581827282905579
test: epoch 26, loss 1.26708984375, acc=0.4555555582046509, loss=1.26708984375
train: epoch 27, loss 0.8490312099456787, acc=0.6512777805328369, loss=0.8490312099456787
test: epoch 27, loss 1.1483122110366821, acc=0.5, loss=1.1483122110366821
train: epoch 28, loss 0.8444911241531372, acc=0.6495000123977661, loss=0.8444911241531372
test: epoch 28, loss 1.2872651815414429, acc=0.4611110985279083, loss=1.2872651815414429
train: epoch 29, loss 0.8209616541862488, acc=0.6590555310249329, loss=0.8209616541862488
test: epoch 29, loss 1.2663254737854004, acc=0.4194444417953491, loss=1.2663254737854004
train: epoch 30, loss 0.7886686325073242, acc=0.6745555400848389, loss=0.7886686325073242
test: epoch 30, loss 1.3261091709136963, acc=0.46666666865348816, loss=1.3261091709136963
train: epoch 31, loss 0.7891510725021362, acc=0.6705555319786072, loss=0.7891510725021362
test: epoch 31, loss 1.3521842956542969, acc=0.46388888359069824, loss=1.3521842956542969
train: epoch 32, loss 0.7813577055931091, acc=0.6747221946716309, loss=0.7813577055931091
test: epoch 32, loss 1.3540557622909546, acc=0.4611110985279083, loss=1.3540557622909546
train: epoch 33, loss 0.7960328459739685, acc=0.6700555682182312, loss=0.7960328459739685
test: epoch 33, loss 1.0502398014068604, acc=0.5611110925674438, loss=1.0502398014068604
train: epoch 34, loss 0.7767324447631836, acc=0.6840555667877197, loss=0.7767324447631836
test: epoch 34, loss 1.2677297592163086, acc=0.4555555582046509, loss=1.2677297592163086
train: epoch 35, loss 0.7876036763191223, acc=0.6749444603919983, loss=0.7876036763191223
test: epoch 35, loss 1.1734983921051025, acc=0.4583333432674408, loss=1.1734983921051025
train: epoch 36, loss 0.7723677158355713, acc=0.6841111183166504, loss=0.7723677158355713
test: epoch 36, loss 1.1489195823669434, acc=0.4444444477558136, loss=1.1489195823669434
train: epoch 37, loss 0.7577950358390808, acc=0.6869444251060486, loss=0.7577950358390808
test: epoch 37, loss 1.0416210889816284, acc=0.5527777671813965, loss=1.0416210889816284
train: epoch 38, loss 0.7385411262512207, acc=0.6969444155693054, loss=0.7385411262512207
test: epoch 38, loss 1.1268521547317505, acc=0.5361111164093018, loss=1.1268521547317505
train: epoch 39, loss 0.7273359894752502, acc=0.6982222199440002, loss=0.7273359894752502
test: epoch 39, loss 1.2742899656295776, acc=0.4749999940395355, loss=1.2742899656295776
train: epoch 40, loss 0.7303056716918945, acc=0.7032222151756287, loss=0.7303056716918945
test: epoch 40, loss 1.3208262920379639, acc=0.46388888359069824, loss=1.3208262920379639
train: epoch 41, loss 0.7608469724655151, acc=0.6859444379806519, loss=0.7608469724655151
test: epoch 41, loss 1.2155427932739258, acc=0.5472221970558167, loss=1.2155427932739258
train: epoch 42, loss 0.7542181611061096, acc=0.690833330154419, loss=0.7542181611061096
test: epoch 42, loss 1.165635585784912, acc=0.5, loss=1.165635585784912
train: epoch 43, loss 0.7111833691596985, acc=0.7047222256660461, loss=0.7111833691596985
test: epoch 43, loss 1.1380324363708496, acc=0.5138888955116272, loss=1.1380324363708496
train: epoch 44, loss 0.7169116139411926, acc=0.7037777900695801, loss=0.7169116139411926
test: epoch 44, loss 1.2376970052719116, acc=0.5138888955116272, loss=1.2376970052719116
train: epoch 45, loss 0.6981925368309021, acc=0.711722195148468, loss=0.6981925368309021
test: epoch 45, loss 1.15116548538208, acc=0.45277777314186096, loss=1.15116548538208
train: epoch 46, loss 0.6984049677848816, acc=0.7103888988494873, loss=0.6984049677848816
test: epoch 46, loss 0.969038724899292, acc=0.5944444537162781, loss=0.969038724899292
train: epoch 47, loss 0.6953087449073792, acc=0.707444429397583, loss=0.6953087449073792
test: epoch 47, loss 0.9895952343940735, acc=0.5888888835906982, loss=0.9895952343940735
train: epoch 48, loss 0.6698605418205261, acc=0.7212222218513489, loss=0.6698605418205261
test: epoch 48, loss 1.1702512502670288, acc=0.5138888955116272, loss=1.1702512502670288
train: epoch 49, loss 0.7129120230674744, acc=0.710444450378418, loss=0.7129120230674744
test: epoch 49, loss 1.2312053442001343, acc=0.5361111164093018, loss=1.2312053442001343
train: epoch 50, loss 0.6726720929145813, acc=0.7268333435058594, loss=0.6726720929145813
test: epoch 50, loss 1.2417185306549072, acc=0.49166667461395264, loss=1.2417185306549072
train: epoch 51, loss 0.6712688207626343, acc=0.721833348274231, loss=0.6712688207626343
test: epoch 51, loss 1.3192572593688965, acc=0.5083333253860474, loss=1.3192572593688965
train: epoch 52, loss 0.6760342121124268, acc=0.7233889102935791, loss=0.6760342121124268
test: epoch 52, loss 1.263967752456665, acc=0.48055556416511536, loss=1.263967752456665
train: epoch 53, loss 0.6648751497268677, acc=0.7248888611793518, loss=0.6648751497268677
test: epoch 53, loss 1.1429723501205444, acc=0.5583333373069763, loss=1.1429723501205444
train: epoch 54, loss 0.6735141277313232, acc=0.7277777791023254, loss=0.6735141277313232
test: epoch 54, loss 1.0633482933044434, acc=0.4972222149372101, loss=1.0633482933044434
train: epoch 55, loss 0.674655020236969, acc=0.7189444303512573, loss=0.674655020236969
test: epoch 55, loss 1.4127014875411987, acc=0.4888888895511627, loss=1.4127014875411987
train: epoch 56, loss 0.671974241733551, acc=0.7208333611488342, loss=0.671974241733551
test: epoch 56, loss 1.0865280628204346, acc=0.5388888716697693, loss=1.0865280628204346
train: epoch 57, loss 0.6428200006484985, acc=0.7328888773918152, loss=0.6428200006484985
test: epoch 57, loss 1.137943148612976, acc=0.5388888716697693, loss=1.137943148612976
train: epoch 58, loss 0.6734609603881836, acc=0.7192777991294861, loss=0.6734609603881836
test: epoch 58, loss 1.069614052772522, acc=0.550000011920929, loss=1.069614052772522
train: epoch 59, loss 0.6235474348068237, acc=0.7440000176429749, loss=0.6235474348068237
test: epoch 59, loss 1.1954978704452515, acc=0.5055555701255798, loss=1.1954978704452515
train: epoch 60, loss 0.6635512709617615, acc=0.7317222356796265, loss=0.6635512709617615
test: epoch 60, loss 1.1037356853485107, acc=0.5583333373069763, loss=1.1037356853485107
train: epoch 61, loss 0.6253498792648315, acc=0.746666669845581, loss=0.6253498792648315
test: epoch 61, loss 1.0200809240341187, acc=0.5361111164093018, loss=1.0200809240341187
train: epoch 62, loss 0.6359996795654297, acc=0.7403888702392578, loss=0.6359996795654297
test: epoch 62, loss 1.296462059020996, acc=0.45277777314186096, loss=1.296462059020996
train: epoch 63, loss 0.6271641850471497, acc=0.7456111311912537, loss=0.6271641850471497
test: epoch 63, loss 1.2212817668914795, acc=0.4694444537162781, loss=1.2212817668914795
train: epoch 64, loss 0.6133618354797363, acc=0.7462777495384216, loss=0.6133618354797363
test: epoch 64, loss 1.1427253484725952, acc=0.5361111164093018, loss=1.1427253484725952
train: epoch 65, loss 0.6309929490089417, acc=0.7402777671813965, loss=0.6309929490089417
test: epoch 65, loss 1.246779441833496, acc=0.5111111402511597, loss=1.246779441833496
train: epoch 66, loss 0.6013774275779724, acc=0.7512778043746948, loss=0.6013774275779724
test: epoch 66, loss 1.1545201539993286, acc=0.4611110985279083, loss=1.1545201539993286
train: epoch 67, loss 0.6086543202400208, acc=0.749833345413208, loss=0.6086543202400208
test: epoch 67, loss 1.1225638389587402, acc=0.519444465637207, loss=1.1225638389587402
train: epoch 68, loss 0.6185396909713745, acc=0.7463333606719971, loss=0.6185396909713745
test: epoch 68, loss 1.161167025566101, acc=0.5444444417953491, loss=1.161167025566101
train: epoch 69, loss 0.6716457009315491, acc=0.7289444208145142, loss=0.6716457009315491
test: epoch 69, loss 1.2409141063690186, acc=0.5027777552604675, loss=1.2409141063690186
train: epoch 70, loss 0.6279296875, acc=0.7432222366333008, loss=0.6279296875
test: epoch 70, loss 1.6307830810546875, acc=0.4416666626930237, loss=1.6307830810546875
train: epoch 71, loss 0.6131251454353333, acc=0.746055543422699, loss=0.6131251454353333
test: epoch 71, loss 1.2615309953689575, acc=0.5444444417953491, loss=1.2615309953689575
train: epoch 72, loss 0.6162083745002747, acc=0.7508888840675354, loss=0.6162083745002747
test: epoch 72, loss 1.2930325269699097, acc=0.4833333194255829, loss=1.2930325269699097
train: epoch 73, loss 0.6037933826446533, acc=0.7536110877990723, loss=0.6037933826446533
test: epoch 73, loss 1.1531524658203125, acc=0.49444442987442017, loss=1.1531524658203125
train: epoch 74, loss 0.6324909329414368, acc=0.7441111207008362, loss=0.6324909329414368
test: epoch 74, loss 1.5523295402526855, acc=0.3861111104488373, loss=1.5523295402526855
train: epoch 75, loss 0.6843365430831909, acc=0.7287222146987915, loss=0.6843365430831909
test: epoch 75, loss 1.1231063604354858, acc=0.5027777552604675, loss=1.1231063604354858
train: epoch 76, loss 0.6121639013290405, acc=0.7546111345291138, loss=0.6121639013290405
test: epoch 76, loss 0.9941598176956177, acc=0.5666666626930237, loss=0.9941598176956177
train: epoch 77, loss 0.6108760833740234, acc=0.7523333430290222, loss=0.6108760833740234
test: epoch 77, loss 1.0087443590164185, acc=0.6000000238418579, loss=1.0087443590164185
train: epoch 78, loss 0.6205887198448181, acc=0.7469444274902344, loss=0.6205887198448181
test: epoch 78, loss 1.2629789113998413, acc=0.4833333194255829, loss=1.2629789113998413
train: epoch 79, loss 0.6488300561904907, acc=0.7387222051620483, loss=0.6488300561904907
test: epoch 79, loss 1.0829516649246216, acc=0.5083333253860474, loss=1.0829516649246216
train: epoch 80, loss 0.6955282688140869, acc=0.7156111001968384, loss=0.6955282688140869
test: epoch 80, loss 1.0556402206420898, acc=0.5861111283302307, loss=1.0556402206420898
train: epoch 81, loss 0.6216352581977844, acc=0.7491111159324646, loss=0.6216352581977844
test: epoch 81, loss 1.3041328191757202, acc=0.4972222149372101, loss=1.3041328191757202
train: epoch 82, loss 0.5904467701911926, acc=0.7600555419921875, loss=0.5904467701911926
test: epoch 82, loss 1.4631941318511963, acc=0.49444442987442017, loss=1.4631941318511963
train: epoch 83, loss 0.6295173168182373, acc=0.7473888993263245, loss=0.6295173168182373
test: epoch 83, loss 1.135270595550537, acc=0.5305555462837219, loss=1.135270595550537
train: epoch 84, loss 0.6386091709136963, acc=0.7412222027778625, loss=0.6386091709136963
test: epoch 84, loss 1.2017459869384766, acc=0.49166667461395264, loss=1.2017459869384766
train: epoch 85, loss 0.6484897136688232, acc=0.7356111407279968, loss=0.6484897136688232
test: epoch 85, loss 1.105799674987793, acc=0.5861111283302307, loss=1.105799674987793
train: epoch 86, loss 0.6324042677879333, acc=0.7382222414016724, loss=0.6324042677879333
test: epoch 86, loss 1.136003851890564, acc=0.574999988079071, loss=1.136003851890564
train: epoch 87, loss 0.6173874139785767, acc=0.7446666955947876, loss=0.6173874139785767
test: epoch 87, loss 1.4043681621551514, acc=0.4888888895511627, loss=1.4043681621551514
train: epoch 88, loss 0.6777112483978271, acc=0.7218888998031616, loss=0.6777112483978271
test: epoch 88, loss 1.0603266954421997, acc=0.5555555820465088, loss=1.0603266954421997
train: epoch 89, loss 0.5821744203567505, acc=0.7554444670677185, loss=0.5821744203567505
test: epoch 89, loss 1.1180087327957153, acc=0.5416666865348816, loss=1.1180087327957153
train: epoch 90, loss 0.6158726215362549, acc=0.7447777986526489, loss=0.6158726215362549
test: epoch 90, loss 1.0171698331832886, acc=0.5472221970558167, loss=1.0171698331832886
train: epoch 91, loss 0.5963338017463684, acc=0.7558888792991638, loss=0.5963338017463684
test: epoch 91, loss 1.250235915184021, acc=0.5083333253860474, loss=1.250235915184021
train: epoch 92, loss 0.5906854271888733, acc=0.7538889050483704, loss=0.5906854271888733
test: epoch 92, loss 1.1076850891113281, acc=0.5944444537162781, loss=1.1076850891113281
train: epoch 93, loss 0.6228291392326355, acc=0.7436110973358154, loss=0.6228291392326355
test: epoch 93, loss 1.2907260656356812, acc=0.5055555701255798, loss=1.2907260656356812
train: epoch 94, loss 0.6347525715827942, acc=0.7368333339691162, loss=0.6347525715827942
test: epoch 94, loss 1.3792160749435425, acc=0.49444442987442017, loss=1.3792160749435425
train: epoch 95, loss 0.6159462928771973, acc=0.7482222318649292, loss=0.6159462928771973
test: epoch 95, loss 1.081814169883728, acc=0.5916666388511658, loss=1.081814169883728
train: epoch 96, loss 0.575621485710144, acc=0.7609999775886536, loss=0.575621485710144
test: epoch 96, loss 1.4397103786468506, acc=0.5277777910232544, loss=1.4397103786468506
train: epoch 97, loss 0.5850750803947449, acc=0.758222222328186, loss=0.5850750803947449
test: epoch 97, loss 0.8971543312072754, acc=0.5972222089767456, loss=0.8971543312072754
train: epoch 98, loss 0.6171835064888, acc=0.746055543422699, loss=0.6171835064888
test: epoch 98, loss 1.1749179363250732, acc=0.5249999761581421, loss=1.1749179363250732
train: epoch 99, loss 0.5684768557548523, acc=0.7629444599151611, loss=0.5684768557548523
test: epoch 99, loss 1.121005654335022, acc=0.5527777671813965, loss=1.121005654335022
train: epoch 100, loss 0.565963864326477, acc=0.7691666483879089, loss=0.565963864326477
test: epoch 100, loss 1.1141016483306885, acc=0.5555555820465088, loss=1.1141016483306885
train: epoch 101, loss 0.6101346015930176, acc=0.7526666522026062, loss=0.6101346015930176
test: epoch 101, loss 1.0337563753128052, acc=0.5944444537162781, loss=1.0337563753128052
train: epoch 102, loss 0.5781586766242981, acc=0.7620000243186951, loss=0.5781586766242981
test: epoch 102, loss 1.0941938161849976, acc=0.5638889074325562, loss=1.0941938161849976
train: epoch 103, loss 0.6010271906852722, acc=0.7534999847412109, loss=0.6010271906852722
test: epoch 103, loss 1.2493607997894287, acc=0.5694444179534912, loss=1.2493607997894287
train: epoch 104, loss 0.5807375311851501, acc=0.7586110830307007, loss=0.5807375311851501
test: epoch 104, loss 1.2409608364105225, acc=0.5944444537162781, loss=1.2409608364105225
train: epoch 105, loss 0.5784780979156494, acc=0.7626110911369324, loss=0.5784780979156494
test: epoch 105, loss 1.060314655303955, acc=0.5861111283302307, loss=1.060314655303955
train: epoch 106, loss 0.6011722087860107, acc=0.7532222270965576, loss=0.6011722087860107
test: epoch 106, loss 1.0004230737686157, acc=0.5916666388511658, loss=1.0004230737686157
train: epoch 107, loss 0.5462287664413452, acc=0.769444465637207, loss=0.5462287664413452
test: epoch 107, loss 1.0717778205871582, acc=0.5944444537162781, loss=1.0717778205871582
train: epoch 108, loss 0.5593011975288391, acc=0.7641666531562805, loss=0.5593011975288391
test: epoch 108, loss 1.1065667867660522, acc=0.5888888835906982, loss=1.1065667867660522
train: epoch 109, loss 0.6298711895942688, acc=0.7419999837875366, loss=0.6298711895942688
test: epoch 109, loss 0.9900162220001221, acc=0.5888888835906982, loss=0.9900162220001221
train: epoch 110, loss 0.5501971244812012, acc=0.7756111025810242, loss=0.5501971244812012
test: epoch 110, loss 1.0711674690246582, acc=0.5916666388511658, loss=1.0711674690246582
train: epoch 111, loss 0.5917473435401917, acc=0.7592777609825134, loss=0.5917473435401917
test: epoch 111, loss 0.9728981852531433, acc=0.5888888835906982, loss=0.9728981852531433
train: epoch 112, loss 0.5887405872344971, acc=0.7598333358764648, loss=0.5887405872344971
test: epoch 112, loss 1.0455501079559326, acc=0.6000000238418579, loss=1.0455501079559326
train: epoch 113, loss 0.5910049080848694, acc=0.7569444179534912, loss=0.5910049080848694
test: epoch 113, loss 1.1081620454788208, acc=0.5861111283302307, loss=1.1081620454788208
train: epoch 114, loss 0.5604121685028076, acc=0.7684999704360962, loss=0.5604121685028076
test: epoch 114, loss 1.227083683013916, acc=0.5472221970558167, loss=1.227083683013916
train: epoch 115, loss 0.5875282287597656, acc=0.7565555572509766, loss=0.5875282287597656
test: epoch 115, loss 0.9803726673126221, acc=0.5888888835906982, loss=0.9803726673126221
train: epoch 116, loss 0.574131190776825, acc=0.7591666579246521, loss=0.574131190776825
test: epoch 116, loss 1.2448947429656982, acc=0.5444444417953491, loss=1.2448947429656982
train: epoch 117, loss 0.5590172410011292, acc=0.7639444470405579, loss=0.5590172410011292
test: epoch 117, loss 1.007610559463501, acc=0.5916666388511658, loss=1.007610559463501
train: epoch 118, loss 0.5279554724693298, acc=0.7772777676582336, loss=0.5279554724693298
test: epoch 118, loss 0.9132740497589111, acc=0.5888888835906982, loss=0.9132740497589111
train: epoch 119, loss 0.6278786659240723, acc=0.7387222051620483, loss=0.6278786659240723
test: epoch 119, loss 1.081959843635559, acc=0.5861111283302307, loss=1.081959843635559
train: epoch 120, loss 0.5861315727233887, acc=0.757611095905304, loss=0.5861315727233887
test: epoch 120, loss 1.2022485733032227, acc=0.5944444537162781, loss=1.2022485733032227
train: epoch 121, loss 0.5318570733070374, acc=0.7758888602256775, loss=0.5318570733070374
test: epoch 121, loss 0.9404908418655396, acc=0.5972222089767456, loss=0.9404908418655396
train: epoch 122, loss 0.5995020270347595, acc=0.7422778010368347, loss=0.5995020270347595
test: epoch 122, loss 0.9613205790519714, acc=0.5944444537162781, loss=0.9613205790519714
train: epoch 123, loss 0.8128619194030762, acc=0.668666660785675, loss=0.8128619194030762
test: epoch 123, loss 1.1032027006149292, acc=0.5249999761581421, loss=1.1032027006149292
train: epoch 124, loss 0.7499865293502808, acc=0.6840000152587891, loss=0.7499865293502808
test: epoch 124, loss 1.1726857423782349, acc=0.5388888716697693, loss=1.1726857423782349
train: epoch 125, loss 0.670681357383728, acc=0.7110000252723694, loss=0.670681357383728
test: epoch 125, loss 1.2126076221466064, acc=0.5305555462837219, loss=1.2126076221466064
train: epoch 126, loss 0.7007263898849487, acc=0.6989444494247437, loss=0.7007263898849487
test: epoch 126, loss 1.1397851705551147, acc=0.5333333611488342, loss=1.1397851705551147
train: epoch 127, loss 0.7194482088088989, acc=0.6939444541931152, loss=0.7194482088088989
test: epoch 127, loss 1.3119285106658936, acc=0.5277777910232544, loss=1.3119285106658936
train: epoch 128, loss 0.6859827637672424, acc=0.6941666603088379, loss=0.6859827637672424
test: epoch 128, loss 1.2044024467468262, acc=0.5388888716697693, loss=1.2044024467468262
train: epoch 129, loss 0.6630371809005737, acc=0.7084444165229797, loss=0.6630371809005737
test: epoch 129, loss 1.127854585647583, acc=0.5361111164093018, loss=1.127854585647583
train: epoch 130, loss 0.6741284728050232, acc=0.7032222151756287, loss=0.6741284728050232
test: epoch 130, loss 1.102847695350647, acc=0.5333333611488342, loss=1.102847695350647
train: epoch 131, loss 0.6615976095199585, acc=0.7070000171661377, loss=0.6615976095199585
test: epoch 131, loss 1.1859451532363892, acc=0.5333333611488342, loss=1.1859451532363892
train: epoch 132, loss 0.642431914806366, acc=0.7127777934074402, loss=0.642431914806366
test: epoch 132, loss 1.1138124465942383, acc=0.5388888716697693, loss=1.1138124465942383
train: epoch 133, loss 0.6593373417854309, acc=0.7084444165229797, loss=0.6593373417854309
test: epoch 133, loss 1.1995693445205688, acc=0.5305555462837219, loss=1.1995693445205688
train: epoch 134, loss 0.6476507186889648, acc=0.7149999737739563, loss=0.6476507186889648
test: epoch 134, loss 1.2705819606781006, acc=0.5388888716697693, loss=1.2705819606781006
train: epoch 135, loss 0.6698824167251587, acc=0.7114999890327454, loss=0.6698824167251587
test: epoch 135, loss 1.1936981678009033, acc=0.5305555462837219, loss=1.1936981678009033
train: epoch 136, loss 0.6590837836265564, acc=0.7079444527626038, loss=0.6590837836265564
test: epoch 136, loss 1.0509552955627441, acc=0.5694444179534912, loss=1.0509552955627441
train: epoch 137, loss 0.567671537399292, acc=0.7671666741371155, loss=0.567671537399292
test: epoch 137, loss 1.2577787637710571, acc=0.5555555820465088, loss=1.2577787637710571
train: epoch 138, loss 0.5766414403915405, acc=0.762666642665863, loss=0.5766414403915405
test: epoch 138, loss 1.0797392129898071, acc=0.5861111283302307, loss=1.0797392129898071
train: epoch 139, loss 0.5937348008155823, acc=0.7550555467605591, loss=0.5937348008155823
test: epoch 139, loss 1.0577712059020996, acc=0.5694444179534912, loss=1.0577712059020996
train: epoch 140, loss 0.5419695377349854, acc=0.7693889141082764, loss=0.5419695377349854
test: epoch 140, loss 1.1993986368179321, acc=0.5638889074325562, loss=1.1993986368179321
train: epoch 141, loss 0.5506674647331238, acc=0.774222195148468, loss=0.5506674647331238
test: epoch 141, loss 1.1265286207199097, acc=0.5527777671813965, loss=1.1265286207199097
train: epoch 142, loss 0.6043428778648376, acc=0.7528333067893982, loss=0.6043428778648376
test: epoch 142, loss 1.0158674716949463, acc=0.5666666626930237, loss=1.0158674716949463
train: epoch 143, loss 0.6698103547096252, acc=0.7411110997200012, loss=0.6698103547096252
test: epoch 143, loss 1.511263370513916, acc=0.5055555701255798, loss=1.511263370513916
train: epoch 144, loss 0.8559387922286987, acc=0.6642777919769287, loss=0.8559387922286987
test: epoch 144, loss 1.2003648281097412, acc=0.5388888716697693, loss=1.2003648281097412
train: epoch 145, loss 0.7155576348304749, acc=0.7018333077430725, loss=0.7155576348304749
test: epoch 145, loss 1.0957280397415161, acc=0.5333333611488342, loss=1.0957280397415161
train: epoch 146, loss 0.6936758160591125, acc=0.6996111273765564, loss=0.6936758160591125
test: epoch 146, loss 1.2728265523910522, acc=0.5388888716697693, loss=1.2728265523910522
train: epoch 147, loss 0.677450954914093, acc=0.7070555686950684, loss=0.677450954914093
test: epoch 147, loss 1.1714050769805908, acc=0.5416666865348816, loss=1.1714050769805908
train: epoch 148, loss 0.6850196123123169, acc=0.7060555815696716, loss=0.6850196123123169
test: epoch 148, loss 1.2673017978668213, acc=0.5416666865348816, loss=1.2673017978668213
train: epoch 149, loss 0.6674871444702148, acc=0.7099444270133972, loss=0.6674871444702148
test: epoch 149, loss 1.208058476448059, acc=0.5249999761581421, loss=1.208058476448059
train: epoch 150, loss 0.6791121959686279, acc=0.7066666483879089, loss=0.6791121959686279
test: epoch 150, loss 1.152136206626892, acc=0.5416666865348816, loss=1.152136206626892
