# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=2.0", "--temp_decay=0.99", "--one_hot=1", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1986752569, receiver_embed_dim=128, save_run=0, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1986752569, receiver_embed_dim=128, save_run=False, temp_decay=0.99, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.710345983505249, acc=0.10344444215297699, loss=2.710345983505249
test: epoch 1, loss 4.761404991149902, acc=0.0972222238779068, loss=4.761404991149902
train: epoch 2, loss 1.4917404651641846, acc=0.38261112570762634, loss=1.4917404651641846
test: epoch 2, loss 4.890931606292725, acc=0.18888889253139496, loss=4.890931606292725
train: epoch 3, loss 0.8714182376861572, acc=0.652388870716095, loss=0.8714182376861572
test: epoch 3, loss 3.8038506507873535, acc=0.2361111044883728, loss=3.8038506507873535
train: epoch 4, loss 0.6029990911483765, acc=0.7656111121177673, loss=0.6029990911483765
test: epoch 4, loss 3.2052979469299316, acc=0.28333333134651184, loss=3.2052979469299316
train: epoch 5, loss 0.4686000645160675, acc=0.8214444518089294, loss=0.4686000645160675
test: epoch 5, loss 2.751267910003662, acc=0.33888888359069824, loss=2.751267910003662
train: epoch 6, loss 0.39693570137023926, acc=0.8471111059188843, loss=0.39693570137023926
test: epoch 6, loss 2.960580587387085, acc=0.3333333432674408, loss=2.960580587387085
train: epoch 7, loss 0.35334256291389465, acc=0.8644999861717224, loss=0.35334256291389465
test: epoch 7, loss 2.826693534851074, acc=0.30000001192092896, loss=2.826693534851074
train: epoch 8, loss 0.313003808259964, acc=0.8799999952316284, loss=0.313003808259964
test: epoch 8, loss 3.0547635555267334, acc=0.30000001192092896, loss=3.0547635555267334
train: epoch 9, loss 0.29170846939086914, acc=0.8886111378669739, loss=0.29170846939086914
test: epoch 9, loss 1.8997979164123535, acc=0.4166666567325592, loss=1.8997979164123535
train: epoch 10, loss 0.2639605700969696, acc=0.9072777628898621, loss=0.2639605700969696
test: epoch 10, loss 2.4175658226013184, acc=0.3305555582046509, loss=2.4175658226013184
train: epoch 11, loss 0.2399469017982483, acc=0.9185555577278137, loss=0.2399469017982483
test: epoch 11, loss 2.378645658493042, acc=0.3777777850627899, loss=2.378645658493042
train: epoch 12, loss 0.2340555489063263, acc=0.9225000143051147, loss=0.2340555489063263
test: epoch 12, loss 2.0012402534484863, acc=0.4416666626930237, loss=2.0012402534484863
train: epoch 13, loss 0.20822051167488098, acc=0.9313333630561829, loss=0.20822051167488098
test: epoch 13, loss 2.5339672565460205, acc=0.4305555522441864, loss=2.5339672565460205
train: epoch 14, loss 0.21343904733657837, acc=0.9275000095367432, loss=0.21343904733657837
test: epoch 14, loss 1.7721933126449585, acc=0.4555555582046509, loss=1.7721933126449585
train: epoch 15, loss 0.19645804166793823, acc=0.9351666569709778, loss=0.19645804166793823
test: epoch 15, loss 2.488388776779175, acc=0.43611112236976624, loss=2.488388776779175
train: epoch 16, loss 0.20340219140052795, acc=0.9327222108840942, loss=0.20340219140052795
test: epoch 16, loss 2.29549503326416, acc=0.4333333373069763, loss=2.29549503326416
train: epoch 17, loss 0.2016468495130539, acc=0.9323889017105103, loss=0.2016468495130539
test: epoch 17, loss 2.3332290649414062, acc=0.4333333373069763, loss=2.3332290649414062
train: epoch 18, loss 0.16074369847774506, acc=0.9469444155693054, loss=0.16074369847774506
test: epoch 18, loss 2.2258362770080566, acc=0.4000000059604645, loss=2.2258362770080566
train: epoch 19, loss 0.1773911565542221, acc=0.9426110982894897, loss=0.1773911565542221
test: epoch 19, loss 2.0266568660736084, acc=0.4166666567325592, loss=2.0266568660736084
train: epoch 20, loss 0.1726566106081009, acc=0.941777765750885, loss=0.1726566106081009
test: epoch 20, loss 1.8894917964935303, acc=0.4722222089767456, loss=1.8894917964935303
train: epoch 21, loss 0.16384375095367432, acc=0.94605553150177, loss=0.16384375095367432
test: epoch 21, loss 1.7643986940383911, acc=0.4694444537162781, loss=1.7643986940383911
train: epoch 22, loss 0.15064123272895813, acc=0.9508888721466064, loss=0.15064123272895813
test: epoch 22, loss 1.7232320308685303, acc=0.5222222208976746, loss=1.7232320308685303
train: epoch 23, loss 0.14862489700317383, acc=0.953499972820282, loss=0.14862489700317383
test: epoch 23, loss 1.780273199081421, acc=0.4749999940395355, loss=1.780273199081421
train: epoch 24, loss 0.15309154987335205, acc=0.9484999775886536, loss=0.15309154987335205
test: epoch 24, loss 1.494869351387024, acc=0.5583333373069763, loss=1.494869351387024
train: epoch 25, loss 0.15172971785068512, acc=0.9506111145019531, loss=0.15172971785068512
test: epoch 25, loss 1.5409477949142456, acc=0.5472221970558167, loss=1.5409477949142456
train: epoch 26, loss 0.14308984577655792, acc=0.9551110863685608, loss=0.14308984577655792
test: epoch 26, loss 1.3387835025787354, acc=0.6166666746139526, loss=1.3387835025787354
train: epoch 27, loss 0.14173412322998047, acc=0.9526666402816772, loss=0.14173412322998047
test: epoch 27, loss 1.7726032733917236, acc=0.5777778029441833, loss=1.7726032733917236
train: epoch 28, loss 0.13189850747585297, acc=0.9595555663108826, loss=0.13189850747585297
test: epoch 28, loss 1.367082953453064, acc=0.699999988079071, loss=1.367082953453064
train: epoch 29, loss 0.126807302236557, acc=0.957611083984375, loss=0.126807302236557
test: epoch 29, loss 1.267482042312622, acc=0.6583333611488342, loss=1.267482042312622
train: epoch 30, loss 0.11806554347276688, acc=0.9629444479942322, loss=0.11806554347276688
test: epoch 30, loss 1.0790704488754272, acc=0.7138888835906982, loss=1.0790704488754272
train: epoch 31, loss 0.10725504159927368, acc=0.9679999947547913, loss=0.10725504159927368
test: epoch 31, loss 0.8001387119293213, acc=0.8222222328186035, loss=0.8001387119293213
train: epoch 32, loss 0.11661417037248611, acc=0.9630555510520935, loss=0.11661417037248611
test: epoch 32, loss 0.9377809762954712, acc=0.7888888716697693, loss=0.9377809762954712
train: epoch 33, loss 0.09443072229623795, acc=0.9718888998031616, loss=0.09443072229623795
test: epoch 33, loss 0.9702334403991699, acc=0.7833333611488342, loss=0.9702334403991699
train: epoch 34, loss 0.11076798290014267, acc=0.965833306312561, loss=0.11076798290014267
test: epoch 34, loss 0.7102973461151123, acc=0.8027777671813965, loss=0.7102973461151123
train: epoch 35, loss 0.09591083228588104, acc=0.9716110825538635, loss=0.09591083228588104
test: epoch 35, loss 0.7136549949645996, acc=0.8416666388511658, loss=0.7136549949645996
train: epoch 36, loss 0.08890155702829361, acc=0.9733333587646484, loss=0.08890155702829361
test: epoch 36, loss 0.7630871534347534, acc=0.8083333373069763, loss=0.7630871534347534
train: epoch 37, loss 0.06964454054832458, acc=0.9793888926506042, loss=0.06964454054832458
test: epoch 37, loss 0.5363379716873169, acc=0.8805555701255798, loss=0.5363379716873169
train: epoch 38, loss 0.07170502841472626, acc=0.9798333048820496, loss=0.07170502841472626
test: epoch 38, loss 0.35365381836891174, acc=0.9166666865348816, loss=0.35365381836891174
train: epoch 39, loss 0.07732010632753372, acc=0.9777222275733948, loss=0.07732010632753372
test: epoch 39, loss 0.37032127380371094, acc=0.9166666865348816, loss=0.37032127380371094
train: epoch 40, loss 0.07077360898256302, acc=0.980388879776001, loss=0.07077360898256302
test: epoch 40, loss 0.4238799810409546, acc=0.9083333611488342, loss=0.4238799810409546
train: epoch 41, loss 0.05944637954235077, acc=0.9833333492279053, loss=0.05944637954235077
test: epoch 41, loss 0.4002130925655365, acc=0.9194444417953491, loss=0.4002130925655365
train: epoch 42, loss 0.06578037887811661, acc=0.9812777638435364, loss=0.06578037887811661
test: epoch 42, loss 0.29196804761886597, acc=0.9222221970558167, loss=0.29196804761886597
train: epoch 43, loss 0.058237988501787186, acc=0.9842777848243713, loss=0.058237988501787186
test: epoch 43, loss 0.19833439588546753, acc=0.9583333134651184, loss=0.19833439588546753
train: epoch 44, loss 0.05414505675435066, acc=0.9848333597183228, loss=0.05414505675435066
test: epoch 44, loss 0.2083360105752945, acc=0.9583333134651184, loss=0.2083360105752945
train: epoch 45, loss 0.03936247155070305, acc=0.987666666507721, loss=0.03936247155070305
test: epoch 45, loss 0.18985410034656525, acc=0.9583333134651184, loss=0.18985410034656525
train: epoch 46, loss 0.04739316925406456, acc=0.9863888621330261, loss=0.04739316925406456
test: epoch 46, loss 0.16603995859622955, acc=0.9583333134651184, loss=0.16603995859622955
train: epoch 47, loss 0.04189104214310646, acc=0.9882222414016724, loss=0.04189104214310646
test: epoch 47, loss 0.21449358761310577, acc=0.9611111283302307, loss=0.21449358761310577
train: epoch 48, loss 0.04868091642856598, acc=0.9868333339691162, loss=0.04868091642856598
test: epoch 48, loss 0.21011663973331451, acc=0.9583333134651184, loss=0.21011663973331451
train: epoch 49, loss 0.04195602983236313, acc=0.9879999756813049, loss=0.04195602983236313
test: epoch 49, loss 0.25322291254997253, acc=0.9361110925674438, loss=0.25322291254997253
train: epoch 50, loss 0.03585246577858925, acc=0.9885555505752563, loss=0.03585246577858925
test: epoch 50, loss 0.2153751254081726, acc=0.9611111283302307, loss=0.2153751254081726
train: epoch 51, loss 0.047109875828027725, acc=0.9851111173629761, loss=0.047109875828027725
test: epoch 51, loss 0.12699368596076965, acc=0.9611111283302307, loss=0.12699368596076965
train: epoch 52, loss 0.03698122128844261, acc=0.9902222156524658, loss=0.03698122128844261
test: epoch 52, loss 0.18844784796237946, acc=0.9583333134651184, loss=0.18844784796237946
train: epoch 53, loss 0.043405432254076004, acc=0.9867777824401855, loss=0.043405432254076004
test: epoch 53, loss 0.18693505227565765, acc=0.9611111283302307, loss=0.18693505227565765
train: epoch 54, loss 0.05109299346804619, acc=0.9856111407279968, loss=0.05109299346804619
test: epoch 54, loss 0.11548865586519241, acc=0.9611111283302307, loss=0.11548865586519241
train: epoch 55, loss 0.03118218295276165, acc=0.9917222261428833, loss=0.03118218295276165
test: epoch 55, loss 0.13720263540744781, acc=0.9611111283302307, loss=0.13720263540744781
train: epoch 56, loss 0.03245106711983681, acc=0.9899444580078125, loss=0.03245106711983681
test: epoch 56, loss 0.14650747179985046, acc=0.9611111283302307, loss=0.14650747179985046
train: epoch 57, loss 0.03138445317745209, acc=0.9911110997200012, loss=0.03138445317745209
test: epoch 57, loss 0.1700376719236374, acc=0.9611111283302307, loss=0.1700376719236374
train: epoch 58, loss 0.040499791502952576, acc=0.9897222518920898, loss=0.040499791502952576
test: epoch 58, loss 0.16281381249427795, acc=0.9611111283302307, loss=0.16281381249427795
train: epoch 59, loss 0.03045635111629963, acc=0.9909444451332092, loss=0.03045635111629963
test: epoch 59, loss 0.15995682775974274, acc=0.9611111283302307, loss=0.15995682775974274
train: epoch 60, loss 0.03492163494229317, acc=0.9900555610656738, loss=0.03492163494229317
test: epoch 60, loss 0.15539264678955078, acc=0.9583333134651184, loss=0.15539264678955078
train: epoch 61, loss 0.029440876096487045, acc=0.9916666746139526, loss=0.029440876096487045
test: epoch 61, loss 0.14621177315711975, acc=0.9611111283302307, loss=0.14621177315711975
train: epoch 62, loss 0.03679915517568588, acc=0.9906666874885559, loss=0.03679915517568588
test: epoch 62, loss 0.22371166944503784, acc=0.9583333134651184, loss=0.22371166944503784
train: epoch 63, loss 0.04205157980322838, acc=0.988444447517395, loss=0.04205157980322838
test: epoch 63, loss 0.16586889326572418, acc=0.9611111283302307, loss=0.16586889326572418
train: epoch 64, loss 0.04583382233977318, acc=0.9885555505752563, loss=0.04583382233977318
test: epoch 64, loss 0.1559206247329712, acc=0.9611111283302307, loss=0.1559206247329712
train: epoch 65, loss 0.03522231802344322, acc=0.9897778034210205, loss=0.03522231802344322
test: epoch 65, loss 0.14009079337120056, acc=0.9611111283302307, loss=0.14009079337120056
train: epoch 66, loss 0.03757743537425995, acc=0.9888333082199097, loss=0.03757743537425995
test: epoch 66, loss 0.09648416191339493, acc=0.9611111283302307, loss=0.09648416191339493
train: epoch 67, loss 0.0419931598007679, acc=0.9892222285270691, loss=0.0419931598007679
test: epoch 67, loss 0.1583344042301178, acc=0.9611111283302307, loss=0.1583344042301178
train: epoch 68, loss 0.035529810935258865, acc=0.9903888702392578, loss=0.035529810935258865
test: epoch 68, loss 0.15348920226097107, acc=0.9611111283302307, loss=0.15348920226097107
train: epoch 69, loss 0.02578035742044449, acc=0.9918333292007446, loss=0.02578035742044449
test: epoch 69, loss 0.1388663798570633, acc=0.9611111283302307, loss=0.1388663798570633
train: epoch 70, loss 0.04208025336265564, acc=0.9870555400848389, loss=0.04208025336265564
test: epoch 70, loss 0.13107065856456757, acc=0.9611111283302307, loss=0.13107065856456757
train: epoch 71, loss 0.03007681481540203, acc=0.9915000200271606, loss=0.03007681481540203
test: epoch 71, loss 0.12477969378232956, acc=0.9611111283302307, loss=0.12477969378232956
train: epoch 72, loss 0.03729942813515663, acc=0.9904444217681885, loss=0.03729942813515663
test: epoch 72, loss 0.11719569563865662, acc=0.9611111283302307, loss=0.11719569563865662
train: epoch 73, loss 0.025379473343491554, acc=0.9912222027778625, loss=0.025379473343491554
test: epoch 73, loss 0.13792665302753448, acc=0.9611111283302307, loss=0.13792665302753448
train: epoch 74, loss 0.02698814682662487, acc=0.9918888807296753, loss=0.02698814682662487
test: epoch 74, loss 0.13194401562213898, acc=0.9611111283302307, loss=0.13194401562213898
train: epoch 75, loss 0.049884188920259476, acc=0.9872778058052063, loss=0.049884188920259476
test: epoch 75, loss 0.12451218068599701, acc=0.9583333134651184, loss=0.12451218068599701
train: epoch 76, loss 0.058947209268808365, acc=0.9851111173629761, loss=0.058947209268808365
test: epoch 76, loss 0.13740138709545135, acc=0.9583333134651184, loss=0.13740138709545135
train: epoch 77, loss 0.025454334914684296, acc=0.992111086845398, loss=0.025454334914684296
test: epoch 77, loss 0.18618237972259521, acc=0.9611111283302307, loss=0.18618237972259521
train: epoch 78, loss 0.05821415036916733, acc=0.98416668176651, loss=0.05821415036916733
test: epoch 78, loss 0.168301522731781, acc=0.9583333134651184, loss=0.168301522731781
train: epoch 79, loss 0.047227952629327774, acc=0.9863888621330261, loss=0.047227952629327774
test: epoch 79, loss 0.10212947428226471, acc=0.9611111283302307, loss=0.10212947428226471
train: epoch 80, loss 0.02966732159256935, acc=0.9920555353164673, loss=0.02966732159256935
test: epoch 80, loss 0.13401050865650177, acc=0.9611111283302307, loss=0.13401050865650177
train: epoch 81, loss 0.03849652782082558, acc=0.9898333549499512, loss=0.03849652782082558
test: epoch 81, loss 0.0922698900103569, acc=0.9611111283302307, loss=0.0922698900103569
train: epoch 82, loss 0.03520841524004936, acc=0.9908333420753479, loss=0.03520841524004936
test: epoch 82, loss 0.13317836821079254, acc=0.9611111283302307, loss=0.13317836821079254
train: epoch 83, loss 0.0332079753279686, acc=0.988444447517395, loss=0.0332079753279686
test: epoch 83, loss 0.1498967707157135, acc=0.9611111283302307, loss=0.1498967707157135
train: epoch 84, loss 0.02615792490541935, acc=0.9924444556236267, loss=0.02615792490541935
test: epoch 84, loss 0.15031558275222778, acc=0.9611111283302307, loss=0.15031558275222778
train: epoch 85, loss 0.0370691679418087, acc=0.9896666407585144, loss=0.0370691679418087
test: epoch 85, loss 0.134443998336792, acc=0.9611111283302307, loss=0.134443998336792
train: epoch 86, loss 0.0477663017809391, acc=0.9865000247955322, loss=0.0477663017809391
test: epoch 86, loss 0.09834258258342743, acc=0.9611111283302307, loss=0.09834258258342743
train: epoch 87, loss 0.0401805117726326, acc=0.9887222051620483, loss=0.0401805117726326
test: epoch 87, loss 0.2394263595342636, acc=0.949999988079071, loss=0.2394263595342636
train: epoch 88, loss 0.0653918981552124, acc=0.9833333492279053, loss=0.0653918981552124
test: epoch 88, loss 0.15269014239311218, acc=0.9583333134651184, loss=0.15269014239311218
train: epoch 89, loss 0.04168686643242836, acc=0.9880555272102356, loss=0.04168686643242836
test: epoch 89, loss 0.13425976037979126, acc=0.9583333134651184, loss=0.13425976037979126
train: epoch 90, loss 0.05467453598976135, acc=0.983222246170044, loss=0.05467453598976135
test: epoch 90, loss 0.13025258481502533, acc=0.9555555582046509, loss=0.13025258481502533
train: epoch 91, loss 0.04560523107647896, acc=0.9856666922569275, loss=0.04560523107647896
test: epoch 91, loss 0.12437725067138672, acc=0.9555555582046509, loss=0.12437725067138672
train: epoch 92, loss 0.050238415598869324, acc=0.9821110963821411, loss=0.050238415598869324
test: epoch 92, loss 0.108087457716465, acc=0.9638888835906982, loss=0.108087457716465
train: epoch 93, loss 0.024576840922236443, acc=0.992111086845398, loss=0.024576840922236443
test: epoch 93, loss 0.14164336025714874, acc=0.9638888835906982, loss=0.14164336025714874
train: epoch 94, loss 0.030247097834944725, acc=0.9908888936042786, loss=0.030247097834944725
test: epoch 94, loss 0.11360509693622589, acc=0.9638888835906982, loss=0.11360509693622589
train: epoch 95, loss 0.0337010882794857, acc=0.9897778034210205, loss=0.0337010882794857
test: epoch 95, loss 0.12711337208747864, acc=0.9638888835906982, loss=0.12711337208747864
train: epoch 96, loss 0.035570211708545685, acc=0.9898333549499512, loss=0.035570211708545685
test: epoch 96, loss 0.14215336740016937, acc=0.9638888835906982, loss=0.14215336740016937
train: epoch 97, loss 0.03596062585711479, acc=0.988277792930603, loss=0.03596062585711479
test: epoch 97, loss 0.11289332062005997, acc=0.9611111283302307, loss=0.11289332062005997
train: epoch 98, loss 0.033141206949949265, acc=0.9903333187103271, loss=0.033141206949949265
test: epoch 98, loss 0.08834633231163025, acc=0.9638888835906982, loss=0.08834633231163025
train: epoch 99, loss 0.03501730412244797, acc=0.9879999756813049, loss=0.03501730412244797
test: epoch 99, loss 0.09843813627958298, acc=0.9638888835906982, loss=0.09843813627958298
train: epoch 100, loss 0.030262121930718422, acc=0.9905555844306946, loss=0.030262121930718422
test: epoch 100, loss 0.13982072472572327, acc=0.9638888835906982, loss=0.13982072472572327
train: epoch 101, loss 0.0337851420044899, acc=0.9899444580078125, loss=0.0337851420044899
test: epoch 101, loss 0.1398344337940216, acc=0.9638888835906982, loss=0.1398344337940216
train: epoch 102, loss 0.029713420197367668, acc=0.9904444217681885, loss=0.029713420197367668
test: epoch 102, loss 0.11345837265253067, acc=0.9638888835906982, loss=0.11345837265253067
train: epoch 103, loss 0.02780202217400074, acc=0.9901111125946045, loss=0.02780202217400074
test: epoch 103, loss 0.21799984574317932, acc=0.9583333134651184, loss=0.21799984574317932
train: epoch 104, loss 0.04779757931828499, acc=0.9874444603919983, loss=0.04779757931828499
test: epoch 104, loss 0.11446098238229752, acc=0.9638888835906982, loss=0.11446098238229752
train: epoch 105, loss 0.0352151058614254, acc=0.9898333549499512, loss=0.0352151058614254
test: epoch 105, loss 0.11485576629638672, acc=0.9638888835906982, loss=0.11485576629638672
train: epoch 106, loss 0.0274119321256876, acc=0.9911666512489319, loss=0.0274119321256876
test: epoch 106, loss 0.10912611335515976, acc=0.9638888835906982, loss=0.10912611335515976
train: epoch 107, loss 0.04008401185274124, acc=0.9897778034210205, loss=0.04008401185274124
test: epoch 107, loss 0.13442383706569672, acc=0.9638888835906982, loss=0.13442383706569672
train: epoch 108, loss 0.02807488478720188, acc=0.9918333292007446, loss=0.02807488478720188
test: epoch 108, loss 0.1069597452878952, acc=0.9638888835906982, loss=0.1069597452878952
train: epoch 109, loss 0.033935610204935074, acc=0.9898889064788818, loss=0.033935610204935074
test: epoch 109, loss 0.1141183003783226, acc=0.9611111283302307, loss=0.1141183003783226
train: epoch 110, loss 0.04243030026555061, acc=0.9886666536331177, loss=0.04243030026555061
test: epoch 110, loss 0.1188376247882843, acc=0.9611111283302307, loss=0.1188376247882843
train: epoch 111, loss 0.03736210614442825, acc=0.9877777695655823, loss=0.03736210614442825
test: epoch 111, loss 0.08842648565769196, acc=0.9638888835906982, loss=0.08842648565769196
train: epoch 112, loss 0.03588392958045006, acc=0.9906666874885559, loss=0.03588392958045006
test: epoch 112, loss 0.10818688571453094, acc=0.9638888835906982, loss=0.10818688571453094
train: epoch 113, loss 0.023652933537960052, acc=0.9926111102104187, loss=0.023652933537960052
test: epoch 113, loss 0.11839008331298828, acc=0.9638888835906982, loss=0.11839008331298828
train: epoch 114, loss 0.025804804638028145, acc=0.9913333058357239, loss=0.025804804638028145
test: epoch 114, loss 0.10785654932260513, acc=0.9638888835906982, loss=0.10785654932260513
train: epoch 115, loss 0.027608435600996017, acc=0.9916666746139526, loss=0.027608435600996017
test: epoch 115, loss 0.09665767103433609, acc=0.9638888835906982, loss=0.09665767103433609
train: epoch 116, loss 0.052829399704933167, acc=0.9865000247955322, loss=0.052829399704933167
test: epoch 116, loss 0.3081592917442322, acc=0.9527778029441833, loss=0.3081592917442322
train: epoch 117, loss 0.040905728936195374, acc=0.9888333082199097, loss=0.040905728936195374
test: epoch 117, loss 0.12545894086360931, acc=0.9638888835906982, loss=0.12545894086360931
train: epoch 118, loss 0.031576886773109436, acc=0.9912222027778625, loss=0.031576886773109436
test: epoch 118, loss 0.16695527732372284, acc=0.9638888835906982, loss=0.16695527732372284
train: epoch 119, loss 0.022532865405082703, acc=0.9931666851043701, loss=0.022532865405082703
test: epoch 119, loss 0.17558853328227997, acc=0.9638888835906982, loss=0.17558853328227997
train: epoch 120, loss 0.025974726304411888, acc=0.9916666746139526, loss=0.025974726304411888
test: epoch 120, loss 0.08940001577138901, acc=0.9638888835906982, loss=0.08940001577138901
train: epoch 121, loss 0.02661641128361225, acc=0.9916666746139526, loss=0.02661641128361225
test: epoch 121, loss 0.15437644720077515, acc=0.9638888835906982, loss=0.15437644720077515
train: epoch 122, loss 0.02851608581840992, acc=0.991777777671814, loss=0.02851608581840992
test: epoch 122, loss 0.040764905512332916, acc=0.9861111044883728, loss=0.040764905512332916
train: epoch 123, loss 0.023755351081490517, acc=0.992388904094696, loss=0.023755351081490517
test: epoch 123, loss 0.042570337653160095, acc=0.9861111044883728, loss=0.042570337653160095
train: epoch 124, loss 0.021702095866203308, acc=0.9930555820465088, loss=0.021702095866203308
test: epoch 124, loss 0.04325176402926445, acc=0.9888888597488403, loss=0.04325176402926445
train: epoch 125, loss 0.014002209529280663, acc=0.9957777857780457, loss=0.014002209529280663
test: epoch 125, loss 0.036826033145189285, acc=0.9888888597488403, loss=0.036826033145189285
train: epoch 126, loss 0.0177867840975523, acc=0.9936666488647461, loss=0.0177867840975523
test: epoch 126, loss 0.017888670787215233, acc=0.9888888597488403, loss=0.017888670787215233
train: epoch 127, loss 0.024255549535155296, acc=0.9935555458068848, loss=0.024255549535155296
test: epoch 127, loss 0.027235738933086395, acc=0.9888888597488403, loss=0.027235738933086395
train: epoch 128, loss 0.018051618710160255, acc=0.9947777986526489, loss=0.018051618710160255
test: epoch 128, loss 0.12682147324085236, acc=0.9861111044883728, loss=0.12682147324085236
train: epoch 129, loss 0.01272306777536869, acc=0.9951666593551636, loss=0.01272306777536869
test: epoch 129, loss 0.03492136299610138, acc=0.9888888597488403, loss=0.03492136299610138
train: epoch 130, loss 0.018941640853881836, acc=0.9943333268165588, loss=0.018941640853881836
test: epoch 130, loss 0.023008400574326515, acc=0.9888888597488403, loss=0.023008400574326515
train: epoch 131, loss 0.015767494216561317, acc=0.9956111311912537, loss=0.015767494216561317
test: epoch 131, loss 0.03147384896874428, acc=0.9888888597488403, loss=0.03147384896874428
train: epoch 132, loss 0.011598143726587296, acc=0.996222198009491, loss=0.011598143726587296
test: epoch 132, loss 0.013623720034956932, acc=0.9944444298744202, loss=0.013623720034956932
