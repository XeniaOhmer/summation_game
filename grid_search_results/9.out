# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.995", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1558897832, receiver_embed_dim=32, save_run=0, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1558897832, receiver_embed_dim=32, save_run=False, temp_decay=0.995, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.600769281387329, acc=0.04205555468797684, loss=3.600769281387329
test: epoch 1, loss 3.452990770339966, acc=0.06388889253139496, loss=3.452990770339966
train: epoch 2, loss 3.507805109024048, acc=0.04844444617629051, loss=3.507805109024048
test: epoch 2, loss 3.3950040340423584, acc=0.07222222536802292, loss=3.3950040340423584
train: epoch 3, loss 3.4826009273529053, acc=0.04899999871850014, loss=3.4826009273529053
test: epoch 3, loss 3.2796108722686768, acc=0.04722222313284874, loss=3.2796108722686768
train: epoch 4, loss 3.448403835296631, acc=0.052666667848825455, loss=3.448403835296631
test: epoch 4, loss 3.3238940238952637, acc=0.05000000074505806, loss=3.3238940238952637
train: epoch 5, loss 3.3182501792907715, acc=0.061222221702337265, loss=3.3182501792907715
test: epoch 5, loss 3.648202896118164, acc=0.05277777835726738, loss=3.648202896118164
train: epoch 6, loss 3.1092238426208496, acc=0.08361110836267471, loss=3.1092238426208496
test: epoch 6, loss 3.720355987548828, acc=0.03888889029622078, loss=3.720355987548828
train: epoch 7, loss 2.959588050842285, acc=0.0972222238779068, loss=2.959588050842285
test: epoch 7, loss 3.694697856903076, acc=0.04444444552063942, loss=3.694697856903076
train: epoch 8, loss 2.8483493328094482, acc=0.10838888585567474, loss=2.8483493328094482
test: epoch 8, loss 3.653956890106201, acc=0.04722222313284874, loss=3.653956890106201
train: epoch 9, loss 2.7803232669830322, acc=0.1142222210764885, loss=2.7803232669830322
test: epoch 9, loss 3.7112319469451904, acc=0.04444444552063942, loss=3.7112319469451904
train: epoch 10, loss 2.724492073059082, acc=0.12244444340467453, loss=2.724492073059082
test: epoch 10, loss 3.6420862674713135, acc=0.05000000074505806, loss=3.6420862674713135
train: epoch 11, loss 2.6768388748168945, acc=0.13161110877990723, loss=2.6768388748168945
test: epoch 11, loss 3.682502031326294, acc=0.03888889029622078, loss=3.682502031326294
train: epoch 12, loss 2.639982223510742, acc=0.13383333384990692, loss=2.639982223510742
test: epoch 12, loss 3.732022523880005, acc=0.05000000074505806, loss=3.732022523880005
train: epoch 13, loss 2.610255241394043, acc=0.13966666162014008, loss=2.610255241394043
test: epoch 13, loss 3.7202794551849365, acc=0.04444444552063942, loss=3.7202794551849365
train: epoch 14, loss 2.5767719745635986, acc=0.1458333283662796, loss=2.5767719745635986
test: epoch 14, loss 3.708479166030884, acc=0.0555555559694767, loss=3.708479166030884
train: epoch 15, loss 2.553880214691162, acc=0.15000000596046448, loss=2.553880214691162
test: epoch 15, loss 3.71749210357666, acc=0.05277777835726738, loss=3.71749210357666
train: epoch 16, loss 2.539821147918701, acc=0.15311111509799957, loss=2.539821147918701
test: epoch 16, loss 3.693493366241455, acc=0.0555555559694767, loss=3.693493366241455
train: epoch 17, loss 2.512270927429199, acc=0.15611110627651215, loss=2.512270927429199
test: epoch 17, loss 3.7187914848327637, acc=0.05277777835726738, loss=3.7187914848327637
train: epoch 18, loss 2.5076096057891846, acc=0.15727777779102325, loss=2.5076096057891846
test: epoch 18, loss 3.6263527870178223, acc=0.05833333358168602, loss=3.6263527870178223
train: epoch 19, loss 2.4896774291992188, acc=0.1582222282886505, loss=2.4896774291992188
test: epoch 19, loss 3.6231026649475098, acc=0.05833333358168602, loss=3.6231026649475098
train: epoch 20, loss 2.48940372467041, acc=0.16144444048404694, loss=2.48940372467041
test: epoch 20, loss 3.588123321533203, acc=0.0555555559694767, loss=3.588123321533203
train: epoch 21, loss 2.4638473987579346, acc=0.1657777726650238, loss=2.4638473987579346
test: epoch 21, loss 3.571634292602539, acc=0.05833333358168602, loss=3.571634292602539
train: epoch 22, loss 2.437443971633911, acc=0.16855555772781372, loss=2.437443971633911
test: epoch 22, loss 3.666940689086914, acc=0.06388889253139496, loss=3.666940689086914
train: epoch 23, loss 2.436513900756836, acc=0.17000000178813934, loss=2.436513900756836
test: epoch 23, loss 3.6290946006774902, acc=0.0555555559694767, loss=3.6290946006774902
train: epoch 24, loss 2.4312593936920166, acc=0.17727777361869812, loss=2.4312593936920166
test: epoch 24, loss 3.6388611793518066, acc=0.0555555559694767, loss=3.6388611793518066
train: epoch 25, loss 2.4247190952301025, acc=0.17399999499320984, loss=2.4247190952301025
test: epoch 25, loss 3.6213531494140625, acc=0.06666667014360428, loss=3.6213531494140625
train: epoch 26, loss 2.4183599948883057, acc=0.17594444751739502, loss=2.4183599948883057
test: epoch 26, loss 3.610232353210449, acc=0.06388889253139496, loss=3.610232353210449
train: epoch 27, loss 2.406748056411743, acc=0.17838889360427856, loss=2.406748056411743
test: epoch 27, loss 3.511915445327759, acc=0.06388889253139496, loss=3.511915445327759
train: epoch 28, loss 2.4000580310821533, acc=0.1802777796983719, loss=2.4000580310821533
test: epoch 28, loss 3.471850633621216, acc=0.0694444477558136, loss=3.471850633621216
train: epoch 29, loss 2.377290725708008, acc=0.1861666738986969, loss=2.377290725708008
test: epoch 29, loss 3.480677604675293, acc=0.07222222536802292, loss=3.480677604675293
train: epoch 30, loss 2.3828392028808594, acc=0.185555562376976, loss=2.3828392028808594
test: epoch 30, loss 3.444615364074707, acc=0.07222222536802292, loss=3.444615364074707
train: epoch 31, loss 2.3792881965637207, acc=0.18522222340106964, loss=2.3792881965637207
test: epoch 31, loss 3.496016502380371, acc=0.07777778059244156, loss=3.496016502380371
train: epoch 32, loss 2.352909564971924, acc=0.19388888776302338, loss=2.352909564971924
test: epoch 32, loss 3.464806079864502, acc=0.07222222536802292, loss=3.464806079864502
train: epoch 33, loss 2.34614634513855, acc=0.19011111557483673, loss=2.34614634513855
test: epoch 33, loss 3.4961650371551514, acc=0.07777778059244156, loss=3.4961650371551514
train: epoch 34, loss 2.328634023666382, acc=0.19433332979679108, loss=2.328634023666382
test: epoch 34, loss 3.416071891784668, acc=0.08611111342906952, loss=3.416071891784668
train: epoch 35, loss 2.3346569538116455, acc=0.19394443929195404, loss=2.3346569538116455
test: epoch 35, loss 3.3847720623016357, acc=0.0833333358168602, loss=3.3847720623016357
train: epoch 36, loss 2.3106377124786377, acc=0.20327778160572052, loss=2.3106377124786377
test: epoch 36, loss 3.277789831161499, acc=0.09166666865348816, loss=3.277789831161499
train: epoch 37, loss 2.303156852722168, acc=0.19511111080646515, loss=2.303156852722168
test: epoch 37, loss 3.1594784259796143, acc=0.09444444626569748, loss=3.1594784259796143
train: epoch 38, loss 2.2921812534332275, acc=0.20222222805023193, loss=2.2921812534332275
test: epoch 38, loss 3.1758646965026855, acc=0.0972222238779068, loss=3.1758646965026855
train: epoch 39, loss 2.279766082763672, acc=0.20233333110809326, loss=2.279766082763672
test: epoch 39, loss 3.072323799133301, acc=0.10555555671453476, loss=3.072323799133301
train: epoch 40, loss 2.273913860321045, acc=0.2065555602312088, loss=2.273913860321045
test: epoch 40, loss 2.9872870445251465, acc=0.10833333432674408, loss=2.9872870445251465
train: epoch 41, loss 2.2525694370269775, acc=0.21155555546283722, loss=2.2525694370269775
test: epoch 41, loss 2.917034864425659, acc=0.1111111119389534, loss=2.917034864425659
train: epoch 42, loss 2.2479453086853027, acc=0.21238888800144196, loss=2.2479453086853027
test: epoch 42, loss 2.8947396278381348, acc=0.10833333432674408, loss=2.8947396278381348
train: epoch 43, loss 2.228801965713501, acc=0.21666666865348816, loss=2.228801965713501
test: epoch 43, loss 2.8283441066741943, acc=0.1111111119389534, loss=2.8283441066741943
train: epoch 44, loss 2.2169065475463867, acc=0.21511110663414001, loss=2.2169065475463867
test: epoch 44, loss 2.837165117263794, acc=0.11388888955116272, loss=2.837165117263794
train: epoch 45, loss 2.2067553997039795, acc=0.22011111676692963, loss=2.2067553997039795
test: epoch 45, loss 2.8361589908599854, acc=0.11388888955116272, loss=2.8361589908599854
train: epoch 46, loss 2.1888885498046875, acc=0.22127777338027954, loss=2.1888885498046875
test: epoch 46, loss 2.7684264183044434, acc=0.125, loss=2.7684264183044434
train: epoch 47, loss 2.1858158111572266, acc=0.22511111199855804, loss=2.1858158111572266
test: epoch 47, loss 2.697972536087036, acc=0.12222222238779068, loss=2.697972536087036
train: epoch 48, loss 2.1747236251831055, acc=0.22438888251781464, loss=2.1747236251831055
test: epoch 48, loss 2.6164088249206543, acc=0.125, loss=2.6164088249206543
train: epoch 49, loss 2.167738914489746, acc=0.23088888823986053, loss=2.167738914489746
test: epoch 49, loss 2.602731466293335, acc=0.13055555522441864, loss=2.602731466293335
train: epoch 50, loss 2.154297113418579, acc=0.2309444397687912, loss=2.154297113418579
test: epoch 50, loss 2.503904342651367, acc=0.14166666567325592, loss=2.503904342651367
train: epoch 51, loss 2.145449638366699, acc=0.24027778208255768, loss=2.145449638366699
test: epoch 51, loss 2.4090018272399902, acc=0.14722222089767456, loss=2.4090018272399902
train: epoch 52, loss 2.1165664196014404, acc=0.2352222204208374, loss=2.1165664196014404
test: epoch 52, loss 2.403665542602539, acc=0.15833333134651184, loss=2.403665542602539
train: epoch 53, loss 2.0927867889404297, acc=0.24161110818386078, loss=2.0927867889404297
test: epoch 53, loss 2.3326499462127686, acc=0.1666666716337204, loss=2.3326499462127686
train: epoch 54, loss 2.0923705101013184, acc=0.24505555629730225, loss=2.0923705101013184
test: epoch 54, loss 2.3220245838165283, acc=0.16111111640930176, loss=2.3220245838165283
train: epoch 55, loss 2.100821018218994, acc=0.24194444715976715, loss=2.100821018218994
test: epoch 55, loss 2.2325994968414307, acc=0.16111111640930176, loss=2.2325994968414307
train: epoch 56, loss 2.083881378173828, acc=0.2546111047267914, loss=2.083881378173828
test: epoch 56, loss 2.233020067214966, acc=0.17222222685813904, loss=2.233020067214966
train: epoch 57, loss 2.0773956775665283, acc=0.24538889527320862, loss=2.0773956775665283
test: epoch 57, loss 2.2131381034851074, acc=0.17777778208255768, loss=2.2131381034851074
train: epoch 58, loss 2.0619966983795166, acc=0.25494444370269775, loss=2.0619966983795166
test: epoch 58, loss 2.1961591243743896, acc=0.16111111640930176, loss=2.1961591243743896
train: epoch 59, loss 2.0199012756347656, acc=0.26705554127693176, loss=2.0199012756347656
test: epoch 59, loss 2.1605381965637207, acc=0.17777778208255768, loss=2.1605381965637207
train: epoch 60, loss 2.0122923851013184, acc=0.26616665720939636, loss=2.0122923851013184
test: epoch 60, loss 2.1485671997070312, acc=0.17499999701976776, loss=2.1485671997070312
train: epoch 61, loss 2.022723913192749, acc=0.26249998807907104, loss=2.022723913192749
test: epoch 61, loss 2.139650344848633, acc=0.18333333730697632, loss=2.139650344848633
train: epoch 62, loss 2.004312515258789, acc=0.2689444422721863, loss=2.004312515258789
test: epoch 62, loss 2.1238889694213867, acc=0.18333333730697632, loss=2.1238889694213867
train: epoch 63, loss 1.9937682151794434, acc=0.2671111226081848, loss=1.9937682151794434
test: epoch 63, loss 2.1084342002868652, acc=0.1805555522441864, loss=2.1084342002868652
train: epoch 64, loss 1.9712482690811157, acc=0.2737777829170227, loss=1.9712482690811157
test: epoch 64, loss 2.085153341293335, acc=0.18888889253139496, loss=2.085153341293335
train: epoch 65, loss 1.967117190361023, acc=0.2714444398880005, loss=1.967117190361023
test: epoch 65, loss 2.077697277069092, acc=0.20000000298023224, loss=2.077697277069092
train: epoch 66, loss 1.9602391719818115, acc=0.27872222661972046, loss=1.9602391719818115
test: epoch 66, loss 2.0873939990997314, acc=0.18611110746860504, loss=2.0873939990997314
train: epoch 67, loss 1.9526705741882324, acc=0.28305554389953613, loss=1.9526705741882324
test: epoch 67, loss 2.039551258087158, acc=0.19722221791744232, loss=2.039551258087158
train: epoch 68, loss 1.9277817010879517, acc=0.28966665267944336, loss=1.9277817010879517
test: epoch 68, loss 2.042304277420044, acc=0.18888889253139496, loss=2.042304277420044
train: epoch 69, loss 1.926127314567566, acc=0.2926666736602783, loss=1.926127314567566
test: epoch 69, loss 2.021003484725952, acc=0.20555555820465088, loss=2.021003484725952
train: epoch 70, loss 1.9123982191085815, acc=0.2872222363948822, loss=1.9123982191085815
test: epoch 70, loss 2.0257246494293213, acc=0.1944444477558136, loss=2.0257246494293213
train: epoch 71, loss 1.9062973260879517, acc=0.29777777194976807, loss=1.9062973260879517
test: epoch 71, loss 2.011523723602295, acc=0.20277777314186096, loss=2.011523723602295
train: epoch 72, loss 1.8939818143844604, acc=0.30061110854148865, loss=1.8939818143844604
test: epoch 72, loss 2.0085980892181396, acc=0.20277777314186096, loss=2.0085980892181396
train: epoch 73, loss 1.8754833936691284, acc=0.3005000054836273, loss=1.8754833936691284
test: epoch 73, loss 1.9962867498397827, acc=0.1944444477558136, loss=1.9962867498397827
train: epoch 74, loss 1.8786797523498535, acc=0.3011666536331177, loss=1.8786797523498535
test: epoch 74, loss 1.9991523027420044, acc=0.20277777314186096, loss=1.9991523027420044
train: epoch 75, loss 1.8614763021469116, acc=0.3052777647972107, loss=1.8614763021469116
test: epoch 75, loss 1.9827369451522827, acc=0.21111111342906952, loss=1.9827369451522827
train: epoch 76, loss 1.8547714948654175, acc=0.3091111183166504, loss=1.8547714948654175
test: epoch 76, loss 1.9694682359695435, acc=0.20555555820465088, loss=1.9694682359695435
train: epoch 77, loss 1.8297327756881714, acc=0.31627777218818665, loss=1.8297327756881714
test: epoch 77, loss 1.9650496244430542, acc=0.20555555820465088, loss=1.9650496244430542
train: epoch 78, loss 1.8219501972198486, acc=0.3179444372653961, loss=1.8219501972198486
test: epoch 78, loss 1.9706007242202759, acc=0.20555555820465088, loss=1.9706007242202759
train: epoch 79, loss 1.8062158823013306, acc=0.3148333430290222, loss=1.8062158823013306
test: epoch 79, loss 1.9624354839324951, acc=0.21388888359069824, loss=1.9624354839324951
train: epoch 80, loss 1.8183125257492065, acc=0.3210555613040924, loss=1.8183125257492065
test: epoch 80, loss 1.9540897607803345, acc=0.2083333283662796, loss=1.9540897607803345
train: epoch 81, loss 1.8051822185516357, acc=0.32572221755981445, loss=1.8051822185516357
test: epoch 81, loss 1.9554314613342285, acc=0.2083333283662796, loss=1.9554314613342285
train: epoch 82, loss 1.7907004356384277, acc=0.3266666531562805, loss=1.7907004356384277
test: epoch 82, loss 1.9488904476165771, acc=0.2083333283662796, loss=1.9488904476165771
train: epoch 83, loss 1.7937061786651611, acc=0.3308333456516266, loss=1.7937061786651611
test: epoch 83, loss 1.9348829984664917, acc=0.21388888359069824, loss=1.9348829984664917
train: epoch 84, loss 1.7724413871765137, acc=0.3309444487094879, loss=1.7724413871765137
test: epoch 84, loss 1.9302878379821777, acc=0.21388888359069824, loss=1.9302878379821777
train: epoch 85, loss 1.764859676361084, acc=0.3328889012336731, loss=1.764859676361084
test: epoch 85, loss 1.930525302886963, acc=0.2222222238779068, loss=1.930525302886963
train: epoch 86, loss 1.7523741722106934, acc=0.33766666054725647, loss=1.7523741722106934
test: epoch 86, loss 1.9202497005462646, acc=0.2222222238779068, loss=1.9202497005462646
train: epoch 87, loss 1.7545671463012695, acc=0.3400000035762787, loss=1.7545671463012695
test: epoch 87, loss 1.8991003036499023, acc=0.2222222238779068, loss=1.8991003036499023
train: epoch 88, loss 1.7431178092956543, acc=0.34672221541404724, loss=1.7431178092956543
test: epoch 88, loss 1.88802969455719, acc=0.22777777910232544, loss=1.88802969455719
train: epoch 89, loss 1.7244043350219727, acc=0.34522223472595215, loss=1.7244043350219727
test: epoch 89, loss 1.883897304534912, acc=0.22777777910232544, loss=1.883897304534912
train: epoch 90, loss 1.7132824659347534, acc=0.3562777638435364, loss=1.7132824659347534
test: epoch 90, loss 1.896092414855957, acc=0.22499999403953552, loss=1.896092414855957
train: epoch 91, loss 1.7166682481765747, acc=0.3512222170829773, loss=1.7166682481765747
test: epoch 91, loss 1.8659586906433105, acc=0.2361111044883728, loss=1.8659586906433105
train: epoch 92, loss 1.6880534887313843, acc=0.3588888943195343, loss=1.6880534887313843
test: epoch 92, loss 1.869228720664978, acc=0.23333333432674408, loss=1.869228720664978
train: epoch 93, loss 1.6916862726211548, acc=0.35811111330986023, loss=1.6916862726211548
test: epoch 93, loss 1.849009394645691, acc=0.24166665971279144, loss=1.849009394645691
train: epoch 94, loss 1.6960240602493286, acc=0.36194443702697754, loss=1.6960240602493286
test: epoch 94, loss 1.8380780220031738, acc=0.24444444477558136, loss=1.8380780220031738
train: epoch 95, loss 1.6750853061676025, acc=0.36133334040641785, loss=1.6750853061676025
test: epoch 95, loss 1.8476163148880005, acc=0.2361111044883728, loss=1.8476163148880005
train: epoch 96, loss 1.6828011274337769, acc=0.3639444410800934, loss=1.6828011274337769
test: epoch 96, loss 1.8355093002319336, acc=0.2361111044883728, loss=1.8355093002319336
train: epoch 97, loss 1.6620516777038574, acc=0.36811110377311707, loss=1.6620516777038574
test: epoch 97, loss 1.835854411125183, acc=0.2361111044883728, loss=1.835854411125183
train: epoch 98, loss 1.6467673778533936, acc=0.3734999895095825, loss=1.6467673778533936
test: epoch 98, loss 1.8263065814971924, acc=0.24166665971279144, loss=1.8263065814971924
train: epoch 99, loss 1.6380544900894165, acc=0.3799999952316284, loss=1.6380544900894165
test: epoch 99, loss 1.817864179611206, acc=0.24722221493721008, loss=1.817864179611206
train: epoch 100, loss 1.6333246231079102, acc=0.3762222230434418, loss=1.6333246231079102
test: epoch 100, loss 1.8216617107391357, acc=0.24444444477558136, loss=1.8216617107391357
train: epoch 101, loss 1.6352261304855347, acc=0.37583333253860474, loss=1.6352261304855347
test: epoch 101, loss 1.8273242712020874, acc=0.25, loss=1.8273242712020874
train: epoch 102, loss 1.6207537651062012, acc=0.38038888573646545, loss=1.6207537651062012
test: epoch 102, loss 1.807523250579834, acc=0.24722221493721008, loss=1.807523250579834
train: epoch 103, loss 1.6062871217727661, acc=0.3806111216545105, loss=1.6062871217727661
test: epoch 103, loss 1.8213785886764526, acc=0.25555557012557983, loss=1.8213785886764526
train: epoch 104, loss 1.612485408782959, acc=0.38155555725097656, loss=1.612485408782959
test: epoch 104, loss 1.8027968406677246, acc=0.24722221493721008, loss=1.8027968406677246
train: epoch 105, loss 1.6038883924484253, acc=0.38377776741981506, loss=1.6038883924484253
test: epoch 105, loss 1.7913360595703125, acc=0.25555557012557983, loss=1.7913360595703125
train: epoch 106, loss 1.6013821363449097, acc=0.3902222216129303, loss=1.6013821363449097
test: epoch 106, loss 1.8041211366653442, acc=0.24722221493721008, loss=1.8041211366653442
train: epoch 107, loss 1.5902400016784668, acc=0.39100000262260437, loss=1.5902400016784668
test: epoch 107, loss 1.8008675575256348, acc=0.25833332538604736, loss=1.8008675575256348
train: epoch 108, loss 1.5736727714538574, acc=0.39855554699897766, loss=1.5736727714538574
test: epoch 108, loss 1.790300726890564, acc=0.2527777850627899, loss=1.790300726890564
train: epoch 109, loss 1.573839545249939, acc=0.39149999618530273, loss=1.573839545249939
test: epoch 109, loss 1.784135341644287, acc=0.2611111104488373, loss=1.784135341644287
train: epoch 110, loss 1.5735671520233154, acc=0.3930000066757202, loss=1.5735671520233154
test: epoch 110, loss 1.776550054550171, acc=0.2527777850627899, loss=1.776550054550171
train: epoch 111, loss 1.5578019618988037, acc=0.4025000035762787, loss=1.5578019618988037
test: epoch 111, loss 1.7499158382415771, acc=0.2611111104488373, loss=1.7499158382415771
train: epoch 112, loss 1.5459723472595215, acc=0.40433332324028015, loss=1.5459723472595215
test: epoch 112, loss 1.7462525367736816, acc=0.26944443583488464, loss=1.7462525367736816
train: epoch 113, loss 1.5426043272018433, acc=0.406333327293396, loss=1.5426043272018433
test: epoch 113, loss 1.7463678121566772, acc=0.27222222089767456, loss=1.7463678121566772
train: epoch 114, loss 1.5482081174850464, acc=0.40299999713897705, loss=1.5482081174850464
test: epoch 114, loss 1.747165322303772, acc=0.27222222089767456, loss=1.747165322303772
train: epoch 115, loss 1.5330300331115723, acc=0.4131111204624176, loss=1.5330300331115723
test: epoch 115, loss 1.7401347160339355, acc=0.25833332538604736, loss=1.7401347160339355
train: epoch 116, loss 1.5398749113082886, acc=0.41405555605888367, loss=1.5398749113082886
test: epoch 116, loss 1.743216633796692, acc=0.2611111104488373, loss=1.743216633796692
train: epoch 117, loss 1.5165945291519165, acc=0.41894444823265076, loss=1.5165945291519165
test: epoch 117, loss 1.732192039489746, acc=0.27222222089767456, loss=1.732192039489746
train: epoch 118, loss 1.514345407485962, acc=0.41483333706855774, loss=1.514345407485962
test: epoch 118, loss 1.749037742614746, acc=0.26944443583488464, loss=1.749037742614746
train: epoch 119, loss 1.514060378074646, acc=0.41850000619888306, loss=1.514060378074646
test: epoch 119, loss 1.7410838603973389, acc=0.27222222089767456, loss=1.7410838603973389
train: epoch 120, loss 1.5087072849273682, acc=0.41927778720855713, loss=1.5087072849273682
test: epoch 120, loss 1.7388205528259277, acc=0.26944443583488464, loss=1.7388205528259277
train: epoch 121, loss 1.5054914951324463, acc=0.42027777433395386, loss=1.5054914951324463
test: epoch 121, loss 1.741136908531189, acc=0.2666666805744171, loss=1.741136908531189
train: epoch 122, loss 1.4973493814468384, acc=0.42322221398353577, loss=1.4973493814468384
test: epoch 122, loss 1.7323439121246338, acc=0.2777777910232544, loss=1.7323439121246338
train: epoch 123, loss 1.4900022745132446, acc=0.42977777123451233, loss=1.4900022745132446
test: epoch 123, loss 1.7277615070343018, acc=0.2805555462837219, loss=1.7277615070343018
train: epoch 124, loss 1.4864863157272339, acc=0.42661112546920776, loss=1.4864863157272339
test: epoch 124, loss 1.716457486152649, acc=0.2777777910232544, loss=1.716457486152649
train: epoch 125, loss 1.4775489568710327, acc=0.42977777123451233, loss=1.4775489568710327
test: epoch 125, loss 1.7168995141983032, acc=0.28333333134651184, loss=1.7168995141983032
train: epoch 126, loss 1.4771500825881958, acc=0.4316111207008362, loss=1.4771500825881958
test: epoch 126, loss 1.7088333368301392, acc=0.2805555462837219, loss=1.7088333368301392
train: epoch 127, loss 1.4639359712600708, acc=0.43638888001441956, loss=1.4639359712600708
test: epoch 127, loss 1.7060596942901611, acc=0.28333333134651184, loss=1.7060596942901611
train: epoch 128, loss 1.4626809358596802, acc=0.43488889932632446, loss=1.4626809358596802
test: epoch 128, loss 1.7000807523727417, acc=0.2805555462837219, loss=1.7000807523727417
train: epoch 129, loss 1.452360987663269, acc=0.4395555555820465, loss=1.452360987663269
test: epoch 129, loss 1.6892132759094238, acc=0.2888889014720917, loss=1.6892132759094238
train: epoch 130, loss 1.450081467628479, acc=0.43922221660614014, loss=1.450081467628479
test: epoch 130, loss 1.6960868835449219, acc=0.2888889014720917, loss=1.6960868835449219
train: epoch 131, loss 1.452751874923706, acc=0.4418888986110687, loss=1.452751874923706
test: epoch 131, loss 1.6987742185592651, acc=0.28333333134651184, loss=1.6987742185592651
train: epoch 132, loss 1.4339419603347778, acc=0.4415000081062317, loss=1.4339419603347778
test: epoch 132, loss 1.6950032711029053, acc=0.2916666567325592, loss=1.6950032711029053
train: epoch 133, loss 1.4398157596588135, acc=0.4407777786254883, loss=1.4398157596588135
test: epoch 133, loss 1.6881358623504639, acc=0.2916666567325592, loss=1.6881358623504639
train: epoch 134, loss 1.4554380178451538, acc=0.44655555486679077, loss=1.4554380178451538
test: epoch 134, loss 1.6864339113235474, acc=0.28611111640930176, loss=1.6864339113235474
train: epoch 135, loss 1.4369767904281616, acc=0.4444444477558136, loss=1.4369767904281616
test: epoch 135, loss 1.6911648511886597, acc=0.28611111640930176, loss=1.6911648511886597
train: epoch 136, loss 1.4328536987304688, acc=0.4511111080646515, loss=1.4328536987304688
test: epoch 136, loss 1.678420901298523, acc=0.27222222089767456, loss=1.678420901298523
train: epoch 137, loss 1.4287652969360352, acc=0.45233333110809326, loss=1.4287652969360352
test: epoch 137, loss 1.6753153800964355, acc=0.28611111640930176, loss=1.6753153800964355
train: epoch 138, loss 1.4204764366149902, acc=0.45350000262260437, loss=1.4204764366149902
test: epoch 138, loss 1.6751993894577026, acc=0.2888889014720917, loss=1.6751993894577026
train: epoch 139, loss 1.4061249494552612, acc=0.4558333456516266, loss=1.4061249494552612
test: epoch 139, loss 1.6751347780227661, acc=0.2805555462837219, loss=1.6751347780227661
train: epoch 140, loss 1.4139642715454102, acc=0.4546111226081848, loss=1.4139642715454102
test: epoch 140, loss 1.6767208576202393, acc=0.27222222089767456, loss=1.6767208576202393
train: epoch 141, loss 1.3978145122528076, acc=0.457111120223999, loss=1.3978145122528076
test: epoch 141, loss 1.6725369691848755, acc=0.28611111640930176, loss=1.6725369691848755
train: epoch 142, loss 1.4083828926086426, acc=0.4601111114025116, loss=1.4083828926086426
test: epoch 142, loss 1.677146553993225, acc=0.28611111640930176, loss=1.677146553993225
train: epoch 143, loss 1.3918561935424805, acc=0.4620555639266968, loss=1.3918561935424805
test: epoch 143, loss 1.6599115133285522, acc=0.2805555462837219, loss=1.6599115133285522
train: epoch 144, loss 1.3771016597747803, acc=0.4692777693271637, loss=1.3771016597747803
test: epoch 144, loss 1.6612918376922607, acc=0.2805555462837219, loss=1.6612918376922607
train: epoch 145, loss 1.3887391090393066, acc=0.4651666581630707, loss=1.3887391090393066
test: epoch 145, loss 1.6641974449157715, acc=0.2805555462837219, loss=1.6641974449157715
train: epoch 146, loss 1.3569481372833252, acc=0.47111111879348755, loss=1.3569481372833252
test: epoch 146, loss 1.6716587543487549, acc=0.2777777910232544, loss=1.6716587543487549
train: epoch 147, loss 1.398865818977356, acc=0.4641111195087433, loss=1.398865818977356
test: epoch 147, loss 1.6588419675827026, acc=0.28611111640930176, loss=1.6588419675827026
train: epoch 148, loss 1.3656651973724365, acc=0.47183331847190857, loss=1.3656651973724365
test: epoch 148, loss 1.6579995155334473, acc=0.2750000059604645, loss=1.6579995155334473
train: epoch 149, loss 1.356091856956482, acc=0.47083333134651184, loss=1.356091856956482
test: epoch 149, loss 1.657077431678772, acc=0.28333333134651184, loss=1.657077431678772
train: epoch 150, loss 1.3750299215316772, acc=0.4741666615009308, loss=1.3750299215316772
test: epoch 150, loss 1.6512686014175415, acc=0.2805555462837219, loss=1.6512686014175415
