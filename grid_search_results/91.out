# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=100", "--receiver_embed_dim=64", "--temperature=2.0", "--temp_decay=1", "--one_hot=true", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_summands=2, n_symbols=100, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=724909796, receiver_embed_dim=64, temp_decay=1.0, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 2.8759472370147705, acc=0.09394444525241852, loss=2.8759472370147705
test: epoch 1, loss 4.344757556915283, acc=0.0833333358168602, loss=4.344757556915283
train: epoch 2, loss 1.7477056980133057, acc=0.296277791261673, loss=1.7477056980133057
test: epoch 2, loss 4.983433723449707, acc=0.14444445073604584, loss=4.983433723449707
train: epoch 3, loss 1.1845645904541016, acc=0.5173333287239075, loss=1.1845645904541016
test: epoch 3, loss 4.607771396636963, acc=0.15000000596046448, loss=4.607771396636963
train: epoch 4, loss 0.8510587215423584, acc=0.6629444360733032, loss=0.8510587215423584
test: epoch 4, loss 3.741010904312134, acc=0.2527777850627899, loss=3.741010904312134
train: epoch 5, loss 0.6744202971458435, acc=0.7318333387374878, loss=0.6744202971458435
test: epoch 5, loss 3.4324123859405518, acc=0.25833332538604736, loss=3.4324123859405518
train: epoch 6, loss 0.5755191445350647, acc=0.773277759552002, loss=0.5755191445350647
test: epoch 6, loss 3.2391397953033447, acc=0.2666666805744171, loss=3.2391397953033447
train: epoch 7, loss 0.4738161563873291, acc=0.8174444437026978, loss=0.4738161563873291
test: epoch 7, loss 3.297691583633423, acc=0.34166666865348816, loss=3.297691583633423
train: epoch 8, loss 0.4310291111469269, acc=0.8369444608688354, loss=0.4310291111469269
test: epoch 8, loss 2.952409029006958, acc=0.28611111640930176, loss=2.952409029006958
train: epoch 9, loss 0.38107630610466003, acc=0.8595555424690247, loss=0.38107630610466003
test: epoch 9, loss 3.014451265335083, acc=0.3055555522441864, loss=3.014451265335083
train: epoch 10, loss 0.3413606286048889, acc=0.8778333067893982, loss=0.3413606286048889
test: epoch 10, loss 3.0857365131378174, acc=0.3305555582046509, loss=3.0857365131378174
train: epoch 11, loss 0.3373875916004181, acc=0.8851110935211182, loss=0.3373875916004181
test: epoch 11, loss 2.4615375995635986, acc=0.375, loss=2.4615375995635986
train: epoch 12, loss 0.2881847620010376, acc=0.8995555639266968, loss=0.2881847620010376
test: epoch 12, loss 2.7658255100250244, acc=0.3499999940395355, loss=2.7658255100250244
train: epoch 13, loss 0.25368955731391907, acc=0.9135000109672546, loss=0.25368955731391907
test: epoch 13, loss 2.825289011001587, acc=0.30000001192092896, loss=2.825289011001587
train: epoch 14, loss 0.2540390193462372, acc=0.9138333201408386, loss=0.2540390193462372
test: epoch 14, loss 2.3077633380889893, acc=0.39444443583488464, loss=2.3077633380889893
train: epoch 15, loss 0.2077644020318985, acc=0.9301111102104187, loss=0.2077644020318985
test: epoch 15, loss 2.780916452407837, acc=0.33888888359069824, loss=2.780916452407837
train: epoch 16, loss 0.21714842319488525, acc=0.929888904094696, loss=0.21714842319488525
test: epoch 16, loss 2.9930455684661865, acc=0.375, loss=2.9930455684661865
train: epoch 17, loss 0.20076394081115723, acc=0.9342222213745117, loss=0.20076394081115723
test: epoch 17, loss 2.1371688842773438, acc=0.4555555582046509, loss=2.1371688842773438
train: epoch 18, loss 0.1981336772441864, acc=0.9358888864517212, loss=0.1981336772441864
test: epoch 18, loss 2.682222843170166, acc=0.3722222149372101, loss=2.682222843170166
train: epoch 19, loss 0.1739300638437271, acc=0.9419999718666077, loss=0.1739300638437271
test: epoch 19, loss 2.1515331268310547, acc=0.43888887763023376, loss=2.1515331268310547
train: epoch 20, loss 0.1546853929758072, acc=0.949222207069397, loss=0.1546853929758072
test: epoch 20, loss 2.1515350341796875, acc=0.40833333134651184, loss=2.1515350341796875
train: epoch 21, loss 0.1648048460483551, acc=0.94477778673172, loss=0.1648048460483551
test: epoch 21, loss 2.0738236904144287, acc=0.49444442987442017, loss=2.0738236904144287
train: epoch 22, loss 0.16186028718948364, acc=0.9470000267028809, loss=0.16186028718948364
test: epoch 22, loss 2.597302198410034, acc=0.375, loss=2.597302198410034
train: epoch 23, loss 0.15865062177181244, acc=0.9492777585983276, loss=0.15865062177181244
test: epoch 23, loss 2.561255693435669, acc=0.4000000059604645, loss=2.561255693435669
train: epoch 24, loss 0.14464254677295685, acc=0.9532777667045593, loss=0.14464254677295685
test: epoch 24, loss 2.1199686527252197, acc=0.375, loss=2.1199686527252197
train: epoch 25, loss 0.13661691546440125, acc=0.9570000171661377, loss=0.13661691546440125
test: epoch 25, loss 2.7721831798553467, acc=0.36944442987442017, loss=2.7721831798553467
train: epoch 26, loss 0.1144310012459755, acc=0.9629444479942322, loss=0.1144310012459755
test: epoch 26, loss 2.4715139865875244, acc=0.4138889014720917, loss=2.4715139865875244
train: epoch 27, loss 0.13490964472293854, acc=0.9536111354827881, loss=0.13490964472293854
test: epoch 27, loss 1.994242548942566, acc=0.4694444537162781, loss=1.994242548942566
train: epoch 28, loss 0.10609438270330429, acc=0.964888870716095, loss=0.10609438270330429
test: epoch 28, loss 2.1377110481262207, acc=0.43888887763023376, loss=2.1377110481262207
train: epoch 29, loss 0.11864090710878372, acc=0.9610000252723694, loss=0.11864090710878372
test: epoch 29, loss 2.6722476482391357, acc=0.3722222149372101, loss=2.6722476482391357
train: epoch 30, loss 0.10073258727788925, acc=0.9674999713897705, loss=0.10073258727788925
test: epoch 30, loss 2.5129308700561523, acc=0.4472222328186035, loss=2.5129308700561523
train: epoch 31, loss 0.12590403854846954, acc=0.9578889012336731, loss=0.12590403854846954
test: epoch 31, loss 2.4359829425811768, acc=0.4444444477558136, loss=2.4359829425811768
train: epoch 32, loss 0.09606441110372543, acc=0.9691666960716248, loss=0.09606441110372543
test: epoch 32, loss 2.0137557983398438, acc=0.4833333194255829, loss=2.0137557983398438
train: epoch 33, loss 0.10128384083509445, acc=0.9670555591583252, loss=0.10128384083509445
test: epoch 33, loss 2.1950395107269287, acc=0.46388888359069824, loss=2.1950395107269287
train: epoch 34, loss 0.10086120665073395, acc=0.9663888812065125, loss=0.10086120665073395
test: epoch 34, loss 2.494574785232544, acc=0.42222222685813904, loss=2.494574785232544
train: epoch 35, loss 0.09397872537374496, acc=0.9697222113609314, loss=0.09397872537374496
test: epoch 35, loss 2.302114963531494, acc=0.38055557012557983, loss=2.302114963531494
train: epoch 36, loss 0.09606850892305374, acc=0.9700000286102295, loss=0.09606850892305374
test: epoch 36, loss 2.4338183403015137, acc=0.43611112236976624, loss=2.4338183403015137
train: epoch 37, loss 0.08060944080352783, acc=0.9731666445732117, loss=0.08060944080352783
test: epoch 37, loss 2.028642416000366, acc=0.4472222328186035, loss=2.028642416000366
train: epoch 38, loss 0.10377051681280136, acc=0.9657777547836304, loss=0.10377051681280136
test: epoch 38, loss 2.6186537742614746, acc=0.3888888955116272, loss=2.6186537742614746
train: epoch 39, loss 0.07711394876241684, acc=0.9758333563804626, loss=0.07711394876241684
test: epoch 39, loss 2.8773858547210693, acc=0.4055555462837219, loss=2.8773858547210693
train: epoch 40, loss 0.07531285285949707, acc=0.9753333330154419, loss=0.07531285285949707
test: epoch 40, loss 2.392855167388916, acc=0.4333333373069763, loss=2.392855167388916
train: epoch 41, loss 0.07655919343233109, acc=0.9746111035346985, loss=0.07655919343233109
test: epoch 41, loss 2.197986602783203, acc=0.5027777552604675, loss=2.197986602783203
train: epoch 42, loss 0.09413624554872513, acc=0.9706666469573975, loss=0.09413624554872513
test: epoch 42, loss 2.679652690887451, acc=0.4555555582046509, loss=2.679652690887451
train: epoch 43, loss 0.07229585200548172, acc=0.9785555601119995, loss=0.07229585200548172
test: epoch 43, loss 2.3262553215026855, acc=0.45277777314186096, loss=2.3262553215026855
train: epoch 44, loss 0.08466919511556625, acc=0.972777783870697, loss=0.08466919511556625
test: epoch 44, loss 2.263841390609741, acc=0.5472221970558167, loss=2.263841390609741
train: epoch 45, loss 0.07732082158327103, acc=0.9755555391311646, loss=0.07732082158327103
test: epoch 45, loss 2.5216386318206787, acc=0.4861111044883728, loss=2.5216386318206787
train: epoch 46, loss 0.08230368047952652, acc=0.9751666784286499, loss=0.08230368047952652
test: epoch 46, loss 2.010572910308838, acc=0.5027777552604675, loss=2.010572910308838
train: epoch 47, loss 0.060670316219329834, acc=0.9821666479110718, loss=0.060670316219329834
test: epoch 47, loss 2.0416784286499023, acc=0.5083333253860474, loss=2.0416784286499023
train: epoch 48, loss 0.07033246010541916, acc=0.9779999852180481, loss=0.07033246010541916
test: epoch 48, loss 2.6018619537353516, acc=0.3777777850627899, loss=2.6018619537353516
train: epoch 49, loss 0.05642791464924812, acc=0.9815555810928345, loss=0.05642791464924812
test: epoch 49, loss 2.3747637271881104, acc=0.48055556416511536, loss=2.3747637271881104
train: epoch 50, loss 0.09387171268463135, acc=0.9732221961021423, loss=0.09387171268463135
test: epoch 50, loss 2.169250249862671, acc=0.4722222089767456, loss=2.169250249862671
train: epoch 51, loss 0.06193213164806366, acc=0.9808333516120911, loss=0.06193213164806366
test: epoch 51, loss 1.9600536823272705, acc=0.4555555582046509, loss=1.9600536823272705
train: epoch 52, loss 0.049832724034786224, acc=0.9853333234786987, loss=0.049832724034786224
test: epoch 52, loss 1.9263437986373901, acc=0.5277777910232544, loss=1.9263437986373901
train: epoch 53, loss 0.07018573582172394, acc=0.9787777662277222, loss=0.07018573582172394
test: epoch 53, loss 2.4819791316986084, acc=0.5027777552604675, loss=2.4819791316986084
train: epoch 54, loss 0.059594158083200455, acc=0.9808333516120911, loss=0.059594158083200455
test: epoch 54, loss 2.175900936126709, acc=0.5111111402511597, loss=2.175900936126709
train: epoch 55, loss 0.053266122937202454, acc=0.9838888645172119, loss=0.053266122937202454
test: epoch 55, loss 2.5317296981811523, acc=0.4888888895511627, loss=2.5317296981811523
train: epoch 56, loss 0.06001158431172371, acc=0.9809444546699524, loss=0.06001158431172371
test: epoch 56, loss 2.0252902507781982, acc=0.550000011920929, loss=2.0252902507781982
train: epoch 57, loss 0.06306403130292892, acc=0.9812222123146057, loss=0.06306403130292892
test: epoch 57, loss 2.110203504562378, acc=0.4888888895511627, loss=2.110203504562378
train: epoch 58, loss 0.05966157466173172, acc=0.9804444313049316, loss=0.05966157466173172
test: epoch 58, loss 1.8467971086502075, acc=0.47777777910232544, loss=1.8467971086502075
train: epoch 59, loss 0.06013692170381546, acc=0.9814444184303284, loss=0.06013692170381546
test: epoch 59, loss 1.9576932191848755, acc=0.5166666507720947, loss=1.9576932191848755
train: epoch 60, loss 0.052277982234954834, acc=0.9828888773918152, loss=0.052277982234954834
test: epoch 60, loss 2.4632415771484375, acc=0.4861111044883728, loss=2.4632415771484375
train: epoch 61, loss 0.0616525337100029, acc=0.9821110963821411, loss=0.0616525337100029
test: epoch 61, loss 2.091454029083252, acc=0.46666666865348816, loss=2.091454029083252
train: epoch 62, loss 0.05666934326291084, acc=0.9844444394111633, loss=0.05666934326291084
test: epoch 62, loss 1.7735016345977783, acc=0.5722222328186035, loss=1.7735016345977783
train: epoch 63, loss 0.050014711916446686, acc=0.984499990940094, loss=0.050014711916446686
test: epoch 63, loss 2.113901376724243, acc=0.5249999761581421, loss=2.113901376724243
train: epoch 64, loss 0.049448736011981964, acc=0.9837222099304199, loss=0.049448736011981964
test: epoch 64, loss 1.853165864944458, acc=0.5527777671813965, loss=1.853165864944458
train: epoch 65, loss 0.05527820810675621, acc=0.9822777509689331, loss=0.05527820810675621
test: epoch 65, loss 2.4462029933929443, acc=0.519444465637207, loss=2.4462029933929443
train: epoch 66, loss 0.049932848662137985, acc=0.9836666584014893, loss=0.049932848662137985
test: epoch 66, loss 1.8252757787704468, acc=0.5249999761581421, loss=1.8252757787704468
train: epoch 67, loss 0.05724950134754181, acc=0.9821110963821411, loss=0.05724950134754181
test: epoch 67, loss 2.5952305793762207, acc=0.5166666507720947, loss=2.5952305793762207
train: epoch 68, loss 0.044544391334056854, acc=0.9869444370269775, loss=0.044544391334056854
test: epoch 68, loss 2.3446695804595947, acc=0.43888887763023376, loss=2.3446695804595947
train: epoch 69, loss 0.05418964847922325, acc=0.9831666946411133, loss=0.05418964847922325
test: epoch 69, loss 1.8054596185684204, acc=0.5527777671813965, loss=1.8054596185684204
train: epoch 70, loss 0.04345662519335747, acc=0.9863888621330261, loss=0.04345662519335747
test: epoch 70, loss 2.1934397220611572, acc=0.5055555701255798, loss=2.1934397220611572
train: epoch 71, loss 0.05490987375378609, acc=0.9828888773918152, loss=0.05490987375378609
test: epoch 71, loss 2.164435625076294, acc=0.550000011920929, loss=2.164435625076294
train: epoch 72, loss 0.05420079827308655, acc=0.9835555553436279, loss=0.05420079827308655
test: epoch 72, loss 2.382612705230713, acc=0.4972222149372101, loss=2.382612705230713
train: epoch 73, loss 0.038191039115190506, acc=0.9879444241523743, loss=0.038191039115190506
test: epoch 73, loss 2.086327075958252, acc=0.5361111164093018, loss=2.086327075958252
train: epoch 74, loss 0.042041290551424026, acc=0.9862222075462341, loss=0.042041290551424026
test: epoch 74, loss 1.8526068925857544, acc=0.6111111044883728, loss=1.8526068925857544
train: epoch 75, loss 0.043636199086904526, acc=0.9866666793823242, loss=0.043636199086904526
test: epoch 75, loss 2.267738103866577, acc=0.5555555820465088, loss=2.267738103866577
train: epoch 76, loss 0.03706913813948631, acc=0.987500011920929, loss=0.03706913813948631
test: epoch 76, loss 2.0019407272338867, acc=0.5472221970558167, loss=2.0019407272338867
train: epoch 77, loss 0.04860169067978859, acc=0.9852222204208374, loss=0.04860169067978859
test: epoch 77, loss 1.5574476718902588, acc=0.625, loss=1.5574476718902588
train: epoch 78, loss 0.03836485370993614, acc=0.9878888726234436, loss=0.03836485370993614
test: epoch 78, loss 2.024789571762085, acc=0.6000000238418579, loss=2.024789571762085
train: epoch 79, loss 0.040467604994773865, acc=0.9879999756813049, loss=0.040467604994773865
test: epoch 79, loss 1.8872199058532715, acc=0.5944444537162781, loss=1.8872199058532715
train: epoch 80, loss 0.032832175493240356, acc=0.9904999732971191, loss=0.032832175493240356
test: epoch 80, loss 1.772212266921997, acc=0.6111111044883728, loss=1.772212266921997
train: epoch 81, loss 0.05115088075399399, acc=0.9856111407279968, loss=0.05115088075399399
test: epoch 81, loss 1.671905517578125, acc=0.5583333373069763, loss=1.671905517578125
train: epoch 82, loss 0.03600885719060898, acc=0.9891111254692078, loss=0.03600885719060898
test: epoch 82, loss 1.558763027191162, acc=0.675000011920929, loss=1.558763027191162
train: epoch 83, loss 0.04818769171833992, acc=0.9863333106040955, loss=0.04818769171833992
test: epoch 83, loss 1.975081443786621, acc=0.5861111283302307, loss=1.975081443786621
train: epoch 84, loss 0.03522809222340584, acc=0.9888888597488403, loss=0.03522809222340584
test: epoch 84, loss 1.884928822517395, acc=0.5722222328186035, loss=1.884928822517395
train: epoch 85, loss 0.04953479766845703, acc=0.9860000014305115, loss=0.04953479766845703
test: epoch 85, loss 1.8712079524993896, acc=0.6027777791023254, loss=1.8712079524993896
train: epoch 86, loss 0.030536293983459473, acc=0.9904999732971191, loss=0.030536293983459473
test: epoch 86, loss 2.41005802154541, acc=0.6166666746139526, loss=2.41005802154541
train: epoch 87, loss 0.032671477645635605, acc=0.9892777800559998, loss=0.032671477645635605
test: epoch 87, loss 1.9709827899932861, acc=0.6305555701255798, loss=1.9709827899932861
train: epoch 88, loss 0.031106779351830482, acc=0.9904999732971191, loss=0.031106779351830482
test: epoch 88, loss 1.654633641242981, acc=0.6499999761581421, loss=1.654633641242981
train: epoch 89, loss 0.047765593975782394, acc=0.9865000247955322, loss=0.047765593975782394
test: epoch 89, loss 1.4472090005874634, acc=0.7222222089767456, loss=1.4472090005874634
train: epoch 90, loss 0.046085357666015625, acc=0.9862222075462341, loss=0.046085357666015625
test: epoch 90, loss 1.1756871938705444, acc=0.6611111164093018, loss=1.1756871938705444
train: epoch 91, loss 0.028322067111730576, acc=0.9913889169692993, loss=0.028322067111730576
test: epoch 91, loss 1.6726281642913818, acc=0.6472222208976746, loss=1.6726281642913818
train: epoch 92, loss 0.04412480443716049, acc=0.9860000014305115, loss=0.04412480443716049
test: epoch 92, loss 1.6519229412078857, acc=0.7194444537162781, loss=1.6519229412078857
train: epoch 93, loss 0.031130898743867874, acc=0.9912222027778625, loss=0.031130898743867874
test: epoch 93, loss 1.4960952997207642, acc=0.6638888716697693, loss=1.4960952997207642
train: epoch 94, loss 0.02599155716598034, acc=0.9919999837875366, loss=0.02599155716598034
test: epoch 94, loss 1.9577133655548096, acc=0.644444465637207, loss=1.9577133655548096
train: epoch 95, loss 0.03393534570932388, acc=0.9913889169692993, loss=0.03393534570932388
test: epoch 95, loss 1.5044727325439453, acc=0.7138888835906982, loss=1.5044727325439453
train: epoch 96, loss 0.03263209015130997, acc=0.9893888831138611, loss=0.03263209015130997
test: epoch 96, loss 1.1974250078201294, acc=0.7472222447395325, loss=1.1974250078201294
train: epoch 97, loss 0.05211769416928291, acc=0.9836111068725586, loss=0.05211769416928291
test: epoch 97, loss 1.309701681137085, acc=0.7194444537162781, loss=1.309701681137085
train: epoch 98, loss 0.02993807941675186, acc=0.9910555481910706, loss=0.02993807941675186
test: epoch 98, loss 1.660009503364563, acc=0.7361111044883728, loss=1.660009503364563
train: epoch 99, loss 0.029097292572259903, acc=0.992111086845398, loss=0.029097292572259903
test: epoch 99, loss 1.3396131992340088, acc=0.7722222208976746, loss=1.3396131992340088
train: epoch 100, loss 0.031168019399046898, acc=0.9909444451332092, loss=0.031168019399046898
test: epoch 100, loss 1.4824635982513428, acc=0.6777777671813965, loss=1.4824635982513428
train: epoch 101, loss 0.02842089533805847, acc=0.9919999837875366, loss=0.02842089533805847
test: epoch 101, loss 1.3944534063339233, acc=0.7222222089767456, loss=1.3944534063339233
train: epoch 102, loss 0.04225710406899452, acc=0.988777756690979, loss=0.04225710406899452
test: epoch 102, loss 1.0759130716323853, acc=0.7416666746139526, loss=1.0759130716323853
train: epoch 103, loss 0.023120924830436707, acc=0.9931666851043701, loss=0.023120924830436707
test: epoch 103, loss 1.0968539714813232, acc=0.7138888835906982, loss=1.0968539714813232
train: epoch 104, loss 0.022840803489089012, acc=0.9932222366333008, loss=0.022840803489089012
test: epoch 104, loss 1.0730546712875366, acc=0.800000011920929, loss=1.0730546712875366
train: epoch 105, loss 0.03136402368545532, acc=0.9908333420753479, loss=0.03136402368545532
test: epoch 105, loss 1.2874902486801147, acc=0.7666666507720947, loss=1.2874902486801147
train: epoch 106, loss 0.02767382562160492, acc=0.9931111335754395, loss=0.02767382562160492
test: epoch 106, loss 0.8229321241378784, acc=0.8222222328186035, loss=0.8229321241378784
train: epoch 107, loss 0.02541678212583065, acc=0.9930555820465088, loss=0.02541678212583065
test: epoch 107, loss 1.0315407514572144, acc=0.7916666865348816, loss=1.0315407514572144
train: epoch 108, loss 0.04248299077153206, acc=0.9886666536331177, loss=0.04248299077153206
test: epoch 108, loss 0.948301374912262, acc=0.7777777910232544, loss=0.948301374912262
train: epoch 109, loss 0.020904330536723137, acc=0.9939444661140442, loss=0.020904330536723137
test: epoch 109, loss 1.3456465005874634, acc=0.6944444179534912, loss=1.3456465005874634
train: epoch 110, loss 0.028949297964572906, acc=0.9908888936042786, loss=0.028949297964572906
test: epoch 110, loss 0.7205963134765625, acc=0.8166666626930237, loss=0.7205963134765625
train: epoch 111, loss 0.01891094632446766, acc=0.9956111311912537, loss=0.01891094632446766
test: epoch 111, loss 1.1182637214660645, acc=0.7888888716697693, loss=1.1182637214660645
train: epoch 112, loss 0.036362141370773315, acc=0.9892777800559998, loss=0.036362141370773315
test: epoch 112, loss 0.9901002645492554, acc=0.8111110925674438, loss=0.9901002645492554
train: epoch 113, loss 0.026040488854050636, acc=0.9928333163261414, loss=0.026040488854050636
test: epoch 113, loss 1.2247260808944702, acc=0.7916666865348816, loss=1.2247260808944702
train: epoch 114, loss 0.02389548160135746, acc=0.9930555820465088, loss=0.02389548160135746
test: epoch 114, loss 1.5325602293014526, acc=0.7666666507720947, loss=1.5325602293014526
train: epoch 115, loss 0.02846083976328373, acc=0.9931666851043701, loss=0.02846083976328373
test: epoch 115, loss 0.8246899843215942, acc=0.8444444537162781, loss=0.8246899843215942
train: epoch 116, loss 0.02318587712943554, acc=0.9932222366333008, loss=0.02318587712943554
test: epoch 116, loss 1.2466479539871216, acc=0.7555555701255798, loss=1.2466479539871216
train: epoch 117, loss 0.02167963795363903, acc=0.9941111207008362, loss=0.02167963795363903
test: epoch 117, loss 1.163732647895813, acc=0.800000011920929, loss=1.163732647895813
train: epoch 118, loss 0.02163541689515114, acc=0.9943333268165588, loss=0.02163541689515114
test: epoch 118, loss 0.846309244632721, acc=0.7583333253860474, loss=0.846309244632721
train: epoch 119, loss 0.024672603234648705, acc=0.9924444556236267, loss=0.024672603234648705
test: epoch 119, loss 0.7880196571350098, acc=0.8138889074325562, loss=0.7880196571350098
train: epoch 120, loss 0.024711735546588898, acc=0.9929444193840027, loss=0.024711735546588898
test: epoch 120, loss 0.8712348341941833, acc=0.7722222208976746, loss=0.8712348341941833
train: epoch 121, loss 0.015717700123786926, acc=0.9950000047683716, loss=0.015717700123786926
test: epoch 121, loss 0.8235712051391602, acc=0.8222222328186035, loss=0.8235712051391602
train: epoch 122, loss 0.024854598566889763, acc=0.9934999942779541, loss=0.024854598566889763
test: epoch 122, loss 1.3104884624481201, acc=0.7749999761581421, loss=1.3104884624481201
train: epoch 123, loss 0.027444858103990555, acc=0.9930555820465088, loss=0.027444858103990555
test: epoch 123, loss 0.702639639377594, acc=0.8444444537162781, loss=0.702639639377594
train: epoch 124, loss 0.022834260016679764, acc=0.9937777519226074, loss=0.022834260016679764
test: epoch 124, loss 0.6541251540184021, acc=0.8388888835906982, loss=0.6541251540184021
train: epoch 125, loss 0.016307387501001358, acc=0.9955000281333923, loss=0.016307387501001358
test: epoch 125, loss 0.8115438222885132, acc=0.8638888597488403, loss=0.8115438222885132
train: epoch 126, loss 0.02168959006667137, acc=0.9942222237586975, loss=0.02168959006667137
test: epoch 126, loss 0.7982125282287598, acc=0.7472222447395325, loss=0.7982125282287598
train: epoch 127, loss 0.022672368213534355, acc=0.9927777647972107, loss=0.022672368213534355
test: epoch 127, loss 0.9898144602775574, acc=0.7777777910232544, loss=0.9898144602775574
train: epoch 128, loss 0.02519713155925274, acc=0.9933888912200928, loss=0.02519713155925274
test: epoch 128, loss 0.6193053722381592, acc=0.8361111283302307, loss=0.6193053722381592
train: epoch 129, loss 0.01678960956633091, acc=0.9955000281333923, loss=0.01678960956633091
test: epoch 129, loss 0.7653231620788574, acc=0.8222222328186035, loss=0.7653231620788574
train: epoch 130, loss 0.01640779711306095, acc=0.9956111311912537, loss=0.01640779711306095
test: epoch 130, loss 0.6274474859237671, acc=0.8722222447395325, loss=0.6274474859237671
train: epoch 131, loss 0.018414270132780075, acc=0.9946666955947876, loss=0.018414270132780075
test: epoch 131, loss 0.8318424224853516, acc=0.8222222328186035, loss=0.8318424224853516
train: epoch 132, loss 0.025644836947321892, acc=0.9936666488647461, loss=0.025644836947321892
test: epoch 132, loss 0.9722177386283875, acc=0.8305555582046509, loss=0.9722177386283875
train: epoch 133, loss 0.016098041087388992, acc=0.9952222108840942, loss=0.016098041087388992
test: epoch 133, loss 0.4211483597755432, acc=0.9194444417953491, loss=0.4211483597755432
train: epoch 134, loss 0.007710661273449659, acc=0.9978888630867004, loss=0.007710661273449659
test: epoch 134, loss 0.7234475016593933, acc=0.8722222447395325, loss=0.7234475016593933
train: epoch 135, loss 0.026505447924137115, acc=0.9940555691719055, loss=0.026505447924137115
test: epoch 135, loss 0.5778241753578186, acc=0.8638888597488403, loss=0.5778241753578186
train: epoch 136, loss 0.02450924552977085, acc=0.9939444661140442, loss=0.02450924552977085
test: epoch 136, loss 0.7651983499526978, acc=0.8416666388511658, loss=0.7651983499526978
train: epoch 137, loss 0.023545263335108757, acc=0.9941111207008362, loss=0.023545263335108757
test: epoch 137, loss 0.5548760294914246, acc=0.8527777791023254, loss=0.5548760294914246
train: epoch 138, loss 0.014099068939685822, acc=0.9965000152587891, loss=0.014099068939685822
test: epoch 138, loss 0.54613196849823, acc=0.9027777910232544, loss=0.54613196849823
train: epoch 139, loss 0.014728831127285957, acc=0.9963333606719971, loss=0.014728831127285957
test: epoch 139, loss 0.5579420328140259, acc=0.8916666507720947, loss=0.5579420328140259
train: epoch 140, loss 0.016345778480172157, acc=0.9952777624130249, loss=0.016345778480172157
test: epoch 140, loss 0.9908314347267151, acc=0.8388888835906982, loss=0.9908314347267151
train: epoch 141, loss 0.022510303184390068, acc=0.9939444661140442, loss=0.022510303184390068
test: epoch 141, loss 0.5592482686042786, acc=0.9055555462837219, loss=0.5592482686042786
train: epoch 142, loss 0.01520654745399952, acc=0.9959999918937683, loss=0.01520654745399952
test: epoch 142, loss 0.4592103064060211, acc=0.9138888716697693, loss=0.4592103064060211
train: epoch 143, loss 0.016255607828497887, acc=0.995722234249115, loss=0.016255607828497887
test: epoch 143, loss 0.5603571534156799, acc=0.8777777552604675, loss=0.5603571534156799
train: epoch 144, loss 0.019614053890109062, acc=0.9959999918937683, loss=0.019614053890109062
test: epoch 144, loss 0.9312220215797424, acc=0.8833333253860474, loss=0.9312220215797424
train: epoch 145, loss 0.014942835085093975, acc=0.9959999918937683, loss=0.014942835085093975
test: epoch 145, loss 0.68998122215271, acc=0.8999999761581421, loss=0.68998122215271
train: epoch 146, loss 0.010824928060173988, acc=0.9972222447395325, loss=0.010824928060173988
test: epoch 146, loss 0.5840752720832825, acc=0.9138888716697693, loss=0.5840752720832825
train: epoch 147, loss 0.017597787082195282, acc=0.9953888654708862, loss=0.017597787082195282
test: epoch 147, loss 0.5546109080314636, acc=0.8861111402511597, loss=0.5546109080314636
train: epoch 148, loss 0.020452085882425308, acc=0.9937777519226074, loss=0.020452085882425308
test: epoch 148, loss 0.6614823341369629, acc=0.9055555462837219, loss=0.6614823341369629
train: epoch 149, loss 0.01442402321845293, acc=0.9966111183166504, loss=0.01442402321845293
test: epoch 149, loss 0.7179130911827087, acc=0.875, loss=0.7179130911827087
train: epoch 150, loss 0.014107123017311096, acc=0.9961666464805603, loss=0.014107123017311096
test: epoch 150, loss 0.43703290820121765, acc=0.9527778029441833, loss=0.43703290820121765
