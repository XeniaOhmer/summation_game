# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=32", "--temperature=1", "--temp_decay=0.99", "--one_hot=1", "--n_layers=3"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=399430284, receiver_embed_dim=32, save_run=0, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=3, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=399430284, receiver_embed_dim=32, save_run=False, temp_decay=0.99, temperature=1.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.2926130294799805, acc=0.059111110866069794, loss=3.2926130294799805
test: epoch 1, loss 3.2413852214813232, acc=0.04722222313284874, loss=3.2413852214813232
train: epoch 2, loss 2.487182140350342, acc=0.1539444476366043, loss=2.487182140350342
test: epoch 2, loss 2.783719539642334, acc=0.08055555820465088, loss=2.783719539642334
train: epoch 3, loss 2.0346004962921143, acc=0.23605555295944214, loss=2.0346004962921143
test: epoch 3, loss 2.7449252605438232, acc=0.1388888955116272, loss=2.7449252605438232
train: epoch 4, loss 1.8044675588607788, acc=0.29466667771339417, loss=1.8044675588607788
test: epoch 4, loss 2.384411573410034, acc=0.19722221791744232, loss=2.384411573410034
train: epoch 5, loss 1.6403697729110718, acc=0.3371666669845581, loss=1.6403697729110718
test: epoch 5, loss 2.047572135925293, acc=0.2361111044883728, loss=2.047572135925293
train: epoch 6, loss 1.5259376764297485, acc=0.3791666626930237, loss=1.5259376764297485
test: epoch 6, loss 2.0342135429382324, acc=0.21666666865348816, loss=2.0342135429382324
train: epoch 7, loss 1.4112995862960815, acc=0.41449999809265137, loss=1.4112995862960815
test: epoch 7, loss 1.9774068593978882, acc=0.22777777910232544, loss=1.9774068593978882
train: epoch 8, loss 1.3334201574325562, acc=0.4422222077846527, loss=1.3334201574325562
test: epoch 8, loss 1.80670166015625, acc=0.2361111044883728, loss=1.80670166015625
train: epoch 9, loss 1.2583365440368652, acc=0.4683888852596283, loss=1.2583365440368652
test: epoch 9, loss 1.6421672105789185, acc=0.2805555462837219, loss=1.6421672105789185
train: epoch 10, loss 1.2337846755981445, acc=0.48027777671813965, loss=1.2337846755981445
test: epoch 10, loss 1.636715054512024, acc=0.29722222685813904, loss=1.636715054512024
train: epoch 11, loss 1.1753290891647339, acc=0.4949444532394409, loss=1.1753290891647339
test: epoch 11, loss 1.7414907217025757, acc=0.28611111640930176, loss=1.7414907217025757
train: epoch 12, loss 1.1380172967910767, acc=0.5103333592414856, loss=1.1380172967910767
test: epoch 12, loss 1.6488393545150757, acc=0.32499998807907104, loss=1.6488393545150757
train: epoch 13, loss 1.111882209777832, acc=0.5130000114440918, loss=1.111882209777832
test: epoch 13, loss 1.747284173965454, acc=0.28611111640930176, loss=1.747284173965454
train: epoch 14, loss 1.091176986694336, acc=0.5236666798591614, loss=1.091176986694336
test: epoch 14, loss 1.7862982749938965, acc=0.30000001192092896, loss=1.7862982749938965
train: epoch 15, loss 1.0722613334655762, acc=0.534166693687439, loss=1.0722613334655762
test: epoch 15, loss 1.718514084815979, acc=0.3499999940395355, loss=1.718514084815979
train: epoch 16, loss 1.058443307876587, acc=0.5375000238418579, loss=1.058443307876587
test: epoch 16, loss 1.5967458486557007, acc=0.31111112236976624, loss=1.5967458486557007
train: epoch 17, loss 1.0249390602111816, acc=0.5447221994400024, loss=1.0249390602111816
test: epoch 17, loss 1.6620988845825195, acc=0.3194444477558136, loss=1.6620988845825195
train: epoch 18, loss 1.0040700435638428, acc=0.559499979019165, loss=1.0040700435638428
test: epoch 18, loss 1.5623303651809692, acc=0.3333333432674408, loss=1.5623303651809692
train: epoch 19, loss 0.9882883429527283, acc=0.5709999799728394, loss=0.9882883429527283
test: epoch 19, loss 1.6754473447799683, acc=0.32499998807907104, loss=1.6754473447799683
train: epoch 20, loss 0.9661991000175476, acc=0.5766111016273499, loss=0.9661991000175476
test: epoch 20, loss 1.706018090248108, acc=0.3472222089767456, loss=1.706018090248108
train: epoch 21, loss 0.9675325751304626, acc=0.5824999809265137, loss=0.9675325751304626
test: epoch 21, loss 1.564156413078308, acc=0.3583333194255829, loss=1.564156413078308
train: epoch 22, loss 0.9555721879005432, acc=0.5888333320617676, loss=0.9555721879005432
test: epoch 22, loss 1.511444091796875, acc=0.38333332538604736, loss=1.511444091796875
train: epoch 23, loss 0.9263563752174377, acc=0.6003888845443726, loss=0.9263563752174377
test: epoch 23, loss 1.5155220031738281, acc=0.3916666805744171, loss=1.5155220031738281
train: epoch 24, loss 0.9189152121543884, acc=0.6031110882759094, loss=0.9189152121543884
test: epoch 24, loss 1.4328383207321167, acc=0.42500001192092896, loss=1.4328383207321167
train: epoch 25, loss 0.9143049120903015, acc=0.605055570602417, loss=0.9143049120903015
test: epoch 25, loss 1.6120082139968872, acc=0.3722222149372101, loss=1.6120082139968872
train: epoch 26, loss 0.9005026817321777, acc=0.6112222075462341, loss=0.9005026817321777
test: epoch 26, loss 1.5007704496383667, acc=0.4166666567325592, loss=1.5007704496383667
train: epoch 27, loss 0.8885282874107361, acc=0.6208333373069763, loss=0.8885282874107361
test: epoch 27, loss 1.3506419658660889, acc=0.4277777671813965, loss=1.3506419658660889
train: epoch 28, loss 0.8706910014152527, acc=0.6309999823570251, loss=0.8706910014152527
test: epoch 28, loss 1.4064692258834839, acc=0.4888888895511627, loss=1.4064692258834839
train: epoch 29, loss 0.8486815094947815, acc=0.6440555453300476, loss=0.8486815094947815
test: epoch 29, loss 1.288704514503479, acc=0.4416666626930237, loss=1.288704514503479
train: epoch 30, loss 0.8458651900291443, acc=0.6497222185134888, loss=0.8458651900291443
test: epoch 30, loss 1.3482823371887207, acc=0.4472222328186035, loss=1.3482823371887207
train: epoch 31, loss 0.8319304585456848, acc=0.6546666622161865, loss=0.8319304585456848
test: epoch 31, loss 1.3946123123168945, acc=0.47777777910232544, loss=1.3946123123168945
train: epoch 32, loss 0.8169159889221191, acc=0.656166672706604, loss=0.8169159889221191
test: epoch 32, loss 1.4051706790924072, acc=0.45277777314186096, loss=1.4051706790924072
train: epoch 33, loss 0.8119277358055115, acc=0.6589999794960022, loss=0.8119277358055115
test: epoch 33, loss 1.2395317554473877, acc=0.4888888895511627, loss=1.2395317554473877
train: epoch 34, loss 0.7895156145095825, acc=0.6704999804496765, loss=0.7895156145095825
test: epoch 34, loss 1.5525721311569214, acc=0.4555555582046509, loss=1.5525721311569214
train: epoch 35, loss 0.7937212586402893, acc=0.6676666736602783, loss=0.7937212586402893
test: epoch 35, loss 1.4088585376739502, acc=0.4888888895511627, loss=1.4088585376739502
train: epoch 36, loss 0.7814321517944336, acc=0.6723333597183228, loss=0.7814321517944336
test: epoch 36, loss 1.4013005495071411, acc=0.4694444537162781, loss=1.4013005495071411
train: epoch 37, loss 0.7782198190689087, acc=0.6754444241523743, loss=0.7782198190689087
test: epoch 37, loss 1.380357265472412, acc=0.4583333432674408, loss=1.380357265472412
train: epoch 38, loss 0.7641208171844482, acc=0.6806666851043701, loss=0.7641208171844482
test: epoch 38, loss 1.2150646448135376, acc=0.4972222149372101, loss=1.2150646448135376
train: epoch 39, loss 0.7476974725723267, acc=0.6901666522026062, loss=0.7476974725723267
test: epoch 39, loss 1.3958239555358887, acc=0.46666666865348816, loss=1.3958239555358887
train: epoch 40, loss 0.7606440186500549, acc=0.6847777962684631, loss=0.7606440186500549
test: epoch 40, loss 1.3457740545272827, acc=0.5027777552604675, loss=1.3457740545272827
train: epoch 41, loss 0.7519350647926331, acc=0.6900555491447449, loss=0.7519350647926331
test: epoch 41, loss 1.2421376705169678, acc=0.46666666865348816, loss=1.2421376705169678
train: epoch 42, loss 0.7246891856193542, acc=0.6976110935211182, loss=0.7246891856193542
test: epoch 42, loss 1.3524836301803589, acc=0.5, loss=1.3524836301803589
train: epoch 43, loss 0.7236444354057312, acc=0.6998888850212097, loss=0.7236444354057312
test: epoch 43, loss 1.4384478330612183, acc=0.4694444537162781, loss=1.4384478330612183
train: epoch 44, loss 0.7160738110542297, acc=0.6997777819633484, loss=0.7160738110542297
test: epoch 44, loss 1.315085530281067, acc=0.49444442987442017, loss=1.315085530281067
train: epoch 45, loss 0.709949254989624, acc=0.7079444527626038, loss=0.709949254989624
test: epoch 45, loss 1.3529771566390991, acc=0.5222222208976746, loss=1.3529771566390991
train: epoch 46, loss 0.697067379951477, acc=0.7098888754844666, loss=0.697067379951477
test: epoch 46, loss 1.333940029144287, acc=0.48055556416511536, loss=1.333940029144287
train: epoch 47, loss 0.6678979992866516, acc=0.726722240447998, loss=0.6678979992866516
test: epoch 47, loss 1.3284382820129395, acc=0.5166666507720947, loss=1.3284382820129395
train: epoch 48, loss 0.6568973064422607, acc=0.7310555577278137, loss=0.6568973064422607
test: epoch 48, loss 1.406441330909729, acc=0.5222222208976746, loss=1.406441330909729
train: epoch 49, loss 0.6579827666282654, acc=0.7319999933242798, loss=0.6579827666282654
test: epoch 49, loss 1.2313989400863647, acc=0.4833333194255829, loss=1.2313989400863647
train: epoch 50, loss 0.6467292308807373, acc=0.7320555448532104, loss=0.6467292308807373
test: epoch 50, loss 1.230586051940918, acc=0.47777777910232544, loss=1.230586051940918
train: epoch 51, loss 0.6361218690872192, acc=0.7407777905464172, loss=0.6361218690872192
test: epoch 51, loss 1.262107014656067, acc=0.5222222208976746, loss=1.262107014656067
train: epoch 52, loss 0.6362758278846741, acc=0.7384999990463257, loss=0.6362758278846741
test: epoch 52, loss 1.380593180656433, acc=0.5305555462837219, loss=1.380593180656433
train: epoch 53, loss 0.6261727809906006, acc=0.7447222471237183, loss=0.6261727809906006
test: epoch 53, loss 1.3702131509780884, acc=0.4888888895511627, loss=1.3702131509780884
train: epoch 54, loss 0.6163313388824463, acc=0.7449444532394409, loss=0.6163313388824463
test: epoch 54, loss 1.4730497598648071, acc=0.5027777552604675, loss=1.4730497598648071
train: epoch 55, loss 0.609028160572052, acc=0.7481111288070679, loss=0.609028160572052
test: epoch 55, loss 1.3338360786437988, acc=0.5111111402511597, loss=1.3338360786437988
train: epoch 56, loss 0.6185476183891296, acc=0.7491666674613953, loss=0.6185476183891296
test: epoch 56, loss 1.3962621688842773, acc=0.4833333194255829, loss=1.3962621688842773
train: epoch 57, loss 0.5937003493309021, acc=0.7553333044052124, loss=0.5937003493309021
test: epoch 57, loss 1.3628208637237549, acc=0.4888888895511627, loss=1.3628208637237549
train: epoch 58, loss 0.5952635407447815, acc=0.7578333616256714, loss=0.5952635407447815
test: epoch 58, loss 1.3091009855270386, acc=0.4833333194255829, loss=1.3091009855270386
train: epoch 59, loss 0.6072515845298767, acc=0.7536110877990723, loss=0.6072515845298767
test: epoch 59, loss 1.3183284997940063, acc=0.4583333432674408, loss=1.3183284997940063
train: epoch 60, loss 0.5995076298713684, acc=0.753944456577301, loss=0.5995076298713684
test: epoch 60, loss 1.3159383535385132, acc=0.48055556416511536, loss=1.3159383535385132
train: epoch 61, loss 0.5841200351715088, acc=0.758388876914978, loss=0.5841200351715088
test: epoch 61, loss 1.3819752931594849, acc=0.5055555701255798, loss=1.3819752931594849
train: epoch 62, loss 0.5823698043823242, acc=0.7606666684150696, loss=0.5823698043823242
test: epoch 62, loss 1.418377161026001, acc=0.4416666626930237, loss=1.418377161026001
train: epoch 63, loss 0.5810670852661133, acc=0.7609999775886536, loss=0.5810670852661133
test: epoch 63, loss 1.3835656642913818, acc=0.4833333194255829, loss=1.3835656642913818
train: epoch 64, loss 0.5696141123771667, acc=0.7659444212913513, loss=0.5696141123771667
test: epoch 64, loss 1.3562313318252563, acc=0.5083333253860474, loss=1.3562313318252563
train: epoch 65, loss 0.5824804902076721, acc=0.7567222118377686, loss=0.5824804902076721
test: epoch 65, loss 1.4342654943466187, acc=0.4888888895511627, loss=1.4342654943466187
train: epoch 66, loss 0.5704378485679626, acc=0.7627221941947937, loss=0.5704378485679626
test: epoch 66, loss 1.4497382640838623, acc=0.49444442987442017, loss=1.4497382640838623
train: epoch 67, loss 0.575019359588623, acc=0.7624444365501404, loss=0.575019359588623
test: epoch 67, loss 1.370552659034729, acc=0.5333333611488342, loss=1.370552659034729
train: epoch 68, loss 0.5641041398048401, acc=0.7676666378974915, loss=0.5641041398048401
test: epoch 68, loss 1.5547397136688232, acc=0.49444442987442017, loss=1.5547397136688232
train: epoch 69, loss 0.566238522529602, acc=0.7641111016273499, loss=0.566238522529602
test: epoch 69, loss 1.4227800369262695, acc=0.5305555462837219, loss=1.4227800369262695
train: epoch 70, loss 0.5658133029937744, acc=0.7670555710792542, loss=0.5658133029937744
test: epoch 70, loss 1.3748958110809326, acc=0.5472221970558167, loss=1.3748958110809326
train: epoch 71, loss 0.5692718029022217, acc=0.7641666531562805, loss=0.5692718029022217
test: epoch 71, loss 1.2851433753967285, acc=0.5444444417953491, loss=1.2851433753967285
train: epoch 72, loss 0.5634675025939941, acc=0.7658888697624207, loss=0.5634675025939941
test: epoch 72, loss 1.3259717226028442, acc=0.4749999940395355, loss=1.3259717226028442
train: epoch 73, loss 0.5609158277511597, acc=0.7675555348396301, loss=0.5609158277511597
test: epoch 73, loss 1.0867687463760376, acc=0.550000011920929, loss=1.0867687463760376
train: epoch 74, loss 0.549189031124115, acc=0.7708333134651184, loss=0.549189031124115
test: epoch 74, loss 1.4428561925888062, acc=0.5222222208976746, loss=1.4428561925888062
train: epoch 75, loss 0.547825038433075, acc=0.7703889012336731, loss=0.547825038433075
test: epoch 75, loss 1.3004450798034668, acc=0.550000011920929, loss=1.3004450798034668
train: epoch 76, loss 0.5607580542564392, acc=0.765999972820282, loss=0.5607580542564392
test: epoch 76, loss 1.279136061668396, acc=0.5472221970558167, loss=1.279136061668396
train: epoch 77, loss 0.5568799376487732, acc=0.7712777853012085, loss=0.5568799376487732
test: epoch 77, loss 1.379870057106018, acc=0.4972222149372101, loss=1.379870057106018
train: epoch 78, loss 0.5459378361701965, acc=0.7717221975326538, loss=0.5459378361701965
test: epoch 78, loss 1.2471660375595093, acc=0.5583333373069763, loss=1.2471660375595093
train: epoch 79, loss 0.5417675971984863, acc=0.7739444375038147, loss=0.5417675971984863
test: epoch 79, loss 1.4046920537948608, acc=0.48055556416511536, loss=1.4046920537948608
train: epoch 80, loss 0.5327003002166748, acc=0.7774999737739563, loss=0.5327003002166748
test: epoch 80, loss 1.3619624376296997, acc=0.5333333611488342, loss=1.3619624376296997
train: epoch 81, loss 0.5425083041191101, acc=0.7748888731002808, loss=0.5425083041191101
test: epoch 81, loss 1.1446771621704102, acc=0.550000011920929, loss=1.1446771621704102
train: epoch 82, loss 0.5436926484107971, acc=0.7726110816001892, loss=0.5436926484107971
test: epoch 82, loss 1.3409596681594849, acc=0.5472221970558167, loss=1.3409596681594849
train: epoch 83, loss 0.5338600873947144, acc=0.7761666774749756, loss=0.5338600873947144
test: epoch 83, loss 1.3612215518951416, acc=0.5166666507720947, loss=1.3612215518951416
train: epoch 84, loss 0.5257672071456909, acc=0.7821111083030701, loss=0.5257672071456909
test: epoch 84, loss 1.3997713327407837, acc=0.49166667461395264, loss=1.3997713327407837
train: epoch 85, loss 0.5322332978248596, acc=0.7760555744171143, loss=0.5322332978248596
test: epoch 85, loss 1.2186998128890991, acc=0.5472221970558167, loss=1.2186998128890991
train: epoch 86, loss 0.527899980545044, acc=0.776888906955719, loss=0.527899980545044
test: epoch 86, loss 1.293077826499939, acc=0.5111111402511597, loss=1.293077826499939
train: epoch 87, loss 0.5219299793243408, acc=0.7832777500152588, loss=0.5219299793243408
test: epoch 87, loss 1.4061803817749023, acc=0.5527777671813965, loss=1.4061803817749023
train: epoch 88, loss 0.5348296761512756, acc=0.7772777676582336, loss=0.5348296761512756
test: epoch 88, loss 1.3340541124343872, acc=0.5333333611488342, loss=1.3340541124343872
train: epoch 89, loss 0.5202563405036926, acc=0.7802222371101379, loss=0.5202563405036926
test: epoch 89, loss 1.3634542226791382, acc=0.5277777910232544, loss=1.3634542226791382
train: epoch 90, loss 0.5268545150756836, acc=0.7793333530426025, loss=0.5268545150756836
test: epoch 90, loss 1.1030954122543335, acc=0.550000011920929, loss=1.1030954122543335
train: epoch 91, loss 0.5189663767814636, acc=0.7792222499847412, loss=0.5189663767814636
test: epoch 91, loss 1.2165671586990356, acc=0.5472221970558167, loss=1.2165671586990356
train: epoch 92, loss 0.515609085559845, acc=0.7827222347259521, loss=0.515609085559845
test: epoch 92, loss 1.1828738451004028, acc=0.5277777910232544, loss=1.1828738451004028
train: epoch 93, loss 0.5144869685173035, acc=0.7822777628898621, loss=0.5144869685173035
test: epoch 93, loss 1.325196385383606, acc=0.5444444417953491, loss=1.325196385383606
train: epoch 94, loss 0.5100847482681274, acc=0.7859444618225098, loss=0.5100847482681274
test: epoch 94, loss 1.1128238439559937, acc=0.5888888835906982, loss=1.1128238439559937
train: epoch 95, loss 0.5154823064804077, acc=0.7826111316680908, loss=0.5154823064804077
test: epoch 95, loss 1.2628413438796997, acc=0.5611110925674438, loss=1.2628413438796997
train: epoch 96, loss 0.5217006206512451, acc=0.7825000286102295, loss=0.5217006206512451
test: epoch 96, loss 1.1444203853607178, acc=0.5472221970558167, loss=1.1444203853607178
train: epoch 97, loss 0.5122070908546448, acc=0.7861111164093018, loss=0.5122070908546448
test: epoch 97, loss 1.2629714012145996, acc=0.5805555582046509, loss=1.2629714012145996
train: epoch 98, loss 0.5176904201507568, acc=0.7830555438995361, loss=0.5176904201507568
test: epoch 98, loss 1.3723918199539185, acc=0.574999988079071, loss=1.3723918199539185
train: epoch 99, loss 0.5043846368789673, acc=0.7858333587646484, loss=0.5043846368789673
test: epoch 99, loss 1.34133780002594, acc=0.5416666865348816, loss=1.34133780002594
train: epoch 100, loss 0.5031675696372986, acc=0.7873888611793518, loss=0.5031675696372986
test: epoch 100, loss 1.412877082824707, acc=0.4972222149372101, loss=1.412877082824707
train: epoch 101, loss 0.5164595246315002, acc=0.781333327293396, loss=0.5164595246315002
test: epoch 101, loss 1.1975278854370117, acc=0.5555555820465088, loss=1.1975278854370117
train: epoch 102, loss 0.5126575827598572, acc=0.7861666679382324, loss=0.5126575827598572
test: epoch 102, loss 1.3032642602920532, acc=0.550000011920929, loss=1.3032642602920532
train: epoch 103, loss 0.5043321847915649, acc=0.7900555729866028, loss=0.5043321847915649
test: epoch 103, loss 0.9851431250572205, acc=0.5527777671813965, loss=0.9851431250572205
train: epoch 104, loss 0.5100480318069458, acc=0.7861111164093018, loss=0.5100480318069458
test: epoch 104, loss 1.3161695003509521, acc=0.5527777671813965, loss=1.3161695003509521
train: epoch 105, loss 0.5019786357879639, acc=0.7877777814865112, loss=0.5019786357879639
test: epoch 105, loss 1.1155885457992554, acc=0.5861111283302307, loss=1.1155885457992554
train: epoch 106, loss 0.4935365617275238, acc=0.7940000295639038, loss=0.4935365617275238
test: epoch 106, loss 1.2957926988601685, acc=0.5555555820465088, loss=1.2957926988601685
train: epoch 107, loss 0.5044357180595398, acc=0.7875555753707886, loss=0.5044357180595398
test: epoch 107, loss 1.3044432401657104, acc=0.5416666865348816, loss=1.3044432401657104
train: epoch 108, loss 0.5171540975570679, acc=0.7821666598320007, loss=0.5171540975570679
test: epoch 108, loss 1.244629144668579, acc=0.5833333134651184, loss=1.244629144668579
train: epoch 109, loss 0.496782124042511, acc=0.7894444465637207, loss=0.496782124042511
test: epoch 109, loss 1.2335656881332397, acc=0.5527777671813965, loss=1.2335656881332397
train: epoch 110, loss 0.4953373372554779, acc=0.792388916015625, loss=0.4953373372554779
test: epoch 110, loss 1.3960641622543335, acc=0.5583333373069763, loss=1.3960641622543335
train: epoch 111, loss 0.4995262622833252, acc=0.7912777662277222, loss=0.4995262622833252
test: epoch 111, loss 1.3895844221115112, acc=0.5333333611488342, loss=1.3895844221115112
train: epoch 112, loss 0.5013307332992554, acc=0.7926111221313477, loss=0.5013307332992554
test: epoch 112, loss 1.272569179534912, acc=0.5527777671813965, loss=1.272569179534912
train: epoch 113, loss 0.4933375418186188, acc=0.7900555729866028, loss=0.4933375418186188
test: epoch 113, loss 1.3295519351959229, acc=0.5777778029441833, loss=1.3295519351959229
train: epoch 114, loss 0.48992112278938293, acc=0.7932778000831604, loss=0.48992112278938293
test: epoch 114, loss 1.1892117261886597, acc=0.5527777671813965, loss=1.1892117261886597
train: epoch 115, loss 0.4988223910331726, acc=0.789222240447998, loss=0.4988223910331726
test: epoch 115, loss 1.2858282327651978, acc=0.5527777671813965, loss=1.2858282327651978
train: epoch 116, loss 0.5010854601860046, acc=0.7903888821601868, loss=0.5010854601860046
test: epoch 116, loss 1.281197428703308, acc=0.574999988079071, loss=1.281197428703308
train: epoch 117, loss 0.5108082294464111, acc=0.7837222218513489, loss=0.5108082294464111
test: epoch 117, loss 1.2648824453353882, acc=0.5972222089767456, loss=1.2648824453353882
train: epoch 118, loss 0.49469470977783203, acc=0.7897777557373047, loss=0.49469470977783203
test: epoch 118, loss 1.3105216026306152, acc=0.574999988079071, loss=1.3105216026306152
train: epoch 119, loss 0.48221689462661743, acc=0.7956666946411133, loss=0.48221689462661743
test: epoch 119, loss 1.3758797645568848, acc=0.5777778029441833, loss=1.3758797645568848
train: epoch 120, loss 0.5047575831413269, acc=0.7876666784286499, loss=0.5047575831413269
test: epoch 120, loss 1.3052879571914673, acc=0.550000011920929, loss=1.3052879571914673
train: epoch 121, loss 0.4922269284725189, acc=0.7931666374206543, loss=0.4922269284725189
test: epoch 121, loss 1.2568856477737427, acc=0.574999988079071, loss=1.2568856477737427
train: epoch 122, loss 0.503646194934845, acc=0.7889999747276306, loss=0.503646194934845
test: epoch 122, loss 1.2775949239730835, acc=0.5444444417953491, loss=1.2775949239730835
train: epoch 123, loss 0.49076002836227417, acc=0.7942222356796265, loss=0.49076002836227417
test: epoch 123, loss 1.1136398315429688, acc=0.5611110925674438, loss=1.1136398315429688
train: epoch 124, loss 0.4808735251426697, acc=0.7973333597183228, loss=0.4808735251426697
test: epoch 124, loss 1.2622913122177124, acc=0.5472221970558167, loss=1.2622913122177124
train: epoch 125, loss 0.49520403146743774, acc=0.7947221994400024, loss=0.49520403146743774
test: epoch 125, loss 1.4625062942504883, acc=0.5583333373069763, loss=1.4625062942504883
train: epoch 126, loss 0.4960611164569855, acc=0.7933889031410217, loss=0.4960611164569855
test: epoch 126, loss 1.3763355016708374, acc=0.5444444417953491, loss=1.3763355016708374
train: epoch 127, loss 0.4780387878417969, acc=0.8002222180366516, loss=0.4780387878417969
test: epoch 127, loss 1.19271719455719, acc=0.5944444537162781, loss=1.19271719455719
train: epoch 128, loss 0.4910813570022583, acc=0.7952222228050232, loss=0.4910813570022583
test: epoch 128, loss 1.0975242853164673, acc=0.5944444537162781, loss=1.0975242853164673
train: epoch 129, loss 0.4798418879508972, acc=0.8004444241523743, loss=0.4798418879508972
test: epoch 129, loss 1.1711968183517456, acc=0.5944444537162781, loss=1.1711968183517456
train: epoch 130, loss 0.4756269156932831, acc=0.801111102104187, loss=0.4756269156932831
test: epoch 130, loss 1.2458858489990234, acc=0.5861111283302307, loss=1.2458858489990234
train: epoch 131, loss 0.4802241921424866, acc=0.7971110939979553, loss=0.4802241921424866
test: epoch 131, loss 1.223312258720398, acc=0.5611110925674438, loss=1.223312258720398
train: epoch 132, loss 0.4913305342197418, acc=0.7956666946411133, loss=0.4913305342197418
test: epoch 132, loss 1.3091200590133667, acc=0.605555534362793, loss=1.3091200590133667
train: epoch 133, loss 0.4723268449306488, acc=0.8013888597488403, loss=0.4723268449306488
test: epoch 133, loss 1.2102552652359009, acc=0.6000000238418579, loss=1.2102552652359009
train: epoch 134, loss 0.47418370842933655, acc=0.8015555739402771, loss=0.47418370842933655
test: epoch 134, loss 1.2665756940841675, acc=0.6027777791023254, loss=1.2665756940841675
train: epoch 135, loss 0.48338571190834045, acc=0.7975555658340454, loss=0.48338571190834045
test: epoch 135, loss 1.1463574171066284, acc=0.6027777791023254, loss=1.1463574171066284
train: epoch 136, loss 0.4746437966823578, acc=0.8023889064788818, loss=0.4746437966823578
test: epoch 136, loss 1.1963087320327759, acc=0.605555534362793, loss=1.1963087320327759
train: epoch 137, loss 0.4801841378211975, acc=0.8010555505752563, loss=0.4801841378211975
test: epoch 137, loss 1.3023351430892944, acc=0.574999988079071, loss=1.3023351430892944
train: epoch 138, loss 0.4877369999885559, acc=0.7972221970558167, loss=0.4877369999885559
test: epoch 138, loss 1.1778672933578491, acc=0.605555534362793, loss=1.1778672933578491
train: epoch 139, loss 0.48018211126327515, acc=0.7993888854980469, loss=0.48018211126327515
test: epoch 139, loss 1.1246055364608765, acc=0.6000000238418579, loss=1.1246055364608765
train: epoch 140, loss 0.46349409222602844, acc=0.8050000071525574, loss=0.46349409222602844
test: epoch 140, loss 1.2161771059036255, acc=0.6027777791023254, loss=1.2161771059036255
train: epoch 141, loss 0.48567771911621094, acc=0.7947777509689331, loss=0.48567771911621094
test: epoch 141, loss 1.2671058177947998, acc=0.5916666388511658, loss=1.2671058177947998
train: epoch 142, loss 0.48159968852996826, acc=0.7975000143051147, loss=0.48159968852996826
test: epoch 142, loss 1.267109751701355, acc=0.605555534362793, loss=1.267109751701355
train: epoch 143, loss 0.4562114477157593, acc=0.8071110844612122, loss=0.4562114477157593
test: epoch 143, loss 1.2494958639144897, acc=0.6000000238418579, loss=1.2494958639144897
train: epoch 144, loss 0.46715572476387024, acc=0.8016666769981384, loss=0.46715572476387024
test: epoch 144, loss 1.2479616403579712, acc=0.605555534362793, loss=1.2479616403579712
train: epoch 145, loss 0.4647795259952545, acc=0.804611086845398, loss=0.4647795259952545
test: epoch 145, loss 1.317541480064392, acc=0.605555534362793, loss=1.317541480064392
train: epoch 146, loss 0.4669276177883148, acc=0.8058888912200928, loss=0.4669276177883148
test: epoch 146, loss 1.2672189474105835, acc=0.605555534362793, loss=1.2672189474105835
train: epoch 147, loss 0.4642699956893921, acc=0.8036110997200012, loss=0.4642699956893921
test: epoch 147, loss 1.1866661310195923, acc=0.605555534362793, loss=1.1866661310195923
train: epoch 148, loss 0.47914066910743713, acc=0.7998889088630676, loss=0.47914066910743713
test: epoch 148, loss 1.2084786891937256, acc=0.6000000238418579, loss=1.2084786891937256
train: epoch 149, loss 0.4702913463115692, acc=0.8017222285270691, loss=0.4702913463115692
test: epoch 149, loss 1.2776544094085693, acc=0.605555534362793, loss=1.2776544094085693
train: epoch 150, loss 0.45826518535614014, acc=0.805388867855072, loss=0.45826518535614014
test: epoch 150, loss 1.1970157623291016, acc=0.605555534362793, loss=1.1970157623291016
