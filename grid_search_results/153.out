# ["--N=20", "--n_epochs=150", "--batch_size=32", "--lr=0.001", "--n_symbols=200", "--receiver_embed_dim=128", "--temperature=2.0", "--temp_decay=0.995", "--one_hot=1", "--n_layers=1"]
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=1, optimizer='adam', preemptable=False, random_seed=1540372047, receiver_embed_dim=128, save_run=0, temp_decay=0.995, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
Namespace(N=20, batch_size=32, checkpoint_dir=None, checkpoint_freq=0, cuda=True, data_scaling=50, device=device(type='cuda'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'), distributed_port=18363, early_stopping_acc=0.99, fp16=False, load_from_checkpoint=None, lr=0.001, max_len=1, n_epochs=150, n_layers=1, n_runs=1, n_summands=2, n_symbols=200, no_cuda=False, one_hot=True, optimizer='adam', preemptable=False, random_seed=1540372047, receiver_embed_dim=128, save_run=False, temp_decay=0.995, temperature=2.0, tensorboard=False, tensorboard_dir='runs/', test_split=0.1, update_freq=1, validation_freq=1, vocab_size=10)
train: epoch 1, loss 3.5114810466766357, acc=0.05194444581866264, loss=3.5114810466766357
test: epoch 1, loss 3.344008207321167, acc=0.05277777835726738, loss=3.344008207321167
train: epoch 2, loss 3.402970790863037, acc=0.054722223430871964, loss=3.402970790863037
test: epoch 2, loss 3.8115031719207764, acc=0.0555555559694767, loss=3.8115031719207764
train: epoch 3, loss 2.8767282962799072, acc=0.12255555391311646, loss=2.8767282962799072
test: epoch 3, loss 7.003698825836182, acc=0.03333333507180214, loss=7.003698825836182
train: epoch 4, loss 2.469769239425659, acc=0.179666668176651, loss=2.469769239425659
test: epoch 4, loss 8.232636451721191, acc=0.03333333507180214, loss=8.232636451721191
train: epoch 5, loss 2.2662510871887207, acc=0.21805556118488312, loss=2.2662510871887207
test: epoch 5, loss 8.904778480529785, acc=0.04722222313284874, loss=8.904778480529785
train: epoch 6, loss 2.1420557498931885, acc=0.24655555188655853, loss=2.1420557498931885
test: epoch 6, loss 9.033408164978027, acc=0.06388889253139496, loss=9.033408164978027
train: epoch 7, loss 2.050352096557617, acc=0.26116666197776794, loss=2.050352096557617
test: epoch 7, loss 9.86087703704834, acc=0.06111111119389534, loss=9.86087703704834
train: epoch 8, loss 1.9888051748275757, acc=0.28255555033683777, loss=1.9888051748275757
test: epoch 8, loss 9.987316131591797, acc=0.06111111119389534, loss=9.987316131591797
train: epoch 9, loss 1.9306460618972778, acc=0.30311110615730286, loss=1.9306460618972778
test: epoch 9, loss 10.19700813293457, acc=0.05833333358168602, loss=10.19700813293457
train: epoch 10, loss 1.8656691312789917, acc=0.32155555486679077, loss=1.8656691312789917
test: epoch 10, loss 9.973014831542969, acc=0.05277777835726738, loss=9.973014831542969
train: epoch 11, loss 1.8271753787994385, acc=0.33283331990242004, loss=1.8271753787994385
test: epoch 11, loss 9.492403030395508, acc=0.0555555559694767, loss=9.492403030395508
train: epoch 12, loss 1.7869873046875, acc=0.347944438457489, loss=1.7869873046875
test: epoch 12, loss 8.974555015563965, acc=0.05833333358168602, loss=8.974555015563965
train: epoch 13, loss 1.7620820999145508, acc=0.35972222685813904, loss=1.7620820999145508
test: epoch 13, loss 8.26472282409668, acc=0.06111111119389534, loss=8.26472282409668
train: epoch 14, loss 1.7200002670288086, acc=0.3725000023841858, loss=1.7200002670288086
test: epoch 14, loss 7.651741027832031, acc=0.08888889104127884, loss=7.651741027832031
train: epoch 15, loss 1.6955225467681885, acc=0.3779444396495819, loss=1.6955225467681885
test: epoch 15, loss 6.965042591094971, acc=0.08611111342906952, loss=6.965042591094971
train: epoch 16, loss 1.6500353813171387, acc=0.3911111056804657, loss=1.6500353813171387
test: epoch 16, loss 6.839742183685303, acc=0.11666666716337204, loss=6.839742183685303
train: epoch 17, loss 1.6286569833755493, acc=0.4048333466053009, loss=1.6286569833755493
test: epoch 17, loss 6.07626485824585, acc=0.10833333432674408, loss=6.07626485824585
train: epoch 18, loss 1.5946325063705444, acc=0.41572222113609314, loss=1.5946325063705444
test: epoch 18, loss 5.79437255859375, acc=0.0972222238779068, loss=5.79437255859375
train: epoch 19, loss 1.5643823146820068, acc=0.41644445061683655, loss=1.5643823146820068
test: epoch 19, loss 5.449295520782471, acc=0.10277777910232544, loss=5.449295520782471
train: epoch 20, loss 1.5557160377502441, acc=0.4304444491863251, loss=1.5557160377502441
test: epoch 20, loss 5.477357387542725, acc=0.1111111119389534, loss=5.477357387542725
train: epoch 21, loss 1.5145156383514404, acc=0.4434444308280945, loss=1.5145156383514404
test: epoch 21, loss 5.43620491027832, acc=0.13055555522441864, loss=5.43620491027832
train: epoch 22, loss 1.4882689714431763, acc=0.46094444394111633, loss=1.4882689714431763
test: epoch 22, loss 5.489196300506592, acc=0.1111111119389534, loss=5.489196300506592
train: epoch 23, loss 1.4683548212051392, acc=0.4615555703639984, loss=1.4683548212051392
test: epoch 23, loss 5.284958839416504, acc=0.11944444477558136, loss=5.284958839416504
train: epoch 24, loss 1.4430652856826782, acc=0.48366665840148926, loss=1.4430652856826782
test: epoch 24, loss 5.1773247718811035, acc=0.125, loss=5.1773247718811035
train: epoch 25, loss 1.4188501834869385, acc=0.4869999885559082, loss=1.4188501834869385
test: epoch 25, loss 5.179699897766113, acc=0.13333334028720856, loss=5.179699897766113
train: epoch 26, loss 1.3810943365097046, acc=0.4967777729034424, loss=1.3810943365097046
test: epoch 26, loss 5.087717056274414, acc=0.14444445073604584, loss=5.087717056274414
train: epoch 27, loss 1.3689908981323242, acc=0.5108333230018616, loss=1.3689908981323242
test: epoch 27, loss 5.197699546813965, acc=0.15000000596046448, loss=5.197699546813965
train: epoch 28, loss 1.3250782489776611, acc=0.5264444351196289, loss=1.3250782489776611
test: epoch 28, loss 5.331056594848633, acc=0.15000000596046448, loss=5.331056594848633
train: epoch 29, loss 1.3095623254776, acc=0.5394999980926514, loss=1.3095623254776
test: epoch 29, loss 5.196582794189453, acc=0.16388888657093048, loss=5.196582794189453
train: epoch 30, loss 1.2901716232299805, acc=0.542722225189209, loss=1.2901716232299805
test: epoch 30, loss 4.972241401672363, acc=0.17777778208255768, loss=4.972241401672363
train: epoch 31, loss 1.2744041681289673, acc=0.5529444217681885, loss=1.2744041681289673
test: epoch 31, loss 4.7698469161987305, acc=0.17777778208255768, loss=4.7698469161987305
train: epoch 32, loss 1.2296316623687744, acc=0.5738333463668823, loss=1.2296316623687744
test: epoch 32, loss 4.808559417724609, acc=0.18611110746860504, loss=4.808559417724609
train: epoch 33, loss 1.2296781539916992, acc=0.5797777771949768, loss=1.2296781539916992
test: epoch 33, loss 4.614268779754639, acc=0.18333333730697632, loss=4.614268779754639
train: epoch 34, loss 1.1829127073287964, acc=0.6000555753707886, loss=1.1829127073287964
test: epoch 34, loss 4.372507095336914, acc=0.20555555820465088, loss=4.372507095336914
train: epoch 35, loss 1.1583497524261475, acc=0.6079444289207458, loss=1.1583497524261475
test: epoch 35, loss 4.362503528594971, acc=0.20277777314186096, loss=4.362503528594971
train: epoch 36, loss 1.1426392793655396, acc=0.6243888735771179, loss=1.1426392793655396
test: epoch 36, loss 4.235716342926025, acc=0.21111111342906952, loss=4.235716342926025
train: epoch 37, loss 1.1002863645553589, acc=0.6329444646835327, loss=1.1002863645553589
test: epoch 37, loss 3.9705333709716797, acc=0.23055554926395416, loss=3.9705333709716797
train: epoch 38, loss 1.1017807722091675, acc=0.643833339214325, loss=1.1017807722091675
test: epoch 38, loss 3.7497410774230957, acc=0.22777777910232544, loss=3.7497410774230957
train: epoch 39, loss 1.0628838539123535, acc=0.6583889126777649, loss=1.0628838539123535
test: epoch 39, loss 3.788745164871216, acc=0.2222222238779068, loss=3.788745164871216
train: epoch 40, loss 1.0297061204910278, acc=0.6794999837875366, loss=1.0297061204910278
test: epoch 40, loss 3.7638373374938965, acc=0.21944443881511688, loss=3.7638373374938965
train: epoch 41, loss 1.0231313705444336, acc=0.6869444251060486, loss=1.0231313705444336
test: epoch 41, loss 3.5970120429992676, acc=0.23888888955116272, loss=3.5970120429992676
train: epoch 42, loss 0.9687953591346741, acc=0.7026666402816772, loss=0.9687953591346741
test: epoch 42, loss 3.6167097091674805, acc=0.25555557012557983, loss=3.6167097091674805
train: epoch 43, loss 0.9625505805015564, acc=0.7082222104072571, loss=0.9625505805015564
test: epoch 43, loss 3.344989061355591, acc=0.24166665971279144, loss=3.344989061355591
train: epoch 44, loss 0.9100419878959656, acc=0.7267777919769287, loss=0.9100419878959656
test: epoch 44, loss 3.3072283267974854, acc=0.2527777850627899, loss=3.3072283267974854
train: epoch 45, loss 0.897021472454071, acc=0.73416668176651, loss=0.897021472454071
test: epoch 45, loss 3.170980453491211, acc=0.26944443583488464, loss=3.170980453491211
train: epoch 46, loss 0.8544535636901855, acc=0.7490555644035339, loss=0.8544535636901855
test: epoch 46, loss 3.079746961593628, acc=0.2611111104488373, loss=3.079746961593628
train: epoch 47, loss 0.8371922969818115, acc=0.7594444155693054, loss=0.8371922969818115
test: epoch 47, loss 2.9921863079071045, acc=0.2527777850627899, loss=2.9921863079071045
train: epoch 48, loss 0.7981016039848328, acc=0.7722777724266052, loss=0.7981016039848328
test: epoch 48, loss 2.902885675430298, acc=0.2638888955116272, loss=2.902885675430298
train: epoch 49, loss 0.7834757566452026, acc=0.7820000052452087, loss=0.7834757566452026
test: epoch 49, loss 2.8018503189086914, acc=0.2777777910232544, loss=2.8018503189086914
train: epoch 50, loss 0.7406144142150879, acc=0.7960000038146973, loss=0.7406144142150879
test: epoch 50, loss 2.7131826877593994, acc=0.28611111640930176, loss=2.7131826877593994
train: epoch 51, loss 0.7277621030807495, acc=0.804111123085022, loss=0.7277621030807495
test: epoch 51, loss 2.6825156211853027, acc=0.2638888955116272, loss=2.6825156211853027
train: epoch 52, loss 0.699516773223877, acc=0.8130555748939514, loss=0.699516773223877
test: epoch 52, loss 2.650261402130127, acc=0.28611111640930176, loss=2.650261402130127
train: epoch 53, loss 0.7006714940071106, acc=0.820555567741394, loss=0.7006714940071106
test: epoch 53, loss 2.7222728729248047, acc=0.25833332538604736, loss=2.7222728729248047
train: epoch 54, loss 0.6780280470848083, acc=0.8253889083862305, loss=0.6780280470848083
test: epoch 54, loss 2.6580111980438232, acc=0.2638888955116272, loss=2.6580111980438232
train: epoch 55, loss 0.6465027332305908, acc=0.8345000147819519, loss=0.6465027332305908
test: epoch 55, loss 2.619781494140625, acc=0.28333333134651184, loss=2.619781494140625
train: epoch 56, loss 0.623618483543396, acc=0.8416110873222351, loss=0.623618483543396
test: epoch 56, loss 2.6710517406463623, acc=0.2666666805744171, loss=2.6710517406463623
train: epoch 57, loss 0.6032844185829163, acc=0.8508333563804626, loss=0.6032844185829163
test: epoch 57, loss 2.526684284210205, acc=0.2611111104488373, loss=2.526684284210205
train: epoch 58, loss 0.5928727984428406, acc=0.8565555810928345, loss=0.5928727984428406
test: epoch 58, loss 2.5236809253692627, acc=0.26944443583488464, loss=2.5236809253692627
train: epoch 59, loss 0.5483934283256531, acc=0.8641111254692078, loss=0.5483934283256531
test: epoch 59, loss 2.415363311767578, acc=0.2805555462837219, loss=2.415363311767578
train: epoch 60, loss 0.5625894069671631, acc=0.8645555377006531, loss=0.5625894069671631
test: epoch 60, loss 2.4103689193725586, acc=0.26944443583488464, loss=2.4103689193725586
train: epoch 61, loss 0.5385031700134277, acc=0.8743333220481873, loss=0.5385031700134277
test: epoch 61, loss 2.348299980163574, acc=0.2527777850627899, loss=2.348299980163574
train: epoch 62, loss 0.525357186794281, acc=0.8769999742507935, loss=0.525357186794281
test: epoch 62, loss 2.3999626636505127, acc=0.28611111640930176, loss=2.3999626636505127
train: epoch 63, loss 0.48816147446632385, acc=0.8869444727897644, loss=0.48816147446632385
test: epoch 63, loss 2.352219581604004, acc=0.27222222089767456, loss=2.352219581604004
train: epoch 64, loss 0.5018897652626038, acc=0.8816111087799072, loss=0.5018897652626038
test: epoch 64, loss 2.383819341659546, acc=0.2638888955116272, loss=2.383819341659546
train: epoch 65, loss 0.4839833676815033, acc=0.8879444599151611, loss=0.4839833676815033
test: epoch 65, loss 2.1966392993927, acc=0.2888889014720917, loss=2.1966392993927
train: epoch 66, loss 0.47277215123176575, acc=0.8926110863685608, loss=0.47277215123176575
test: epoch 66, loss 2.2437965869903564, acc=0.2750000059604645, loss=2.2437965869903564
train: epoch 67, loss 0.4602053165435791, acc=0.8959444165229797, loss=0.4602053165435791
test: epoch 67, loss 2.2212328910827637, acc=0.26944443583488464, loss=2.2212328910827637
train: epoch 68, loss 0.4363771080970764, acc=0.8982222080230713, loss=0.4363771080970764
test: epoch 68, loss 2.1880831718444824, acc=0.2805555462837219, loss=2.1880831718444824
train: epoch 69, loss 0.4455471932888031, acc=0.8991666436195374, loss=0.4455471932888031
test: epoch 69, loss 2.1221513748168945, acc=0.2805555462837219, loss=2.1221513748168945
train: epoch 70, loss 0.4565846621990204, acc=0.902388870716095, loss=0.4565846621990204
test: epoch 70, loss 2.1642725467681885, acc=0.2750000059604645, loss=2.1642725467681885
train: epoch 71, loss 0.4130929708480835, acc=0.9049999713897705, loss=0.4130929708480835
test: epoch 71, loss 2.048279285430908, acc=0.28611111640930176, loss=2.048279285430908
train: epoch 72, loss 0.4277893006801605, acc=0.9083333611488342, loss=0.4277893006801605
test: epoch 72, loss 2.0242996215820312, acc=0.29722222685813904, loss=2.0242996215820312
train: epoch 73, loss 0.4161669611930847, acc=0.9104999899864197, loss=0.4161669611930847
test: epoch 73, loss 2.0673165321350098, acc=0.30000001192092896, loss=2.0673165321350098
train: epoch 74, loss 0.4240174889564514, acc=0.9100000262260437, loss=0.4240174889564514
test: epoch 74, loss 2.123494863510132, acc=0.28611111640930176, loss=2.123494863510132
train: epoch 75, loss 0.4000231623649597, acc=0.9117777943611145, loss=0.4000231623649597
test: epoch 75, loss 2.0727732181549072, acc=0.2888889014720917, loss=2.0727732181549072
train: epoch 76, loss 0.4127735197544098, acc=0.914555549621582, loss=0.4127735197544098
test: epoch 76, loss 2.056962251663208, acc=0.3166666626930237, loss=2.056962251663208
train: epoch 77, loss 0.3936934471130371, acc=0.913777768611908, loss=0.3936934471130371
test: epoch 77, loss 2.1352763175964355, acc=0.2944444417953491, loss=2.1352763175964355
train: epoch 78, loss 0.3787223696708679, acc=0.9176666736602783, loss=0.3787223696708679
test: epoch 78, loss 2.091672420501709, acc=0.28611111640930176, loss=2.091672420501709
train: epoch 79, loss 0.3904104232788086, acc=0.9171110987663269, loss=0.3904104232788086
test: epoch 79, loss 2.034397840499878, acc=0.28333333134651184, loss=2.034397840499878
train: epoch 80, loss 0.3997378349304199, acc=0.9177777767181396, loss=0.3997378349304199
test: epoch 80, loss 1.941354751586914, acc=0.2944444417953491, loss=1.941354751586914
train: epoch 81, loss 0.36776676774024963, acc=0.9215555787086487, loss=0.36776676774024963
test: epoch 81, loss 1.971376895904541, acc=0.2944444417953491, loss=1.971376895904541
train: epoch 82, loss 0.3603973984718323, acc=0.9236111044883728, loss=0.3603973984718323
test: epoch 82, loss 1.9975552558898926, acc=0.30000001192092896, loss=1.9975552558898926
train: epoch 83, loss 0.35532042384147644, acc=0.9268888831138611, loss=0.35532042384147644
test: epoch 83, loss 2.018616199493408, acc=0.3027777671813965, loss=2.018616199493408
train: epoch 84, loss 0.3890959322452545, acc=0.9200555682182312, loss=0.3890959322452545
test: epoch 84, loss 1.965132713317871, acc=0.31388887763023376, loss=1.965132713317871
train: epoch 85, loss 0.3360684812068939, acc=0.9266111254692078, loss=0.3360684812068939
test: epoch 85, loss 2.0255300998687744, acc=0.2916666567325592, loss=2.0255300998687744
train: epoch 86, loss 0.3624987304210663, acc=0.9224444627761841, loss=0.3624987304210663
test: epoch 86, loss 1.981603741645813, acc=0.2944444417953491, loss=1.981603741645813
train: epoch 87, loss 0.35695892572402954, acc=0.9223889112472534, loss=0.35695892572402954
test: epoch 87, loss 1.965058445930481, acc=0.30000001192092896, loss=1.965058445930481
train: epoch 88, loss 0.36112910509109497, acc=0.9246666431427002, loss=0.36112910509109497
test: epoch 88, loss 1.9944610595703125, acc=0.29722222685813904, loss=1.9944610595703125
train: epoch 89, loss 0.3544197380542755, acc=0.9253888726234436, loss=0.3544197380542755
test: epoch 89, loss 1.9094252586364746, acc=0.3027777671813965, loss=1.9094252586364746
train: epoch 90, loss 0.3673645555973053, acc=0.9272778034210205, loss=0.3673645555973053
test: epoch 90, loss 1.9048380851745605, acc=0.29722222685813904, loss=1.9048380851745605
train: epoch 91, loss 0.3626713156700134, acc=0.9277777671813965, loss=0.3626713156700134
test: epoch 91, loss 1.9430758953094482, acc=0.3083333373069763, loss=1.9430758953094482
train: epoch 92, loss 0.354216992855072, acc=0.9260555505752563, loss=0.354216992855072
test: epoch 92, loss 1.9404162168502808, acc=0.29722222685813904, loss=1.9404162168502808
train: epoch 93, loss 0.35711267590522766, acc=0.9258333444595337, loss=0.35711267590522766
test: epoch 93, loss 1.9411708116531372, acc=0.3166666626930237, loss=1.9411708116531372
train: epoch 94, loss 0.3423604965209961, acc=0.9284999966621399, loss=0.3423604965209961
test: epoch 94, loss 1.8567513227462769, acc=0.3166666626930237, loss=1.8567513227462769
train: epoch 95, loss 0.36248579621315, acc=0.9260555505752563, loss=0.36248579621315
test: epoch 95, loss 1.8773058652877808, acc=0.3194444477558136, loss=1.8773058652877808
train: epoch 96, loss 0.34515735507011414, acc=0.9278333187103271, loss=0.34515735507011414
test: epoch 96, loss 1.912020206451416, acc=0.29722222685813904, loss=1.912020206451416
train: epoch 97, loss 0.3361333906650543, acc=0.929888904094696, loss=0.3361333906650543
test: epoch 97, loss 1.8696404695510864, acc=0.3027777671813965, loss=1.8696404695510864
train: epoch 98, loss 0.3301008939743042, acc=0.9290555715560913, loss=0.3301008939743042
test: epoch 98, loss 1.9225527048110962, acc=0.2944444417953491, loss=1.9225527048110962
train: epoch 99, loss 0.32136327028274536, acc=0.9306666851043701, loss=0.32136327028274536
test: epoch 99, loss 1.8699164390563965, acc=0.3083333373069763, loss=1.8699164390563965
train: epoch 100, loss 0.3343175947666168, acc=0.9306666851043701, loss=0.3343175947666168
test: epoch 100, loss 1.8834881782531738, acc=0.3027777671813965, loss=1.8834881782531738
train: epoch 101, loss 0.3295590579509735, acc=0.9323333501815796, loss=0.3295590579509735
test: epoch 101, loss 1.8533905744552612, acc=0.3166666626930237, loss=1.8533905744552612
train: epoch 102, loss 0.3143582344055176, acc=0.9314444661140442, loss=0.3143582344055176
test: epoch 102, loss 1.8727149963378906, acc=0.3055555522441864, loss=1.8727149963378906
train: epoch 103, loss 0.32552680373191833, acc=0.9294999837875366, loss=0.32552680373191833
test: epoch 103, loss 1.7788622379302979, acc=0.32499998807907104, loss=1.7788622379302979
train: epoch 104, loss 0.33125126361846924, acc=0.9326666593551636, loss=0.33125126361846924
test: epoch 104, loss 1.797493815422058, acc=0.33888888359069824, loss=1.797493815422058
train: epoch 105, loss 0.340875506401062, acc=0.9286666512489319, loss=0.340875506401062
test: epoch 105, loss 1.7761033773422241, acc=0.32777777314186096, loss=1.7761033773422241
train: epoch 106, loss 0.3302210569381714, acc=0.9294999837875366, loss=0.3302210569381714
test: epoch 106, loss 1.7681610584259033, acc=0.3305555582046509, loss=1.7681610584259033
train: epoch 107, loss 0.3354838788509369, acc=0.9301111102104187, loss=0.3354838788509369
test: epoch 107, loss 1.766876459121704, acc=0.3166666626930237, loss=1.766876459121704
train: epoch 108, loss 0.34972408413887024, acc=0.9290000200271606, loss=0.34972408413887024
test: epoch 108, loss 1.7717640399932861, acc=0.3305555582046509, loss=1.7717640399932861
train: epoch 109, loss 0.3503819704055786, acc=0.9290555715560913, loss=0.3503819704055786
test: epoch 109, loss 1.7876685857772827, acc=0.31388887763023376, loss=1.7876685857772827
train: epoch 110, loss 0.3392561972141266, acc=0.929111123085022, loss=0.3392561972141266
test: epoch 110, loss 1.7249308824539185, acc=0.3194444477558136, loss=1.7249308824539185
train: epoch 111, loss 0.3372269570827484, acc=0.9319444298744202, loss=0.3372269570827484
test: epoch 111, loss 1.7557657957077026, acc=0.34166666865348816, loss=1.7557657957077026
train: epoch 112, loss 0.31979700922966003, acc=0.9314444661140442, loss=0.31979700922966003
test: epoch 112, loss 1.8013794422149658, acc=0.33888888359069824, loss=1.8013794422149658
train: epoch 113, loss 0.3285141885280609, acc=0.9308333396911621, loss=0.3285141885280609
test: epoch 113, loss 1.7213172912597656, acc=0.32499998807907104, loss=1.7213172912597656
train: epoch 114, loss 0.33945798873901367, acc=0.9284444451332092, loss=0.33945798873901367
test: epoch 114, loss 1.708457112312317, acc=0.33888888359069824, loss=1.708457112312317
train: epoch 115, loss 0.3374265134334564, acc=0.9283333420753479, loss=0.3374265134334564
test: epoch 115, loss 1.7205085754394531, acc=0.3305555582046509, loss=1.7205085754394531
train: epoch 116, loss 0.349190354347229, acc=0.92894446849823, loss=0.349190354347229
test: epoch 116, loss 1.679538369178772, acc=0.34166666865348816, loss=1.679538369178772
train: epoch 117, loss 0.3194003403186798, acc=0.9318888783454895, loss=0.3194003403186798
test: epoch 117, loss 1.6902544498443604, acc=0.3444444537162781, loss=1.6902544498443604
train: epoch 118, loss 0.32793399691581726, acc=0.9342222213745117, loss=0.32793399691581726
test: epoch 118, loss 1.7326064109802246, acc=0.33888888359069824, loss=1.7326064109802246
train: epoch 119, loss 0.31989696621894836, acc=0.9297778010368347, loss=0.31989696621894836
test: epoch 119, loss 1.641552448272705, acc=0.3444444537162781, loss=1.641552448272705
train: epoch 120, loss 0.3498058319091797, acc=0.9273889064788818, loss=0.3498058319091797
test: epoch 120, loss 1.693826675415039, acc=0.3194444477558136, loss=1.693826675415039
train: epoch 121, loss 0.3384864628314972, acc=0.9274444580078125, loss=0.3384864628314972
test: epoch 121, loss 1.648321270942688, acc=0.36666667461395264, loss=1.648321270942688
train: epoch 122, loss 0.33569586277008057, acc=0.9282222390174866, loss=0.33569586277008057
test: epoch 122, loss 1.6590133905410767, acc=0.33888888359069824, loss=1.6590133905410767
train: epoch 123, loss 0.33413299918174744, acc=0.9300000071525574, loss=0.33413299918174744
test: epoch 123, loss 1.680306077003479, acc=0.3472222089767456, loss=1.680306077003479
train: epoch 124, loss 0.3459792137145996, acc=0.925944447517395, loss=0.3459792137145996
test: epoch 124, loss 1.6538074016571045, acc=0.3638888895511627, loss=1.6538074016571045
train: epoch 125, loss 0.33523809909820557, acc=0.930055558681488, loss=0.33523809909820557
test: epoch 125, loss 1.6387062072753906, acc=0.38055557012557983, loss=1.6387062072753906
train: epoch 126, loss 0.3451153039932251, acc=0.9265000224113464, loss=0.3451153039932251
test: epoch 126, loss 1.6239688396453857, acc=0.3777777850627899, loss=1.6239688396453857
train: epoch 127, loss 0.3388388454914093, acc=0.9272222518920898, loss=0.3388388454914093
test: epoch 127, loss 1.6286017894744873, acc=0.375, loss=1.6286017894744873
train: epoch 128, loss 0.32567036151885986, acc=0.9291666746139526, loss=0.32567036151885986
test: epoch 128, loss 1.6355761289596558, acc=0.3444444537162781, loss=1.6355761289596558
train: epoch 129, loss 0.3476189076900482, acc=0.9268888831138611, loss=0.3476189076900482
test: epoch 129, loss 1.619205117225647, acc=0.3722222149372101, loss=1.619205117225647
train: epoch 130, loss 0.3280456066131592, acc=0.9255555272102356, loss=0.3280456066131592
test: epoch 130, loss 1.6173981428146362, acc=0.3333333432674408, loss=1.6173981428146362
train: epoch 131, loss 0.33071446418762207, acc=0.9287222027778625, loss=0.33071446418762207
test: epoch 131, loss 1.6337809562683105, acc=0.3583333194255829, loss=1.6337809562683105
train: epoch 132, loss 0.330726683139801, acc=0.9299444556236267, loss=0.330726683139801
test: epoch 132, loss 1.6057589054107666, acc=0.3638888895511627, loss=1.6057589054107666
train: epoch 133, loss 0.35420912504196167, acc=0.9242777824401855, loss=0.35420912504196167
test: epoch 133, loss 1.650142788887024, acc=0.3499999940395355, loss=1.650142788887024
train: epoch 134, loss 0.3352411687374115, acc=0.9236111044883728, loss=0.3352411687374115
test: epoch 134, loss 1.5865031480789185, acc=0.375, loss=1.5865031480789185
train: epoch 135, loss 0.3374806046485901, acc=0.9291666746139526, loss=0.3374806046485901
test: epoch 135, loss 1.5896973609924316, acc=0.38333332538604736, loss=1.5896973609924316
train: epoch 136, loss 0.3318212628364563, acc=0.9292222261428833, loss=0.3318212628364563
test: epoch 136, loss 1.5799589157104492, acc=0.3611111044883728, loss=1.5799589157104492
train: epoch 137, loss 0.3525761067867279, acc=0.925000011920929, loss=0.3525761067867279
test: epoch 137, loss 1.5973575115203857, acc=0.36666667461395264, loss=1.5973575115203857
train: epoch 138, loss 0.3435738682746887, acc=0.9250555634498596, loss=0.3435738682746887
test: epoch 138, loss 1.559636116027832, acc=0.36944442987442017, loss=1.559636116027832
train: epoch 139, loss 0.34599342942237854, acc=0.9247778058052063, loss=0.34599342942237854
test: epoch 139, loss 1.5447604656219482, acc=0.40833333134651184, loss=1.5447604656219482
train: epoch 140, loss 0.34581753611564636, acc=0.9225000143051147, loss=0.34581753611564636
test: epoch 140, loss 1.5583113431930542, acc=0.3583333194255829, loss=1.5583113431930542
train: epoch 141, loss 0.3570195436477661, acc=0.9241666793823242, loss=0.3570195436477661
test: epoch 141, loss 1.570102334022522, acc=0.3777777850627899, loss=1.570102334022522
train: epoch 142, loss 0.3578944206237793, acc=0.9215555787086487, loss=0.3578944206237793
test: epoch 142, loss 1.5517826080322266, acc=0.3888888955116272, loss=1.5517826080322266
train: epoch 143, loss 0.3568359911441803, acc=0.92166668176651, loss=0.3568359911441803
test: epoch 143, loss 1.5575525760650635, acc=0.36944442987442017, loss=1.5575525760650635
train: epoch 144, loss 0.3464360535144806, acc=0.9222221970558167, loss=0.3464360535144806
test: epoch 144, loss 1.57640540599823, acc=0.3861111104488373, loss=1.57640540599823
train: epoch 145, loss 0.33831241726875305, acc=0.9253333210945129, loss=0.33831241726875305
test: epoch 145, loss 1.5360021591186523, acc=0.39444443583488464, loss=1.5360021591186523
train: epoch 146, loss 0.35321369767189026, acc=0.921833336353302, loss=0.35321369767189026
test: epoch 146, loss 1.5444223880767822, acc=0.3861111104488373, loss=1.5444223880767822
train: epoch 147, loss 0.32867199182510376, acc=0.9234444499015808, loss=0.32867199182510376
test: epoch 147, loss 1.515048623085022, acc=0.4000000059604645, loss=1.515048623085022
train: epoch 148, loss 0.3552922308444977, acc=0.9235000014305115, loss=0.3552922308444977
test: epoch 148, loss 1.5351488590240479, acc=0.41111111640930176, loss=1.5351488590240479
train: epoch 149, loss 0.36005517840385437, acc=0.9210555553436279, loss=0.36005517840385437
test: epoch 149, loss 1.5443155765533447, acc=0.4138889014720917, loss=1.5443155765533447
train: epoch 150, loss 0.35454654693603516, acc=0.9198889136314392, loss=0.35454654693603516
test: epoch 150, loss 1.5056020021438599, acc=0.39722222089767456, loss=1.5056020021438599
